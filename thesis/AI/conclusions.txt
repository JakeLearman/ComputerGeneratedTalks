The habilitation thesis aims at presenting a selection of research results obtained in the field of artificial intelligence, mainly dealing with intelligent agents, multiagent systems, optimization and machine learning methods. 
Concerning multiagent role allocation, a method is proposed by which the agents can self-organize based on the changes of their individual utilities. Agents have different preferences regarding the features of the tasks they are given and their adaptive behaviour is based on the psychological theory of cognitive dissonance, where an agent working on a low-preference task gradually improves its attitude towards it. The total productivity is shown to increase as an emergent property of the system. 
Another demonstration of emergent behaviour is based on the design of an interaction protocol for a task allocation system, which can reveal the formation of social networks. The agents can improve their solving ability by learning and can collaborate with their peers to deal with more difficult tasks. The average number of connections and resources of the agents follows a power law distribution.  
Also, a simple set of interaction rules is proposed that can generate overall behaviours with different levels of complexity, from asymptotically stable to chaotic. It is shown that very small perturbations can have a great impact on the evolution of the system, and some methods of controlling such perturbations are investigated in order to have a desirable final state. 
Another contribution in the subfields of planning and learning is a method that includes a learning phase into the plan itself, so that the agent can dynamically recognize the preconditions of an action when the states are not fully determined, and it can even directly choose its actions based on learning results. 
The notion of state attractor is introduced, which allows the agents to compute their actions based on the proximity of their current state to the nearest state attractor. This technique is considered to be an alternative way of approaching difficult multiagent reinforcement learning problems. 
Considering autonomous learning, a system for solving classification and regression problems is proposed, which involves competition between different types of agents. They use neural networks to solve external problems given by the user, but they can also build their own internal problems out of their experience in order to increase their performance.  
Due to the recent research advances in quantum computing, ideas from this field have been increasingly used as a source of inspiration for new evolutionary algorithms. Two variants of quantum-inspired evolutionary algorithms are proposed, characterized by a steady state model (population-based elitism), a repairing procedure to keep all the individuals feasible and an evolutionary hill-climbing phase meant to further improve the quality of the solution. The first one uses binary encoding and the second one uses real-valued encoding. These algorithms are applied for solving multi-attribute combinatorial auction problems and for finding nearoptimal outcomes of multi-issue multi-lateral negotiation in multiagent systems.  
 Another contribution is the application of classification methods to chemical engineering problems. One example is the prediction of the liquid crystalline property of compounds, using methods such as neural networks, decision trees, instance-based techniques etc. The best results are obtained with an original algorithm, Non-Nested Generalized Exemplars with Prototypes, NNGEP.  
The classification algorithms are also applied for 27-class problems and on 4-class reduced problems of protein fold classification. 
 A modelling methodology based on stacked neural networks is suggested by combining several individual networks in parallel, whose outputs are weighted to provide the output of the stack. For time series forecasting, a stacked neural network is designed containing one normal multilayer perceptron with bipolar sigmoid activation functions, and the other with an exponential activation function in the output layer. Other two chemical engineering phenomena are modelled with stacked neural networks, and they are shown to outperform individual networks in terms of generalization capabilities. 
 The final section of the thesis is concerned with the combination of different modelling and optimization methods into hybrid ones. In case of chemical processes which are difficult to model analytically, a hybrid model is obtained by combining a simplified phenomenological model with a neural network which approximates the difficult parts of the reaction.   Different types of evolutionary algorithms, such as classical genetic algorithms, standard differential evolution and self-adaptive differential evolution are used to determine both the architecture and the internal parameters of a neural network that models other chemical processes. 
Neural networks can handle multiple outputs corresponding to multiobjective optimization problems. The Non-dominated Sorting Genetic Algorithm, NSGA-II, is adapted with alternative metrics such as the set coverage metric and the spacing metric in order to increase the solution diversity. 
A stacked neural network is designed using an evolutionary hyperheuristic, called NSGA-II-QNSNN, based on the NSGA-II algorithm as a global optimization method and incorporating a Quasi-Newton algorithm as a local optimization method for training the neural networks of the stack. The results prove to be very good not only in terms of accuracy, but also in terms of structural complexity of the stacks. 
The most prevalent finding is that openness in AI stems from a variety of reasons. As mentioned, researchers value the ability to publish their work and employees appreciate companies that allow this. Especially for young talents within the AI field, openness is a huge gain for their career (Bostrom 2016, p. 4). Learning from and teaching others is another benefit.  
However, the field of AI is very influenced by businesses and business leaders. Money is the primary factor, which will determine what happens in the future. The world will change depending on the future developments of AI, whether it comes to industries, governments or social life.  
Matters of interest at the moment are recruiting outstanding talent, driving autonomy forward, advancing techniques and models, solving important societal problems, and reforming the field by introducing more transparency.  
Declaring AI as a commons would not have immediate implications but would transfer the focus to its immediacy in human life. Not only looking at the existential risk it poses, but also looking at the improvements it could offer humanity if handled in a transparent way. Findings show that the risk of corruption within the AI space is quite high and that wealth emerges exponentially, but is concentrated instead of distributed equitably. Therefore, many experts recommend a new model of ownership to block gains concentration within the hands of a few and to promote a fair sharing system (see chapter 4.2.4.).  
Considering law and rules surrounding AI technology, the findings show that it will be most important to guide wealth distribution and technological infrastructure to alleviate the digital divide. Additionally, it will be important to create laws which guide developers as well as practitioners to work on and produce AI properly and ethically. The easiest mechanism is through transparency. Transparency maximizes sincerity towards the general public and reduces corruption within the AI space. It also leads to higher interaction between experts and between other individuals, by strengthening the communal co-operation and therefore the community (Hibbard 2016, p. 2; 2008, p. 2). Software by itself is barrier and border free, but it is unclear if this could happen worldwide. Law makers and experts should sit together and discuss future strategies. There are already committees, and other forms of expert consortiums, which discuss ethical issues surrounding AI. Most often they discuss practical topics like the application of self-driving cars or how to make a safe code which acts morally right (Stone et al. 2016, pp. 20-21). New committees could form with the aim to close the digital divide and also transform AI to a commons which would presumably lead to a higher common good.  
The distinction that something which is freely accessible is not useful to all, is very important. As Gourley mentioned, there are still a lot of humans which cannot make any use of AI software. Others are concerned that their jobs might get automated by AI (see chapter 4.2.4). As a result, ¡°Open AI¡± cannot fight inequalities of the global digital divide. It will only accelerate it if nothing is being done to change that fact (Gourley; Hibbard 2016). 
Declaring AI as a commons would not change that fact. It is more important to include developers in decisions, recognize their significance, and even take away some of their power in the establishment within the creation of AI.  
As Weber remarks, ¡°[a] good business model is simply one which succeeds adding additional value on the edge of the commons¡± (Weber 2005, p. 218). 
The specific objectives of this thesis (see section 1.2), have all been achieved during the course of this EngD research project, summarised here below. Specific Objective 1: Identification of a multi-objective optimisation algorithm to utilise as a starting and comparison point for the optimisation process.
This objective was achieved through a thorough investigation of the state of the art multi-objective optimisation algorithms. The investigation was performed and a benchmark algorithm (NSGA-II) was selected on the basis of it showing excellent performance across a number of computationally complex optimisation problems and also being commonly used in research and practice. Additionally, the two main machine-learning approaches that were being considered as a basis for improving performance had built upon NSGA-II. This choice of NSGA-II was verified by the performance of the algorithm on the set of benchmark problems for the water distribution system design, which are
similar in nature to the problem studied in this thesis. The testing on these problems and on the Dalmarnock catchment systems monstrated the validity of the algorithm choice.
Specific  Objective  2:  To  develop  a  multi-objective  optimisation methodology and implement it through an object-oriented structured
software engineering approach with a suitable user-interface, as one of the requirements for this EngD is software that can be further utilised in practice 
Once the multi-objective optimisation algorithm had been selected, the software was developed identifying and following the requirements of the industrial partner co-sponsoring this project. This software allowed for the running of our selected multi-objective optimisation algorithm in a modular fashion (which allows for the addition of elements to the code-base with ease, as the modules are loosely linked) and further development and testing of various methodologies investigated in this thesis. A suitable user interface was then developed for this software (see Appendix III ¨C SAM-Risk Settings, which contains screenshots of this user interface in order to show settings). This user interface met HR Wallingford¡¯s requirements and was developed with practitioner¡¯s needs in mind. In addition to the utility of the software and the new interface being demonstrated on the Dalmarnock case study in this thesis, the successful achievement of this objective was verified by subsequent application by third-party modellers at HR Wallingford on the EU TRUST project (Boelee and Kellagher, 2015). In this application of the technology, a different approach to reducing rainfall periods needed was used, and the base NSGA-II algorithm was used for optimisation. However, the software used is the software developed for this thesis (with minor modifications).
Specific Objective 3: To formulate the overall optimisation problem for the multi-objective optimisation algorithm that will best describe the drainage system flood risk management problem based on:
a. Expected annual damage, and
b. Capital cost of intervention strategy.
The previously developed methodology and software (SAM-Risk) was modified in order to improve its user-interface code and make it usable in a number of practical situations. This was replaced by the user-interface software developed in this thesis. In the process the SAM-Risk implementation was developed as a module, which could be run easily by any other software to calculate expected annual damage for an arbitrary drainage system. This was to be utilised for the computation of the first objective function in this thesis. The formulation allows a reduced of set of rainfall duration/return-period events to be used and compared to the full set so that a suitable improvement in computational speed can be achieved without a noticeable loss in accuracy. The second objective function for the drainage system risk optimisation multi-objective algorithm, was developed to give a cost estimate of the changes being undertaken to the network. It is based upon the cost of the pipes required, plus the cost of excavation for storage, and fixed material costs. The cost calculation is customisable so that scaling can be applied to update this measure and make it appropriate for a particular situation.
Specific Objective 4: To test the objective functions for performance, and investigate methodologies for reducing the computational burden of these functions to allow efficient and effective drainage system flood risk optimisation
Evaluation of the EAD objective function requires a large number of simulation model runs rendering optimisation almost impossible due to excessive times needed to complete it.
To alleviate this, the EAD objective function was modified to cache drainage networks where the network is unchanged between rainfall runs, the solutions were modified to cache objective function scores where they have not been altered between evaluations, and some optimisation of the EAD software such as using LINQ (Pialorsi and Russo, 2007) where appropriate and restructuring iterations to reduce unnecessary complexity. This yielded a performance gain of around 15% for each EAD value calculated, and the caching reduced the time
taken for multiple EAD value calculations to a linearly increasing value rather than an exponentially increasing value.
To further improve this performance, the development of a methodology for identifying a reduced rainfall set as undertaken. This methodology allows for EAD to be estimated using a reduced number of rainfall events for a given network, vastly reducing the complexity of calculating EAD. This reduced the runtime to around 45 seconds per EAD calculation, from a value of 5 hours per calculation.
Finally, even a 45 second objective function is too large to allow for a large number of iterations to be completed, so methods were investigated to reduce the number of necessary iterations, or improve the performance of the optimisation.
The LEMMO algorithm (Jourdan et al., 2005), which was originally used for optimisation of WDS design decisions, uses classifier based meta-models to improve computational efficiency of an optimisation algorithm. The original algorithm was based on the use of decision-tree methods as the main classifier. An approach based on employing Artificial Neural Networks (ANN) has been investigated in this thesis and shown to be effective in combination with a multiobjective genetic algorithm. This approach was developed by the author. The new methodology was applied and tested in depth on a set of WDS test problems that are similar in nature to the problem of drainage system risk optimisation, but require less simulation times, hence are faster to converge to a good Pareto front. Furthermore, a best-known Pareto front is available for each of the test cases (Wang et al., 2014), thus allowing easy evaluation of any optimisation algorithm. The new LEMMO-ANN algorithm was shown to work best with a four-layered ANN achieving a good approximation of the best-known Pareto fronts on a range of WDS problem sizes, ranging from 1.48¡Á10 m to 1.00¡Á10 ohh .
Specific Objective 5: To test and verify computational efficiency and effectiveness of the new methodology on a real case study involving
drainage system flood risk optimisation.
After the successful test of the new methodology on the WDS design test cases, it was applied to the Dalmarnock test problem, with the EAD objective and the capital expenditure (cost of network alteration) objective. The size and complexity of the problem were such that this particular case study could not be tackled using existing tools. Even with the new LEMMO-ANN methodology, only one long (1000 iterations plus) run on this case study was performed due to excessive run times and computational resources required. Based on this run it was seen that selected solutions from the Pareto front represent a considerable improvement over the base unmodified network which was the optimisation starting point.
From this completed work several conclusions can be drawn. The main is that it is possible to run a full flood risk versus capital expenditure multi-objective optimisation on mainstream desktop computer hardware. This optimisation progresses towards a Pareto front that should aid in identifying networks with an improved performance in terms of EAD vs capital expenditure. The caveat to that
is, that it is only possible with appropriate improvements to the state-of-the-art optimisation methodology, including reduction of rainfall set, and use of a cutting edge machine learning and multi-objective optimisation algorithm. As a product of the above conclusion, it is possible to reduce the rainfall set used to evaluate flood-risk, provided that a specific set of rainfall suitable for the catchment is identified. This can be achieved with only minor loss of accuracy, which allows for this less accurate EAD to be used as a multi-objective optimisation algorithm objective. The end-product of that optimisation can then be checked using the full rainfall set to ensure that full accuracy checks of the lower-accuracy result have occurred. Additionally, one can conclude that the use of machine-learning based meta-models within optimisation algorithms is highly promising. Both for application to future problems of this nature, and to other highly complex combinatorial type problems. For this particular application, it has been shown to produce good results on test-problems. Improving on a base NSGA-II algorithm both in terms of final output and throughout the algorithms execution, and optimising well and producing good results on a true flood-risk vs. network modification cost case-study.
The first steps to continuing the work outlined within this thesis would be to continue the testing performed. In particular, with more real case-studies to examine how well the LEMMO-ANN combination and reduced rainfall set identification perform on different kinds of urban environment. There is also potential for combining an alternating block hyetograph type method with the methodology outlined here for identification of a reduced rainfall set. The durations could, for example, be combined for each return period and then the
methodology applied to identify return periods. There is some potential in improving the way in which LEMMO is integrated into the NSGA-II base algorithm. It could be possible to develop an approach which would avoid the outcomes seen in Chapter 5, where a poorly  erforming meta-model within the LEMMO algorithm results in less optimal performance than the base NSGA-II network This work would be a clear candidate for the application of distributed or high- powered computing due to the highly parallel nature of genetic algorithm based optimisation algorithms. In theory, applying enough computers to this problem could reduce the time taken for each iteration of the algorithm to the time required for one simulation. The downside to experimenting with this kind of approach, is that it would render the software unable to run on a single desktop computer, and would require extensive licensing costs, were Infoworks CS still utilised as part of the software.
The present thesis is divided into four independent but, at the same time, com-plementary parts. The main objective is to use artificial intelligence and molec-ular dynamics/statics tools to study the behaviour of plasticity elements (e.g. grain boundaries, dislocations) at an atomic scale. This section gives a summary of the study performed on each part.
Part i: Minimization methods Molecular statics simulations are usually carried out by applying discrete deformation increments to a sample. After each step, the system is relaxed until a new equilibrium is reached. This relaxation procedure is the most computationally expensive part of the simulation and, depending on the complexity of the problem, it can take considerable time. In order to optimize the search for an equilibrium after each deformation increment, a comparison of the convergence rates and final structures obtained with seven different relaxation methods is presented and applied to molecular statics simulations of dislocations. It is found that the fastest algorithms are the Fast Inertial Relaxation Engine (FIRE) and the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).
Additionally, it turns out that in cases where the stress field rather than the overall energy is the quantity of real interest, equilibration is a particularly delicate operation, and can lead to errors in simulations if not handled carefully.
To address this issue, the equilibration of a single crystal, a system containing a dislocation and one with a grain boundary are analysed. The use of the overall energy as sole parameter to end a relaxation is discussed and compared with the local energy and the global and local stresses. From this, a new and more accurate method to determine whether a system has reached static equilibrium,
based on the stress and the standard deviation of the atomic energies, is proposed. 
Part ii: Dislocations
The Peierls stress (¦Ó p ) is one of the most fundamental properties of dislocations since it is a measure of the resistance of the lattice to plasticity. Despite its importance, it is still not very clear why there are disagreements of up to two orders of magnitude between the reported experimental and simulated values for face-centred cubic materials. In the present part ¦Ó p is calculated for aluminium using molecular statics simulated with two different interatomic potentials, and the obtained values are compared with those available in the literature. Based on an analysis of the observed motion of partial dislocations, it was found that the stacking fault strip can act as a tensile/compressive spring or as a rigid connection between the partials. This fact, which complements the analytical
work of Schoeck and P¨¹schl (G. Schoeck and W. P¨¹schl, Mat. Sci. Eng. A 189, 61 (2001)), is used to propose a direct explanation for the wide spread of the reported values of ¦Ó p . It is concluded that the one of the main reasons for this spread is the idealized width of the stacking fault that separates the partials. 
Part iii: Grain Boundaries Generation 
Generation of realistic Grain Boundaries (GBs) in bicrystals for atomistic simulations requires a search in a multidimensional space for a configuration with the atoms at the GB organized in such a way that the energy of the whole system is minimized. This chapter presents the commonly used ¡°brute force¡± method and a Genetic Algorithm (GA). The proposed GA allows to find low energy GB configurations by optimizing three main criteria: the local arrangement of the atoms, the relative translation between the two grains that compose the GB and an overall expansion/contraction of the system. The GA is designed to make a wider and more effective search through the energy landscape compared to other traditional methods, giving access to more configurations and increasing the possibility of finding the global minimum in energy. 
Energy
Artificial Neural Networks (ANNs) have been used in a few domains of materials science, but never for the prediction of Grain Boundary (GB) energies.In the present chapter, an ANN is used to generate ¨Cfor the first time¨C a function for the GB energy in terms of its five macroscopic degrees of freedom. The proposed approach is verified for GBs of Body Centred Cubic iron. Part of the database calculated by Kim et al. (Kim, H.-K., Ko, W.-S., Lee, H.-J., Kim, S. G., Lee, B.-J. Scripta Materialia, 64(12), 1152-1155 (2011)) is used as training data for the ANN. After the ANN has been trained (i.e. after it has learned to replicate and predict the function), the magnitude of the errors in predicted GB energies for the remaining part of the database is about 4%, which is lower than the error of 10% that is typical for experimental GB energy measurements. 
Part iv: Grain Boundaries & Dislocations
Grain boundaries (GBs) and dislocations are responsible for the plastic behaviour of crystalline materials. Despite their importance, as of now, most of the knowledge available in the literature about the way that they interact with each other is largely qualitative. In the present part quantitative methods based on the energy and local environment of the atoms, and on the Burgers vector of the dislocations are presented. These methods are then applied to an aluminium sample system to analyse the dislocation spread and the changes in GB width
caused by an interaction event. For different interaction points, results show an average change of GB width of 3.87? and differences in the dislocation spread of up to 20?. The availability of these newly developed tools opens up a door for the incorporation of atomistic events in larger scale computational modelling methods.
After an exploratory phase of formulating a valuable and feasible problem to tackle, the main research question of this research raised in Chapter 1 is worded as follows: Can a conceptual design process of knowledge-intensive business processes be created, such that the cognition utilised in the process can be enhanced by integrating Artificial Intelligence software? The answer to this main research question is that such a conceptual design process can be created, where the research described in this thesis acts as proof. The conducted research delivers a conceptual design process that consists of six components, displayed in Figure 8.1. Four of the components of the conceptual design process comprise design steps to construct a conceptual design of cognition enhanced business processes. First, the Cognitive Requirement Design Process (CRDP) entails the extraction of the cognitive needs of a business process in the current situation. These needs are formulated into functional requirements for the cognition utilised in the business process to ensure proper business operation, so called cognitive requirements. The CRDP can be conducted by performing the following five steps: 
1A. Decomposing the business process to tasks and roles and formulating stakeholders¡¯ requirements 
1B. Collecting preliminary domain knowledge
1C. Identifying sub-tasks and types of knowledge that are required to perform the tasks under review
1D. Extracting the cognitive activities from the tasks at hand
1E. Formulating cognitive requirements based on the cognitive activities extracted
Second, the Cognitive Capabilities Design Process (CCDP) of the conceptual design process establishes the functionalities of an AI software package under review that express a cognitive skill. These functionalities are formulated into so called cognitive capabilities. The CCDP can be conducted by performing the following four steps:
2A. Decomposing the selected AI software to its services
2B. Identifying the corresponding methods and techniques for each service
2C. Establishing the cognitive functionalities of each service of the software
2D. Formulating cognitive capabilities based on the cognitive functionalities established
These two design processes are input for a third component, the Cognitive Possibilities Design Process (CPDP), in which the requirements from the CRDP and the capabilities of the CCDP are mapped to each other, possibly resulting in match. These matches lead to a set of possibilities for AI software to fulfil the cognitive requirements of the business process. The CPDP can be conducted by
performing the following three steps:
3A. Categorising the cognitive requirements and cognitive capabilities by cognitive function 
3B. Structurally matching the cognitive capabilities to the cognitive requirements
3C. Analysing each potential match and formulating the rationale for each match
The outcomes from the CPDP can be used to draft a set of design alternatives of AI-driven business processes, which can then be refined and assessed based on pre-defined metrics. On these metrics the best scoring design alternative can be selected to draft a subsequent preliminary design. These design steps form together the Design Alternative Generation & Selection Process (DAGSP) of the conceptual design process. Due to time constraints, the DAGSP fourth design process is left out of the research scope and therefore not unravelled in this research. To construct a coherent conceptual design process next to these four sub-design processes, this research found that two other components should be included in the conceptual design process. First, a component responsible for the co-ordination of the design exercise when utilising the conceptual design process is considered essential and therefore created, called Design Process Co-ordination (DPC). This component is present on a more strategic level than the components mentioned before, executing the strategy of the overall design process by allocating resources (such as time and information) amongst the sub-design processes. Second, the component Data Availability & Quality Assessment (DAQA) completes the conceptual design process created in this research, covering the specific need to ensure that the design being created fits the data present in the business process and thus its context. The DAQA thereby finds itself on a more operational level than the sub-design process components of the conceptual design process. The six components identified, studied and specified in this research form together the final conceptual design process and thereby the main deliverable of this research. The conceptual design process indicates that conceptually designing cognition enhanced business processes can be done by identifying and matching the requirements of the business process and capabilities of AI software scrutinised from an integrated systems perspective on cognition, to generate design possibilities. A project aiming to draft a conceptual design of a business process that leverages an AI driven software system can utilise this conceptual design process and/or the components it consists of. The conceptual design process is created based on two case studies comprising a claim-assessment process of health-insurance companies. After validation, the conceptual design process is viewed to be applicable to assessment business processes, which indicates its universal applicability to processes with
the same characteristics. Furthermore, attributable to its methodical nature, the conceptual design process is considered to provide to some extent rigour to a conceptual design exercise, limiting the design process to freewheel to improper preliminary designs.
It is now very clear that the biological and computing world has a lot to gain from neural networks because of their ability to learn by example which makes them very flexible and powerful. In computation, they are very well suited for real time systems because of their fast response which are due to their parallel architecture. In areas of research such as neurology, they are used to model parts of living organisms and to investigate the internal mechanisms of the brain. Even though neural networks have a huge potential, scientist will only get the best of them when they are combined with computing. In general, putting a neural network into a computer allows it to make intelligent judgments. Computerized neural networks can function in ways that are a huge improvement upon the brain's own processing mechanisms. Although, for some, it is easy to visualize how the huge number of neurons in the brain could provide the computational power that a person might need, it is difficult to explain why the brain is not better at forming complex mathematical judgments.
In all, neural network is a rich area of research which has the potential to capture perhaps a greater range of the operation of the brain than with only computational models. It should be noted that neural network do not perform magic, but can produce very exciting results if used intelligently, as this paper has attempted to explain some of its benefits, and the kind of task that a neural network excels at in computation.
Artificial intelligence is a large and challenging research field. The progress we have made in the last few decades is both immense and minimal. I feel like there is much to be learned in this field and I am sure that real progress is imminent. In this paper I covered
many relevant subjects in AI research and while this was just a scratch on the surface I think this is a good start going into real AI research. I covered the fields in AI research and questioned the possibility of a strong AI. I talked about the relevant approaches to
programming an AI. I mentioned the practical applications of AI in todays¡¯ society as well as its¡¯ relevance. Then, I took the dangers of AI into account, while talking about future research goals both in the long and short term.
In the practical part of this paper I described the methodology and language I used to program my AI. I went into detail about how AIML works and how I used it in my program. I covered the program I use to run the AI, program-o, as wells as how to get it to work. I then
argue why a chatterbot is an AI and give my thoughts about the project as a whole. AI is a broad topic and there are many things that I still have to learn about it. I feel like for every tidbit of information that I learn about AI, I ask twice as many new questions. I
hope that this paper has helped you to understand the capabilities and possibilities that artificial intelligence presents for humanity as well as the possibly negative things it can bring with it. And with that, I finish this paper, gaining a new view on modern science and technology.
Software quality measurement is one of the most important tasks in software engineering. Initially the measurement metrics were taken from the existing hardware components. That creates a problem as two situations are different. Artificial intelligence  software  has  several  issues  for  quality measurement. Considering the market demands this area should be considered as thrust area for research.
This chapter presented a design for a prototype demonstration system, in accordance with the analysis of Chapter 3. The design for the
syntax of the Tala conceptual language is fairly general and flexible, addressing issues such as compound nouns, gerunds, compound verbs,
verb tense, aspect and voice, nested prepositions, clitic possessive determiners, gerundive adjectives, shared dependencies, coordinating
and subordinating / structured conjunctions, subject-verb agreement, etc. This coverage indicates a Tala syntax could be comprehensive  or English, though developing such a comprehensive syntax is a large effort that could occupy multiple researchers. The design for a prototype conceptual framework includes representations of perceived reality, subagents, a Tala lexicon, encyclopedic knowledge, mental spaces and conceptual blends, scenarios for nested conceptual simulation, executable concepts, grammatical constructions, and event memory. The design for prototype conceptual processes includes interpretation of executable concepts with pattern-matching, variable binding, conditional and iterative expressions, transmission of mental speech acts between subagents, nested conceptual simulation, conceptual blending, and composable interpretation of grammatical constructions.