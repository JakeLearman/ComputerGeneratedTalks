Combinatorial auctions are auctions in which bidders can place bids on
combinations of items (packages or bundles) instead of individual items.
They are motivated by the non-linear valuation of the requested resource
bundles due to the inherent interdependencies of the auctioned goods or
services, when the bidders are interested in multiple heterogeneous items
and their valuations are non-additive. This kind of resource allocation is a
common research topic in multi-agent systems.
Traditionally, the search for an optimal solution to the combinatorial
auction problem (CAP) was mainly done by integer programming
(Andersson, Tenhunen & Ygge, 2000) or branch-and-bound (Sandholm,
2002). Due to the high computational effort of such approaches, heuristics
are also used to solve the CAP, which try to reach a balance between
solution quality and computational effort.
There are many real-world problems which require the exploration
of multiple local optima in order to find the global optimum. Evolutionary
algorithms (EAs) proved to be very successful optimization methods,
especially in cases where classical differential-based techniques are difficult
or even impossible to use. As population-based methods, EAs demonstrated
a clear potential for finding the global optimum, despite typical problems
such as premature convergence or low solution precision, especially for
complex optimization problems.
With the recent advances and popularization of quantum mechanics
and quantum computing, ideas from this field have been increasingly used
as a source of inspiration for new variants of evolutionary algorithms, with
the main goals of increasing their speed and avoiding premature
convergence.
Real-world applications of combinatorial auctions include transport
services, which are often highly interdependent (Ledyard et al., 2002;
Caplice & She, 2003). Practical transport auctions in the United States can
handle an average annual procurement volume of 150 million USD, and the
whole auction process can take up to a few months.
Combinatorial auctions have also been used for truckload
transportation, bus routes, and industrial procurement, and have been
proposed for airport arrival and departure slots, as well as for allocating
radio spectrum for wireless communication services. Combinatorial
auctions for radio spectrum have been conducted in the United States and
Nigeria. In each case, the motivation for the use of a combinatorial auction
is the presence of complementarities among the items, which differ across
bidders. For example, a truckerís cost of handling shipments in one lane
depends on its loads in other lanes. Similarly, a mobile phone operator may
value licenses in two adjacent cities more than the sum of the individual
license values, since roaming between cities is important for the customers
of the operator (Cramton, Shoham & Steinberg, 2006).
The CAP is also known as the winner determination problem,
according to the traditional task of the auctioneers to identify the winner.
Many algorithms have been proposed for the CAP, for example a method
based on stochastic local search (Hoos & Boutilier, 2000), a method for
multi-unit combinatorial auctions (Leyton-Brown, Shoham & Tennenholtz,
2000), an optimal algorithm based on depth-first branch-and-bound for
generalized combinatorial auctions, i.e. multi-unit, double auction and price
reservation (Sandholm & Suri, 2000), or a genetic algorithm with random
key encoding (Khanpour & Movaghar, 2006).
The performance of three different heuristics for CAP have been
compared (Schwind, Stockheim & Rothlauf, 2003): the greedy approach,
simulated annealing and a genetic algorithm (GA), and it was found that the
GA, using a random key encoding, shows the best results with acceptable
execution times.
The CAP is NP-hard, i.e. there is no polynomial-time algorithm that
is guaranteed to compute the optimal allocation. Moreover, it is not
uniformly approximable, i.e. there exist no polynomial-time algorithm and a
constant d such that the algorithm
Since crossover in a quantum-inspired evolutionary algorithm is
usually made by means of a rotation gate (equation 3.10), some authors
have used a differential approach to fine-tune the rotation angle, i.e.
gradients of the objective functions to determine the size of the rotation
angle (Li, Wu & Liu, 2012).
However, it can be argued that the nature of this operation is not
exactly in line with the philosophy of the evolutionary approach, where the
objective function may not be differentiable. In the present paper, in order to
improve the quality of the solution, an evolutionary hill-climbing phase was
added after the termination of the QIEA: a set of solutions is generated
starting from the current solution, based only on mutation, and the best
solution among those is selected as the new current solution. The process is
repeated for the same number of generations as the main QIEA.
In addition, a steady state model is used. A new population is
created, but it does not automatically replace the old one. The two
populations are merged, sorted by non-domination and crowding, like in
NSGA-II, and the next generation is produced by selecting the best half.
In order to generate valid candidate solutions, and hence to increase
the diversity of the population by avoiding to discard individuals which do
not satisfy the constraints, a repairing procedure was applied. A greedy
approach is impossible, because with multiple attributes it cannot be

determined which gene will have the lowest or highest impact on the fitness
of a chromosome. Therefore, a randomly selected gene with the value of 1
in the measured individual is set to 0, thus decreasing the number of
violated constraints, until the chromosome comes to satisfy all the
constraints of the problem.
where S is the solution of the algorithm (the set of all non-dominated
solution vectors), A is the set of attributes, W is the set of all utility
functions, represented here as the set of all possible combinations of weights
(provided that the sum of all weights is 1),
su )( a
is the utility of a solution
vector for an attribute, i.e. the value of a partial fitness function, and
su )(
max
a
is the maximum utility obtainable for that attribute, i.e. the optimal result of
an optimization for that attribute alone. p is an intensity function which
gives the probability density of each utility function
?Ww .

From the implementation point of view, the integral is approximated
as a Monte Carlo sampling with 1 million samples, where each trial
generates a combination of normalized weights. Since any two simulations
give the same result for U, it is considered that this number of samples is
sufficient for three attributes. The number of samples is independent from
the number of attributes, and thus this approach can also be applied for a
larger number of attributes, although the approximation will likely have less
precision.
Figure 3.1 shows two solutions of a bi-attribute problem (the
medium-sized problem considered for analysis, only with attributes A1 and
A negotiation problem can be defined as a situation where multiple agents
try to come to an agreement (or deal), given that each agent has a preference
over all possible deals. A solution to this problem is a deal accepted by all
the agents involved. Agents want to maximize their own utility, but they may
also face the risk of a breakdown in negotiation, or expiration of a deadline
for an agreement (Vidal, 2007). In the general case where several agents
negotiate over several issues, it is much difficult to find the optimal deal.
In this study (Leon, 2012c), we choose a centralized approach to this
problem, by using some evolutionary optimization methods improved with
ideas from quantum computing.
Combinatorial optimization problems were among the first to benefit
as applications of quantum-inspired genetic or evolutionary algorithms.
However, quantum-inspired optimization has also been applied for various
continuous benchmarks problems (Liao, Wang & Qin, 2010; Zongyao &
Zhou, 2011; Li, Wu & Liu, 2012). Quantum-inspired variants have been
proposed as well for differential evolution (Storn & Price, 1997), used to
solve both binary problems such as the knapsack (Hota & Pat, 2011) and
continuous optimization ones (Wang et al., 2012). Other applications
include: training fuzzy neural networks (Zhao et al., 2009), options price
model calibration in financial modelling (Fan et al., 2007), data clustering
(Xiao et al., 2008), image segmentation (Melo et al., 2008) or electric power
applications (Babu, Das & Patvardhan, 2008; Vlachogiannis & Lee, 2008).
Although different from evolutionary algorithms, particle swarm
optimization is also a model of biological behaviour, namely, the social
flocking behaviour of insects or birds. This algorithm was adapted with the
help of ideas from the quantum computing, with applications to numerical
benchmark problems (Pant, Thangaraj & Abraham, 2008), feature selection
and parameter optimization for classification based on neural networks
(Hamed, Kasabov & Shamsuddin, 2011) and linear array antenna synthesis
in the field of electromagnetic research (Mikki & Kishk, 2006).
The evaluation of the results of multi-agent negotiation protocols is not
straightforward. However, there are some parameters that can be used to
evaluate these protocols: efficiency, stability, existence of time constraints,
and whether side payments, or money transfers, are allowed (Kraus, 2001).
A qualitative criterion to identify the states that are optimal from a
social point of view is the identification of Pareto optimal states (the Pareto
frontier), i.e. states where it is not possible to increase the utility of some
agents without reducing that of any of the others. As mentioned in section
1.1, commonly used quantitative solutions for the bargaining problem are,
among others, the Nash solution and the utilitarian solution.
Beside these, other solutions which are not always Pareto optimal
have been proposed, such as the egalitarian solution, where all the agents
receive the same utility and the sum of utilities is maximum, and the Kalai-
Smorodinsky solution, which is Pareto optimal only if all the deals are
possible, and where each agent receives an utility proportional to the
maximum utility that it could be able to receive.
A concrete negotiation problem is the following example (Bo & Li,
2011) with three agents: supplier agent, storage agent and sales agent, and
four issues: the commodity price (with a range from 1000 to 1150 yuans), the
number of commodity (100-200 pieces), the date of payment (5-12 days),
and maintenance period (1-16 months).
The second variant, QIEA2, is similar to the first, however it uses
elitism to improve the solution. A new population is created using the same
procedure, but it does not automatically replace the old one. The two
populations are merged, sorted by the values of the fitness, and the next
generation is created by selecting the best half (the so-called ìsteady stateî
approach of generating multiple solutions from the information contained in
a single qubit is much closer to the philosophy of quantum computing. A
resetting mutation is used instead of a ìflipî mutation: a qubit is randomly
reinitialized, in order to introduce a greater diversity into the population and
avoid premature convergence. Also, a population-based elitism was used,
similar to the idea from QIEA2. Finally, since some authors use a differential
approach to fine-tune the rotation angle (Li, Wu & Liu, 2012), it was
considered that the nature of this operation was not exactly in line with the
philosophy of the evolutionary approach, where the objective function may
not be differentiable. However, in order to improve the quality of the
solution, an evolutionary hill-climbing phase was added after the termination
of the QIEA. Here, a set of solutions is generated starting from the current
solution, based only on mutation, and the best solution among those is
selected as the new current solution. The process is repeated for the same
number of generations as the main QIEA.
In all cases, the divisions are normalized in order to comply with the
constraint that
It must be mentioned that none of the three QIEA variants are actual
quantum algorithms meant to be executed on a quantum computer. They are
sequential algorithms for traditional computing architectures, but include
ideas from quantum computing.
In order to evaluate the performance of the quantum-inspired algorithms,
several experiments were made while varying the parameters: the number of
generations, the population size and the mutation rate. Table 3.19 displays
the results in terms of average fitness and best fitness.
One can see that the results improve as the number of generations
increases. With 1000 generations, the best fitness comes very close to the
optimum value. Also, the accuracy of the results increases with the
population size, which ensures a greater diversity. The mutation rate must be
larger compared to the one usually used in a classical EA, but if it is much
greater (e.g. 25%), the results are affected by randomness and their quality
decreases. Therefore, an acceptable mutation rate was found to be around
10%.
The results in italics were repeated for convenience. The best results
in each group are marked in bold. Finally, the results with the best
combination of parameters are presented.
Table 3.20 shows the results of the algorithm with different
parameters when applied to the 3 x 3 problem, and Table 3.21 displays the
corresponding results for the 10 x 10 problem.
In the polymersí field, the efficient design of new materials requires the
prediction of the properties of the candidate polymers and the selection of
the best structure from all the potential possibilities. To solve this problem,
a quantitative structure-property relationship is necessary. One of the most
interesting properties of polymers is the liquid crystalline behaviour,
because in this state, the materials combine two essential properties of the
matter: the order and the mobility. But, because of the complexity of the
liquid crystalline phase, it is not at all easy to predict the occurrence of a
mesophase, i.e. an intermediate state of matter between liquid and solid.
Also, the reduction of the number of experimental trials represents a
requirement that is more and more felt in the field of the study and analysis
of physical, chemical and biological phenomena. In particular, this tendency
is encouraged by the economical advantages that experimental research can
get from it in terms of time and money savings. The determination of the
liquid crystal properties of some organic compounds based on their structure
and quantitative structure-property relationship is a major research subject
in computational chemistry.
In order to test the underlying research question and to capture subjective views of ex-
perts9 and their real-life context, I collected data through semi-structured and guideline-
based interviews with industry experts in San Francisco, USA. A conscious decision was
made not to conduct a highly structured survey, instead to conduct semi-structured inter-
views as qualitative interviewing has greater flexibility and allows the opportunity to cap-
ture the industry expertís perspective on the subject of study in greater detail while at the
same time allowing for quantitative analysis of the interview responses (Leech 2002, p.
665). The overarching aim of the expert interviews was to collect data on (i) developersí
motivation to develop open-source AI and (ii) their perspective on AI as a commons. In
order to guarantee a certain degree of comparability between the different interviews, I
conducted them on the basis of an interview guide with a number of open-ended questions
depending on interesting aspects during the conversation. The interview design was the
result of comprehensive desk research: prior to the interviews, I analyzed the websites of
AI companies, conducted a market overview, assessed reports of experts and conducted
personal conversations. The interview guide can be found in the Appendix of this thesis.
The interview partners were selected based on the key informant technique in order to
attain expert knowledge, since key informants are characterized as individuals possessing
meaningful knowledge and are willing to communicate with the researcher (Marshall
1996, pp. 92-96). The target group had mixture of diverse fields and business back-
grounds whose current work is related to open-source AI as well as experts who were not
new to the AI space and had insights about recent business developments. Six AI experts
were interviewed. Most studied Computer Science or related fields.
That was particularly helpful in assessing the first part of the research question about the
incentives and motives of involved actors. Overall, I had a high response rate, which em-
phasized the great relevance of the topic. The initial contact mode was an interview re-
quest via e-mail with attached information about the purpose of the study. As part of the
sampling technique, I made use of snowball sampling asking at the end of each interview:
ìIs there anyone else in the field of AI you know or can think of who I should definitely
speak with?î Though controlling for this, I was not referred to any new interviewee which
9 An expert in this study is defined as a person who has specialized knowledge, experience in the field of
open-source AI and/or has privileged access to information about decision processes regarding open-
source AI.was not already part of the initial population, thus indicating that a representative sample
had been chosen. The interviews were conducted in English with predetermined open-
ended questions. Each interview consisted of about 12 questions and had a duration of
about 30 minutes. Two interviews have been conducted via telephone. The interviews
were recorded, transcribed and analyzed. The interview questions can be found in the
Appendix of the thesis. In general, the interviews were structured along the following
four question blocks: (1) personal motivation, (2) open-source AI movement, (3) AI as a
commons and (4) the future of AI. A tabular representation of the interview partners can
be found in Table 1.
Given the possibility that some of the experts came from multiple disciplines before en-
tering the field of open-source AI and/or general AI research, questions about their moti-
vation and their journey were asked first. Questions about the reasons developers chose
to make their work open-source in regard to the OpenAI company, were of interest.
Crowdsourcing, the accessibility of data for AI researchers and developers, the ownership
of powerful AI systems, and regulation of AI were also discussed. Furthermore, the ac-
cessibility of AI systems, the contingency of AI as a commons and the future of AI were
talked about at the end of the interview.
Genetic algorithms are based loosely upon Darwinís theory of natural selection
(Darwin, 1859), which suggests (to simplify greatly) that organisms evolve based
on the more useful elements of an organism enabling that organism to breed
more, thus passing on those elements to some or all of its offspring, who will,
again, breed more. Eventually the organisms with the useful traits will either
supplant or exist alongside the original organisms. These useful elements would
initially be produced by a mutation of existing elements, before being passed
down parent to child in this manner. Genetic algorithms are immensely popular
optimisation algorithms due to their suitability for non-linear, non-convex, multi-
modal and discrete problems with which traditional gradient descent derived
algorithms may perform poorly in comparison (Nicklow et al., 2010).
The process of a genetic algorithm can be separated into four distinct sub-
processes: generation, selection, crossover and mutation. Generation involves
building an initial population of potential solutions either by random creation or
some other method. Selection is where the population is evaluated, and then a
subset of that population is selected by one of many possible selection algorithms
(e.g., fitness proportionate (Back, 1996), stochastic universal sampling (Ghimire
et al., 2013), tournament selection (Miller and Shaw, 1996; Nicklow et al., 2010)
etc.) for the generation of a child population via the next stages of crossover and
mutation. Most modern implementations use either truncation or tournament
selection (Nicklow et al., 2010) as these are scaling invariant and inherently elitist,
which has been shown (Bayer and Finkel, 2004; Reed et al., 2000; Yoon and
Shoemaker, 2001) to enhance the effectiveness of the genetic algorithm.
Crossover is the process of generating new chromosomes by combining aspects
from previous solutions chosen by the selection algorithm via one of several
possible crossover algorithms (single-point, multi-point, uniform, partially mapped
crossover, etc.) in the hope of producing a ìchildî chromosome more fit then
either of its ìparentî chromosomes. Mutation involves introducing a chance of
making random changes to the chromosomes, which helps to prevent premature
convergence and allow a fuller exploration of the search space by including genes
that were not present in the initial random population. A final process that is not
essential to the function of the algorithm, but which vastly improves its efficiency
and effectiveness, is called ìelitismî. Elitism ensures that the best scoring
chromosomes from each population make the transition from parent to child
population intact. This ensures that promising search areas are not lost to the
algorithm part way through iteration.
Genetic algorithms have been around since the 1960ís (Holland, 1962). However,
they only began to gain wider acceptance as an effective and efficient
optimisation strategy in 1975. This was due to both the publication of ìAdaptation
in Natural and Artificial Systemsî (Holland, 1975) and the thesis entitled ìAn
analysis of the behaviour of a class of genetic adaptive systemsî (De Jong, 1975).
Holland (1975) presented the concept of adaptive algorithms utilising the
concepts of mutation, selection and crossover, and De Jong (1975) showed that
genetic algorithms could perform exceptionally well on discontinuous and noisy
data that is challenging for many other optimisation techniques. Genetic
algorithms have been utilised on many and varied problems since their
development (Goldberg and Wang, 1997; Huang et al., 2009; Montana and
Davis, 1989; Santarelli et al., 2006; Scully and Brown, 2009) with good success
rates. The annealing process in metalworking inspired the simulated annealing
algorithm. Annealing is the process of heat-treating metal to achieve desired
properties within the material by heating it up and then allowing it to cool very
slowly. Annealing occurs because over time, the atoms within the metal align
themselves towards the equilibrium state when the bonds between atoms have
been broken (hence the heat).
Simulated annealing (Kirkpatrick et al., 1983) is a computational emulation of this
process, in order to apply it to optimisation problems. A temperature is tracked
within the algorithm, beginning at a high level and gradually decreasing
throughout the execution of the algorithm. The algorithm usually halts when the
temperature reaches a pre-determined level. Initially, one solution to the problem
in question is generated and the score obtained through the objective function
represents the ìenergyî of that particular state. At each cycle of the algorithm, this
state is altered to generate a new state. This new state is then evaluated and if
its energy is lower, it replaces the current state. If the energy of the new state is
higher, then it still may replace the current state, but that is based upon chance
influenced by the current temperature and the difference in energy. As the
temperature lowers, the chance of inferior solutions replacing the main solution
drops swiftly (Smith and Savic, 2006).
It has been proven (Geman and Geman, 1984) that with a sufficiently drawn out
cooling schedule the simulated annealing algorithm will always converge to the
best possible solution. Most implementations of simulated annealing are,
however, on far faster cooling schedules in order to be of use in providing an
answer within a reasonable time frame. Simulated annealing does generally
perform well over shorter intervals, however, and has proven to be an extremely
effective solution for single objective optimisation problems.
2.3.4 Ant-Colony Optimisation
Ant-colony optimisation is a relative newcomer to the field of optimisation
algorithms, having been first introduced in the early 1990ís by M. Dorigo and his
colleagues in Italy (Dorigo, 1992; Dorigo et al., 1997, 1991) as an algorithm for
solving combinatorial optimisation problems. Like evolutionary algorithms, ant-
colony optimisation is a meta-heuristic algorithm inspired by nature. In this case,
by the methods that ants in the natural world use to guide other members of their
colony to discovered food sources, i.e., pheromone trails.
When ants are exploring an area around their nest for food, they initially explore
in a wholly random manner. When an individual ant discovers a food source, it
evaluates the quantity and quality of this food source, and then carries a portion
back to the colonyís nest leaving a pheromone trail behind it. The pheromone trail
varies depending upon the quantity and quality of the food source. Other ants are
attracted to follow this trail and will then discover the food and leave their own
pheromone trail. The pheromones laid to mark these trails evaporate over time,
so over time longer trails will become weaker than shorter ones and attract fewer
ants in consequence. In this manner, although no direct communication has taken
place, greater quantities of ants will be drawn towards the best food sources, the
shortest distance from the nest (Dorigo and Blum, 2005).
Ant-colony optimisation has been applied to various problems successfully
(Dorigo and Stutzle, 2004) since it was developed. A proof of convergence
focusing on a particular implementation of ant-colony optimisation, called ìGraph
Based Ant System (GBAS)î to an optimum solution was published in 2000
(Gutjahr, 2000). This was followed by a more generalised proof of convergence
to any optimal solution in 2002 (Gutjahr, 2002). Practical applications of GBAS
have, however, been rare (Dorigo and Blum, 2005) and work continued on
proving convergence of more commonly used ant-colony algorithms with an
included positive lower bound. This has finally been completed with a proof for
convergence in value and solution (Dorigo and Stutzle, 2004, 2002).
Ant-colony optimisations main advantage over evolutionary algorithms, or other
optimisation techniques, lies in its ability to be run on-line and swiftly compensate
for live alterations to the problem being solved. It can be used with great effect
for route planning, network planning, and similar problems, due to these
capabilities. Evolutionary algorithms do, however, have a longer record of
accomplishment and are considered a safer option, particularly where the
problem has no element of volatility and does not have to be solved on-line.
2.3.5 Multi-Objective Optimisation
The vast majority of optimisation algorithms are designed around the idea of a
single objective; therefore, there is a proliferation of highly efficient, accurate
algorithms to deal with single-objective problems that are highly documented (De
Jong, 1975; Dorigo, 1992; Dorigo et al., 1997; Holland, 1975, 1962; Kirkpatrick et
al., 1983). The field of multi-objective optimisation is more challenging and more
useful. Most real-world problems do not involve simply trying to find the most
optimal approach to a problem to achieve one fixed solution. They are a matter
of weighing different options against each other, based on several separate
criteria, and attempting to select the approach with the most usable balance.
Multi-objective optimisation algorithms are designed to work with more than one
objective function. The algorithm works to minimise or maximise each of the
objectives simultaneously. The objective functions are usually in conflict with
each other, as if there was no conflict between the two objective functions it would
be more efficient and possibly more accurate to develop a number of single-
objective optimisation algorithms and find the optimum value for each objective
in this way (Coello, 1999).
In words, multi-objective optimisation is the problem of finding from a given set,
which satisfies the constraints listed in 2 and 3, the sub-set that is composed of
the optimum values of all objective functions. This set is known as a ìPareto setî
(Pareto, 1896), the non-inferior or non-dominated sets, which contain Pareto
optimal solutions. A point is considered Pareto optimal if no vector exists which
would improve the score of one criterion without causing a simultaneous
deterioration in some other criterion.

The first mention of the concept of a truly functional multi-objective genetic
algorithm (i.e., a genetic algorithm that could handle multiple objectives without
resorting to objective function aggregation) dates back to the 1960ís (Rosenberg,


1967). However, no multi-objective genetic algorithm was developed at that time.
An attempt was made in 1983 (Ito et al., 1983) to develop a multi-objective
genetic algorithm, but usually credit is given to Schaffer with his Vector Evaluated
Genetic Algorithm (VEGA) for developing the first fully functioning multi-objective
genetic algorithm (Schaffer, 1985, 1984). VEGA offered a credible multi-objective
genetic algorithm, but it failed to include a mechanism for multi-objective elitism.
This dramatically affects the speed at which an algorithm converges to good
solutions, as promising solutions may be lost throughout the process.
After the development of VEGA, the most popular approaches for multi-objective
genetic algorithms were aggregating functions. The most commonly used
versions of these were the weighted-sum approach (Coello, 1999; Jones et al.,
1993; Liu et al., 1998; Syswerda and Palmucci, 1991; Wilson and Macleod, 1993;
Yang and Gen, 1994), goal programming (Charnes and Cooper, 1961; Coello,
1999; Ijirii, 1965; Sandgren, 1994; Wienke et al., 1992), goal attainment (Coello,
1999; Wilson and Macleod, 1993), s-constraint (Coello, 1999; Quagliarella and
Vicini, 1997; Ranjithan et al., 1992; Ritzel et al., 1994).
These aggregating functions had several common problems, including a difficulty
in working well on non-convex search spaces. Furthermore, where weights were
used within the algorithm a very good knowledge of the objective functions in
question was required for the values of those weights to be decided. So the need
for improvement in the field was still very obvious.
The initial ideas of including the concept of pareto-optimality in multi-objective
algorithms arose in Goldbergís book in 1989 (Goldberg, 1989). Whilst criticising


VEGA, he suggested that the use of non-dominated ranking of solutions with
selection could move a population towards the Pareto front. There was no

implementation of this idea for an algorithm supplied, but the majority of multi-
objective algorithms developed after the publication of this book drew in a large

part upon his ideas and suggestions (Coello, 2005), most notable the non-
dominated sorting genetic algorithm (NSGA) (Srinivas and Deb, 1994), the

niched Pareto genetic algorithm (Horn et al., 1994), and the multi-objective
genetic algorithm (MOGA) (Fonseca and Fleming, 1993). Additionally, in 1992 a
method was developed (Tanaka and Tanino, 1992) to incorporate user
preferences into a multi-objective evolutionary algorithm.
From this point onwards, a focus shift occurred. The problem of building effective
algorithms had been solved, and the goal was now to produce ever more effective
and efficient algorithms (Coello, 2005). One of the main initial steps moving

towards efficiency and effectiveness was the introduction of elitism to the multi-
objective evolutionary algorithm playing field. Elitism involves artificially

preserving the most optimal chromosomes produced at each point where
chromosomes may be lost from the algorithm process, to ensure that promising
solutions are not lost. Although early studies hinted at the possibility of application
of elitism to multi-objective evolutionary algorithms, the formal introduction of this
concept to the subject is usually credited to Echart Zitzler (Zitzler and Thiele,
1999) and his strength Pareto evolutionary algorithm (SPEA). After the
publication of Zitzlerís paper the majority of multi-objective evolutionary
algorithms implemented some form of elitism (Coello, 2005). The most common
form of elitism within a multi-objective evolutionary algorithm involves an external

population comprised of all generated non-dominated solutions. Every solution
entered into the external population must be non-dominated with regard to that
population, and replaces any solution that it dominates within it.
The most popular current multi-objective evolutionary algorithms are SPEA
(Zitzler and Thiele, 1999, 1998), SPEA2 (the second iteration of SPEA) (Zitzler
et al., 2002), the pareto-archived evolution strategy (PAES) (Knowles and Corne,
2000), and NSGA II (the second iteration of NSGA) (Deb et al., 2002, 2000).

Although attempts have been made to convert simulated annealing to multiple
objective optimisation due to its effectiveness as a single objective algorithm
(Bandyopadhyay et al., 2008; Smith and Savic, 2006), it does not lend itself to
the concept in the same way as evolutionary algorithms do, with their large
population based approach. Attempts have generally revolved around objective
function aggregation, similar to earlier multiple-objective evolutionary algorithm
attempts (Smith and Savic, 2006).
2.3.8 Multi-Objective Ant-Colony Optimisation
Similarly, to multi-objective simulated annealing, attempts have been made to
develop multi-objective ant-colony optimisation algorithms (LÛpez-Ib·Òez et al.,

2004; LÛpez-Ib·Òez and St¸tzle, 2012; LÛpez-Ib·Òez and Stutzle, 2010). Ant-
colony optimisation does not, however, lend itself so conveniently to multi-
objective optimisation and finding Pareto sets as evolutionary algorithms do.

Additionally, ant-colony optimisation is a fairly recent algorithm and does not have

a comparable body of published applications to some other algorithms to back up
its effectiveness.


Machine learning is a branch of artificial intelligence techniques dealing with
algorithms that can learn from data. The most common usage is for data mining,
and they can be used to great effect for classification of data (Kotsiantis, 2007).
There are two main types of learning undertaken by machine learning algorithms,

commonly referred to as ìsupervisedî and ìunsupervisedî learning (Kotsiantis,
2007).
Data sets for training machine learning algorithms may be continuous,
categorical, or binary. Where instances within the data set are provided with
known labels (i.e. the correct outputs) the training process is known as a
ìsupervisedî process (Kotsiantis, 2007). Where there are no known labels, the
process is known as ìunsupervisedî (Kotsiantis, 2007). Algorithms designed to
undertake unsupervised learning generally work with clustering techniques such
as Bayesian techniques (Neal, 1995). Clustering techniques are methods of
identifying similarities between data instances. Those instances are then given
(often varying degrees of) membership of ìclustersî in an attempt to identify
unknown but potentially useful classifications of data. These have been used on
such diverse problems as road sign recognition (Prieto and Allen, 2009), water
resources (Kalteh et al., 2008) and text detection with character recognition
(Coates et al., 2011).
In this thesis the concentration is on classification algorithms, specifically artificial
neural networks, and supervised training. This is because part of the work
performed will be following on from previous work on developing neural network
meta-models for multi-objective optimisation (Behzadian et al., 2009).
Additionally, the second area where machine-learning techniques are utilised is
within the LEMMO (Learning Evolution Model for Multiple-objective Optimisation)
algorithm (Jourdan et al., 2005). The LEMMO algorithm is designed in the same
way as the LEM algorithm that it was built upon (Michalski et al., 2000) to utilise


any machine-learning algorithm. Artificial neural network (ANN) code was already
implemented, so ANNs will be utilised for the machine learning part of this
algorithm to minimise development time.
2.4.2 Artificial Neural Networks

The first neural network model which featured digital neurons was developed as
early as 1943, although in this model no capability for learning was initially
included (McCulloch and Pitts, 1943), limiting the usefulness of the model. In
1958 Frank Rosenblatt developed the ìPerceptronî model (Rosenblatt, 1958),
however Rosenblatt was unable to identify a reliable mathematically accurate
mechanism for allowing multi-layer perceptrons to learn. The next major advance
in artificial neural networks occurred in 1974, when Werbos (1974) succeeded in

discovering the back-propagation algorithm, which was also independently re-
discovered in 1982 (Parker, 1982). The application of neural networks to varied

and complex problems is, today, a common occurrence (Behzadian et al., 2009;
Biswajeet et al., 2010; Rowley et al., 1998).
There are two methods of training an artificial neural network ñ supervised and
unsupervised (Kotsiantis, 2007). Supervised learning requires a set of training
data that is pre-processed such that, along with each instance of data, there is
an included expected output for the artificial neural network. The most common
model for supervised-learning neural networks architecture is a feed forward
network (see Figure 2). This is an arrangement of different layers of ìnodesî, most
commonly an input layer, a ìhiddenî layer, and an output layer. Each layer within

this arrangement has connections to the outputs of nodes of the previous layer,
and each of these connections has an associated weight (Lippman, 1987).

Data then enters into the network at the ìinputî points (see Figure 2) and
proceeds through the network node by node. At each connection it is multiplied
by the value of the weight attached to that connection. At each node it is
processed by a function ñ usually a differentiable function to facilitate training, of
which the most popular are the logistic function (sigmoid function, see equation
6), and the Gaussian function (see equation 7). Artificial neural networks using
Figure 2 - Feed forward artificial neural network structure using a sigmoid activation
function


sigmoid or Gaussian functions have been shown to be capable of approximating
any arbitrary continuous function on a limited size domain, with varying accuracy
depending on the number of neurons in the network (Cybenko, 1989; Hartman et
al., 1990; Hornik et al., 1989; Park and Sandberg, 1991). Indeed, it has been
shown (Hornik, 1991) that the choice of activation function is not as critical in
allowing for the potential of universal approximation as the feed-forward
architecture.

Training of feed forward artificial neural networks is accomplished by modifying
the weights within the network to move them closer to achieving a desired output.
The most commonly used algorithm to achieve this is the back-propagation

algorithm (Parker, 1982; Rumelhart et al., 1986; Werbos, 1974). Back-
propagation is a supervised learning technique that involves propagating error

backwards through the network. It is a gradient descent method and because of
this in its pure form it will be trapped at any localised optima that occur in the
search space. A nearly ubiquitous addition to back-propagation in order to avoid

this effect is ìmomentumî (Rumelhart et al., 1986). Momentum allows the back-
propagation algorithm to be influenced by recent trends in the error surface,

reducing the likelihood of, but not eliminating, the possibility of being stuck in local
optima.
Back-propagation can be viewed as a simple gradient descent algorithm
optimising the weights with the error from the artificial neural network performing
the role of an objective function to be minimised. Taking this viewpoint makes it
clear that any optimisation algorithm could be utilised to the same purpose.
Genetic algorithms (see 2.3.2), simulated annealing (see 2.3.3), ant-colony
optimisation (see 2.3.4) or any other optimisation algorithm can also be utilised,
therefore, as part of the training process of an artificial neural network (Montana
and Davis, 1989). This is also true for other versions of error propagation
algorithms (Heaton, 2014; Igel and H¸sken, 2000).
2.4.3 Bayesian Belief Networks
Bayesí rule is a mathematical rule, which identifies the way in which existing
beliefs should be altered, given a set of evidence previously unavailable. This can
be simply illustrated with an example utilising white/black marbles as measures
of belief. If it is imagined that a new-born baby sees the sun set, and wonders

whether it will rise again. As the child has no prior knowledge, it assigns a fifty-
fifty chance that the sun will rise again the next morning, and represents this by

placing a white marble and a black marble into a bag. The following day, the sun
does rise. The child, therefore, places another white marble into the bag to
represent his increase in belief that the sun will rise. The probability that a random
marble selected from the bag will be white (i.e. the childís belief that the sun will
rise), has gone from 50% to 66.67%. As time passes and the bag becomes nearly

entirely full of white marbles, the child becomes increasingly convinced that
sunrise will come each day (Anonymous, 2000).
Mathematically, this can be represented as in equation 9 (Bayes and Price, 1763;
Laplace, 1986) where equation 8 denotes the probability that a random variable
ëRí has the value ërí given that the evidence is equal to ëeí. A Bayesian belief network is a model that reflects the states of something, and
how those states are described by probabilities. A Bayesian belief network can
be utilised to model almost anything ñ with all possible states of a model
representing all possible ways the states of that model can be configured. It works
on the principle that some states are more likely to be true, when other states are
also true. For example, if a person had a model of their body, they are more likely
to have a sore throat if they also have a blocked nose. They are more likely to
have sore eyes, if their eyes are watering.
Although Bayesian belief networks have many applications and are extremely
effective when applied to the correct problems, for the purpose of this thesis a

neural network is more easily utilisable and therefore is the approach that has
been selected.

This algorithm, presented by Behzadian et al. (2009) involves the use of an
adaptive neural network. Initially the algorithm is run for a number of iterations
with no neural network in place. During these initial iterations the data from each
objective function evaluation is collected. This data is then used after a fixed
number of iterations to construct a training set for an initial training cycle for the
neural network. The function is then altered so that the neural network first
evaluates all solutions produced by the genetic algorithm. The solutions that rank
above a specified cut-off after this first evaluation are then re-evaluated for
accuracy by the full objective function. All of the data from these re-evaluations
is stored and every 'n' iterations the neural network undergoes a further training
cycle using this new data.


The goal of this process is to initially train a neural network, which can then be
used as a kind of ìfilterî, to remove the poor solutions from the set before using
the computationally intensive full objective function on the remaining solutions.
The re-training should make this network progressively more accurate,
particularly for solutions that more closely match the Pareto front (as the solutions
the network is being trained on should be moving closer to the Pareto front with
each training cycle) (Behzadian et al., 2009).
This approach has been successfully applied to a sampling design (Kapelan et
al., 2005) approach (Behzadian et al., 2009). In the test cases described the
MOGA-ANN (multi-objective genetic algorithm with adaptive neural networks)
approach performs almost as well as a standard MOGA approach. It does so with
far fewer evaluations of solutions with the computationally costly objective
functions, thus increasing performance by around twenty-five times with only a
very minor effect on Pareto front accuracy. The LEMMO algorithm (di Pierro et al., 2009; Jourdan et al., 2005, 2004) is based
on the LEM (Learning Evolution Model) single-objective algorithm (Michalski et
al., 2000). The LEMMO algorithm involves a decision tree classifier within the
NSGA-II algorithm which is used as a feature identifier for characteristics of
solutions which perform well.
At a high level the algorithm works by performing a number of iterations of a
standard NSGA-II (Deb et al., 2002) algorithm and storing data on which of the

generated solutions are ìgoodî solutions and which are ìpoorî solutions. It then
uses this data to train a machine-learning algorithm to distinguish between good
and bad functions, and uses the outcome of this training in some fashion to
generate new solutions, which, according to the trained machine-learning model,
are ìgoodî solutions. These are then integrated with the main population and the
algorithm continues with further iterations based on pure NSGA-II and further
iterations of machine learning.
There are five variants of this algorithm that have been tested (Jourdan et al.,
2005), and these are described briefly below.
ï Variant 'LEMMO-1'
o Learning is run when there has been no change to the population
for two successive iterations.

ï Variant 'LEMMO-Fix1'
o Every ten full iterations a learning iteration is entered, utilising the
initial population of the previous evolution phase as the ìbadî set
and the final population of that phase as the ìgoodî set.

ï Variant 'LEMMO-Fix2'
o A learning iteration is run every ten iterations, using the twenty
individuals most recently inserted into the Pareto set as the ìgoodî
set and the remaining individuals as the ìbadî set.

ï Variant 'LEMMO-Fix3'
o Every ten generations, a learning iteration is run, with random
individuals from the current approximation of the Pareto set as the

ìgoodî set and the remaining individuals of the current population
as the ìbadî set.
ï Variant 'LEMMO-Fix4'
o Run a learning iteration every ten generations using the best 30%
of the individual solutions found so far on one of the objectives
(randomly chosen at each learning phase) as the ìgoodî set, and
the worst 30% of those solutions as the ìbadî set.

When considering urban flooding in a risk-based fashion there is a complex
system to analyse, as the physical flooding event must be included, along with
the inhabitants of the areas to be flooded, human infrastructure present on any
affected flood plains, the ecosystem present, and private personnel who will
either influence or be influenced by the flooding and flood impacts.
The benefit of a risk-based approach over a more traditional approach is that a
risk-based approach allows the evaluation of a system in terms of the
consequences of the failure of that system, rather than in terms of the
performance of that system. This allows for a decision to be made based on the
consequences of system failure. In this thesis we focus on the economic
consequences, however almost any consequence could be used as a measure.
Generally, risk based analysis involves modifying variables that describe the
flood system in terms of pipe diameters and storage node volumes, then
analysing the results of those changes on the flood model and the effect they
have had on the risks of flooding occurring.
3.3 Flood Risk Analysis Toolset

The work in this thesis has been based upon the project that was undertaken by
a research consortium to develop a risk-based methodology and tool set for
evaluating drainage systems in terms of their flood-risk, and to improve
knowledge in this area of research (Kellagher et al., 2009). The classic risk calculation is based on magnitude of the probability and the
damage that would be caused if that probability occurred. This can be expressed
as risk (R) being equal to probability of an event (P) multiplied by damage caused
(D) by that event (see equation 10) (Kellagher et al., 2009). In order to find the probability of the event in question not occurring, 1-P, can then
be used, as the sum of all possible outcomes must have a value of one. Since a
design-storm has a specific probability, the probability of non-exceedance of the
threshold can be found via the equation below (equation 11) (Kellagher et al.,
2009). Where ëní represents the number of events in one year. Although the main
purpose of applying a risk-based methodology is to evaluate the damages for a
given flood event, these do vary for any given location during the same return
period, depending on the duration of the storm event used. It is important that
damage associated with each probability of non-exceedance is based on the
critical duration for each manhole. The critical duration is the duration with which
the maximum flood volume at a given node is associated. At the top of a drainage
system, therefore, the critical duration will be fairly short, whereas at the bottom
end of a large network it will be considerably longer (Kellagher et al., 2009).
There is also a desire to associate the flood damage with the assets that initially
flooded during some analyses. In order to achieve this, the total damages
occurring at all impact events must be proportionally distributed to the manholes
based on the damage of the critical duration event (see equation 12 where ëDí is
equal to damage at a manhole, ëdí is equal to the damage at an impact zone, ëmí
is the total number of manholes, ëií is the current manhole and ëIí is the total
number of impact zones) (Kellagher et al., 2009). Impact zones are identified
within pre-processing of the flood plain in RFSM and represent topographical 
depressions where water will collect in case of flooding (Lhomme et al., 2008). This ensures consistency when associating damages at IZís with the manholes
that initially flooded (see equation 13) (Kellagher et al., 2009). The EAD (expected annual damage) is the integration of the risk due to every
probability of non-exceedance. The simple trapezoidal integration method is
used, and as a consequence the expected annual damage for every impact zone
and manhole can be written as a function of the probabilities DNOP and damages
associated JNOP with every return period ED+ as shown in equation 14
(Kellagher et al., 2009).
When the modelling system is being run, expected annual damage evaluates
shortest return periods first, and then continues through all return periods in
ascending order until convergence is achieved.
In order to avoid an overestimation of damages, the return period threshold of
every manhole/impact zone must be found. This is the return period at which
flooding first occurs at this manhole/impact zone (Kellagher et al., 2009).
This means that D1 (the damage of the shortest return period event) is a very
small value (zero or close to zero) and P1 is an event for which flooding occurs.
When this process (see Figure 5) is plotted, EAD can be seen to increase
asymptotically (see Figure 4) (Kellagher et al., 2009). The RFSM (Rapid Flood-Spreading Model) (Lhomme et al., 2008) performs the
function of spreading volumes of water over a specified flood area. RFSM acts
as a simplified hydraulic model that produces answers to an acceptable level of
accuracy, but produces them considerably faster than more accurate and less
simplified methods (Lhomme et al., 2008).
The water level in each impact zone is then used to assess the depth of flooding
against a national property database provided by the environment agency and
information gleaned from the Middlesex multi-coloured manual on flood damage
(Penning-Rowsell et al., 2005). The damage assessment tool includes average
property valuations, based on information from the land registry office, floor space
and rateable values, flood damage curves for residential and non-residential
areas, and varied codes associated with the multi-coloured manual (Wallingford,
2009). The major performance problem with this framework involves the runtime of the
Infoworks CS (Innovyze, 2007, 2011) model within this risk assessment
framework. The process of evaluating expected annual damage is the most
computationally intensive task involved in the optimisation algorithm. A significant
proportion of that computational effort is involved in setting up and initialising
Infoworks simulation runs. This is because for each expected annual damage
assessment a selection of rainfall events with different return periods (two, five,
and ten, then steps of ten to three hundred, five hundred, seven hundred and fifty
and one thousand years) and different durations (thirty, sixty, ninety, then steps
of thirty minutes all the way to six hundred) have to be run as Infoworks
simulations. This totals seven hundred Infoworks simulation runs. With each of
those runs of seven hundred rainfall events taking approximately five hours
(depending on computational performance), this is not a tractable problem. In a
genetic algorithm, which might have a population of one hundred, to be run for
one thousand iterations (conservatively), the total runtime would be in excess of
fifty-five years. To a certain extent that could be mitigated by running on a more
modern machine than the testing machine but even with a desktop machine at
the forefront of modern consumer technology, the run-time could not be expected
to be reduced to a usable time-frame (i.e. weeks or months, rather than years). The second intended objective of the multi-objective algorithm is ìcostî. In this
case it means the cost of the proposed changes to the network (I.e. a network
that matches the original network, is zero cost). These changes will take the form
of alterations to pipe diameters, and storage node volumes. The constants in the
cost calculation are all customizable (they are constant in the sense that they
should not change during the algorithm run), and sensible defaults are provided
to give a ìrough ideaî. These defaults are based upon work undertaken by HR
Wallingford and Mouchel consulting as part of the project on which this thesis is
based (Kellagher et al., 2009). This work also formed the basis for the cost model
developed here. It would be expected that for a given flood risk scenario,
constants for these cost calculations would be modified to be in line with the real
figures for that particular scenario. These defaults have been used for testing
purposes for all experiments described in this paper, they are: Mobilization Cost
(M) of £50,000 for making any change to a network; Pipe Intervention Cost (I) of
£1,000 per metre of pipe replaced; Storage Intervention Cost (S) of £500 per
metres cubed; and Storage Base Cost (b) of £10,000 for making any change to
a storage node.

All cost calculations initially determine whether a network has been modified at
all, or whether it is identical to the original network. All modified networks cost
calculations include an initial sum to reflect the ìmobilizationî costs ñ i.e. the costs
of hiring contractors, getting them and their equipment on site, and other
associated costs with initially beginning a task of this nature. The pipe alteration
costs are then estimated by multiplying the product of a constant ìIntervention
Costî value (I) to represent costs of piping purchase, excavation, etc. and the
length of pipe (L) in question by the cross-section area (c) of the pipe. Storage
alteration costs are estimated by adding the product of an ìIntervention Costî
constant (S) signifying cost of materials, etc., and the area of the storage node in
meters squared (a), to a second ìBase Costî constant (b) that represents the
costs associated with excavation, removal of existing storage node if necessary,
etc. All non-modified pipes or storage nodes result in zero cost. Orifices are
included in the cost model and have a flat cost (o) associated with any change to
their original setting.
Therefore, the total cost of a network is the mobilization cost (M), plus the cost of
each modified pipe in the network, plus the cost of each modified storage node
in the network, plus the cost of each modified orifice in the network.
Environment in AI refers to the surroundings in which the intelligent agents acts. An agent
chooses the action based on the environment it perceive through the sensors and does the
action through the actuators. Here sensors are camera and microphone while actuators are
wheel and arm. The environment type determines how the agent is should be build.

Environments are classified by Russell and Norvig (2009) into seven types -

1. Deterministic: In this type of environment, the next state of environment can be
predicted by the current state, so the agent can act accordingly.
2. Fully Observable: All the sensors of the agent gives complete information about
the environment at each point of time.
3. Episodic: Each episode consist of agent perceiving the environment and
performing an action. The quality of action depends only on the particular episode
and independent of other episodes.
4. Static: As the environment remains unchanged, the agent can focus on deciding the
action. So the environment doesnít change with passage of time but the agent
performance score changes.

5. Discrete: In this environment there are clearly fixed percepts and actions similar to
fixed number of turns by each player in chess. While car driving with variable speed
is non-discrete.
6. Single Agent: An agent operating by itself is termed a single agent. If there is more
than one agent then it is termed multi-agent.
7. Knowledge: If an agent understands the laws governing an environment, then the
environment is known to the agent such as red signal is to stop the vehicle and wait
for the green signal.

16

Problem Solving

? The agentís world i.e. the environment is represented by discrete set of states.
? Discrete set of operators are used to represent agentís actions.
? The world is considered to be deterministic and static.
? A simple example is that of a vacuum cleaner which is a single-state agent moves
from a particular position and cleans the dirt.

A single-state problem is defined by the following characteristics:

1. Initial state of the agent such as the position where it is currently being placed.
2. Collection of possible actions available to an agent in the form of action-state pairs.
They are formulated using Successor function S(x).
3. The goal test which determines whether the desired goal has been achieved or not.
4. A path cost function which assigns a cost to a path which is represented by g. The
path cost is calculated by the individual actions along a given path.

Hence the solution for the problem is determined. A solution is a sequence of actions
which lead from initial state to goal state. Problem solving methods is divided into distinct
categories namely

17

1. General Purpose
2. Special Purpose

Special purpose methods are made for solving particular problems while general
purpose can be applied for solving wide range of problems.

General purpose problem solving techniques used in AI are reduction, difference,
incremental and step-by-step. Special purpose problem solving techniques are used for
solving mathematical proofs, finding the move that leads to win the game etc.

18

Applied AI in Search Engines

Search engines play a key role in todayís high-tech world. For every answer to our
questions we refer to Google or any other popular search engine. Google being the leading
search engine of the web gets over a billion searches every month round the world. Being
the top search engine, they change the algorithm more often to give relevant result to the
users. Search engines primarily earn the revenue from advertising such as pay per click on
the sponsored links. Well Google search engine we see today is not the complete search. It
is only 5 percent of the search engine of the future. The future search engine would be
made completely of AI.

19
For getting relevant links and answers to the users, AI has been applied in top search
engines such as Google. Technologies such as natural-language processing (NLP) and
statistical machine learning has been applied in web searches for better and more accurate
results. NLP can be used to examine full-question queries for their linguistic structure and
to make connections between words such as verbs and nouns. This is definitely more
intelligent approach and will lead accurate results. AI enables search engine to respond to
natural-language queries rather than only keyword based queries. AI could overcome the
problem of keyword limit and the precision of searches. It is a fact no search engine today
uses AI as a major component. Mostly in the form of beta versions, search engines roll out

20
new features, so in future we can see highly intelligent search engine which can compute
highly complex queries. Initially the search engines tried to give answers to users queries
by matching the keywords on the webpages and showing those pages with high keyword
density. Search engines found that people filled their web pages with lot of high value
keywords and wrote it in the form which was meant only for the search engines. It lead to
lot of artificial content and search engines started penalizing for such websites by either
giving them warnings or banning their web pages. Well this lead to development of
PageRank an intelligent algorithm by Google for finding and ranking web pages. It ranks
the pages not only on based on keywords on the article but also based on the number of
relevant pages hyperlink to the webpage. This made the search engine results much
accurate and prevented wrong web links from showing up on the search engine. Medstory,
a health search engine has almost applied AI by using Markov modelling and Bayesian
analysis which uses statistical inference to calculate probability that a hypothesis is true.
This search engine can accept queries such as ìinfection, treatment?î or full question such
as ìHow do you treat infectionsî. Hence it can understand any type of queries of user
whether questions or keywords and prescribe the treatment. It is sure that in future doctors
prefer to use such AI search engine to prescribe them treatment with high probability of
being cured.

21
Well search engines that search over text is easy to construct when compared with
search engines that search over the images. As to interpret the visual data requires a lot of
computational memory and server space. Such search engines is constructed with fuzzy
logic to find images with matching visual signatures. Even the technology which reads
street signs, names on conference badges uses text-recognition algorithms. The limitation
with image search engines is that it takes a lot of computational power, but such an image
search engine can be build which can tag a photo based on the image automatically without
need of manual tagging. Such a detection algorithm can save a lot of time of users and
administrators so that photos can be tagged correctly. Its practical application such as
ability to tag the disease of various health scan reports can save a lot of time of doctors and
helps in quick treatment of the patients.

Building a search engine that uses Artificial Intelligence can be daunting as the
engine must be designed efficiently because a lot of memory and processing power is
required to search through a large database. Building efficient search engine requires use
of tools such as C Language Integrated Production System (CLIPS).



IBM Watson an artificially intelligent computer developed by IBM can answer
questions intelligently than humans. In February 2011, Watson defeated Brad Rutter and
Ken Jennings in the Jeopardy quiz show. This quiz show is known for complex and tricky
questions and only extremely intelligent humans can solve these questions. During the
show, Watson was given access to 200 million pages of data sizing 4 terabytes but not

IBM Watson uses cognitive technology to process information just like human
beings. It can understand natural language and form hypothesis by evidences. It has
dynamic learning capability i.e. it can relearn and correct itself from the user feedbacks. In
just three seconds Watson was able to answer any complex question accurately due to its
cognitive learning capability.

Watson runs on DeepQA software made by IBM and supports programming
languages such as Java, C++ and Prolog. Unlike search engines where we have to find the
answer from the list of possibilities, Watson uses open-source OpenNLP a natural language
processing program, OpenCyc for conceptual logic reasoning. Watson is designed to
diagnosis and treat lung cancer at Memorial SloanñKettering Cancer Center.


This is a useful application of Artificial intelligence used by Multi-national
companies round the globe. This service provides automated customer support to any
companies customers. The advantage of using this service is the company can provide
customer support 24 hours a day and 7 days a week. Companies gets around 30% decrease
in work load by using this technology.

Popular banks like Royal Bank of Scotland and websites like Ebay and Paypal use
this service to provide support to its customers. The online assistant is made up of a dialog
system and an avatar. The dialog system gets the textual data from the user and processes

it in language the agent can understand, then does natural language processing to
understand the text and processes the reply in a way as normal as a person would speak
after finding the data from the database similar to the query that was asked by the user. An
avatar is kept on these chat systems with a name and photo of that of a person so that it
appear as a human to the customer. Such an automated online assistant can be implemented
by yahoo messenger, twitter or any other chat systems.

Most atomistic studies of dislocations require the artificial insertion of disloca-
tion(s) into the initial system. To this end several approaches have been pro-
posed [1ñ7].

Due to the fact that the objectives of this work include being able to insert

dislocations into bicrystals that can be used to make a connection with a compu-
tational method that works at a higher scale ñtwo Dimensional Discrete Disloca-
tion plasticity (2D-DD) [8]ñ the following two requirements need to be fulfilled:

ï Only edge dislocations are considered
ï The crystal that will contain the dislocations needs to be periodic along
the dislocation line -??


Because of its simplicity, a method similar to the one used in references [1, 6]
is adopted, but modified in such a way that, instead of a pair, only one edge
dislocation is generated. The procedure of dislocation insertion can be divided
into three stages: removal of planes, application of a displacement field and
atomistic relaxation.

25

2.1 removal of planes
Edge dislocations are line defects characterized by the lack of atomic half-planes
in an otherwise perfect crystal. These planes are perpendicular to the Burgers
vector (-?b ) [9] and contain the dislocation line -??


. Depending on the crys-
talline structure of a material, a different number of planes have to be removed

(or inserted) in order to generate an edge dislocation.
In face centred cubic (FCC) materials the edge dislocation with the lowest
energy and, thus, the most common type is -?? = h211i and -?b =
1
2
h110i. The

reason for this is that the shortest lattice vector is 1
2
h110i. The atomic planes
perpendicular to -?b have a stacking sequence ∑ ∑ ∑ ABABAB ∑ ∑ ∑ where A and B
are the two possible layer positions in this direction [10]. In order to generate a
perfect dislocation two half planes (one A and one B) have to be removed (see
Figure ii.1a-b).
Similarly, in body centred cubic crystals (BCC) materials, where the most
common dislocation is defined by -?? = h211i and -?b =
1
2
h111i [2, 11], and
the stacking sequence in the direction of -?b is ∑ ∑ ∑ ABCABCABC ∑ ∑ ∑ , three half
planes need to be removed.

2.2 application of a displacement field

Starting with a perfect crystal, the necessary half-planes are removed (see ii.1a-
b) and a displacement field predicted by the elastic isotropic theory of dislo-
cations, centred in the core of the dislocation, is applied to all the atoms in

the system (see Figure ii.1b-c) in order to get a first approximation to realistic
atomic positions around the dislocation. This displacement field is given by the
following equations [11]:


where ua is the displacement to be applied to each atom in the a direction;
b is the magnitude of the Burgers vector (in this case -?b =
1
2
h110i); y and z are
the original coordinates of the atoms, and ? is the Poissonís ratio of the material.

26

Removal of planes

Application of a
Displacement field

Atomistic relaxation
Figure ii.1: Generation of a single dislocation for MD simulations. a) Perfect FCC crystal
with h211i pointing towards -?x , h111i towards -?y and h110i towards -?z . a-b)
Removal of two h110i half-planes. b-c) Application of the displacement field
predicted by isotropic elastic theory of dislocations [11]. c-d) Relaxation of
the system using MD; the inserted edge dislocation splits in two partials.

27

It is important to note here that the crystal has to be oriented in such a way that
-?x is parallel to -?? ,

-?y is perpendicular to the slip plane and -?z is parallel to -?b .

2.3 atomistic relaxation

The final step needed for the generation of a dislocation for atomistic simula-
tions, consists on the relaxation of the system (Figure ii.1d). For this, several

methods can be used (see Appendix A) and the idea is to allow the atoms to
re-accommodate to find the positions that minimize the energy of the system.

In the case of aluminium, copper and FCC materials in general, after relax-
ation, the inserted dislocation splits into two partial dislocations separated by a

stacking fault ribbon. This is consistent with experimental results and the reac-
tion can be written in terms of the burgers vector of the initial dislocation (-?b1)

and that of the two generated partials (-?b2,


(ii.3)
Figure ii.1d shows the detail of the separation of a perfect dislocation into
two partials in the case of aluminium modelled with the interatomic potential
developed by Mishin et al. [12].
Various approaches have been proposed to measure tp using atomistic simu-
lations, each one of them having its own advantages and disadvantages. Here,

four different types of methods are mentioned and briefly explained with the

aim of justifying the choice made. More detailed reviews can be found in refer-
ences [2] and [57].

The first and simplest type (Group I) [24, 58, 59] consists of inserting a single
dislocation in an otherwise perfect crystal and selecting all the atoms outside a
given radius from the dislocation line. All the selected atoms are then displaced

using an elastic field that mimics the long range strains generated by the dislo-
cation and are then fixed to their new positions. Finally the movement of the

dislocation is enforced by the application of a uniform strain field to the system.
In this approach, periodicity is only maintained along the dislocation line.

30

The main disadvantage of this group of methods is that when the dislocation
starts to move, the applied elastic field on the atoms that are away from the
dislocation is no longer correct, which can lead to an overestimation of the
value of tp [57].
The second type (Group II) [60, 61] is similar to the previous one but with

more sophisticated ways of dealing with the boundary conditions. Here, a dis-
location is inserted into a perfect crystal while enforcing periodicity along the

dislocation line. Then, the influence of the boundary forces is dynamically elimi-
nated by by determining their contribution and correcting for their effect [57, 62]

or by using flexible boundary conditions [63].
The main point in favour of these methods is that the size of the system
can be considerably reduced while still obtaining a correct core structure and
value of tp, but the choice of a small system hinders the simulation of long
range interactions [2]. Another method that can also be included in this group
is the quasi-continuum method, which takes into account the boundary forces
by linking the system to a continuum model [64].
The third group (Group III) contains the methods that use periodic simulation
boxes along the direction of the Burgers vector and of dislocation line [2, 24, 29,
30, 65]. For these methods, a dislocation is inserted and the first layers of atoms
at the free boundaries (perpendicular to the glide plane) are kept fixed at their
positions. It is important to note that the forces generated by these immobilized
atoms do not have a component in the glide direction [57].
A disadvantage of this group of methods is that the dislocation ìfeelsî the
presence of its periodic images. Nevertheless, if the system is big enough in the
glide direction, the core size can be correctly represented [63] and, since there is
symmetry in the periodic images, the net force acting on the dislocation caused
by the periodicity cancels out.
The last group of methods (Group IV) uses a fully periodic system [6, 66].
For this group, the selection of the boundary conditions obliges the insertion of
dislocations to be done in pairs, this being the cause of one of its major flaws:
all the quantities that can be measured for one of the dislocations are corrupted
by the stress (strain) field generated by the presence of the other one.
Due to the simplicity of the methods and the fact that it has been successfully
used to determine quantitative properties of dislocations [2], a methodology

that can be classified as belonging to Group III is adopted here, taking spe-
cial care of the influence of the boundary conditions and making sure that the

relaxation after every deformation step is fully achieved [67].

31

a) b) c) d) x
y
z
Figure ii.2: Construction of an edge dislocation in a Face-Centred Cubic (FCC) crystal. a)

Removal of two h110i half planes. b) Relaxation of the system using Molec-
ular Dynamics (MD). c) Insertion of two semi-rigid slabs of atoms. d) MD

relaxation of the system keeping the atoms in the inserted slabs attached to
harmonic springs to straighten the system.

3.2 simulation set-up
For the set-up of the simulation, an FCC crystal is generated in such a way that
the directions 
112  Ø

,

111 Ø

and [110] point towards the global -?x ,

-?y and -?z axis,
respectively. Periodic boundary conditions are imposed along -?x and -?z , while
the boundaries along -?y are left as free surfaces. In order to insert a single edge
dislocation, two (110) half planes are removed (Figure ii.2a) and a displacement

field ñcorresponding to an edge dislocationñ calculated using isotropic elastic-
ity theory [10] is applied to all the atoms. The system is subsequently relaxed

using the method FIRE [68] while maintaining the pressure set to zero along
the directions of the dislocation line (-?x ) and of the Burgers vector (-?z ).

As a result of the relaxation the inserted dislocation splits into two Shock-
ley partials separated by a stacking fault ribbon (Figure ii.3). Additionally, the

whole system becomes bent due to the difference in the number of planes in the
upper and lower part (Figure ii.2b). This bending causes the sliding plane of the
dislocation to be non-straight and, therefore, has an influence on the movement
of the dislocation and on its Peierls stress. In order to straighten the dislocated
crystal, two slabs of atoms are added (Figure ii.2c): one to the top and one to

the bottom along the non periodic direction (-?y ). These slabs contain atoms at-
tached by harmonic springs to ìanchor points" which are located at the same

initial positions of the newly inserted atoms in the slabs. All the system is then
further relaxed, becoming almost perfectly straight (Figure ii.2d). The y position
of the inserted slabs is set in such a way that the average force measured by the
springs in the -?y direction is zero.


Figure ii.3: Relaxed 1
2
h110i{111} edge dislocation ñusing the MFMP potentialñ in an
otherwise perfect FCC crystal. After relaxation the dislocation splits in two
partials separated by a stacking fault. Grey atoms are FCC, lighter atoms
Hexagonal Close-Packed (HCP) and darker atoms are neither FCC nor HCP.
Only part of the configuration is shown.

3.3 measurement of the peierls stress
The shear stress is measured in the bottom and top atoms ñtbottom[110] and
ttop[110]
respectivelyñ, using the following relation:

where PFtop[110] and PFbottom[110] are the sums of the components of
the forces exerted by the harmonic springs in the direction [110] on the top and
bottom atoms, respectively, and A(111 Ø )

is the area of the
111 Ø

plane.
After checking that the initial configuration is fully relaxed, the anchor points
attached to the top atoms are incrementally displaced in the direction of the
Burgers vector [110] ñgenerating a shearing strain (?xz)ñ in order to force the
dislocation to move. This displacements are done by increments of 2.14◊10-4≈,
each of them being followed by a full energy minimization of the system using
FIRE [68] and the MFMP potential. The relaxation is only stopped after the
values of tbottom[110] and ttop[110]

converge to the same magnitude. This
deformation-relaxation process is repeated several times in order to force the
dislocation to overcome the Peierls barrier.

33

Table ii.1: Values of the Peierls stress (tp) determined for different system sizes. MFMP
potential.
System Size 
≈3

To verify that there is no influence of the boundary conditions the calculation
of tp is done for three different system sizes, as shown in Table ii.1. Note that
since the length of the system along the dislocation line has no influence on the
value of tp, it is kept constant.
In Figures ii.4a, ii.4b, and ii.4c the results of the simulations for the three
systems are shown. In this work, tp is calculated as the maximum shear stress
(maximum absolute value of tbottom[110] and ttop[110]

). The values are shown
in Table ii.1. Based on these values, System 2 ñfor which tp = 0.0151 MPañ
is chosen as a sufficiently large system to study in more detail the evolution
of the dislocation as it overcomes the Peierls barrier. Note that although the
displacement is applied through the anchor points, the strain is measured using
the positions of the attached atoms.

Using the same method and system size, tp is also determined for the alu-
minium EA potential. A plot of the shear stress vs the applied shear strain is

shown in the lower part of Figure ii.6b. Note that, despite the fact that the only
difference between this simulation and the one used to generate Figure ii.4b
(repeated in extended form in the lower part of Figure ii.6a) is the interatomic
potential, The shape of the stress curves is not the same. Additionally, for the
EA potential the calculated value of tp is found to be 2.24 MPa, more than two
orders of magnitude larger than for MFMP.

The results obtained with the EA potential are of the same order of magni-
tude as other MD calculations in the literature using this same potential; they

also agree with recent calculations done with orbital-free density functional the-
ory [24], see Table ii.2.

The reason for the large difference between the values of tp for the two po-
tentials will be discussed in the following section.

3.4 analysis
Table ii.2 presents a a compilation of experimental and simulated values of tp
available in the literature, where the wide spread in the data can be seen. For

Figure ii.4: Shear stress vs. shear strain strain curves ñobtained using the MFMP

potentialñ for systems 1, 2 and 3 containing a dislocation. The only differ-
ence between the systems is their size (see Table ii.1).

potentials are used. The results obtained in the current work and those pre-
sented in Tableii.2 show that the variations are of approximately two orders of

magnitude.
In order to understand this spread the most natural thing to begin with is
to look at the unrelaxed generalized stacking fault curves [76] of the potentials
used, since their maximum slope is a measure of the theoretical shear strength
of the lattice [77] and, indirectly, of the Peierls barrier [23, 78]. These curves are
presented in Figure ii.5 for the MFMP and the EA potentials. The ratio of their
maximum slopes is 1.39, not at all large enough to explain such a big ratio in tp.
In addition, the slope of the curve is higher for MFMP, which does not match
with the fact that in the current work tp is higher for EA. A similar conclusion


Figure ii.5: Unrelaxed generalized stacking fault curves for the MFMP and EA potentials.
Note that the maximum slopes of these curves, which are measures of the
Peierls stress, do not differ enough to account for the difference of two orders
of magnitude obtained for tp.

can be drawn from the comparison between EA and the potential developed by
Baskes et al. (BAM) [23, 75].
A second possible explanation is presented in reference [23], where tp is
calculated with the BAM and the EA potentials. In their work the difference in
tp is explained by the fact that the ground state of the dislocations is different:
for EA the dislocation spontaneously splits into two Shockley partials, while in
the case of BAM the dislocation core remains compact. The authors argue that
this compactness generates an increase of tp. This hypothesis of the increase of

tp with decreasing dislocation core width is supported by the classical Peierls-
Nabarro model,

where µ is the shear modulus (ò 32 GPa) [12], ? the core radius and b the
magnitude of the Burgers vector. Nevertheless, in the current case, neither of

the potentials produce a compact dislocation core; the initial separations be-
tween the partial dislocations, i.e. in the presence of the Peierls potentials, are

9.1 ≈ and 16.4 ≈ for MFMP and EA, respectively (coincident with the data from
reference [23]). Here, the position of the cores of the partial dislocations are ap-
proximated by the coordinates of the atoms that have the highest tensile stresses

along -?z . More importantly, contrary to the predictions of the Peierls-Nabarro
model, the dislocation with a wider separation of the partial dislocations (EA)
produces a higher value for tp.
Since the two previous arguments fail to explain the difference in the relative
order and of the two orders of magnitude between the values of tp for the
MFMP and EA potentials, a detailed analysis of the initial equilibrium positions
and the motion of each of the partial dislocations was carried out.

As mentioned earlier, the initial equilibrium separation of the partial dislo-
cations for the EA potential is 16.4 ≈ and there are ten planes between the ref-
erence planes that are used to approximate the position of the partials. This

gives an average distance of approximately 1.49 ≈ along the 
112  Ø

direction for

the planes in the tensile region generated by the dislocation within the stack-
ing fault, which is bigger than the equilibrium atomic bulk separation of 1.43 ≈

along the same direction. This means that the deformation is not only concen-
trated in the dislocation cores of the partials, but it spreads throughout the

width of the stacking fault. A similar analysis for the MFMP potential yields an
initial equilibrium separation of 9.1 ≈, five planes between the reference planes
and an average atomic separation of approximately 1.52 ≈ along 
112  Ø

within

the stacking fault.
A plot of the displacements (u) of the leading (ul) and trailing (ut) partial
dislocations, and their separation (dl-t) as a function of the applied shear strain
is presented in the top parts of Figures ii.6a and ii.6b for the MFMP and EA
potentials, respectively.
An schematic representation of the movement of the two partial dislocations
is shown in Figure ii.7b, where the partial dislocations are drawn as point
masses, the staking fault between them as a spring and the Peierls potential
as a sinusoidal path. For the EA potential, the initial separation of the partial

dislocations remains constant as the dislocation travels through the Peierls land-
scape in a discontinuous stick/slip mode, making ìjumpsî of approximately

1.43 ≈, following the initial build-up of the necessary stress bias (2.17 MPa),
seen in Figure ii.6b. This is the situation that in [51] is called ìstrong couplingî
with an idealized separation distance of the partials being an integer multiple
of the Peierls period. The hypothetical possibility that the two partials are fully
uncoupled and move simultaneously simply because they experience the same
shear stress level at the same moment can be discarded. The presence of the
stacking fault strip between the partials, with its associated energy, is already
proof by itself for the existence of coupling.

The situation with the MFMP potential is more complex. As shown in Fig-
ure ii.6a the separation between the partials does not remain constant, in con-

Figure ii.6: Upper part of the figures: displacement of the leading (ul

) and trailing (ut)
partial dislocations and the distance between them (dl-t) as a function of the
applied shear strain. Lower part of the figures: shear stress vs. shear strain
as the dislocation moves through the Peierls landscape. Results presented for
the MFMP (a) and EA (b) potentials.


Figure ii.7: Schematic model of the movement of two partial dislocations linked by a
stacking fault represented by two point masses and a spring, respectively, in
an FCC material. The applied shear strain (?xz) points to the right. In the
case of the MFMP potential (a) the two partial dislocations do not move at the
same time, and their motion is assisted by the compression/extension of the
stacking fault. For the EA potential (b), the two partials move in phase and
the distance between them remains constant. Stages (i) to (v) are explained
in the main text. Note that, although it is not shown in this schematic view,
the two potentials produce a different separation of the partial dislocations.

40

trast with the EA potential: the leading and trailing partials do not move in

phase. The trailing partial moves first and it is then followed by the leading par-
tial, making ìjumpsî of approximately 1.52 ≈. Figure ii.7a shows a schematic

representation of their movement as they travel to the next stable position in the
Peierls potential. Again one should note the difference between the continuous
motion displayed in Figure ii.7a and the stepwise ìfrozenî motion displayed

in Figure ii.6a. In fact, the behaviour in Figure ii.6a corresponds to the time se-
quence (i), (iii), (v) of Figure ii.7a, repeated indefinitely. It is again the ìstrong

couplingî case [51], but this time with the idealized separation distance of the
partials being a half integer multiple of the Peierls period.
For the MFMP case, the behaviour of the partial dislocations and the stacking
fault between them can be explained in the following manner (see Figure ii.7a):
ï Initially the leading and trailing partials are in their equilibrium positions
(i). The stacking fault can be thought of as a pre-tensed spring.

ï As the magnitude of the applied shear displacement increases, both par-
tials start feeling its effect. Since the trailing partial is at the same time

being ìpulledî by the stacking fault, it climbs the Peierls potential hill (ii)
and reaches the next valley, while the leading partial stays trapped (iii).
At this point, the partial dislocations find a new equilibrium, one that is
different from (i). Here, the stacking fault acts like a compressed spring.

In fact, this compressed spring exerts so much force on the leading par-
tial that the applied stress needed to move the leading partial forward,

becomes negative (see Figure ii.6a).
ï As the magnitude of the shear displacement continues to increase, the
leading partial climbs over the next Peierls hill (iv) and subsequently goes

down the valley (v). This happens because it is being ìpushedî simultane-
ously by the stacking fault. A new equilibrium position equivalent to the

initial configuration (i) is reached. As said, for both potentials, the only
equilibrium stages are those where both partial dislocations sit in a valley;
the other stages are transient and are only shown for the sake of clarity.

Note that in this model the energy needed to perform the caterpillar-like mo-
tion (Figure ii.7a) is lower than that needed to move the two point masses

in phase (Figure ii.7b). This phenomenon where the stacking fault acts like a

tensed-compressed spring changing the dynamics of the motion of the disloca-
tion explains to a large extent the difference in the calculated values of tp using

different potentials.
This analysis is different from that of reference [50], where it is proposed that
the size of the stacking fault can be modified by the presence of point defects
along the path of motion of a dislocation, and that the stress needed to move


a dislocation segment may in this way be reduced almost to zero [79]. It is
also different to the analysis given in references [72], [23] and [24], where the
compactness of the dislocation core takes all the credit for the so called Peierls
stress controversy [23] generated by the wide spread in the values of tp for FCC
metals reported in the literature.
Finally, the current analysis is set-up as the realistic atomistic version of the

analytic analysis pioneered in reference [51]. It is observed here, for the two stud-
ied potentials, two widely different cases of partial dislocation movement under

applied stress, which agree with the integer and half-integer period separation
cases of the analytic approach. The values of the Peierls stresses themselves vary
widely: no less than two orders of magnitude. The first thing that needs to be done to generate a computational model of a

grain boundary is to choose the misorientation of the grains. Although, theoret-
ically, every misorientation can be generated, if periodic boundary conditions

need to be enforced (which can be useful for MD simulations) there have to
be atoms ìmatchingî in the interfacing lattices. This means that the periodic

55

length of one grain has to be a multiple of that of the other one along the two
perpendicular directions lying in the GB plane. When this ìmatchingî exists, it
is said that the two grains form a Coincident Site Lattice (CSL) [1, 3].
To further understand the concept of the CSL, it is helpful to consider two
identical interpenetrating lattices with a common origin. For this new system
there will be certain rotations of one of the lattices (Figure iii.1a,b) that will

result in the coincidence of a subset of the lattice points (Figure iii.1c). This co-
incident lattice points compose the CSL and one of its properties is that it is

periodic. This means that GBs between grains whose extended lattices form a

CSL (Figure iii.1d), are specially good choices for MD simulations where peri-
odicity is wanted.

The CSL is usually described in terms of a single value S which represents
the ratio between the volume of the CSL unit cell and that of the unit cell of
the composing lattices. A simple equation to calculate the orientations of cubic
lattices that result in a CSL is shown in the following equations [21]:


where N, a and b are integers > 0 and U, V, W and ? represent the misorien-
tation axis and angle, respectively, as described by the misorientation scheme

(eq. iii.2).

5.2 microtranslations
Apart from the 5 ìmacroscopicalî degrees of freedom that were chosen in the
previous section, there is a set of 3 more DoFs that need to be given in order to
uniquely describe a GB. These extra DoFs are the three components of a vector
that defines the rigid microtranslations -?T = (Tx, Ty, Tz)

that the grains can
suffer with respect to each other. Note that Tx and Ty are rigid translations
along the GB plane and Tz perpendicular to it.
Since one of the goals is to find the vector -?T that gives rise to the lowest
GB energy, a systematic search through the possible values that this vector can
take is performed. In the two planar directions, the values of Tx and Ty are
varied from zero to the length of the respective minimum periodic unit with
increments of a0/10, where a0 is the lattice constant of the material. In the

Figure iii.1: Example of a grain boundary and its Coincident Site Lattice (CSL). a) Lattice
A: a 2 dimensional cubic lattice represented by black dots. b) Lattice B: cubic

lattice represented by empty circles obtained by rotating Lattice A. c) Super-
position of lattices A and B; the places where both lattices coincide define

5.3 atom removal
As a result of the relative rigid translations applied to the grains, there exists
the possibility that some of the atoms that are close to the interface overlap with
each other and thereby lead to non-physical situations. When this happens, the
dilemma is which of the two atoms should be removed, since, depending on the
choice, the resulting energy may be different. To solve this issue, four different
solutions are adopted.
The first solution consist in removing all the overlapping atoms from the
arbitrarily chosen grain A, while for the second one the same is done for grain
B. The third choice is to remove both of the atoms, and the fourth is to randomly
choose which of the two overlapping atoms is removed.

5.4 grain boundary selection
Once all the initial configurations are generated, the energy of each one of them

is minimized by letting the atoms find their most energetically favourable po-
sition. Since the criterium to select the most realistic system is usually, but not

always, linked to a low energy, the grain boundary energy is calculated for all
the relaxed systems.
The grain boundary energy is determined by calculating the difference in
energy of a perfect crystal and a bicrystal with the GB. If this energy is divided
by the area of the GB, the GB energy per unit of area E

GB is obtained. In practice,

this calculation consists of three steps:
ï Selection of boundary atoms:
The first step is the selection of the atoms that are going to be considered
as belonging to the GB. In our case, we divide the bicrystal with the GB in
the middle into four equally high parts along the direction perpendicular
to the GB plane and select the atoms that are in the second and third
quarter. The selection is done in this way in order to avoid selecting the
atoms that form the free surfaces on the top and bottom of the bicrystal (-?z
direction) and to avoid neglecting any atoms whose energies are affected
by the relaxation of the GB. Note that the bicrystal is periodic in the -?x
and -?y directions (Figure iii.4).
ï Energy of the atoms

Figure iii.4: The atoms in the grey are are selected as belonging to the GB region for the
calculation of the GB energy.

Since the GB energy is calculated at a temperature of 0 K, the only con-
tribution to the energy of the atoms comes from the potential energy. The

potential energy of each atom (ei) is calculated using MD; a schematic
curve of atomic potential energy vs atomic position (zi) perpendicular to
the GB ñfor a typical GBñ can be seen in Figure iii.5.
ï Energy of the grain boundary

The energy of the grain boundary will correspond then to the region de-
limited by the line representing the energy of an atom in bulk (ebulk)and

the potential energy of the atoms (dark region in Figure iii.5). Mathemati-
cally, the grain boundary energy per unit area is expressed as


where AGB represents the area of the grain boundary and NGB the num-
ber of atoms that were previously selected as part of the GB.

Finally, after the GB energies of all the systems are known, and assuming that
the GBs with lower energies are more likely to exist in nature, the GB with the
lowest energy is selected for further analyses. The rest of the GBs are also saved
in order to compare their properties and behaviour.

z position (zi)

Potential energy (ei)

Figure iii.5: Schematic representation of the potential energy per atom plotted against

the position of the atoms measured perpendicular to the GB. The dark re-
gion represents the GB energy

5.5 application of the algorithm
Two examples are selected to show the performance of the algorithm:
the 
tilt GBs in aluminium and cop-
per, respectively. The main reason for the choice of these specific boundaries is

the availability of experimental observations using high-resolution transmission
electron microscopy (HRTEM) [22ñ24].
5.5.1 Tilt GB in aluminium
Simulation details
For this particular GB 

, systems containing approximately 2000
atoms, depending on the atom removal choice, are generated. Typically, their
dimensions are 21.043 ◊ 26.247 ◊ 60.615 ≈3

. An image of one of these initial

samples is shown in Figure iii.6a.
The minimum periodic lengths along x and y are 7.015 ≈ and 26.247 ≈. In
these two directions the micro-translations have a step of a0,Al/10 = 4.05 ≈/10 =
0.405 ≈, yielding 17 and 64 translations, respectively. Along -?z five translations

in copper.

are performed and there are four alternative criteria for removing overlapping
atoms. In total, 21760 initial configurations are considered for relaxation.
Results and comparison with experiments
A GB potential energy plot of the lowest energy configurations after relaxation is
shown in Figure iii.7a. The structure of the GB with the lowest energy obtained
from the simulations is shown in Figure iii.8 side to side with an HRTEM image
of a real GB with the same misorientation [22]. Note that since some of the
initial configurations are very similar, several GBs might converge to the same
energy after relaxation. The structure of the simulated and experimental GBs is
the same, confirming the applicability of the ìbrute forceî method for this type
of GBs.


in aluminium obtained via
de ìbrute forceî method and an HRTEM image of a GB with the same
misorientation [22].

moval of the atoms are applied in the same way as for the previous boundary,
giving rise to 11760 initial systems.
Results and comparison with experiments
A plot with the GB potential energy of the lowest energy configurations after
relaxation is shown in Figure iii.7b. Additionally, the structure of the lowest
energy GB is presented on Figure iii.9 and compared with experimental HRTEM
measurements [25]. Also in this case, the simulated and experimental results
coincide with each other very closely. 
When two mutually misoriented crystals are put together there are a number
of parameters, apart from the five macroscopical DoFs, that will influence the
geometry and, hence, the energy of the formed interface. The objective of the
1 Adapted from: Echeverri Restrepo S., Tamayo Giraldo S. and Thijsse, B. J. A genetic algorithm for
generating grain boundaries. Modelling and Simulation in Materials Science and Engineering, 21(5),
055017 (2013).

67

a) b) c)
Figure iii.10: Criteria included in the function to minimize. a)Position of the atoms.

b)Relative rigid translations. c)Scaling of the system.

GA presented here is to find the optimal combination of these parameters that
will produce the system with the lowest GB energy (EGB).
The energy to minimize is a function of the following parameters, which are
graphically represented in Figure iii.10:

ï The positions -?r of each of the N atoms in the GB region [N ◊ 3 param-
eters]

This corresponds to an array containing the x, y and z,coordinates of all
the atoms in the GB region. Their coordinates can be modified by the
algorithm.
ï The rigid translations -?T between the two crystals [3 parameters]
During the formation process of a GB one of the grains can rigidly change
its position with respect to the other. In the algorithm this is accounted

for by a vector with three components, two of them corresponding to dis-
placements along the GB plane and one to displacements perpendicular

to it.
ï The scaling -?S of the system [3 parameters]
During the process of relaxation, a system containing a GB can expand or
contract to evolve towards a more favourable energy state. The algorithm
takes this into account by optimizing the contraction/expansion of the
system along the -?x ,

Earlier simulations in the literature [15] have shown the importance of chang-
ing the number of atoms to access different thermodynamic states; the algorithm

presented here can incorporate this feature by allowing for the variation of the

68

number of atoms in the GB region, however, different runs of the algorithm
need to be performed.

6.2 general description of the algorithm
GAs belong to a class of stochastic search methods that work iteratively on a
population of candidate solutions to a problem (called individuals), performing
a search guided by a fitness function [26]. In the present case this function
determines the GB energy (equation iii.5). In particular the lower the energy,
and therefore the higher the fitness, the more the properties (genes) of a solution
are likely to be propagated to the next iterations. This Darwinian principle is
emulated with specific reproduction, mutation and survival operators, which
are applied with stochastic mechanisms that make the GA explore solutions
with increasing fitness (lower GB energy).

The implementation of GAs to the GB generation problem offers great ad-
vantages, as it does not rely on specific a priori hypotheses (e.g. continuity and

convexity), ensuring a wider mapping of the solution multidimensional solution
landscape.
6.2.1 System set-up

To generate the initial system two crystals with the desired relative misorienta-
tion are inserted into a molecular dynamics simulation box (see Figure iii.11),

leaving an empty region between them (along -?z ) where the interface is going
to be located. Periodic boundary conditions are imposed along the GB plane
directions (-?x ,

-?y ); perpendicular to it (-?z ) periodicity is not enforced so that the

bicrystal is bounded by two free surfaces.
This initial system can be divided into three regions: the atomic displacement
region (AD), the rigid translation region (RT) and the fixed region (F). The AD
region is delimited by the initially empty space between the two crystals. In
this unfilled space atoms are inserted at the start and subsequently modified
by the evolutionary algorithm; their positions can evolve individually and as
a result of expansions/contractions of the system. The RT region contains the
atoms of the initial system located towards the positive -?z direction with the
AD region as reference. The atoms in this region can change their position as
a rigid body and due to expansions/contractions of the system. Finally, the F
region contains the remaining atoms which can only scale their positions when
the system contracts/expands.

Figure iii.11: Regions of the grain boundary system. AD is the atomic displacement re-
gion; RT the rigid translation region and F the fixed region. (hkl)1

and

(hkl)2
indicate the sets of lattice planes in each grain which are parallel to
the grain boundary.

6.2.2 Generation of the initial population
Within the GA jargon, a population can be defined as a group of individuals,
and an individual as a possible solution to a given problem. In this particular
case, every individual is characterized by the three aforementioned parameters,
namely -?r ,

-?T and -?S . As an example, for N = 250 atoms in the GB, they com-
prise 756 adjustable parameter values.

The initial coordinates of the AD atoms of all individuals are randomly gen-
erated. For these first ìsolutionsî, it is required that the distance between any

two atoms is greater than a given value. It is also imposed that all the atoms
have to be inside the AD region. These requirements keep the atomic positions
of the atoms of the initial solutions away from physically unrealistic situations.
In the same way, values for the initial rigid translations and scaling of the
system are randomly assigned to each individual.
6.2.3 Evaluation of the population (fitness function)

Since the final goal of this algorithm is to minimize an energy function, popu-
lations are evaluated according to the energy of their individuals. In this work

the energy of each individual is calculated as the total energy of the system
(including all three regions) via use of an embedded atom method (EAM) [27]
potential for aluminium [28] implemented in the MD software CAMELION [29].
It is important to note that the choice of this potential for the calculation of the
energy is arbitrary and any other sensible method could be employed.

70

Once all the individuals of a population are evaluated, they are ranked ac-
cording to their energy and the best-adapted energy (i.e. the lowest) is used to

check for convergence, as will be explained in the following sections.
6.2.4 Reproduction

The main idea of reproduction is to produce a new population from the individ-
uals of the current, i.e. latest, population. This new population will consist of

a new generation of individuals. For this reproduction stage individuals are ar-
ranged in couples in such a way that the number of couples corresponds to the

number of individuals of the previous population. Note that a given individual
can be assigned to more than one couple. A higher probability for reproduction

is assigned to individuals with lower energy so that they reproduce more of-
ten than the ones with higher energy. This is done to accelerate convergence by

saving the properties (genes) of the best individuals through generations.

Unlike previous research in the literature [15ñ17] where new individuals (off-
springs) are generated by interchanging atoms from a pair of individuals (par-
ents), in the present work to form an offspring the values of the rigid translation,

the scaling of the system and the coordinates of each atom from the parents are
averaged and assigned to the offspring. For example, if the coordinates of atom
i of two parents (father f, mother m) forming a couple are -?r fi and -?r mi, the
position of atom i of the offspring (o) will be given by -?r oi =

6.2.5 Mutation
The main purpose of the mutation stage is to prevent the algorithm from getting
trapped in local minima and to allow it to explore a wider region of the energy
landscape. Mutations are random perturbations that are applied to randomly
selected individuals of the last generated population. Similarly to reproduction,
mutations affect all the variables of an individual: the coordinates of the atoms,
the values of the rigid translations and the values of the system scaling. In all
cases, mutations simply consist of the addition of random values to the desired
properties. For instance, if an individual (offspring o) is chosen to mutate and
the coordinates of its atom i are defined by the vector -?r oi, the coordinates of
this same atom after mutation

R i
is a random vector whose length is calculated independently in each
Cartesian direction.

71

6.2.6 Static relaxation
After every new generation of solutions is produced one final step is carried out:

the energy of each individual ñhaving still unrelaxed atomic coordinatesñ is min-
imized using the relaxation algorithm proposed in reference [30] while keeping

the volume fixed. The coordinates of each offspring are then updated and the
algorithm returns to the ìEvaluation of the populationî step (Section 6.2.3).

This process iterates until energy convergence is reached. Convergence is mea-
sured using the individual that has the lowest energy after each iteration.

6.2.7 Input parameters
The specific operation of a GA is controlled by certain input parameters. In
virtually all cases different input parameters of a GA will produce different
results. Therefore, to assure and accelerate convergence, different values for
these input parameters have to be tried out. In the present work an iterative
procedure was developed to evaluate the results of different combinations of the
GA parameters. For the problem of finding low energy configurations of GB, the
algorithm was designed to work with the following concrete input parameters:
ï Size of the population: Number of individuals (configurations) generated
in each generation (step).
ï Mutation rate of the population: Percentage of individuals selected to
mutate in each generation.
ï Mutation rate of the atoms: Percentage of atoms of the individual that is
selected to mutate whose positions are modified (mutated).
ï Survival rate: Percentage of individuals that are kept from one generation
to the next without being modified. These individuals are selected on an
energy basis, meaning that individuals with lower energy have a bigger
probability of being transferred to following generation.
ï Maximum distance for mutation: Maximum distance that an atom can be
displaced during the mutation stage. This same parameter is used for the
scaling -?S and the translation -?T .

6.3 tuning of the algorithm

The tuning of the algorithm involves the sampling of several value combina-
tions of the previously mentioned input parameters in a system for which the

72

minimum energy configuration is well known: a twin GB. Once the effective
parameters for the algorithm are found they can be used for more complicated
systems, as will be shown in Section 6.4.
The input parameters are sampled as follows: the size of the population is
kept constant at 100 individuals; there are five allowed values for the mutation
rates of the population and of the atoms, which are varied from 10% to 90% with
increments of 20%; four allowed values for the survival rate, varied from 0% to
15% with increments of 5%, and seven allowed values for the maximum distance
for mutation: 0.03, 0.1, 0.3, 0.5, 1.3, 2.0 and 2.7 ≈. Altogether these constitute 700
possible combinations for the input parameters. Each trial optimization runs
for 50 iterations. This number of iterations was found to be a good compromise
between computational time and convergence. Note that since all the systems
have the same number of atoms and their free surfaces are identical, measuring
the energy per atom of the whole system reflects the relative value of the GB
energy.
To set up the problem two grains with orientations defined by the interface
plane scheme as 

(430),(430), 0


(twin GB) are put in a simulation box sepa-
rated by a distance of approximately 8 ≈, as seen in Figure iii.12a. Note that even

though the choice of this separation is rather arbitrary, the system will evolve to
find the value for this parameter that will minimize the energy. Each crystal has
444 atoms. The initial dimensions of the system are 22.72 ◊ 20.94 ◊ 58.56 ≈3
in

the -?x ,
-?y and, -?z directions, respectively. These dimensions are chosen in such
a way that the intended periodicity along -?x and -?y can be imposed. Finally,
each individual of the first population is generated by randomly inserting 240
atoms without overlap in the AD region (see Figure iii.12b for a typical first
individual).
A plot with the energy per atom for the best final individual of each of the
combinations of input parameters is shown in Figure iii.13. The results show
that the algorithm performs very well for this problem. The minimum energy
found by the algorithm is indeed equal to the energy of a perfect twin GB
in Al, which, in this case, was calculated based on its known structure and
using the same interatomic potential as the one used for the GA. From the 700
combinations of parameters that were tried, around 600 were able to converge
to the geometry of a perfect twin. A visual inspection of the obtained system
confirms that the geometry corresponds to that of a perfect twin (Figure iii.14).
The following set of parameter values are selected as being optimum for this
application of the algorithm. This set is arbitrarily selected from the many sets
of parameter combinations that converge to the correct energy:
ï Size of the population: 100 individuals
ï Mutation rate of the population: 70%

ï Mutation rate of the atoms: 10%
ï Survival rate: 15%
ï Maximum distance for mutation: 1.342 ≈
Figure iii.15 shows the convergence behaviour of the energy for the selected
parameters. Note that only ten generations were needed to reach convergence.

6.4 application of the algorithm to an un-
known grain boundary

To obtain a minimum energy configuration for a different GB, i.e. 
the same procedure was followed as the one for the twin GB. First, two crystals
separated by a distance of approximately 8 ≈ and misoriented in such a way
that the desired symmetric tilt GB is obtained are inserted into a simulation cell
(Figure iii.16) of dimensions 19.71 ◊ 30.13 ◊ 53.31 ≈3

. Each crystal is composed
of 504 atoms and each individual of the first population is generated with 272
atoms in the AD region.
Since an effective set of input parameters was already selected in the previous
section there is no need to repeat that search. Five independent runs of the GA
are performed with the selected parameters in order to monitor the effects of
the statistical choices made by the algorithm.
The convergence behaviour of the five runs is presented in Figure iii.17. It can
be seen that they all converge to the same energy. This strongly suggests that
the obtained value is indeed the global minimum energy for the selected GB.
Nevertheless, it is still not possible (by this or any other method) to guarantee
a configuration with lower energy does not exist.

tilt grain boundary. Five independent runs are performed for the selected
input parameters. All of them converge to the same energy.

obtained by the two methods is the same. This structure is shown in detail in
Figure iii.18.
Apart from the fact that GAs are designed to perform a more thorough search

through the energy landscape compared to conventional algorithms, the num-
ber of iterations required to achieve convergence is usually smaller. For both the

ìbrute forceî method and the GA the most time consuming part is that of the

relaxation that is done after each configuration is generated. Here is also impor-
tant to note that the time consumed by the GA itself is negligible compared to

the times needed for relaxation.
In the present case, for the 


tilt GB, the GA converges in
approximately 10 to 50 iterations. Although the GBs and materials considered
are not the same as in earlier reports [15ñ17], the GA presented here shows a
major improvement in the number of iterations needed for convergence. In the
earlier works [15ñ17], hundreds to thousands of iterations were required.
There is also a performance improvement compared to the ìbrute forceî
method, since in that approach the number of initial systems to relax can be
as high as 50 000 or more [19]. In the present GA, in each generation there are

100 individuals, giving rise to the need of only approximately 1000 to 5000 re-
laxations.
ANNs are models that attempt to embody the information processing capabil-
ities of nervous systems such as the human brain. Based on the simultaneous

processing of incoming stimuli, ANNs are capable of learning what is the best-
fitting output for a set of inputs (Figure iii.19).

These models emerged after the notion of ìsimplified neuronî was intro-
duced by McCulloch and Pitts [45], and nowadays they are one of the main

stepping stones in the path of artificial intelligence. From a practical point of

view, ANNs can learn form experience (ìlearning by exampleî), which is par-
ticularly appealing when one has only little or at best incomplete understanding

of the relation between inputs and outputs. This is why they have found a sub-
stantial number of applications in the fields of prediction and classification.

ANNs are composed of artificial neurons (Figure iii.20), which are processing
units that receive and combine m inputs xi

to generate an output signal y. The
nodes of the input layer are passive, meaning they do not modify the data, but
simply distribute them to the neurons to which they are connected. Within a
neuron of the hidden and output layers there is typically a non-linear function,
called ìactivation functionî of the form y = g
 Pm
i=1
wixi

that processes the

input data to generate the output of each neuron.
An ANN is operational when several neurons linked by interconnection strengths
wi

(known as synaptic weights) receive a set of inputs and provide a set of out-
puts. The network acquires its knowledge through a learning process (training)

that modifies the synaptic weights of each neuron.

Figure iii.19: Typical structure of an Artificial Neural Network (ANN). An ANN is an
interconnected group of nodes (neurons) that usually consist of one input
layer, one or more hidden layers and one output layer. The number of
inputs and outputs can vary depending on the purpose of the ANN; in the
present case these numbers are, respectively, five and one. The number of
hidden layers, the number of neurons in each layer and the topology of the
connections can be modified to improve the performance (also known as
ìfitnessî) of the ANN.

Figure iii.20: Detail of an artificial neuron. Neurons are composed of (1) m input den-
drites that provide the information carried by the input values (x1 . . . xm),

81

7.2 input data
The generation of an ANN requires a set of examples to be used for its training.
In the present chapter, part of the 68 705 data points calculated by Kim et al. [43]
for Body Centred Cubic iron (BCC-Fe) GBs are used. The remaining part of the
data points is used for verification, i.e. for assessing the accuracy of the ANN.
Each data point consists of five input parameters, namely the five DoFs that
define the misorientation ñthree Euler (f1, F, f2), one polar and one azimuth
angleñ of the GB [1ñ3], and one output parameter, which is the corresponding

GB energy. In their work, 66 339 out of the 68 705 GBs, were mechanically se-
lected by discretising each of the five input DoFs in such a way that an even

sampling of the five-dimensional parameter space is obtained. The remaining

2366 GBs are so called ìspecialî GBs ñwhich were not selected using this au-
tomatic procedure but were selected manually. These special GBs tend to have

lower energies than the other general grain boundaries and, thus, will occur
with higher probability in real material microstructures.

In the work by Kim et al.[43], once the input data is chosen for a particu-
lar GB, the corresponding energy is calculated using molecular statics and a

second Nearest-Neighbour Modified Embedded-Atom Method potential (2NN
MEAM)[46] that defines the interatomic interactions. For this, two spherical
crystal samples are prepared, and one spherical crystal is rotated in such a way
that the desired misorientation with respect the other one is obtained. Half of
each sphere is then cut out, the remaining halves are put together and the new

system is relaxed to 0 K using molecular statics. The energy of the GB is sub-
sequently calculated as the difference between the energy of the new system

and that of one of the initial spheres. Note that the spheres are so large that the
difference between the surface energies of the two half spheres can be neglected.
This dataset was selected mainly due to its complete and even coverage of
the five-dimensional space that defines a GB. All the bicrystals were prepared
according to the same algorithm and the energies were not further minimized
by varying the microscopic DoFs [14, 19].
A plot of the full dataset of 68 705 GB energies is presented in Figure iii.21.
The data is organized by increasing Euler (f1, F, f2) and inclination (polar and
azimuth) angles. Note that no apparent trend is present in the corresponding
energy data. This clearly illustrates the complexity of the mathematical function
that is supposed to connect the GB energy to the five input parameters (which
are continuous, i.e. which can be specified to arbitrary precision).
In the present work an ANN is proposed to generate such a function for the
GB energy. Out of the different types of networks that can be conceived, one
with the ìcascadeî architecture is selected, for it avoids having to define the
number of layers and neurons up front [47, 48]. In this type of network the

Figure iii.21: Full data set of grain boundary energies in BCC-Fe as calculated by Kim et

al.[43]. The data is organized by increasing Euler (f1, F, f2) and inclina-
tion (polar and azimuth) angles. 25% of these data is used for the training

of the artificial neural network and the remaining 75% serve to test its pre-
dicting capabilities.

input layer of source nodes and the output layer of neurons connect the hidden
layers of the ANN to the training set, as defined by the cascade architecture [48].
The input nodes receive the data contained in the training set, in this case the
five input angles of each GB, and the output neurons generate solutions in the
form of energies. These solutions are then compared to the known energies
of the GBs, so that the weight factors and number of neurons can be further
optimized.
Since the main goal of this ANN is to predict the GB energy of all possible
GBs in BCC-Fe, and not only the GBs that are already present in the training
set, 25% of the data (including the ìspecialî GBs) are randomly selected and
used for its training, while the remaining 75% are reserved to verify the ANNís

performance when fed with data outside the training set. This process is re-
peated to generate three different training datasets ñI, II and IIIñ, used in three

independent ANN trainings in order to capture the effect of different inputs on
the predictive proficiency of ANNs. Note that a training set size of 25% of the
total is a somewhat arbitrary choice. If a larger fraction had been chosen, the
predictive error would be reduced.


7.3 training
One of the main issues in the construction of an ANN is to decide upon the

number of hidden neurons and layers, because different topological configura-
tions of the network can lead to different results [49]. To address this matter,

the Cascade 2 training algorithm [48, 50, 51] is used. This algorithm obtains the

best-adapted number of neurons for a given set of input examples by evaluat-
ing different candidate neurons and keeping the one with the lowest prediction

error. The selected candidate will be installed as a single-neuron layer, as de-
scribed by the cascade architecture, with connections to all previously added

neurons, and to the neurons in the input and output layers [48].
In all neurons different activation functions g, and steepness parameters s are
allowed to optimize the solution. In the current work ten activation functions
and four values of the steepness parameter ñ0.25, 0.50, 0.75 and 1.00ñ are tested
in each candidate neuron (Table iii.1).
During each iteration of the Cascade 2 method the network is trained with a
modified resilient backpropagation algorithm (iRPROP- [52]) which has emerged

as one of the best tools for the design of ANNs amongst several training tech-
niques [48, 49, 52]. Backpropagation algorithms make a search in order to mini-
mize the error of the predictions by modifying the weights of the neurons using

a gradient descent method. Once a minimum is found, the vector containing the
obtained weights, which is considered to be a training solution, is then fed into
the Cascade 2 algorithm

Figure iii.22: Average absolute error (h|e|i) as a function of the number of hidden neu-
rons. Results are presented for the three different partitions of the full

dataset. The T rain curves represent the error for the training datasets and

the T est curves represents the error of the predictions for the GBs not in-
cluded in the training sets.
The quantification of the spread of a dislocation absorbed by a GB requires,
first, the selection of a method capable of quantifying the local changes in the
atomic arrangement and, second, the definition of a cut-off value to differentiate
the atoms that are affected by the dislocation from the ones that are not. In
the present work, a method based on the changes in the atomic energies is
introduced, since energy differences always play a crucial role in processes of
change in materials.
The reason to use the energy for the calculation of the spread instead of the
LSP ñused in the previous sectionñ is that the larger changes in energy normally

identify the place where the larger disordering has occurred. In contrast, a big-
ger value of the change of the LSP does not necessarily reflect a larger atomic

disorder.

Energy plots for the expelled and absorbed cases corresponding to a disloca-
tion in plane 1 are presented in Figures iv.6a and iv.6b, respectively. Atoms in

the bulk have an equilibrium energy of -3.36 eV, according to the interatomic
potential [18], while atoms closer to the GB plane tend to have higher energies.

104

a) b)

Figure iv.6: Energy of each atom for a dislocation interacting with plane 1 of the se-
lected configuration (see Figure iv.1). The figure on the left (a) represents the

expelled case (no dislocation), while the figure on the right (b) shows the
absorbed case. Note that the absorption of the dislocation generates a local
change in the atomic energies that permits the detection of the impingement
site.

Observe that there are local variations in energy near the GB for the absorbed
case that are not present in the expelled situation. These local variations show
the place where the impingement took place.
To analyse the influence of the absorption of a single dislocation by the GB
and, thus, its spread, the energy difference between each atom in the absorbed
and expelled configurations is calculated. A plot of this difference is shown in
Figures iv.7a and iv.7b. The point where the dislocation impinges is the point
where the largest energy differences are obtained, in this example (plane 1) at
y ò 170 ≈.
For the measurement of the spread, a limiting value for the change in energy
has to be defined (?Emin). All atoms that have an absolute change in energy
higher than this are said to be affected by the dislocation; the greatest distance

between two atoms in the affected region is defined as the spread of the dislo-
cation. Since there is no obvious choice for the magnitude of ?Emin, different

values were tried.
Figure iv.8 shows the results and, as expected, the spread decreases as ?Emin
increases. The graph also shows that the selection of ?Emin has a big influence

on the calculated spread, and that for all selected values the maximum differ-
ence in spread between different impact planes is approximately 20 ≈ or ò 7b.

Note also that, as in the calculation of the width, there is a periodicity of five

Figure iv.7: Energy change of each individual atom caused by the absorption of a single
edge dislocation impinging on plane 1 (see Figure iv.1). Figure a shows the
energy change as a function of the y and z coordinates of the atoms. Figure
b shows the same data but only as a function of y in order to emphasize
the spreading of the incoming dislocation when it interacts with the grain
boundary.

Figure iv.8: Spread of the dislocation after it interacts with the grain boundary. Re-
sults shown for each of the five possible impingement planes of the non-
orthogonal repetitive cell, for seven different values of ?Emin.

planes, which is explained by the size of the non-orthogonal repetitive cell (Fig-
ure iv.1).

A comparison between these values and the minimum resolution of approx-
imately 7b ò 20.1 ≈ inherent to linear elasticity theory, where b is the Burgers

vector, shows that the influence of the place where dislocation impinges into
a GB should be considered, or at least more carefully studied, in larger scale
simulation methods.
8.4.2 Spread of the Burgers vector
Manual method
In the two previous sections it was shown via properties of the atoms that the
effect of a dislocation impinging upon a GB is not local, but it also extends along
the GB plane. Apart from the changes in the LSP and in the energy of the atoms,
this spreading phenomenon can also be analysed by determining the changes
in the Burgers vector of the incoming dislocation before and after interaction
ñalways taking into account its conservation laws.

A first analysis of the evolution of the Burgers vector for an incoming disloca-
tion along plane 2 is performed by manually drawing the Burgers circuits A, B

107

and C surrounding the region of the GB where the interaction takes place. These
circuits are presented in Figures iv.9a and iv.9b, where the subscripts ìabsî and
ìexpî denote the absorbed and expelled situations, respectively.
The outermost Burgers circuits (Cexp, Cabs) are drawn in such a way that
they encompass the full defect, also after spreading. The remaining Burgers
vector in the GB after absorption is 1
2
h110i and it is represented by the closure
failure of the circuit Cabs in Figure iv.9b. The magnitude of this vector is the
same as that of the inserted edge dislocation, which verifies the conservation
law of the Burgers vector. The two inner circuits (Aabs and Babs) also show a
closure failure, but a smaller one. The Burgers vector contained in each of them
is of the form 1
6
h112i, and, if they are added, give as result the Burgers vector

enclosed by the circuit Cabs.

The fact that it is possible to isolate different parts of the total absorbed Burg-
ers vector by making smaller circuits confirms that there is indeed a spread

happening as a result of the impingement. Nevertheless, with this manual tech-
nique, it is not possible to determine exactly the result of the spreading, nor the

exact location, direction or magnitude of the fragmented Burgers vectors in the
GB.
Non-Uniform Rational Basis-Splines (NURBS), are a more recent type of 3D
format, also known as a rubber sheet geometry, which are topological geometries
defined by algorithms. The curves and surfaces in NURBS geometries are based
on splines, which are defined and manipulated by ëcontrol pointsí, ëweightsí and
ëknotsí, to define perfectly smooth lines and surfaces. NURBS have the advantage
of being able to produce extremely complex curved geometry in a very efficient
computational way. As Dunn (2012) noted, before digital technologies, curved
surfaces and forms were the product of approximations using tangents to circular
arcs and straight-line segments that were translated from drawings to the building
site.

Fig. 13. Example of the wireframe of a NURBS of a croissant 3D model.

NURBS are mathematical representations of any curve or surface, from the most
basic straight line to the most complex irregular surface, including organic shapes.
The use of NURBS has many advantages. It defines its geometry through
mathematical and very accurate geometrical equations, making the amount of
information stored in a NURBS file smaller and more accurate than that of faceted
primitives, such as polygon meshes. NURBS curve equations are defined by
degree, control points, knots and evaluation rule.
The degree is a number, typically 1,2,3,4 or 5. Straight geometry is degree 1 or
linear, circles degree 2 or quadratic, and free form curves range from degree 3 to
5 or cubic, but can be higher. The control points are a list of points which have an
associated number called weight. Depending on the weight, curves can be
rational or non-rational. Curves such as circles, ellipses and other regulated
geometries are rational, but most curves drawn by following NURBS control points
are non-rational, and called splines. The knots are a list of numbers defined by the
number of control points, as shown in Fig. 13. The evaluation rule is the
mathematical formula that defines each exact point of the curve based on its

37

degree, control points and knots. The evaluation rule generates each point of the
ëbasis splineí through its mathematical formula. According to Piegl (1991) and
Rogers (1991), the advantages of using NURBS are:
-They offer a common mathematical form for both, standard analytical shapes
and free form shapes;
-They provide the flexibility to design a large variety of shapes;
-They can be evaluated reasonably fast by numerically stable and accurate
algorithms;
-They are invariant under affine as well as perspective transformations;
-They are generalizations of non-rational B-splines, and non-rational and rational
Bezier curves and surfaces.
The mathematical formulation of a Basis-spline curve (Equation 01) is defined by
the following polynomial function:

Equation 01. Base polynomial definition of a Basis spline.

Where ëP ií are the control points defined by its coordinates (Xi, Yi, Zi), and ëuí is
the knot vector (Figs. 14 and 15).

38

Fig. 14. An example of a NURBS Basis spline, showing the control points ëP ií, the knot vector ëN i,pí

and the final resulting smooth curve ëC(u)í.

Fig. 15. A set of individual NURBS Basis splines or C(u) used to generate a croissant 3D model.

The mathematical definition of a NURBS Curve is a vector-valued polynomial
function (Equation 02), defined by Piegl (1991), and Piegl et al. (1997):


Equation 02. Base polynomial definition of a NURBS curve.

39

Where:
C(u): NURBS Curve
w_i : weights
Pi : control points (vector)
N_i,k : normalized B-spline basis functions of degree k
p: Curvature degree.
In a similar way to how a NURBS curve is defined by its (u) vector, a NURBS
surface is defined by its (u,v) vector coordinates (Equation 03). Similarly, by
combining ëuí and ëví Basis splines, the following polynomial function is obtained:


Equation 03. Base polynomial definition of a NURBS surface.



NURBS surface is defined by its (u,v) coordinates, corresponding basically to two
sets of basis splines in two directions (Fig. 16). NURBS polynomial functions
define splines or open single surfaces, but not solid objects. Multi-surface objects
are modeled in NURBS as a set of combined single surfaces, often referred as
ëpoly-surfacesí. When a ëpoly-surfaceí is closed, it is referred to as a solid.



Fig. 16. An example of a NURBS Surface, showing the (u,v) coordinates and control points.


40

2.1.6.3 Point Clouds
Known as unstructured point clouds, there are groups of unconnected points that
range from just a few to millions. This format is the simplest type of 3D format, as
the points typically do not have connectivity information among them, point files
are simpler than other formats. A point cloud contains point coordinates referred
to as an origin on a coordinate system. The vertices are positioned in space by
their coordinates (X,Y,Z). Sometimes the files also contain additional information,
such as color (RGB), normals (tangents), luminosity, temperature, reflectivity, etc.
The mathematical formulations and file formats are quite simple, however to
represent a continuous line or a simple surface for instance, it is necessary to
define a large number of points, making up the datasets, often too large,
compared to other formats (Fig. 17). Some of the most commonly used point
formats are PTS, PTX, ASC, LAS, PLY, PCD and BPC.

Fig. 17. Example of an unstructured point cloud of a croissant 3D model.

Typically, when using this format, objects are represented by point samples of
their exterior surfaces, the interior of the objects being hollow. Point samples are
referred to as splats, for rendering purposes. As mentioned in sub-chapter 2.1.4.,
unstructured point clouds are the common format used by laser 3D scanners,


which captures them automatically and yields a dataset from real objects. As
points are zero-dimensional, they do not have mass, so they do not have volume,
area or length, which is the main reason why points are not commonly used for 3D
applications, and why they are mainly used only as a reference entity, which later
generates lines, surfaces and volumes. As explained in sub-chapter 4.1.5, PBR
uses splats as rendering entities, which are small oriented surfaces applied to
points, to simulate a continuous surface. Unstructured point clouds are
extensively used in the case studies as a novel way to merge 3D data obtained
directly from the ëreal worldí with ëartificially generatedí 3D data.
The vertical consolidation diagram (Fig. 95) shows Minato Wardís vertical density
in gray-scale. The darker shade represents lower-rise buildings which open up the
possibility for new high-rise urban developments. This is due to the fact that Tokyo
high-rise buildings are developed through the reorganization of small land plots,
which puts pressure on less dense areas to be developed into high-rises, as
explained in the previous paragraph.

176

ï Accessibility
The train station proximity diagram (Fig. 96A) shows gray-scale circles centered
on subway stations with a radius of 500m (walking distance of 5 to 10 min). When
several train lines overlap in one station, the circles overlap as well, producing a
darker gray tone which indicates multiple access points to public transportation. In
Tokyo, there is a strong correlation between the development of high-rise
buildings and their proximity to public train transportation. When showing the
footprint of buildings over 100m in black over the station diagram, this correlation
of high-rise buildings and proximity to train stations is easily observed (Fig. 96B).
In the final accessibility diagram, the public space and already regulated spaces
from the previous diagrams are subtracted (Fig. 96C). In this diagram, we can
observe how the darker areas, which are in close proximity to public transportation,
are more likely to experience further high-rise developments.


Fig. 96. Gray-scale Diagrams of the subway stations with a radius of 500m (Walking distance of 5 to

10 min), and allocation of buildings over 130m.

177

ï Allocation Parameters
With these various inputs, a final gradient probabilistic map combining the land
ownership, existing master plans, vertical consolidation and accessibility was
developed (Fig. 97), which was later used as the basis for the computer model.
This map is the result of the overlap of the vertical consolidation diagram and the
accessibility diagram (Fig. 95 and Fig. 96C). The darker gray area reflects a high
probability for future high-rise developments to occur due to less urban
consolidation, as well as their proximity to public transportation. Lighter gray areas
represent a lower probability of further vertical growth and, finally, the white areas
show where construction of vertical developments is prohibited or highly unlikely.
The gradient plan was produced with simple parameters and is used as a
probabilistic basis to determine areas with higher probability for new skyscraper
construction.


Fig. 97 Allocation diagram produced from overlapping Figs. 95 and 96C. The darker gray areas

show the higher probability of future high-rise developments.

178
ï Economic and Real Estate Parameters
The evolutionary process incorporates economic and demographic data to
predict the number of new high-rise developments annually. Understanding the
overall economic context and its relation to the construction of high-rise buildings
is an important factor. In 1991, Japanís economy entered a recession which, as
Daniell (2008) noted, was the result of a real estate bubble that was fed by easy
access to loans using over-valuated properties as collateral. As Krugman (2008)
explains, ìJapan did not merely undergo a single year of catastrophic economic
decline,î rather the economy gradually slowed down with growth rates below 2%
since 1992. Starting in the 1990s with the fall of land prices, some developers
began acquiring significant numbers of small adjacent land in order to undertake

large building projects in a way that had not been viable before. Most of the high-
rise construction in Minato Ward followed this pattern of development. As Pazos

(2014) argued, high-rise developments in Tokyo tend to increase with low
economic growth, in part due to lower land prices, the introduction of economic
stimulus, and less restrictive building regulations as the government attempts to
boost productivity across the economy (Fig. 98 and 99). High-rise construction
has been more of a tool to boost economic development than a result of
economic growth itself. Since 1960, a total of 51 buildings over 130m were
completed in Minato Ward with 31% of them during the year 2003 alone. The
reason for this anomaly is that urban regulations regarding high-rise construction

were eased in the year 2000 by the Urban Regeneration Act and the typical high-
rise building takes an average of 3 years to build. According to Pazos (2014),

attempts by the government to improve the economy have led to an increase in
high-rise construction since each drastic fall of the economic growth rate results in
more quantitative easing, which produces a wave-like pattern (Fig. 5). These
economic patterns and their relation to the number of buildings over 130m

179

completed per year serve as the basis for the evolutionary computation process in
predicting the number of future buildings per year.

Fig. 98. Number of buildings over 150 meters in height, built in Tokyo since 1960 in bars, and the

GDP growth overlapped on a graph.

Fig. 99. Number of buildings over 130 meters in height, built per year in Minato Ward from 1960 to
2015, combined with Japanís Gross Domestic Product (GDP) growth rates and economic stimulus

packages.

180
7.4 Vertical Growth Evolutionary Algorithm
By combining the probabilistic gradient map (Fig. 97) with the historical economic
data (Fig. 99), a computer model to predict the construction of high-rise buildings
over 130m in Tokyoís Minato Ward was developed and tested. This study used the
gradient plan (Fig. 97) as a base to predict the most likely allocations for new
high-rise buildings using a variety of determining factors: only areas where new
developments are allowed, proximity to the public transportation network, the
current location of high rise buildings, as well as variance in land prices, density,
and population. The economic data, which were obtained from the World Bank
(2016), cover the period from 1991 to 2015, when Tokyo experienced its high-rise
boom. These data were then used to predict the number of buildings that would
be built per year, as well as the height of each building. The model is based on the
assumption that the current conditions of vertical growth will remain constant and
that there will be no major changes in the governmentís urban policy. In order to
complete this task, the economic data for the region were used to statistically
determine the number of buildings that would be developed within that area from
2016 to 2019, as well as their respective building height based on previous
patterns of development and current urban regulations. The data parameters
contain 184 economic indicators, such as population growth, fuel exports, foreign
direct investment, deposit interest rates, etc. Regarding the construction data from
1991 to 2015, the data were obtained from specialized websites, Emporis Building
Directory (2017), and The Global Tall Building Database of the CTBUH (2017).
Once the economic data parameters were decided, a hybrid genetic algorithm
was created (Mathias et al., 1994). The feature selection (Kudo and Sklansky,
1998), feature transformation (Liu and Motoda, 1998) and parameter selection
(Hurvich and Tsai, 1990) were made simultaneously to create an adjusted linear
regression model using R-Squared as a measure of performance in the

181

evolutionary process. The genetic algorithm uses mathematic operators to
refactor input variables in order to find a suitable solution. All 12 available
transformations in the evolutionary process used in this study are represented in
diagrams (Fig. 100). Any continuous mathematical function could have been
used; however, this specific subset was selected based on previous experiences.

Fig. 100. The 12 available sets of mathematical transformations that were used to determine the

evolutionary process.

The genotype of the individuals (high-rise buildings) is an array of the functions
described above. The genetic algorithm attempts to calculate the best possible
combination of selections and transformations for all the input features. The
workflow of the hybrid genetic algorithm previously described is shown on a
flowchart (Fig. 101). Through the evolutionary process, the best combinations of
transformations in the input variables, which maximize the previously determined
objective function, were determined.

182

Fig. 101. Workflow diagram of the hybrid genetic algorithm. The Feature Selection, Feature
Transformation and Parameter Selection were made simultaneously to maximize the objective

correlation function (R-Squared).

During the feature selection process, the possibility of determining the usability of
a variable when assigning null selection is known (Fig. 101). For the best
individuals of the population, the parameters with the best adaptation were
selected (parameter selector) to create the final regression model. The termination
criteria of the evolutionary process are connected to the performance of the
current individuals of a population in relation to the average individual of the
population. When the average difference is lower than the preset threshold value
for a homogeneous population, the iterative search process comes to an end. For
the cases in which the threshold value is not reached, a maximum number of
iterations are used to finalize the process.

183

Operators Add, subtract, sin, cos, tan, asin, acos,
atan, log, exp, sqrt, and inverse
Initialization Ramped half-and-half
Fitness function R-Squared correlation
Recombination Strategy 1-point crossover
Mutation Strategy Leave-flipping
Mutation rate (pm) 0.05
Crossover rate 0.90
Selection Strategy Proportional Roulette Wheel
Replacement Strategy Invert-fitness

Table 04. Technical specifications of the algorithm proposed.

For this particular case, the genetic algorithm works over the entry data and
attempts to maximize the R-Square regression value (Table 4). It uses a vector
formed by the previously mentioned 184 indicators (variables) as training data and
objective data, the buildings built per year, as well as their median height.

Table 5 shows the capacity of the predictive model in relation to the determination
coefficient R-squared and F-Test analysis (Seber and Lee, 2012) for both the
number of buildings and the median height, respectively. All the results reported
refer to the performance obtained in validation, using 10-fold cross-validation and
50 independent runs. This probability is low enough to reject the null hypothesis
using the common significance level of 0.05.

Table 05. Regression Analysis for both models

Equations 14 and 15 show the selected/transformed variables during the
evolutionary search of the best adapted individual. For these individuals, the
optimal parameters are calculated according to the input data to extrapolate the
generated model.


Equation 15. Variable selection / transformation formula obtained in the evolutionary search for
Minato Ward, used to predict the average heights of probable new buildings.

Once both predictive models are determined, the gradient probabilistic plan is
used (Fig. 97) for the generation of a stochastic roulette wheel based on
Stochastic Universal Sampling (Baker, 1987). A total of 100 independent
simulations were performed according to the 2015 map by estimating the

186

possible locations of the buildings in Minato Ward over the 2016-2019 period. The
number of buildings and their heights were determined using both predictive
models. A probabilistic map was then generated, where the darker gray-scale
tone represents higher likelihood of new buildings over 130m to be developed (Fig.
102).

Data regarding future high-rise construction and the data predictions obtained
from the computer model for the 2016-2019 period were compared to evaluate
the results (Table 6). The left side of the table shows the real estimated data for
future high-rise (undergoing construction), while the middle portion shows the
data predictions made by the computer model. The difference between both sets
of data is shown in the right column for evaluation purposes. According to the
evolutionary model, a total of 6 new buildings over 130 meters should be built over
this period. The table also includes the median height of the buildings and shows
that a total of 6 buildings over 130m were undergoing construction or were
planned for construction in Minato Ward from 2016 to 2019. The current (as
September 2017) observed developments under construction are: ëSumitomo
Roppongi Grand Towerí 2016 (230m), ëAkasaka Intercity

187

Airí 2017 (205m), ëPark Court Akasakaí 2018 (170m), ëTGMM Shibauraí 2018
(169m), ëNissei Crea Towerí 2018 (166m), and ëToranomon 2-10-Okura Hotelí
2019 (195m). It should be noted that there is a possibility that additional buildings
over 130m that have not been identified will be completed by 2019. There is also a
possibility that some of the buildings currently undergoing construction scheduled
to be finished by 2019 could be delayed beyond the parameters of this case study.
Due to these facts, it will not be possible to verify with total accuracy the observed
real data until the end of 2019 and, thus, these data should be used only as an
estimate for the purpose of pre-evaluating the results.
The algorithm simulation predicted 6 buildings in Minato Ward for 2016-2019,
which matches with at least 6 developments, either under construction or planned
to be completed, over the same period. Thus, the algorithm was 100% accurate in
predicting the total number of buildings when contrasted with the observed
current construction data over a four-year period. This occurred despite the fact
that the algorithm had an error of one building per year, which suggests that, even
if the algorithm was accurate in predicting the overall number of buildings, it was
not accurate in predicting the exact time of construction as it deviates by a few
months. Once again, it is important to note that construction delays or additional
developments are still possible which might further skew these numbers. The
algorithm prediction was not accurate regarding the average building height, with
an average error of 55m or 20%. This result was probably due to the fact that the
maximum possible building height entered into the algorithm was 300m. Even
though buildings reaching 300m in height are allowed to be built in Tokyo (for
example, there are several 300m buildings planned to be completed in the 2020s),
approval for such projects are only granted under special circumstances and
currently there are no buildings over 260m completed in central Tokyo. Thus,
placing a height limitation of 260m into the algorithm would have probably

188

resulted in a smaller deviation. In order to test the accuracy of the algorithm in
predicting building heights, further research is necessary.

Fig. 102. Probabilistic gradient where new high-rise developments over 130m are likely to occur,
darker gray shows higher probability. Highlighted in red are the high-rise buildings already planned

to be completed in that area during the 2016-2019 period.

189

The six dots on the gray-scale map (Fig. 102) show the exact location of the
already planned and undergoing construction projects to be built by 2019. A total
of 4 buildings appear on the dark gray areas that the algorithm predicted and only
two buildings, the ëTGMM Shibauraí and the ëSumitomo Roppongi Grand towerí,
fall into the medium gray area, with one of them allocated right on the edge of an
area of higher probability. It can be said that the results are in line with the
probabilistic plan generated by the algorithm, with 66.67% of the buildings
allocated in the dark gray area versus a 60% result from the computer simulation,
leaving a 6.67% margin of error. 33.33% of the observed construction is allocated
in the medium and light gray areas, versus a combined 40% (25%+15%) by the
computer model, again with a margin of error of 6.67%. Due to the small size of
the population sample (6 buildings), these deviations are considered acceptable.

7.5 Morphological Parameters: Plan Type, Area and Heights
In addition to the location and average height of the new high-rise developments,
their quantity, size and massing geometry are of interest to this doctoral thesis, as
they define the overall morphology of the skyline. What determines the final
geometry and appearance of the skyline is the overall number of buildings, their
exact location within the urban fabric and its massing morphology. A classification
of the current existing buildings by their volume and typology reveals recurring
formal patterns that provide information on the morphology of future high-rise
developments (Fig. 103).

190

Fig. 103. 3D model classification of the 45 Minato Wardís buildings over 130m by building height

and plan footprint geometry.

The existing buildings in Minato Ward were classified by its plan geometry,
building footprint, area, overall height and slenderness (Fig. 104). High-rise
constructions have characteristic recurring geometries and share common
morphological characteristics, as most building typologies have very specific
requirements based on floor plan efficiency, function, natural light, structural
integrity, egress, vertical circulation, construction methods and economic
efficiency.

Fig. 104. Classification diagrams of Minato Wardís buildings over 130m by their plan footprint

geometrical type, showing recurring geometrical patterns.

191

As shown on Table 07, a total of 75.6% of the buildings have rectangular or
square plan geometry, as it is by far the most efficient and economic geometry. In
addition, but to a lesser extent some other geometrical configurations are found,
such as polygonal, triangular, cylindrical or oval geometry.

Plan Typology Number of buildings Percentages
Square (Almost Square) 18 40.0%
Rectangular A<2B 9 20.0%
Rectangular A>2B 7 15.6%
Triangular / Angular 7 15.6%
Round / Oval 4 8.9%
TOTAL 45 100%
Table 07. Percentage of buildings over 130m in Minato Ward, classified by its plan footprint

geometry.

Table 08 shows how a total of 84.4% of the buildings are between 130 to 200m in
height, with only 15.6% over the 200m mark, with an overall height average for
buildings over 130m of 170m.



Table 08. Building height classification by percentages.

192

The footprint areas vary between 1000m2 and 7000m2, being the most common
area between 1500m2 and 3000m2, with a total of 53.3% of the building within
that range, as it optimizes structure, vertical circulation, usable area and natural
light. The average building footprint area is 2788m2, as shown in Table 09.


Table 09. Towers footprint area classification by percentages.



As the relation between the floor plan area and building height tends to be
proportional, taller buildings have generally larger plan footprints in order to
maximize the vertical circulation and usable floor area. Another factor is structural
stability, as very slender tall buildings will not have a good structural behavior to
lateral forces, such as seismic and wind stresses. Slenderness for the purpose of
this study was defined as the footprint area divided by its total height:

7.6 Skyline Parametric Morphogenesis
The previous sub-chapter analyzed the massing geometry of the existing high-rise
buildings over 130m in Minato Ward, identifying the recurring morphological
patterns of Minato Wardís high-rise buildings. These parameters were used in
combination with an evolutionary algorithm, to simulate the evolution of the
morphology of the skyline. Table 12 shows the results by the proposed GA.
195

The proposed methodology is a combination evolutionary computation and
parametric process. Self-organizing systems, such as as cities, show recurrent
patterns in their growth, but it is important to note that randomness encounters or
interactions play a crucial role in these systems (Johnson, 2001). In that sense,
the possibilities of combination of the different parameters of a self-organized
system are infinite. Some combinations, however, are much more likely to occur
than others. All the solutions are different, but share similar characteristics. The
process is similar to a biological natural system, as all the individuals of the same
species have similar characteristics, and share a common biological structure, but
none of them are identical. The GA and parametric methodology proposed in this

thesis do not predict the future of Minato Ward high-rise developments, but auto-
generate very likely solutions, somehow generating solutions that have a similar

morphology and distribution that to those most likely to happen in the future. The
proposed methodology generates random scenarios with high probability to
occur, particularly from a morphological point of view.

196

Random individual decisions impossible to predict, made by land owners, banks,
developers, politicians, citizens, architects, planners, etc. play a key role in the city
transformation for each independent location. Moreover, individual events such as
inheritances, business deals, technological developments, catastrophic events,
investments, loans, etc. have an impact on whether a new high-rise development
will occur on a specific location. The proposed methodology uses a random
stochastic roulette (Baker, 1987) methodology instead, within pre-established
parameters, equivalent to the randomness of a self-organizing system. The thesis
develops one example of growth, but as previously mentioned, the possibilities
are basically infinite, as in any biological system.
Building shapes, heights, areas and slenderness is based on the parametric
relations shown in Tables 13 and 14. By random generation of values within these
pre-established parameters, the values in Tables 108 and 109 were generated,
which show just two random set of values that meet the required parameters. The
data were generated by random assignation of values within the pre-established
parameters. The data obtained are just two possible examples within the infinite
possible combinations.


Fig. 109. Example B. In blue, the 3D simulation of the results obtained for example B.

198

The data resulting from the parametric random process generated the two
previous examples, which were mapped over the areas of high probability
generated by the GA, in sub-chapter 7.5. The areas in dark gray were assigned a
60% probability and the areas in medium gray a 25% probability, whereas the
areas in lighter gray shade a 15% probability. The results were modeled in 3D and
overlapped the current 3D model of the Minato Ward skyline. The two resulting
skyline interactions generated by a combination of an evolutionary and parametric
process are shown in Fig. 108 and 109, for examples A and B, respectively.

When the results obtained are compared with the buildings currently under
construction, planned to be completed by 2019 (Fig. 107), the results are different,
however of similar density, distribution and geometry.
The two examples shown in orange and blue (Fig. 108 and 109) are just two
random samples of the infinite possible combinations generated by the
evolutionary and parametric process proposed. Thus, the proposed methodology
does not exactly predict urban vertical growth, but simulates real city growth,
bringing the boundaries between real and artificial closer together. In Fig. 110 the
overlap of the real high-rise buildings under construction in red, and examples A
and B generated by the proposed methodology, are shown all together.

199

Fig. 110. In Red, real buildings planned to be completed between 2016 and 2019. In orange and
blue, two different examples of buildings generated by the GA and parametric algorithms.
7.7 Results
In the same manner as organisms (Johnson, 2001), cities experience constant
change and transformation through endless mutations in what constitutes the
ultimate and most visible expression of civilization. The constantly changing
skylines of large urban centers have come to define their identity. The

morphological evolution of cities and biological growth are both driven by a self-
organizing process. In this case study, an adaptive evolutionary model was tested

through the use of genetic algorithms to predict the likeness of future vertical
growth in Tokyoís Minato Ward. First, the areas with high potential for future high-

rise developments based on previously identified recurring patterns of growth
(regulations, vertical density and accessibility) were identified over a gradient map.
Then, using data from previous economic patterns and high-rise construction, the
algorithm predicted the number of new buildings expected to be built per year and
their respective heights to generate a probabilistic map of new buildings.
The results obtained from the proposed approach were then compared to the real
construction data of buildings over 130m planned to be completed in Minato
Ward by 2019. After testing the genetic algorithm predictions for the 2016-2019
period and contrasting the results with the real projects underway, it can be
concluded that the growth estimates by the algorithm were accurate regarding the
total number of buildings (100%) and their likely locations (+/-6.67%). However,
the algorithm did not accurately predict the exact year of the developments (+/-
one year) and the height of the buildings (19.50% deviation), suggesting that
further studies should be conducted on these areas. In future case studies using
this methodology, a larger sample area should be tested, since the current study
was partially limited by the small size of the population sample (6 buildings).
Nonetheless, it can be concluded that the use of evolutionary computation yielded
acceptable results when used to predict future urban vertical growth.

Fig. 111. In darker gray, future high-rise developments, as one example out of an infinite number of
possible scenarios, of a 12-year evolutionary growth of the Minato Ward skyline.

In addition, based on the identified recurring patterns, a parametric process in
combination with the biological evolutionary algorithm simulated two different
examples of vertical growth.
First, the algorithm estimated different patterns of growth, or buildings to be built
per year. Then, it allocated those buildings on Minato Ward plan, based on the
likeness for new developments over Fig. 102 diagram, and then generated the
building mass morphologies, which were randomly determined by random
combination of the pre-established parametric relationships.

202

Fig. 112. In darker gray, future high-rise developments, as one example out of an infinite number of
possible scenarios, of a 12-year evolutionary growth of the Minato Ward skyline.
Figs. 111 and 112 show a hypothetical example for a 12-year period (from 2015),
as one of many possible scenarios of how the Minato Ward skyline could look by
2027. The proposed methodology was able to generate real evolution of a
complex system based on pre-identified patterns of behavior, creating real 3D
objects before they happened, furthering blurring the boundaries between ëreal
urban growthí and ëartificially generated urban growthí (Fig.112 & 113).
Theoretical Perspective on Cognition Enhanced Businesses Pro-
cesses

Before elaborating further on the methodology used in this research, some general notions should be
made on the perspective of this research regarding the important concept it makes use of: Artificial
Intelligence, in the light of business processes. From experience obtained throughout this research, it is

noted that multiple viewpoints can be taken to analyse the topic of this research: purely from a Busi-
ness perspective, purely Information Technology perspective, purely Computer Science perspective, or

a more integrated perspective. This indicates that a clear explanation of the perspective taken in this
research is of value, and even necessary to describe this research and its results. This section therefore
elaborates on its topic and how it adds to the perspective taken this research.
Artificial Intelligence studies the design and development of intelligent systems, systems that express
some form and degree of intelligence. AI methods and techniques have the potential to create a
software system that expresses (some form and degree of) intelligence (Russell and Norvig, 2003). As
formulated by (Langley, 2012), AI research were, in its early days, guided by the common vision of
“understanding and reproducing, in computational, i.e. machine systems, the full range of intelligent
behaviour observed in humans”. The same perspective is initially used for this research.
However, some scholars in the field state that, for a couple of decades, the field of AI emerged in
a way in which it somewhat abandoned these initial and grand goals (Langley, 2012; Heylighen, 2011;
Brachman and Lemnios, 2002). Langley (2012) states a couple of factors that caused this change of
focus of AI research:

13

Chapter 2. Research Methodology & Setup

• Increased computer speed and storage has aided simple-minded CPU-intensive and memory-
based approaches;

• Emphasis on quantitative performance metrics has encouraged incremental progress on standard-
ised problems;

• Influence of mathematics has led to “theorem envy” and to an optimality obsession, encouraging
a focus on simple tasks;
• Commercial success on narrowly defined problems has fostered research on similarly limited tasks.
According to Langley (2012), these trends together have transformed AI in to the research field as
it is to date, adopted more restricted goals than its initial vision, for example focusing more on specific
algorithmic challenges in sub-fields such as Machine Learning.

This research studies the enhancement of this cognition of business processes by leveraging the (intelli-
gent) computational capabilities of machines (AI). Humans play a very important role in organisations

and their (business) processes: they are the entities that exhibit intelligence and cognition, using it in
favour of the business. This is the intelligence and cognition utilised by business processes to achieve
their goals. Therefore, this research also largely takes into account human intelligence, and thus not
solely built on machine intelligence. Hence, a purely Computer Science perspective does not suit the
intentions of this research.
As a result, the modern approach of the field of AI is not used in this research, since a more
comprehensive perspective is desired. However, it identifies itself for a bigger part with the initial
broad vision of the field of AI, as mentioned above. Its view on the design of intelligence thus calls for
a perspective that integrates this human- and machine intelligence, addressing integrated intelligent
systems, opposed to fragmented algorithmic components of intelligence.
A field of study addressing the design of these integrated intelligent systems is found in the paradigm of
Cognitive Systems, a term championed by Brachman and Lemnios (2002). This field of study refers to
the discipline that designs, constructs, and studies computational artifacts that exhibit the full range
of human intelligence. According to Langley (2012), this field is not, at heart, a new movement, but
rather a continuation of the old, initial vision of the field of Artificial Intelligence. To make clear,
this research does not oppose itself to AI research, but identifies its way of thinking to the Cognitive
Systems paradigm – a paradigm that also not opposes the field of AI, but, besides its similarities to
AI, also has some fundamental differences. Put differently, the Cognitive Systems paradigm broadens
the thinking of the field of Artificial Intelligence towards the design of human-level intelligent systems.

In his article on Artificial Intelligence and Cognitive Systems, Langley (2012) characterises the Cog-
nitive Systems paradigm by means of several assumptions that it adopts, in which it differs from the

characteristics of the field of AI. To describe and clarify the perspective of this research on the design
of intelligent systems, these characterising features as formulated by Langley (2012) is here briefly
adopted and elaborated. They provide understanding of the concept of Cognitive Systems and set the

14

Cognition Enhanced Business Processes

scene for further deliberation on cognition enhanced business processes.
High-level cognition The Cognitive Systems paradigm takes into account high-level cognition, a
feature that can be clearly distinguished from current researched AI capabilities to recognise

concepts, perceive objects, or execute complex motorised skills. These abilities are clearly im-
portant for agents that operate in physical environments, but they are not real distinguishing

features of intelligent systems. Rather, this intelligence here entails the capacity to engage in
abstract thought that goes beyond immediate perceptions and actions, having the capacity to

engage in multi-step reasoning, to understand the meaning of natural language, to design inno-
vative artifacts, to generate novel plans that achieve goals, and even to reason about their own

reasoning as humans are able to do.
Structured representations Early AI researchers also assumed that structured representations play
a central role in intelligence, which in turn depend on the ability to represent, create, and interpret
content encoded in such representations. This position is closely related to the fundamental
insight – arguably the foundation of the 1956 AI revolution – that computers are not simply
numeric calculators but rather general symbol manipulators. This emphasis runs counter to
recent trends in many branches of AI, which, over the past few decades, have retreated from this
position. Some sub-fields, such as Machine Learning, have abandoned almost entirely the use of
interpretable symbolic formalisms, caring only about performance (such as improving even more
the prediction accuracy of Machine Learning models).
System-Level Research A third feature that characterised much early AI work was an emphasis
on system-level accounts of intelligence. Because researchers envisioned comprehensive theories
of the mind, they naturally recognised the need for their programs to comprise a number of
interacting components. The argument for this approach was compelling: because intelligence is
clearly such a complex and multifaceted phenomenon, even partial accounts should incorporate
multiple capabilities and aim to explain how these different processes can work together to
support high-level mental activities of the sort observed in humans. Despite these promising
beginnings, by the 1990s many researchers had come to focus on component algorithms rather
than integrated systems.
Heuristics and Satisficing Another central assumption of initial AI research was that intelligence
involves heuristic search. Using heuristics clearly differentiated AI from mainstream computer
science, which emphasised ‘algorithms’ that provided formal guarantees. The resulting systems
often satisfied by finding acceptable rather than optimal solutions, and even these were not
guaranteed. However, in practice, heuristic methods could often solve problems that the more
constrained approaches could not. AI today turns away from this practical attitude and adopt
other fields’ obsession with formal guarantees.
Links to Human Cognition The design and construction of intelligent systems has much to learn
from the study of human cognition. Many central ideas in knowledge representation, planning,
natural language, and learning were originally motivated by insights from cognitive psychology
and linguistics. The field also looked to human activities for likely problems that would challenge
existing capabilities, and design intelligent systems that could offer support. However, attention
15

Chapter 2. Research Methodology & Setup
moved instead to problems on which computers can excel using simple techniques combined with
rapid computing and large memories, like data mining and information retrieval. Langley (2012)
claims that they reveal little about the nature of intelligence in either humans or machines, and
there still remains a need to research this.
Exploratory Research Because in the early days of AI few examples of intelligent artifacts existed,
a common strategy was to identify some intellectual ability of humans, design and implement
a system that exhibited it, and demonstrate its behaviour on a set of convincing examples:
exploratory research. However, when people began to develop new approaches to established
problems, it became natural to compare the behaviours of different methods. Performance on
metrics became increasingly important, however, the broad coverage of intelligence was not yet
achieved. As a result, there remains a need for exploratory research on cognitive systems that
demonstrate a wider range of capabilities, even if they are not as efficient or accurate as current
techniques.
This research embraces the paradigm of Cognitive Systems to study the enhancement of cognition of
business processes by AI software. It is thus a more integrated perspective, combining a human-centric
approach to the design of intelligent systems and a machine-centric approach. Furthermore, it uses
systems thinking principles to structurally analyse and design such systems (see Figure 2.1).

Human-centric
perspective on
cognition

Machine-centric
perspective on
cognition

Integrated
systems
perspective on
cognition

Figure 2.1: Visualisation of the perspective taken in this research, formatted in bold

The context of this study involves organisations, and more specifically the business processes that
structure their expressed business efforts. Each process existing in an organisation contributes to this
overall organisation goal in its own way. Utilising AI in a business process is pursued to better achieve
the objective of the business process. Goal-directedness is therefore important in the design of cognition
enhanced business processes; the cognition enhancement should contribute towards achieving this goal.
In the two case studies analysed in this research, the cognition enhancement takes into consideration
the pursue of the business process to achieve its highest goal. In addition, the goal of the business for
the transformation of a business process to its cognition enhanced state is therefore also important in

16

Cognition Enhanced Business Processes
order to be able create a proper design, since it forms the input, i.e., rationale for the actual design
exercise.
Furthermore, because the conceptual design process does not necessarily focus on the enhancement
of cognition of human beings, it focuses more generally of cognition utilised in the process (systems
perspective) and higher this level of cognition. The word enhancement covers this description in a
correct way. If it would specifically focus on the cognition of people in the business process, it would
be called augmentation – a concept also studied in scientific literature.
So far, this chapter intended to clarify the topic of this research by discussing Artificial Intelligence
(software), human- and machine intelligence, their role in business processes, and the overarching
integrated perspective on intelligent systems of the theory on Cognitive Systems. How to leverage
these AI software in a business process context such that it increases the cognition that is utilised in
the process, is a complex question, but a question this research aims to tackle. To answer this question,
one has to research cognition in the context of business processes, which is done in the next chapter
(Chapter 3) of this thesis. This thesis first introduces and explain the approach and methods applied
by this research to in the end deliver insightful and useful results.

2.2 Conceptual Design Process for Cognition Enhanced Business Pro-
cesses

The main question of this research that should be answered is how the conceptual design process of
knowledge-intensive business processes, enhanced by integrating Artificial Intelligence software, looks
like. Utilising the theoretical perspective as explained in Section 2.1, this enhancement can be seen as
cognitive enhancement. Chapter 1 already provided a description of a cognitive enhancement, as it is
the transformation of the current state to the desired state of a business process in which it possesses
increased cognitive capabilities (see Figure 1.1). The conceptual design process supports the future
design steps of cognition enhanced business processes, and thereby addresses the transformation of the
current state of the process into its enhanced state. The ultimate goal of such a cognition enhancement,
i.e. transformation, is to create actual business value, e.g. more efficiency and/or more consistency of
its operations.
Recall Figure 1.4 in Chapter 1, schematically representing the decomposition of a enhanced business
process into different phases, based on the Systems Development Life-Cycle (The US Department of
Justice, 2003) and Design theory from Dym and Little (2010). They presents some general steps to
construct a conceptual design of a system through the design phase and design process. However,
they are therefore not very specific for a cognitive enhancement of a business process. Regarding the
context of this research, this specification is highly needed, since commercial AI software is just seeing
light to commercial application and therefore very little knowledge about the concepts in this practical
domain exists. Besides, no experience is present which can be built on. Furthermore, even no shared
vocabulary is used by professionals initiating the design of these new technology driven process within
their organisations.

17

Chapter 2. Research Methodology & Setup
These factors demand a more sophisticated design process that is tailored to cognition enhanced
business processes. First, this envisioned design process should grasp the (human-) cognitive aspects of
the business process and deal with them, since this research also utilises a human-centric focus next to
a machine-centric focus, as explained earlier. Second, the design process should deal with commercial
AI software, since this research is studying how actual implementable business software can be of value
to the business. Third, its use should at least be understandable for relevant stakeholders, such as
process architects, software system developers, end-users, process managers and innovation officers.
Reformulating the steps of Dym and Little (2010) to suit the theoretical perspective on Cognitive
Systems provides an overview of the stages of the conceptual design phase that suits this research:
1. Establish the requirements of a business process which indicate the opportunities for AI software
(opportunities to enhance its cognition). From now on, these are called cognitive requirements
(CR).
2. Establish the capabilities of AI software to fulfil the opportunities in the business process and
thereby providing the cognition enhancement that is demanded. From now on, these are called
cognitive capabilities (CC).

3. Establish a way to match the cognitive requirements of business processes and cognitive capabil-
ities of AI software. This can be seen as the way to establish means to enhance the cognition of

business processes.
4. The fourth and last stage is the integration of these three stages, i.e. design sub-processes into
the final conceptual design process, by including the fourth, fifth and sixth step of the conceptual
design phase of Dym and Little (2010).
With this knowledge, Figure 1.4 can be contextualised for this research, which results in Figure 2.2.

For clarity, note that only the stages addressed in this research are visualised. The steps Generate de-
sign alternatives, Refine and apply metrics to design alternatives and Choose a design are not displayed.

Regarding the application of the envisioned conceptual design process, this research assumes that a
business process as object for the enhancement is already chosen on forehand. This also applies to the
software suite that is selected to perform the enhancement. Thus a selection process is likely to be
executed, before starting with the cognition enhancement process as described in this research. Such
selection will probably be based on a business assessment, considering largely the financial aspects
of such a project. To be clear, this selection process based on business/financial assessments are out
of scope of this research: it only considers the potential possibilities of cognition enhanced business
process by AI software.
The three stages presented above together form the envisioned Conceptual Design Process (CDP),
besides the process for generating and selecting design alternatives. This research however argues that
the context specific issues for the conceptual design phase of cognition enhanced business processes
are mainly present in the first three steps of the conceptual design phase (see Section 1.4). That
is, because in these first steps the possibilities for AI technologies in business processes are actually
18

Cognition Enhanced Business Processes

Conceptual design

transformation

Systems Analysis Development Implementation

Systems
Development
Life-cycle

Current state of
business process

Cognition enhanced
state of business
process

Systems
Analysis &
Design

Conceptual design Detailed design

Systems Design Preliminary
analysis

Design
communication

Stage 1
Establish the requirements of a
business process which indicate the
opportunities to enhance its
cognition
(= cognitive requirements)

Stage 2
Establish the capabilities of AI
software to fulfil the opportunities
in the business process and
thereby providing the cognition
enhancement that is demanded
(= cognitive capabilities)

Stage 3
Establish a way to map or match the cognitive requirements of business
processes and cognitive capabilities of AI software (= establish a match)
Conceptual Design

Design
process Stage 4
Problem definition Preliminary design

In scope of this research
Out of scope of this research
Legend

Figure 2.2: Transformation of a business process to its enhanced state, highlighting the conceptual

design phase as made applicable for this research

investigated. The generation of design alternatives and subsequent design steps can be executed similar
to other design exercises. However, for a complete picture, these latter steps are jointly represented
in a separate process and included in the CDP. As of now, the following sub-design processes can be
formulated:
1. Cognitive Requirements Design Process (CRDP): establishing the cognitive requirements
(CR) of a business process
2. Cognitive Capabilities Design Process (CCDP): establish the cognitive capabilities (CC)
of AI software
3. Cognitive Possibilities Design Process (CPDP): establish means to enhance the cognition
of business processes by matching the CC’s to CR’s.

4. Design Alternatives Generation & Selection Process (DAGSP): construct design alter-
natives from the results of the CPDP, select based on pre-defined metrics.

Based on the above, the structure of the CDP can be visualised as in Figure 2.3. It indicates the
different components the CDP can consists of. This breakdown is used in this research to construct
the CDP as envisioned.

19

Chapter 2. Research Methodology & Setup

CRDP
Cognitive
Requirements Design
Process

CCDP
Cognitive Capabilities
Design Process

CPDP
Cognitive Possibilities
Design Process

DAGSP
Design Alternatives
Generation & Selection
Process

CDP
Conceptual Design Process

part of part of part of part of

Figure 2.3: A schematic visual of the envisioned conceptual design process and its components –

the striped component is outside the scope of this research

The above described the distinct components of an envisioned CDP. These components together form
the basis to draft designs for cognition enhanced business processes, the main purpose of the CDP. It
is preferable to know on which aspects a CDP can be assessed, next to its content. These important

aspects are here called criteria, and indicate why and to what extent a conceptual design process is suit-
able for use if met. From informal interviews with designers and developers of AI systems in business

processes in the preliminary analysis of this research, a set of four criteria is drafted and listed below.
Throughout this thesis, the criteria relevant to each part of the CDP constructed in this research are
mentioned and argued for. In this way, more substantiated argumentation can be provided to answer
the main research question raised in Chapter 1.
Universally applicable The CDP is ought to be applicable to different business processes within
the domain it is constructed for, as well as different AI software packages. The subject and
scope of this research limits the CDP to be made for knowledge-intensive (claim-) assessment
business processes, thereby, all business processes matching this characteristic should be suitable
for cognition enhancement by the CDP. The same holds for the AI software packages used in
a cognition enhancement project. The CDP thus should not exclude certain business processes
or software packages by its structure, content or otherwise. Hence, universal applicability is
pursued.
Methodical in essence For the CDP to be useful in projects that strive for cognition enhancement,
the CDP is ought to be usable for designers and developers of AI systems in business processes
in general. Furthermore, the CDP is ought to support their practice by providing rigid to the
design process on the one hand, but also room to manoeuvre in the design and design process
on the other hand. Hence, the CDP can steer the design process to an extent while remaining
variety in the designs it produces. This asks for the CDP to be methodical in its essence and its
different components should accommodate a systematic or established procedure.
Flexible but robust As mentioned in the introduction of this thesis, the AI technology is evolving.
The CDP should be flexible to changes in the state-of-the-art, such that it is still useful when
technology capabilities change during a design process. That is, it is ought to embrace the
20

Cognition Enhanced Business Processes
changes in a software package and adjust its design to. The same holds for business processes,
which in reality not always follow the same work flow according to procedures. The CDP should
be able to deal with these changes to produce a suitable design. On the contrary to its flexibility
to changes the design should adapt to, the CDP should be robust to external disruptions. For
example, a design produced by the CDP should resilient to the disappearance of a member of
the design team, and not to jam when this occurs. The degree of flexibility and robustness at
the same time is a balancing act, considered in the construction of the CDP.
Context-aware At last but still important is the fact that the CDP should be constructed in such
a way that it accommodates the design (cognition enhanced business process) to the context it
is exists in. That is, it is ought to perform and work is such a way that its design is suitable
to its environment. Here, the people performing activities in the business process, information
that they use and the data that underlies this information are considered as the environment.
For example, it should not rush certain design steps when stakeholders do generally speaking not
agree on their outcomes. In this case it is considered to be better if more effort and time is put
in these steps.
Furthermore, the influence of data to the design process and thus design is assumed to be of
importance in Chapter 1. Data powers AI technology, and is therefore relevant for investigation
what role data plays in the context-awareness of the CDP. The CDP is therefore ought to indicate
the designer when and how to take into account data during the design process. For example,
if data is not available or considered to be of insufficient quality, the number of possible designs

can decrease. Thus, data can be of importance when designing cognition enhanced business pro-
cesses. If the data is of too bad quality, projects can be initiated in the organisation to create

(new) data needed for possible solutions. The latter is not further discussed in this thesis.
With the more specific conceptual design phase as showed in Figure 2.2 and structure of a CDP in
Figure 2.3, a related design process can be inferred. However, this is not a straightforward assignment,
due to the lack of knowledge, experience and shared vocabulary that qualify the design of cognition
enhanced business processes. Partly due to these unknowns, a hand full of examples of such designed
enhanced processes only exist in both the business community and/or scientific literature. This results
in even more unknowns on forehand such as how such a enhanced process actually will look like and
what exactly is making the design of these enhanced processes so difficult. Furthermore, the problem
overarches more than one field of study (Artificial Intelligence, Cognitive Systems, Business Process
Engineering, et cetera) and is therefore interacting with an evolving set of interlocking issues and
constraints.
This characterisation of the problem, the design of a cognition enhanced business process, indicates
that it can be denoted as a so called wicked problem (Rittel and Webber, 1973). Another noticeable
characteristic of wicked problems is that they do not have a defined goal. In case of a wicked problem,
if you cannot define the problem, how can you tell when it’s resolved? Therefore, wicked problems
do not have a perfect solution (Rittel and Webber, 1973), the solution is rather good or bad. When
solving a wicked problem, the designer keeps iterating and refining the solution – or goes back and
considers other solutions. The research and design process does not have an ending, since a better
21

Chapter 2. Research Methodology & Setup
solution is always possible. However, the problem solving process ends when you run out of research
resources, such as time, information sources or money (Rittel and Webber, 1973). These statements
suggest a scientific research approach is needed that is able to deal with these kinds of unknowns and
uncertainties. The research approach Research through Design is known for exactly these purposes,
and is described in the next section.
2.3 Research Approach and Methods

Research through Design (RtD) is described in literature as an approach to structurally and con-
tinuously design and refine an envisioned artifact (Godin and Zahedi, 2014). It is a conceptualising

research done by means of the skillful practice of design activity, revealing research insights (Krogh

et al., 2015). More specifically, this knowledge is gained by conducting a design exercise and contin-
uously extracting information by means of direct and indirect observations, beliefs and experiences

(van Langen, 2015a). The approach has a highly iterative character, switching frequently between a
theoretical and a practical application perspective (van Langen, 2015b).
Research through Design lends itself for addressing wicked problems through its holistic approach
of integrating knowledge and theories from across many disciplines, and its iterative approach to
re-framing the problematic situation and the preferred state as the desired outcome of the research
(Zimmerman et al., 2010). The resulting artifact can be seen as a proposition for a preferred state
or as a placeholder that opens a new space for design, allowing other designers to make artifacts that
then better define the relevant phenomena in the new space (Zimmerman et al., 2010). Furthermore,

design researchers have claimed that RtD can result in conceptual frameworks and guiding philoso-
phies as well as community discourse on preferred states, identification of gaps in current theories

from other disciplines, and indications of new materials (technology) that would be especially valu-
able to invent. Finally, literature describes how RtD leads to new artifacts (products, environments,

services, and systems) where the artifact is itself is a type of implicit, theoretical contribution. The
power of these artifacts was described in how they codify the designers’ understanding of the current
state. This includes the relationships between the various phenomena present, and furthermore the
description of the preferred state as an outcome of the artifact’s construction (Zimmerman et al., 2010).
Based on the description above, Research through Design is considered to be a suitable approach to
use in this research. By means of iteratively conducting a design exercise and frequent use of relevant
theories (which is done in both case studies), insights are extracted for designing a cognition enhanced

business process. This can result in valuable input for the construction of the conceptual design pro-
cess this research is pursuing. To provide the definition of a design process as used this research, the

terminology of van Langen (2002) is used: a design process is in this thesis defined as “a sequence of
design activities, such as civil engineering or software design”. A design activity is defined as “an act of

designing, such as the refinement or structuring of a design problem, or the generation of a design so-
lution” (the final version of the design). Leveraging this definition, the sub-design processes as pointed

out in Figure 2.2 will thus consist of a sequence of design activities. By utilising the Research through
Design approach, these design activities of the envisioned design process of cognition enhanced business
processes are therefore revealed through the design exercise this approach entails. Appendix A further

22

Cognition Enhanced Business Processes
elaborates on Research through Design as a scientific research approach and describe the Research
through Design-process this research went through.
To show that the cognition of business processes can be enhanced, this research illustrates and validates
this design process to an actual case: a claim-assessment process within the health insurance business.
Currently Jibes Data Analytics, a small sized IT advisory organisation in the Netherlands, runs projects
at two health insurance companies to design, build, implement and test AI software in their current
claim-handling processes. The goal of these two projects is to enhance the processes, for two main
reasons: to be more efficient (less time per claim assessment, at least maintaining the same assessment
quality) and to assess the claims more consistent (assessing claims with similar characteristics the
same way) (van der Hulst, 2015). These two projects are used in this research as case studies; one for
illustrative purposes, one for validation purposes.
In this process, subject matter experts (SME’s) assess health insurance claims from patients and

health providers and decide if these claims will be granted or not. These SME’s have extensive knowl-
edge of and experience with medical treatments, which they use to assess the claims. However, to

assess these insurance claims they also use a lot of information from several external sources: law
books, internal protocols, state-of-the-art literature, and so forth. To search for and collect all the
right and relevant information for a particular case, and make a right and well-founded decision, is a
complex and time consuming activity. Altogether, this makes the process very knowledge-intensive.
The next chapter further elaborates on these two case studies.

2.4 Chapter Summary

First and foremost, this chapter introduces and argues the perspective take in this research (Sec-
tion 2.1). This perspective comprises a systems perspective on cognition, found in literature under the

name of Cognitive Systems. Furthermore, the research’s both human-centric focus and machine-centric

focus denote the integrated nature of its perspective. Next, this chapter further describes the envi-
sioned conceptual design process where Chapter 1 left of, identifying the sub-design processes it consist

of. These sub-design processes are design processes on themselves: a Cognitive Requirement Design
Process (CRDP), a Cognitive Capabilities Design Process (CCDP), a Cognitive Possibilities Design
Process (CPDP) and a Design Alternative Generation & Selection Process (DAGSP) (Section 2.2).
This chapter concludes with an introduction to the approach utilised by this research, called Research
through Design (Section 2.3). It provides a more detailed description of this approach, which has an
exploratory nature.
Besides this, this second chapter also served as the introduction to the ‘way of thinking’ this
research embraces and presumes when reading this thesis. This research is proposed and conducted
with constant thoughts of experimenting in a (new) field which is not trusted by all people. This
research thrived based on creativity and bold ideas, not only focusing on precision, but also on hunches
and intuitions.

23

3 | Theoretical Notions on Cognitive Sys-
tems in Business Processes

This chapter presents the research that is conducted on the first research issue (RI1) raised Chapter 1
in Figure 1.4: Establish theoretical foundations of cognition enhanced business processes. That is, it
describes a way to describe and analyse cognition in business processes from an integrated systems
perspective (see Section 2.1). Thereby, it pursues to deliver a theoretical foundation for the design of
the envisioned conceptual design process of cognition enhanced business processes.
First, this chapter introduces the first of the two case studies used in this research in Section 3.1.
Throughout this and following chapters, this case is used to provide examples of the theoretical concepts
and methods explained. Next, Section 3.2 elaborates on Cognitive Systems, the concept of cognition
analysed from a cybernetics perspective. It defines the important concepts that are used in this thesis,
such as system, agent, intelligence, their properties and their components. Thereafter, these theoretical
concepts are linked to the main object under study in this research, business processes, in Section 3.3.
Section 3.4 concludes this chapter by summarising the takeaways on the concepts addressed.
3.1 Introduction to the Claim-Assessment Process
As explained in Chapter 2, two cases are studied to execute this research. One case for illustrative
purposes throughout the first three sub-design processes (CRDP, CCDP and CPDP), one for validation
purposes of the final design process. Both case studies entail the claim-assessment process of a health
insurance company.
Health insurance companies offer insurances to clients. They reimburse claims they receive from their
clients to compensate for medical related treatments they receive from health suppliers. Stakeholders
that play a role in such a health insurance market are clients (the insured), the health practitioner,
the health specialist, the health supplier, the insurance company and the government.
Part of their business consists of checking whether a particular client should be compensated for
a health treatment or not. That is, assessing if the claim should be approved for reimbursement or
not. This business process of a health insurer is called the claim-assessment process. Within this
claim-assessment process in this case study, two sub-processes can be identified, which both have the

same goal: assessing whether the claim should be approved or not. However, the first sub-process is ex-
ecuted by a claim handler that uses standardised tools (such as a decision-tree tool or other protocols)

25

Chapter 3. Theoretical Notions on Cognitive Systems in Business Processes

to assess the claim. The second sub-process is executed by a subject matter expert (SME), which as-
sess the claim utilising its own expertise. Expertise is here described as both knowledge and experience.

This claim-assessment process is a relatively straight forward process, as is argued in Section 4.1.
However, it its a rather difficult process because substantive knowledge and experience is necessary
to perform the process properly. The latter makes it a valuable object to analyse in this research, as
Chapter 1 signified the potential of Artificial Intelligence applications to knowledge-intensive business
processes.
Within the assessment process different forms and degrees of expertise are experienced. SME’s are
required to do training before starting their work, and the claim handler employees develop expertise
handling claims and their assessment using the tools available to them.
These claim-assessment processes are quite representative for other assessment processes, such as the
assessment process of a bank which provides or refuses a loan to a client (mortgage to house owner,
loan to company to innovate, etc.). Both have similar characteristics: contextualised information (all
information that is relevant to a specific case) is gathered, identified and analysed in such a way that
it forms the basis for a particular decision. This decision is always made by a person accountable for
making this decision.

The goal of this research is to deliver a conceptual design process that supports the design of cog-
nition enhanced business processes. This implies that there is a rationale behind this enhancement,

or transformation as stated in the introduction chapter of this research. This rationale is considered
to be established by the business and related to financial aspects. Determining the rationale for this
transformation is outside the scope of this research. In this illustrative case, the business goal for this
transformation is determined by means of an interview with relevant stakeholders. The rationals in
this illustrative case study are to (1) lower throughput time and increase of quality (2a. less errors,
2b. more consistency) (van der Hulst, 2015).
3.2 From Business Processes and Intelligence to Cognitive Systems

As briefly touched upon in the previous chapters, AI software is considered to provides business pro-
cesses with clever automation solutions. In order to study these ‘intelligent’ functionalities of AI

software and what they can bring to business processes, this chapter further introduces and describe
the perspective this research takes regarding business processes and Artificial Intelligence. To do this,
a deep dive into intelligence and even more into cognition within business processes is needed in the
first place. This research is searching for a way to analyse this cognition within business processes and
thus a detailed specification of how one can scrutinise the cognition that is utilised. Namely, if there is
a way to describe cognition in a process, this can be leveraged to reason about how one can enhance
this cognition. As stated before, this research investigates how AI software can play a role in this.
This section first elaborates on business processes and the entities that express intelligence in a
process, called agents, in Subsection 3.2.1. Subsection 3.2.2 presents the definition of cognition as used
in this research. Next, the systems perspective is introduced more in detail in Subsection 3.2.3 which
is the foundation of the perspective taken in this research. Finally, Subsection 3.2.4 links the concepts
26

Cognition Enhanced Business Processes
of Subsection 3.2.2 and Subsection 3.2.3 to each other, elaborating on the cognitive functions that a
system in a business process could posses.
3.2.1 Agents and Business Processes
A business process is defined as a set of business activities that represent the steps required to achieve
a particular business objective (Object Management Group, 2011). A business process consists of
sub-processes (distinct parts of the process that can perform independently) and tasks (the work in
the process that is not broken down to a finer level of process model detail). The execution of tasks
is performed by entities in the business (process), functioning according to a certain role. Such role
can be fulfilled by a human being, a software service or a combination of the two (Object Management
Group, 2011).
Using the vocabulary of Artificial Intelligence field of study, these entities can be called agents. In the
preliminary research conducted in the field of Artificial Intelligence, I noticed that a broad range of
definitions and descriptions exists of agents. As described by Wooldridge (1999), there is no agreement
in literature about what an agent exactly is and no universally accepted definition of the term agent
exists. “There is a good deal of ongoing debate and controversy on this very subject” (Wooldridge,
1999).
One definition of an agent, created by Russell and Norvig (2003), is that an agent is “anything
that can be viewed as perceiving its environment through sensors and acting upon that environment
through effectors”. An example of another definition provided by Wooldridge (1999), is that “an agent
is a [...] system that is situated in some environment, and that is capable of autonomous action in this
environment in order to meet its design objectives”.

Humans can be defined as intelligent agents (Russell and Norvig, 2003), but according to their defini-
tion of an intelligent agent (“an autonomous entity which observes through sensors and acts upon an

environment using actuators and directs its activity towards achieving goals and may also learn or use
knowledge to achieve their goals”), a thermostat is also considered an intelligent agent (complex versus
simple agents). According to Wooldridge (1999), a distinction between agent and intelligent agents

should be made. Wooldridge (1999) states that “an intelligent agent is [an agent] that is capable of flexi-
ble [(reactivity, pro-activeness, social-ability)] autonomous action in order to meet its design objectives”.

Besides their definition of an agent, Russell and Norvig (2003) state that there are agents in different

forms such as human agents, robotic agents and software agents. This research is, however, not inter-
ested in these agents solely human or machine-like, but particular interested in agents as in an entity

representing a combination of a human and machine (robotic or software service), where the machine

is facilitating the human in its activities. This is because in a business process, such as the claim-
assessment process, humans are executing tasks within the process, leveraging machines to support,

i.e. facilitate them in the activities to perform these tasks.

27

Chapter 3. Theoretical Notions on Cognitive Systems in Business Processes
I am fully aware that this field of agents within Artificial Intelligence is very broad. Furthermore,
I recognise that I hereby only touch upon a small part of agent theory, which studies the design of
(intelligent) agents. However, agents and their intelligent behaviour as described in the field of Artificial
Intelligence inspired me to conduct this research, and is of interest to the development of this research.
Besides, it is a logical starting point for this research, since we are here studying AI applications within
business processes.

Before presenting the definition of an agent as used in this research, first some notions on intel-
ligence and cognition ought to be made, considered from a human point of view. This is regarded

as a legitimate choice, since we are studying humans that are performing tasks within a particular
organisation (structured by business processes), facilitated by the machines they can utilise.
3.2.2 Intelligence and Cognition
Human beings are seen as entities that possess intelligence, i.e. behave intelligently. Intelligence is a
term describing one or more capacities of the (human) mind. Human intelligence is studied widely
in literature, all together covered by the umbrella term cognitive science. Cognitive Science consists
of multiple research disciplines, including psychology, artificial intelligence, philosophy, neuroscience,
linguistics, and anthropology (Thagard, 2014). Probably this is the reason that no uniform definition
of intelligence exists.
To some extent, machines also can carry out intelligent behaviour, but very little relative to humans

(Power, 2015). Recall the system that can play Go as introduced in Chapter 1. This system is con-
sidered to be capable in playing this one game. However, it cannot play other games, not to mention

other capabilities that humans consider as very normal. This human intelligence is closely related
to cognition. Intelligence can be seen as a subset of cognition, and popularly described as the set of
conscious mental processes of a human being (Hendriks et al., 1997). Cognition is the collection of
mental process and activities used in perceiving, remembering, thinking, and understanding, as well as
the act of using those processes (Ashcraft, 2006). This is one of the many definitions of cognition; such
as the concepts of agents and intelligence, multiple fields of study hold multiple definitions of cognition.
Nevertheless, some sort of definition of cognition is important for this research. The definition of
cognition used in this research is composed out of several books on cognition, such as Ashcraft (2006),
Anderson (2013) and Hendriks et al. (1997) by extracting the main concepts they address, and is
worded as follows:
Cognition – The ability to execute processes of thought including perception, recognition,
memory, learning, knowledge, language, comprehension, goal generation, decision-making,

judgement, reasoning and problem solving

To research cognition enhanced business processes, the following gap of knowledge to tackle is
how this cognition is considered in business processes. Systems thinking on cognition, as studied in
cybernetics, provides a perspective to research this knowledge gap.

Cognition Enhanced Business Processes

3.2.3 System Perspective on Agents and Cognition
Above, I briefly touched upon agents, described from an Artificial Intelligence point of view. This
research is however particularly interested in entities representing an entity that is a combination of
a human and machine (robotic or software service), where the human is facilitated by the machine to
execute its assigned tasks. Therefore, this particular entity is in this research related to what has been
said before about agents, by leveraging a definition of an agent from systems (cybernetic) theory. The
definition of an agent that is used in this research is obtained from Heylighen (2011): an agent is a
goal-directed system that tries to achieve its goals by acting on its environment. “Agents are typically
organisms, such as animals or people. However, they can also be artificial systems, such as robots or
software agents, with pre-programmed goals. They can even be organisations or other social systems,
such as a firm, a football team, or a country, that consist of coordinated individual agents with a shared
set of goals (e.g. making profit for a firm, winning for a football team)” (Heylighen, 2011). To depict
this description, think about an agent as a coherent and collaborating whole of a human and machine
within a business environment.
The systems philosophy can be summarised by the well-known saying that “the whole is more than the
sum of the parts”. A whole possesses emergent properties, i.e. properties that are not properties of
its parts. For example, an organism has the property of being alive; the same cannot be said of the
atoms and molecules that constitute it. A song has the properties of melody, harmony and rhythm,
unlike the notes out of which it is composed.
A system can be defined as a number of parts connected by relations or interactions. The connections
are what turn a collection of parts into a coherent whole. What thus counts are the relations between the
elements, not the elements themselves (its emergent character). The essence here is organisation, that
is, the pattern of connections and the information that is passed on along them, which give the system
its coherence. An systems is always considered to be embodied in its environment, thereby interacting
with this environment by inputs and outputs: they exchange matter, energy and/or information, and
thus mutually affect each other. This leads us to define two other basic systems concepts:
• Input: what enters the system from the environment
• Output: what leaves the system to end up in the environment
Because we know that humans express cognition, i.e. execute cognitive activities, we can say that their
abstraction in systems also hold cognition and the corresponding (cognitive) activities. Next to that,
human beings live and only because they are alive, they are able to express their cognition. Therefore,
a systems abstraction of a human can be denoted as (living) cognitive system.
Ideally, a system exists if its purpose is of value to the world it is embodied in. For example, within a
company, a particular business process only exists if it adds value to the business, that is, it contributes
to the general goal of the business. If it is not of any value, there is no need for the business to exist,
because the burden of its existence (use of business’s resources such as time, people, money, etc.) is
higher that the gain it delivers to the business (which is then zero). This is the principle of a ‘business

Chapter 3. Theoretical Notions on Cognitive Systems in Business Processes
case’. However, there should be noted that in practice, organisations deal with legacy systems that do
not entail value to the business.

This results in the principle that systems are goal-directed (Heylighen, 2011). Besides, all liv-
ing systems are intrinsically goal-directed: “they try to maintain and (re)produce themselves, in spite

of perturbations from the environment” (Heylighen, 2011). A system is pursuing a particular goal,
whereby achieving this goal it creates value. In the example of a business process, the process has a
particular goal. By achieving this goal, it adds certain value to the business by which it is coming
closer to achieve its (overall) business goal.
In a business organisation however, humans are also working towards a certain goal: getting their work
done (individual goal), such that the overarching goal of the organisation (business goal) is achieved.

With that, a systems implicit goal or value is fitness. Fitness is defined as the quality of being suit-
able to fulfil a particular role or task. Thus, in the example of a business, a business process’ fitness

describes the quality of the process being suitable to fulfil the task it is assigned to by the overall busi-
ness. If the fitness of a business process rises by the cognitive enhancement, the business process could

perform better because of the cognitive enhancement. Futhermore, the fitness of a human-machine
system to the business process describes the quality of the system of being suitable to fulfil the task
in the business process it its responsible for. When such a system performs better due to a cognition
enhancement, its fitness in the business process will improve.
From the literature on cybernetics, goal-directedness is achieved via control (Ashby, 1962; Heylighen
and Joslyn, 2001). Control is the successful reduction of deviations from the goal by appropriate
counteractions. For example, hunger is a deviation from the state of sufficient energy. Its counteraction
is to find and eat food. The agent is in control if it manages to eat sufficient food not to stay hungry.
A deviation thus triggers an action (finding and eating food), which produces a reduced deviation
(less hungry), which in turn produces a further action (eat more), and so on, until all deviations have
been eliminated (not hungry anymore). Thus, control is characterised by a negative feedback loop.
A feedback loop is a circular coupling of a system with itself via the environment. The feedback is
called negative when it reduces deviations, positive when it increases deviations.
Thus, agents are control systems, pursuing a goal and able to act to achieve this goal. Generally,
such a system consists out of the following elements, also see Figure 3.1:
• perception (P): information enters the agent from the environment, representing the situation
as experienced by the agent.
• goal (G): internal representation of the ideal or preferred situation for the agent.
• action (A): the agent affects the environment in order to bring the perception closer to the goal.
• diversions (Di): changes in the environment that affect the situation independently of the agent
(i.e. that are not under control of the agent), making it deviate from its present course of action.
Can be diversions that help the agent achieve its goal, i.e. positive diversions (affordances) or
just the opposite, counteract the agent to achieve its goal, i.e. negative diversions (disturbances).

When an agent experiences a (current) situation (perception) which is not the same as its desired sit-
uation (goal), a difference (deviation) in situation is experienced/noticed by the agent. This difference

30

Cognition Enhanced Business Processes

Figure 3.1: Schematic illustration of a Agent and its Environment (Heylighen and Joslyn, 2001)
in situation is called a problem. If there would be no difference, the agent would be perfectly satisfied
and would have no reason to act. Two notions on problems here:
• A problem in this sense is not necessarily negative or unpleasant: it is sufficient that the agent
can conceive of some way to improve its situation and is motivated to seek such a improvement.
For example, if I feel like drawing, then my “problem” is defined as the difference between an
empty page and a page with an esthetically pleasing sketch on it.
• A problem should also not be seen as something purely intellectual: if the cup I am holding tilts
a little bit too much to the left, so that coffee may leak out, this defines a problem that I must
resolve by restoring the balance.
Referring to state-determined systems, any problem is characterised by:
• an initial state, i.e. the situation you start from which is unsatisfactory (such as a car that does
not start, or an unsolved puzzle).
• a goal state, i.e. a conceivable situation that would satisfy your criteria for a problem solution
(such as a car that drives, or a puzzle where all the pieces have fallen into place).
With perceiving its current situation (initial state) and its willingness to reach its goal (goal/desired
state), the agent’s task is to bridge this gap between these to situations, i.e. solve the problem. To
solve its problem, the agent has to select and perform one or more actions that together eliminate (or
at least minimise) that difference (using the presumptions created by negative feedback loop).
For example, the claim-assessment process tries to minimise the number of claims in the cue that
(still) need assessment. To achieve this, it executes tasks to perform this assessment of the claims.
This is a sequential process: claims are assessed in sequence.
3.2.4 Systems and Cognitive Functions
As stated earlier, to execute certain tasks an agent in a company needs some form and degree of
cognition. Thus, from a systems (cybernetic) perspective, when a system encounters a problem that
31

Chapter 3. Theoretical Notions on Cognitive Systems in Business Processes
it wants to solve, cognition is required. If this system is capable of executing tasks (associated with a
particular business process), it is capable of successfully expressing the right cognitive functions to get
closer to its intended goal(s).
Recall the definition of cognition as used in this research: cognition is “the ability to execute processes

of though such as/including perception, recognition, memory, learning, knowledge, language, compre-
hension, decisions, judgements, reasoning and problem solving”. From the definition of cognition, a

decomposition of the cognitive functions that exist, i.e. that a cognitive system holds (and needs
to solve a problem) can be made. This decomposition is largely based on (adapted from) Heylighen
(2011).
Perception (P) The agent needs to sense as precisely as possible what deviations there may exist,
and in how far previous actions have affected these.
Representation (R) Perception produces an internal representation of the outside situation, such as
a pattern of activation across neurons. Note that this representation is not an objective reflection
of external reality as it is, but a subjective experience of how the agent’s personal situation may
deviate from the preferred situation. There is also no reason to assume that a representation
consists of discrete units (symbols) that represent separate objects or aspects of the situation.
Fundamentally, the agent’s cognition does not represent objective phenomena, but subjective
sensations that depend on the agent’s goals.

Information processing (IP) The agent needs to process or interpret the information in the rep-
resentation, and in particular determine precisely in what way it differs (or may start to differ)

from the goals, and which actions could be used to reduce those differences. This requires some
process of inference.

Decision making (De) The agent needs to select an action to perform. In general, only one ac-
tion can be performed at a time; if several actions seem appropriate, the best one needs to be

determined.
Knowledge (K) To make adequate decisions, the agent has to know which action is most appropriate
to reduce which deviation. Otherwise the agent would have to try out an action at random, with
little chance of success, and thus a high chance of losing the competition with more knowledgeable
agents.
Intelligence (I) If the problem is complex-so that solving it requires more than one interpretation
and/or action-the agent may need to look ahead at likely future situations by making inferences,
exploring their consequences, and developing a plan to deal with them, i.e. by designing a
sequence of well-chosen, coordinated actions that as much as possible take into account the
intricacies of the situation.
Note that these distinct functions do not necessarily correspond to distinct components in the
cognitive system: the same component (e.g. a nerve connecting a sensor to an effector) may perform
more than one function (e.g. perception, representation, knowledge, etc). Let us follow through the
control process outside the agent, noting how the environment too participates in solving (or worsening)
the problem:

32

Cognition Enhanced Business Processes
Action (A) The agent should be able to perform a sufficiently broad repertoire of actions to affect
the environment in the needed way: the more variety there is in the diversions, the more variety
there must be in the actions to deal with them. This requires sufficiently powerful and flexible
effectors.
Affected variables (AV) Only certain aspects of the environment are affected by the agent’s actions:
for example, the agent cannot change the weather, but may be able to find or make a shelter
against the rain.
Dynamics (Dy) Changes in the environment, whether produced by the agent or by diversions (i.e.
all events not produced by the agent), generally lead to further changes, according to the causal
laws or dynamics governing the environment. For example, a stone pushed over a cliff by the
agent will fall down to the bottom, where it may break into pieces. This dynamics may help or
hinder the agent in achieving its goals. It may even perform some of the required information
processing, like when the agent adds stones together to perform calculations (calculus = Latin
for “small stone”).
Observed variables (OV) The agent cannot sense all changes in the environment, whether caused
by its own actions, diversions or dynamics; the variables it can perceive should ideally give as
much information as possible relevant for reaching the goal; irrelevant variables are better ignored
since they merely burden the cognitive system.
In cybernetic literature, systems that are considered to hold and can express (some form of) cognition
are called cognitive systems. A cognitive system can be defined as a goal-directed system that tries
to achieve its goals by acting on its environment (Heylighen, 2011).
When we put the different internal and external components of the control process together, we
end up with the following more detailed scheme as presented in Figure 3.2 and obtained from Heylighen
and Joslyn (2001).
In general, the cognitive system cannot be certain which action is appropriate (Heylighen and
Joslyn, 2001). This is because the environment is infinitely complex: every phenomenon in the universe
has potentially some influence on what can happen here and now. Moreover, every situation is unique:
even seemingly identical situations can produce different outcomes.
Furthermore, the cognitive system cannot have perfect knowledge of what to do for each possible
situation. This implies that some of the actions the cognitive system performs will not be optimal,
or not even adequate (Heylighen and Joslyn, 2001). However, this is not grave because errors can
generally be corrected by subsequent actions. The only real requirement is that actions must be more
likely to improve than to worsen the situation. If that condition is met, a long enough sequence of
actions will eventually bring the cognitive system close to its goal.

Cognitive Systems are studied in scientific literature. The Elsevier journal of Cognitive Systems Re-
search published its first volume in 1999 (Honavar et al., 2015), thus is fairly new to the scientific

body. The journal Advances in Cognitive Systems publishes research articles, review papers, and es-
says on the computational study of human-level intelligence, integrated intelligent systems, cognitive

architectures, and related topics since 2012.

33

Chapter 3. Theoretical Notions on Cognitive Systems in Business Processes

Figure 3.2: Schematic illustration of a System embodied in its Environment, denoting the Cogni-
tive and Physical aspects of control of these two concepts (Heylighen and Joslyn, 2001)

3.3 Cognitive Systems as Used in this Research
In this research, the concepts and Figure 3.2 from Heylighen and Joslyn (2001) and the earlier presented
definition of cognition is used to study cognition enhanced business process by AI system more in detail.
To emphasise, a cognitive systems is thus considered here as an entity that consists of a human agent
and a machine agent, and thereby purely a conceptual notion.
From a cybernetics perspective, such a cognitive system can thus be seen as a system where a
human and a machine interact with each other (Heylighen, 2011). In other words, a human which is
being facilitated by a machine. This machine may have to have some form and to some degree have
artificial intelligence, of which its design is studied in the Artificial Intelligence and Computer Science.
In an organisation, humans interact with each other and the organisation depends on the interaction
of these people. Often multiple roles are present in a business process, and thus multiple cognitive
systems. Each cognitive system behaves individually, trying to achieve its own goal(s). However, in
an organisation consisting of multiple cognitive systems, the cognitive systems are collectively working
together to achieve the organisation’s goal (Dignum, 2013). One studying business processes is therefore
obliged to say something about these interactions between the people and the machines they are using
(could be collective use: shared computer services, e.g. same data processing environment) in the
organisation. Leveraging concepts from systems theory, one can denote that these ‘(cognitive) systems’
or agents are coupled and interact with each other and form as a group a system on itself, a so called
multi-agent system (Dignum, 2013).
Cognitive systems can furthermore be identified on different organisational levels. Furthermore,
systems can be systems that consists of other systems, called sub-systems, allocated in a hierarchical

34

Cognition Enhanced Business Processes
order. Therefore, a group of cognitive systems performing the tasks of a business process can, from a
holonic perspective, be seen as one cognitive system itself. That is, a system – consisting of a collective
of cognitive systems – executing the business process with a certain performance. Thereby, this system
is also capable of conducting cognitive performances, i.e. inheriting the (collective) cognition of the
cognitive systems it consists of, and thus indeed can be (also) called a cognitive system. Such systems
are called holonic systems in literature (Clegg, 2007).
However, this research however only focuses on the lowest organisational level where individual
humans (workers) are performing individual tasks by executing a set of activities. That is, it
does not take into account the interactions between systems on the same hierarchical level (multiagent
systems) nor systems on other hierarchical levels (holonic systems).
According to Russell and Norvig (2003) and thus stated from an Artificial Intelligence point of view,
“intelligent agents need knowledge about the world in order to reach good decisions”. Furthermore,
according to the cybernetician Ashby (1962), this appropriate selection of sequential actions
(i.e. making the right decisions) is the essence of intelligence. These statements corresponds what has
been stated about a business process executed by human beings such as the claim-assessment process
mentioned earlier; humans (and certainly subject matter experts) utilise their knowledge to be able
to make decisions when presented a complex and difficult tasks. This research therefore identifies its
view on intelligence in business processes with this statement on intelligence as appropriate selection
of sequential actions by Ashby (1962).
The relation between cognitive systems, roles, tasks and business processes as considered in this research
are schematically visualised in Figure 3.3, utilising the Business Process Modelling Notation (Object
Management Group, 2011) to draft the Organisation worldview in this figure. One can see in this
figure that a cognitive system fulfils a certain role, thereby executing certain tasks within the business
process. The cognitive systems that fulfil a role in a business process are performing tasks all together
to execute the business process.
To execute tasks, a (cognitive) system needs to utilise its cognition to execute the necessary
activities to perform a certain task at hand, thereby expressing cognition. Thus, a cognitive system
performs tasks within the business process, using the capabilities it possesses, included in these are
its cognitive capabilities. From a task-point of view, to perform a certain task, certain cognitive
capabilities are required to perform the tasks in (at least) a sufficient way.
3.3.1 Cognition Enhanced Business Processes
When the cognitive capability of a cognitive system is enhanced, the cognitive system is more capable
of performing the task, i.e. the performance with which the tasks is executed will be raised – bluntly,

the task will be ‘better’ executed. This performance raise depends on the goal of the (cognition) en-
hancement of the business process (efficiency, quality, et cetera). Thus, to make clear, it is not the

business process itself that is being enhanced as meant in this research. What is being enhanced is the
cognitive system that performs activities in the business process, as a conceptual entity.

Figure 3.3: The relation between the Organisation world and Agent / Cognitive Systems world
Recalling Figure 3.2, the interaction between the components through their connecting processes form
the ‘cognitive activities’ the agent/cognitive system can express. Thus, if one increases the performance
of these cognitive processes (perception, information processing, action, etc.), one thereby increases
(enhances) the quality of the cognitive activities executed by the cognitive system, and thus its overall
cognitive performance. These cognitive processes can thereby be leveraged as categories or types of
cognitive enhancements.
Based on all above, one could characterise cognitive requirements and capabilities through the cognitive
functions as identified and argued in this chapter. The Cognitive Requirements of a business process
can be seen as the (form and level of) cognition that the tasks in the business process require in order

to be performed such that it creates value for the business process (and thereby for the whole busi-
ness). The Cognitive Capabilities of AI software as introduced in Chapter 2 are thus the capabilities of

the software to ‘boost’ the cognitive functions of the cognitive system (human), thereby boosting the
performance of the tasks performed by humans. Finally, a cognitive enhancement can now be defined
as the increase of the ‘level’ of cognition, i.e. the difference between the current state and the desired
(enhanced) state.

To summarise, the link between the behaviour of elements of business processes and cognition is pre-
sented, which is found in theories on cognitive systems. Now this understanding is created, one can

denote what is meant by enhancing this cognition and what it entails. Recalling Figure 3.2, represent-
ing the components and processes of a cognitive system. The interaction between these components

through their connecting processes form the ‘cognitive’ activities the agent/cognitive system can ex-
press. Thus, if one increases the performance of these cognitive processes (perception, information

processing, action), one thereby increases (or: enhances) the quality of the cognitive activities exe-
cuted of the agent/cognitive system, and thus its (overall cognitive) performance. These cognitive

processes can be used as categories or types of cognitive enhancements.

36

Cognition Enhanced Business Processes
Now recall the conceptual design phase from Chapter 1 to conceptually design a system (Figure 2.2),
which is the focus of this research. This conceptualisation phase forms the underlying setup of this
research, and the question is now how this design process of cognition enhanced business processes
looks like.
3.3.2 Cognitive Systems Applied in Illustrative Case
Recall the description of the claim-assessment process in Section 3.1. With the concepts such as

business processes, intelligence and cognitive systems introduced and described in Section 3.2 and Sec-
tion 3.3, the claim-assessment process is analysed here for illustration purposes.

Consider the claim handler and the machines that he or she uses to process the claims as a cognitive
system. This system starts by observing the claim and its corresponding record it receives (perception)
and determining that an assessment is needed if the claim should be reimbursed or not (goal). The
claim handler will then make an internal representation of all the information available on the record,
which is processed in order to infer an assessment of the claim (information processing / reasoning).
Based on this reasoning, the claim handler decides if the assessment and his or her recommendation
is complete and correct (enough) (decision), the claim handler sends the record including the drafted
assessment and recommendation back to the person who sent the claim. If not, the claim handler
takes an action such that it is better able to draft a complete and correct assessment. Examples
of possible actions are gathering more information, consulting with colleagues (language), et cetera
(problem solving loop). These actions cause changes to the variables of the claim record (affected and
observed variables), which are again perceived by the claim handler.
A claim handler in this case study has a lot of knowledge on different kinds of domains: the health
insurance domain, biology domain, knowledge on internal protocols and guidelines set by the health
insurance company, and more. All relevant knowledge from its memory is used to reason about the
information at hand and draft an assessment and recommendation. The expertise of the claim handler
is here considered as the ability to use its cognitive capabilities properly, such that the claim handler
will produce better outcomes.
To process the information such that this information is actually digested by the claim handler, he of
she needs to comprehend the information at hand, which can then be used to reason and to be used
to come to judgement. The final most correct judgement, according to the claim handler, will most
likely be transformed into the final decision if the claim should be reimbursed or not. Otherwise, it
will be a judgement that says that more information is needed to come to such a decision. When a
claim-handler has assessed multiple claims, it generates experience, by reasoning about each individual
assessment and all assessments overall to infer certain patterns (learning).
The enhancement of the cognition of the cognitive system here focuses on what is considered to be
the most difficult cognitive processes when handling claims as described above. An interview with
stakeholders from a health insurance company, responsible for the handling of claims, resulted in the
observations that the most difficult activities are related to deducing the correct conclusion from the
37

Chapter 3. Theoretical Notions on Cognitive Systems in Business Processes
right information. The question that remains to be answered is thus what possibilities exist to enhance
the cognitive functions expressed in the business process by AI software, i.e. cognitive system such as
formally conceptualised in this research. Chapter 4 presents the first step of the envisioned conceptual
design process that contributes to the answer to this question.
3.4 Chapter Summary
This chapter provides insights in the concepts that are used throughout this thesis. The main insight

that it generates is that a human facilitated by machines, performing certain activities which are asso-
ciated with a particular business process, can be viewed as a cognitive system. In this research, these

cognitive systems are studied individually and only those systems that are on the level of performing
tasks. Next, it gives a description of such a cognitive system, emphasising its cognitive function, which
is primarily used further in this research. Furthermore, the notion how a cognition of a business process
can be enhanced is described. It states that this can be achieved by enhancing the performances of
the cognitive functions of a cognitive system within the business process, by designing these cognitive
systems utilising the cognitive capabilities that can be leveraged by (AI) software. Finally, this chapter
illustrated how the concept of cognitive systems and its related notions can be used by applying them
to a case. This case on the claim-assessment process of a health insurance company is further studied
in the next chapters of this thesis.

38

4 | Cognitive Requirements of Business

Processes

This chapter presents the research that is conducted on the second research issue (RI2) raised Chapter 1
in Figure 1.4: Establish requirements to enhance the cognition of business processes. As explained in
the Chapter 2, this research is interested to know how to match the properties of business processes
that are (in some form and degree) related to cognition to the AI software and the technologies they
utilise (e.g. machine learning). Therefore, one needs first to examine business processes in such a way
that these cognitive properties can be identified and analysed, and formulated in such a way that they
represent the cognitive requirements (CR) of the business process; the cognitive activities that are
required to properly execute the tasks in the business process and thus the process as a whole. These
cognitive requirements can be used to match suitable AI software and corresponding technologies to
the business process at hand. Thereby, this chapter creates the first part of the conceptual design
process, the Cognitive Requirements Design Process (CRDP) (see Section 2.2). The chapter
shows a way to elicit the cognitive requirements of a business process, by sequentially describing the
theoretical underpinnings of this design process, focused on eliciting these cognitive requirements and
its use in the illustrative case introduced in Section 3.1.
Section 4.1 kicks off by describing and explaining what is exactly needed for the formulation of
cognitive requirements of business processes. More specifically, it describes how a business process is
analysed and decomposed in sub-processes and tasks and how these can be specified utilising Business
Processes Modelling theory and notation. Section 4.2 elaborates on Cognitive Task Analysis and how
this theory and its methods can help to identify the cognitive elements within a (knowledge-intensive)
task. When the cognitive elements are identified, they can be formalised as cognitive requirements such

that their specification is consistent in terms of granularity. Section 4.3 therefore addresses Require-
ment Engineering as a scientific concept and business standard, describing how it can be leveraged

to formulate these cognitive requirements from the cognitive elements identified in Section 4.3. By
combining Business Process Modelling, Cognitive Task Analysis and Requirement Engineering from
respectively Section 4.1, Section 4.2 and Section 4.3, a description of the first part of the engineered
design process on how to formulate (i.e. design) cognitive requirements of a business process is given in
Section 4.4. This last section furthermore assesses and discusses the constructed CRDP on the criteria
stated in Chapter 2, which thereby concludes this fourth chapter.

39

Chapter 4. Cognitive Requirements of Business Processes
4.1 From Business Processes to Tasks and Roles
The goal at hand is to formulate the cognitive requirements of a business process. We can thereby
identify the starting point of our quest by analysing business processes of organisations. To date,
it is common to describe organisations as sets of business processes (Pidd and Melao, 2000). More
specifically, we are interested in what happens in a business process or, put differently, what the
execution of such a business process entails. The Object Management Group (2011), an organisation
describing and maintaining definitions of technological standards worldwide, defines a business process
as “[...] a defined set of business activities that represent the steps required to achieve a business
objective. It includes the flow and use of information and resources”.
4.1.1 Business Process Modelling
According to the above mentioned definition of a business process, business activities are those activities
that have to be executed such that the goal of the business process is achieved. For this research, these
business activities are of interest; when an overview of these business activities would be available, we
would be a step closer to identify the cognitive properties that are needed to execute these business
activities, and thereby the business process as a whole. Such overview of the activities of a business
process can be drafted by employing approaches that identify and analyse business processes and their
activities. Such an approach is Business Process Modelling and is well known in both the business
world as the scientific world, abbreviated to BPM (Pidd and Melao, 2000).
Using Business Process Modelling one is thus able to decompose and formulate the distinct tasks in
the process (Object Management Group, 2011). Although many notations for business process models
exist, a well known BPM notation is the Business Process Modelling Notation (BPMN) standard
(Object Management Group, 2011). BPMN is a formal notation standard of business process modelling,
and commonly used in and adopted in practice by organisations. In this modelling notation business
processes are decomposed in sub-processes, which on their turn consist out of tasks (in increasing
order of granularity). A sub-process is a process that is included in another process, thereby lower in
hierarchy and can entirely be executed stand alone (Object Management Group, 2011). Next, a tasks
is denoted as ’the work in the process [that] is not broken down to a finer level of process model detail’
(Object Management Group, 2011).
4.1.2 Application to this Research
Within an organisation, tasks can be executed by a person, an application, or both. In the light of
this research, such an application could thus be an AI technology driven piece of software, i.e. AI
software service. BPM visually denotes these participants in a business process. It can be a specific
organisational entity (e.g. department) or a role (e.g. assistant manager, doctor, student, vendor).
In BPMN the rectangular boxes represent these specific organisational entities or roles. One or more
tasks present in a business process are in BPMN visually differentiated by the specific organisational
entity or role played by an organisational entity executing these tasks.
Recapitulating, these specific organisational entities and roles described in a business process model
indicate the entities that execute the tasks in a business process. In the light of the theoretical notions

40

Cognition Enhanced Business Processes
on cognitive systems in Chapter 3, such an entity (a human worker, facilitated by a machine) is viewed
as a cognitive system. These cognitive systems are, conceptually speaking, thus the entities executing

these tasks. Because these tasks are considered to be executed utilising some form and degree of cogni-
tion, these entities (cognitive systems) are expressing cognition when executing these tasks. Section 3.3

furthermore showed that it is possible to analyse cognitive systems to identify the cognitive functions
of such a system. The line of reasoning corresponds to insights from the field of psychology on humans
working in businesses, which state that the activities performed by humans in an organisation involve
‘some form and some degree’ of cognition, and these humans are thus expressing cognitive process
(Klein et al., 1989).
Based on the insights above, one can infer that the tasks in a Business Process Model (specified
according to the BPMN standard) are the objects that can entail cognitive properties when executed.
Furthermore, the entities executing these tasks (in Chapter 2 referred to as cognitive systems) are
considered to possess the cognition needed and utilise their cognitive functions to execute these tasks.
As a result, the tasks of a business process are considered to be very valuable to look at to succeed
in our quest to the cognitive properties of business processes. A business process modelling exercise is
thereby considered to be a valuable approach in this research, as it will result in a clear set of tasks a
particular business process consists of (Object Management Group, 2011).
This set of tasks can be filtered to extract the tasks that are valuable for further investigation.
As mentioned before in Chapter 2, this research is interested in enhancing the cognition utilised in
business processes. The tasks performed in the business process model that entail a relatively high
degree of cognition are most interesting here, since that indicates that when this cognition is enhanced,
the corresponding tasks could be performed in a better way.
Next to the decomposition of the business process, the current viewpoint lends itself to investigate the
requirements of the stakeholders of the business process under review. It is important to identify their
needs and define them specifically into stakeholder requirements, such that the design will also reflect
their needs. These defined stakeholder requirements can be taken into account in the design process.
4.1.3 Example by Means of Illustrative Case
To illustrate the conceptual notions and BPM exercise as described above, the business process model
as visualised in Figure 4.1 describes the claim-assessment process of this case study. The case study
only considers claim assessments where a human being is needed in the assessment process for to
perform the actual assessment, thereby neglecting assessments that are performed automatically by
(computer) systems.

41

Chapter 4. Cognitive Requirements of Business Processes Business process architecture layer Information architecture layer Claim handler Subject Matter Expert
Receive claim

Send claim approval

Send claim decline

Approve or
decline
claim
?

Scientific articles Law and regulations Internal protocols

Check claim for
completeness

Claim
complete
?
Send incompleteness message
Assess claim
utilising
protocols

approved

declined

Decision
possible
based on
standardized
rules
?
yes

Send claim approval

Send claim decline
Approve or
decline
claim
?
approved

declined

no

Send claim to
corresponding
expert
department

Determine field
of study

Assess claim (manually)

Receive claim

Claim assessment Decision Tree

Internal
database

Excerpts from
online sources

Client
database

Claim records
database
clientRecord claimRecord

Figure 4.1: Business Process Model of case study 1

42

Cognition Enhanced Business Processes
In this business process model, two architecture layers are presented. These layers are considered
to be of most value here, since we are interested in the different process tasks within the process and,
because we are dealing with knowledge-intensive business processes, the information that is involved
in these tasks.

Figure 4.1 shows that two different sub-processes exist within this organisation that are both respon-
sible for the assessment of claims. The first sub-process, executed by a claim handler, assesses if the

claim is complete. If not, the claim is send back to the applicant. If the claim is complete, the claim
handler tries to assess the claim based on standardised protocols (such as decision-trees). If the claim
handler succeeds, the claim handler can approve or decline the claim properly. If, for a certain reason,
the claim handler is not able to assess the claim in a proper way, the claim is send to a subject matter
expert (SME). The subject matter expert advises, amongst other things, on more complex medical
claims (second sub-process). All the information on the claim is send to the SME, who conducts a

second assessment. With his or her in depth knowledge on medical concepts and experience assess-
ing many different claims, the SME is able to come to a legitimate assessment and decision of the claim.

From the business process model, the tasks relevant to the assessment of claims can be extracted. The
following tasks can thus be extracted from Figure 4.1:
• Check claim for completeness
• Assess claim utilising protocols
• Determine field of study
• Send claim to corresponding expert department
• Receive claim
• Assess claim (manually)
Reviewing the list above, the tasks Assess claim utilising protocols and Assess claim (manually)
seem play an important role in the whole business process, since they comprise the essential activity
of the process: assessing if a claim should be approved for reimbursement or not. Furthermore, a
significant portion of cognition is needed to execute these tasks: both knowledge about medical concepts
and claim insurance is needed to adequately perform such a claim assessment, utilising thought to
perceive, process and judge the information in the claim. This also borne witness of the fact that
one needs a sufficient amount of training before one can actually assess claims in a proper way. At
the insurance company this case study is conducted, claim handlers are being trained for half a year.
The two highlighted tasks are essentially the same – both entail the assessment of a claim, however
executed by different roles in the process (claim handler and subject matter expert). Based on these
reasons the tasks Assess claim utilising protocols and Assess claim (manually) are further investigated
in this first sub-design process of the conceptual design process. Therefore, the focus on the rest of
this chapter is on these two tasks.
4.1.4 Intermediate Conclusion
To conclude the story so far, Business Process Modelling (here shown according to the BPMN standard)
43

Chapter 4. Cognitive Requirements of Business Processes
can aid one in eliciting the cognitive properties of a business process, namely the cognitive aspects of
its tasks. Therefore, it can be utilised to perform the first step of the cognitive requirements design
process – the construction of an overview of the elements of a process – from here referred to as the
design step A. Business Process to Tasks. For this research, the identification of sub-processes, tasks
and roles is especially valuable, although the BPMN standard notion is much more comprehensive
than only this selection. As already stated in this section, the tasks are executed by entities (viewed
from a systems/cybernetic perspective) that entail some form and degree of cognition. Performing this
business process modelling exercise will result in a list of tasks. Thus, the cognitive aspects of these
tasks are of particular interest for this research to analyse. The question that raises is how to reveal
these cognitive aspects of the tasks of interest.
4.2 From Tasks and Roles to Cognitive Activities
With the overview of the tasks in a business process formed in Section 4.1, the designer is able to take
a closer look at these sub-processes and tasks. To be able to formulate the cognitive requirements of
a business process, the cognitive aspects of the tasks need to be identified and extracted in some way.
A method that can be utilised to guide this identification and extraction process is desirable.
From psychological and management literature, theories under the scientific umbrella term Task
Analysis are found. Task Analysis consists of a variety of techniques for identifying and understanding
the structure, the flow, and the attributes of tasks, required for a user to complete a task or achieve
a particular goal (Jonassen et al., 1999). Task Analysis is studied in the System Engineering field as
well. According to Jonassen et al. (1999) in the light of designing a new system, “task analysis makes it
possible to design and allocate tasks appropriately within the new system. The functions to be included
within the system [...] can then be accurately specified”. A branch of Task Analysis is Cognitive Task
Analysis, focusing more on the cognitive aspects of the task under study. Cognitive Task Analysis
seems promising for achieving the objective presented above. Therefore, this section further elaborates
on Cognitive Task Analysis and how it is used in this research.
4.2.1 Cognitive Task Analysis
Cognitive Task Analysis (CTA) is a category of Task Analysis, focusing on the cognitive aspects of
executing tasks, i.e. describing and representing the cognitive activities that underlie goal generation,
decision making, judgements, etc., not on evaluating the outcomes of task executions (Hoffman, 2005;
Schraagen et al., 2000). One can recognise (a part of) the definition of cognition as presented in
Section 3.2.
Cognitive Task Analysis analyses and represents the cognitive activities users utilise to perform

certain tasks. CTA describes approaches to the understanding of cognitive activities required for man-
machine systems (Hollnagel, 2003). Some of the steps of a cognitive task analysis are: the mapping

of the task, identifying the critical decision points, clustering, linking, and prioritising them, and
characterising the strategies used (Klein et al., 1989). There is a collection of methods available for
conducting a cognitive task analysis. Applied Cognitive Task Analysis (ACTA), the Critical Decision
Method (CDM), Skill-Based CTA Framework, Task-Knowledge Structures (TKS) and the Cognitive
Function Model (CFM) are a few examples. Cognitive task analysis has been used to examine for

44

Cognition Enhanced Business Processes
example the decision-making process of experts, the development and evolution of mental models and
the information requirements for command and control systems (Klein et al., 1989).
Although there are many varieties of CTA methods, most methods follow a five-stage process
(Schraagen et al., 2000; Clark et al., 2006; Coffey and Hoffman, 2003; Cooke, 1994; Hoffman et al.,
1995; Jonassen et al., 1999):
1. Collect preliminary knowledge: getting familiar with the content, systems and procedures being
analysed;
2. Identify knowledge representations: examining each task to identify its underlying activities and
types of knowledge required to perform it;
3. Apply focused knowledge elicitation methods: collect the knowledge identified in the prior stage,
using methods that are appropriate to the targeted knowledge type;

4. Analyse and verify data acquired: sometimes the knowledge elicitation techniques are less for-
mal and require that the analyst code and format the results for verification, validation, and

applicability for use in their intended application;
5. Format results for the intended application: for less formal CTA methods, such as those described
here, the results must be translated into models that are appropriate to aid the design and
development of the system under review.

The first two stages will be referred to as respectively the design step B. Preliminary knowledge col-
lection and C. Knowledge representation identification. These design steps mainly resulted in the

information of the claim-assessment process as noted in Section 3.1. From here, the third to fifth stage
of the CTA will be combined and referred to as one design step for the purpose of clarity: D. Tasks
to Cognitive activities. This stage is actually responsible for the extraction and formulation of the
cognitive activities from the tasks under review. This design step is scrutinised and constructed in the
next subsection.
4.2.2 Application to this Research
CTA is focused mainly on the design of training systems, but also other systems where cognitive skills
are valuable input for the design of that system. CTA focuses, more specifically, on expertise. Expertise

can be described as knowledge on how to execute a particular skill set in the best way. The characteris-
tics of CTA match with the intention of eliciting the cognitive requirements within the design process,

since it focuses on the design of cognitive systems in a business process. Furthermore, the business
processes that are considered in this research are delineated to processes that are knowledge-intensive.
Hence, CTA is considered as relevant to this research.
To elicit the cognitive activities expressed (by a cognitive system) when executing particular tasks,
Cognitive Tasks Analysis seems thus to be a method that is able to achieve this, making it a valuable
method in the process of identifying the cognitive properties of business processes and thus valuable
for formulating the Cognitive Requirements of business processes.
As stated earlier, many variants of CTA methods exist. The challenge is to choose the method
that delivers the outcomes that suit the design of a systems as intended by the designer. To choose
wisely, for this research a couple of criteria are formulated to which the method should comply to:
45

Chapter 4. Cognitive Requirements of Business Processes
1. The method should be relatively easy to conduct for people that are no experts in psychology,
because this research constructs a design process intended for business system designers and AI
software experts.
2. The method should not be very time consuming, since it will be used in a business context where
time is costly.
3. The method should deliver a high probability of high quality outcomes, otherwise the design
process will not be very valuable.

Based on a literature search and brief assessment, Applied Cognitive Task Analysis (ACTA) is consid-
ered most appropriate in view of these three criteria (Clark et al., 2006; Militello and Hutton, 1998)

and therefore utilised in this part of the design process. ACTA is an approach where you sequentially
conduct three structured interviews: an interview for constructing a Task Diagram, an interview where
you audit the knowledge available and an interview where you extract additional knowledge through
a simulation.

Task Diagram Through the first interview, you develop a Task Diagram that gives a broad repre-
sentation of task and that specifically allows you to hone in on complex cognitive processes that

merit further consideration. This interview is intended to elicit a very broad overview of the
tasks, thereby ensuring that one will not try to extract detailed knowledge from the interviewee
(Rouse and Morris, 1986). Therefore, the delineation of each tasks should be limited to six steps
(Militello and Hutton, 1998), thereby complying to criteria 2.
Knowledge Audit The second interview yields a Knowledge Audit, which probes the expert on the
skills and knowledge applied to the tackle specific component tasks or decision points in the

overarching task process. It draws directly from the extensive research literature on expert-
novice differences and critical decision method studies (Hoffman et al., 1995; Klein et al., 1989;

Militello and Hutton, 1998). The knowledge audit is organised around knowledge categories
that have been found to characterise expertise: diagnosing and predicting, situation awareness,

perceptual skills, developing and knowing when to apply tricks of the trade, improvising, meta-
cognition, recognising anomalies, compensating for equipment limitations. It is a relatively non

labour-intensive method (thereby complying to criteria 1), focusing not on the extensive detail
and sense of dynamics such as the critical decision method (Klein et al., 1989). However, they do
provide enough detail to retain the appropriate context of the task (Militello and Hutton, 1998),
thereby complying to criteria 3.
Simulation Interview The third and last interview involves presenting the expert with a specific
and relevant scenario designed to elicit insight into the cognitive processes used by the expert in
the scenario context, i.e. conducting a simulation interview. Klein et al. (1989) and Howell, W.
C. and Cooke (1989) have asserted that identification and exploration of information surrounding
high consequence, difficult decisions can provide a sound basis for generation of effective training
and systems design. Simulation- and incident-based interviews have been used successfully in
many domains (Klein et al., 1989; Flanagan, 1954), all in all complying to criteria 3.
Furthermore, each of these interviews, i.e. steps generate a separate easy formatted and structured
outcome (Clark et al., 2006; Militello and Hutton, 1998). Also, ACTA is developed as a streamlined
46

Cognition Enhanced Business Processes
CTA method intended for use by instructional designers and systems designers rather than knowledge
engineers, cognitive psychologists, and human factors/ergonomics professionals (Clark et al., 2006),
thereby complying both to criteria 1.
To leverage the process steps of both Cognitive Task Analysis and Applied Cognitive Task Analysis,
the third process step of CTA is replaced by the three process steps of ACTA, resulting in a seven step
process which is suited to the goal of this part of this research. These seven steps are described and
elaborated in Appendix B.
4.2.3 Example by Means of Illustrative Case
To illustrate the conceptual notions and the CTA exercise described above, the CTA is performed
on case study one. First, preliminary knowledge is gathered about the tasks in the claim-assessment
process by means of conducting informal interviews and reading relevant documents. The following
insights are extracted in this step:
• A claim-assessor is asked to give advice on whether or not a claim should be reimbursed. Such
advice is always written down in text. An advice is furthermore always structured in three
parts: a general sentence indicating what is asked (‘a request for reimbursement for a patient of
age X, suffering from disease Y and treatment Z’), factual information about the patient, from
literature and regulatory frameworks, and the actual assessment of the claim in the form of a
line of reasoning supported by all relevant information. A proper advice can only be given if the
request is properly formulated.
• A request for reimbursement is mostly textual in form, sometimes supplemented with pictures,
casts, etc. In a request a proper formulated question must be present explicitly, in order to be
assessed.
Next, more in depth knowledge about what kind of information is dealt with during the execution of
the tasks, and which knowledge is required to perform the tasks, are examined. This step resulted in a
terminology to use for proper communication about the concepts and insights what type of knowledge
is required in order to be able to assess a claim: internal assessment frameworks, laws and regulations
regarding health care, professional groups standard protocols, communication with colleagues, medical
institutions, etc.
Third, an ACTA-interview is conducted with a claim-assessment expert of the company this case study
is performed at. This step created the most valuable insights for this research, since it addresses the
cognitive elements that are involved in performing a claim assessment. Note that only the tasks Assess
claim utilising protocols and Assess claim (manually) in the business process model in Figure 4.1 are
considered, as explained in Section 4.1.
• As stated in Section 4.2.2, first a Task Diagram is constructed together with the claim-assessment
expert. This resulted in the Task Diagram as visualised in Figure 4.2. For each step in the task,
the expert is asked to describe the step in detail, which information is needed to perform this step
and, most importantly, which cognitive functions he/she needs to perform this step properly (a
list of cognitive functions plus description is provided in advance of the interview). This first part

47

Chapter 4. Cognitive Requirements of Business Processes
of the ACTA resulted in a table with extracted cognitive steps performed by the claim-assessment
expert, which can be seen in Appendix B.

Gather the claim
information

Comprehend the
claim information

Extract
relevant and
valuable facts

Compose a
sound line of
argumentation

Draft the advice

Figure 4.2: Task Diagram from the Cognitive Task Analysis conducted in case study 1
• In the second part of the ACTA-interview, the expert is asked to reflect on a couple of probes
presented by the interviewer, the Knowledge Audit, to extract the cognitive functions that are
employed by people having experience assessing claims. For different aspects of expertise, the
expert was asked to formulate the cues and strategies used to perform the assessment of a claim
and what is difficult about it. The main results of this step include the extension of the insights
generated by the Task Diagram and the identification of three abstraction levels of a claim
assessment, denoted by the interviewee: micro (one claim specific), meso (claims that belong to
the same case) and macro (assessment of general treatments on national level). On meso level,
more than one claim assessment is considered and ultimately judged if these claims can be dealt
with similarly. All results of this second part of the ACTA-interview resulted in Appendix B.

• The third part of the ACTA-interview is the Simulation interview, resulting in more understand-
ing how the experts thinks when assessing a claim. For all events occurring in the simulation, the

expert is asked for each event which actions he/she performs, how he/she assesses the situation
at the time of the event, the critical cues that the experts uses when performing the action and
the potential errors a novice would be likely to make when he/she has to deal with this particular
event. The results of this third part of the ACTA-interview resulted in Appendix B.
The results of this interview are analysed, processed and formatted such that they are of value in the
next step of the CRDP of the conceptual design process. Due to the fact that all the results together
are a big amount of information, only an excerpt is visualised in Figure 4.3 but nevertheless illustrates
its purpose and value to this research. In this figure, in literature called a Cognitive Demand Table
(Militello and Hutton, 1998), the steps Extract relevant and valuable facts and Compose a sound line
of argumentation (meso level) are elaborated by describing their relevant difficult cognitive activities,
necessary cognitive skills, why these are difficult to perform, what the common errors are and finally
what cues and strategies are used by the claim handler.

48

Cognition Enhanced Business Processes

Step Difficult Cognitive activity Cognitive skills Why difficult? Common errors Cues and strategies used

Perception: percept the facts in the
information
Recognition: recognize the facts contained
in the information
Reasoning: reason which facts are
relevant for the judgement of the claim
Reasoning: reason which facts are
valuable for the judgement of the claim
Judgement: determine which facts are
relevant to come to a legit judgement of
the claim
Judgement: determine which facts are
valuable to come to a legit judgement of
the claim
Indicate which facts relevant for
the line of reasoning are missing
Find the necessary extra facts

Reasoning: reason which of the
characteristics of the claim are comparable
to a existing case
Recognition: recognise that this claim has
come up before
Meta-cognition - Reasoning: reason if the
similar claim has been treated already →
pattern recognition
Reasoning: reason which historic claim has
comparable characteristics (= similar case)
Recognition: recognise the line of
reasoning of a claim

Determine that a case can be
treated in the same way as a
similar claim

Judgement: determine that standardisation
is possible without loss of quality

Checking if the claims are indeed
the same

Know that a claim can be treated
in the same way as a similar claim

Learning: learn when a case can be
treated in the same way as a similar claim

Checking if the current claim is
indeed the same as a previous
one

Misinterpretation of the facts.
Incorrect match to a previous
claim.

Misjudgement of the equality of
claims
Do to this, lot of experience is
needed with the assessment of
claims, since only then patterns
could be noticed. One should be
able to relate the words/concepts
that are present in the information
are similar to a previous claim.
To reason what relevant and
valuable facts are missing, proper
knowledge about medical
concepts and claim assessment is
needed

Words and terms present in the
information.
Search through previous claims to
find a similar one.

Word and terms present in the
information.
Appeal to knowledge and
experience on medical concepts
and claim assessment.
Consideration of each fact.
Looking out for 'key' facts; facts
that often are important for the
assessment of the claim.

Words and terms present in the
information, and reason which
words and/or terms are likely to be
relevant
The value of the facts are not

assessed properly, leading to miss-
judgements.

The relevance of the facts are not

assessed properly, leading to miss-
judgements.

Wrong judgement, although based
on the correct facts.

Reasoning and judging about the
relevance and value of all facts in
the claim information is difficult,
mainly because these processes
are performed very iteratively. The
relevance and value of a fact can
constantly change when a new
fact is considered, resulting in a
(mental) list of facts sorted on their
relevance and value.

Scanning through all information,
paying attention to medical terms
and words describing the context
of the claim.
Not all facts are recognised in the
first shot, which can have several
reasons

Neglect relevant facts that should
be included.
Include irrelevant facts that should
not be included
To recognise the facts in the claim
information, one needs a proper
understanding of a wide variety of
medical concepts, master the
corresponding vocabulary
(knowledge) and how such claim
dossier is constructed (knowledge)

Reasoning: reason which facts are missing

Indicate which facts are relevant
for the line of reasoning
Indicate the facts within the
information

Know that a similar claim already
has been treated

The judgement of a claim by
establishing a line of reasoning
(Meso)
The extraction and listing of
relevant and valuable facts

Figure 4.3: Excerpt of the Cognitive Demand Table from the Cognitive Task Analysis performed in case study 1

49

Chapter 4. Cognitive Requirements of Business Processes
For the task Extract relevant and valuable facts, Figure 4.3 indicates that a difficult cognitive activity
is to report all facts in the information. This is due to the fact that a claim dossier often consists of
multiple documents, each with multiple pages, and these facts are ‘hidden’ in the information, spread

throughout the dossier. This difficult cognitive activity can be performed by the cognitive skills Per-
ception and Recognition; one should perceive (the facts in) the information and recognise the facts

contained in the information. A Cognitive Demand Table such as in Figure 4.3 thus provides informa-
tion which cognitive skills are executed in the task under review, indicated by the cognitive functions

listed.
In the case of the claim assessment process, different forms and degrees of expertise are noticed (see
Section 3.1). Both Claim handlers and SME’s are required to do training before starting their work,

and develop expertise handling claims and their assessment. However, the SME’s have a lot more in-
depth medical knowledge, whereas Claim handlers are experts themselves in assessing claims efficiently

and effectively. The detailed description of the illustrative case (based on case study 1) can be found
in Appendix B.
4.2.4 Intermediate conclusion
To summarise this section, Cognitive Task Analysis and Applied Cognitive Task Analysis can be
utilised to drill down the tasks in a business process to the cognitive activities expressed when these
tasks are executed. Thus, from a systems perspective, the cognitive activities that are executed to
perform the tasks in a business process, can be considered the lowest level of detail for describing the
cognitive properties of a business process.
Now the cognitive activities of tasks within a business process are identified and described, we are
looking for a way to formulate these cognitive activities more specific and formal, such that they can
be used more sophisticatedly in the matching process to the cognitive capabilities of AI software later
on in this research (see Chapter 6). Thus, the question that now raises is how could one formalise and
specify the cognitive activities as elicited in this section such that their level of granularity is lowered.
4.3 From Cognitive Activities to Cognitive Requirements
The previous section resulted in a (long) list of cognitive activities demanded for proper execution of
the tasks in a business process. In a later sub-design process of the conceptual design process, these
demanded cognitive activities should be matched to the capabilities that AI software can deliver. For
proper matching, these demanded cognitive activities and software capabilities should syntactically
and semantically be on the same level. One is therefore looking for a formal specification of these
cognitive activities, since their specificity will influence the likelihood of a complete and correct match
to one or more AI software capabilities.
4.3.1 Requirement Formulation
To overcome this hurdle, Requirement Engineering (RE) theory and its practical standard in ISO 15288
are considered valuable to use. Requirement Engineering is a systematic approach to come to system
requirements and software requirements. The latter is not in the scope of this research, since we are
exploring the conceptual design phase, not the systems development phase (see section Section 2.3).
50

Cognition Enhanced Business Processes
The system requirements consist of functional requirements that have to be met in order to complete
the task and process at hand (ISO/IEC/IEEE-42010, 2015).
Requirements are statements of what the system must do, how it must behave, the properties it
must exhibit, the qualities it must possess, and the constraints that the system and its development
must satisfy. The Institute of Electrical and Electronics Engineers (IEEE) defines a requirement as “a

condition or capability needed by a user to solve a problem or achieve an objective, a condition or ca-
pability that must be met or possessed by a system or system component to satisfy a contract, standard,

specification, or other formally imposed document” (Institute of Electrical and Electronics Engineers
(IEEE), 2011).

According to ISO 15288 standard of the field of Systems Engineering, the process of Requirement En-
gineering consists of 5 process steps: a Stakeholder Requirement Definition Process (1), a Requirement

Analysis Process (2), an Architectural Design Process (3), a Verification Process (4) and a Validation
Process (5).

The purpose of the Requirements Analysis Process is to transform the stakeholder, requirement-
driven view of desired services into a technical view of a required product that could deliver those

services. This process builds a representation of a future system that will meet stakeholder require-
ments and that, as far as constraints permit, does not imply any specific implementation. It results in

measurable system requirements that specify, from the supplier’s perspective, what characteristics it
is to possess and with what magnitude in order to satisfy stakeholder requirements.
Characteristics of good requirements according to the (Institute of Electrical and Electronics Engineers
(IEEE), 1998) are:
1. Unitary (Cohesive) The requirement addresses one and only one thing.
2. Complete The requirement is fully stated in one place with no missing information.
3. Consistent The requirement does not contradict any other requirements and is fully consistent
with all authoritative external documentation.
4. Non-Conjugated (Atomic) The requirement is atomic, i.e., it does not contain conjunctions.
E.g., ‘The postal code field must validate American and Canadian postal codes’ should be written
as two separate requirements: (1) ‘The postal code field must validate American postal codes’
and (2) ‘The postal code field must validate Canadian postal codes’.
5. Traceable The requirement meets all or part of a business need as stated by stakeholders and
authoritatively documented.
6. Current The requirement has not been made obsolete by the passage of time.
7. Unambiguous The requirement is concisely stated without recourse to technical jargon, acronyms
(unless defined elsewhere in the Requirements document), or other esoteric verbiage. It expresses
objective facts, not subjective opinions. It is subject to one and only one interpretation. Vague
subjects, adjectives, prepositions, verbs and subjective phrases are avoided. Negative statements
and compound statements are avoided.

8. Specify Importance Many requirements represent a stakeholder-defined characteristic the ab-
sence of which will result in a major or even fatal deficiency. Others represent features that may

51

Chapter 4. Cognitive Requirements of Business Processes
be implemented if time and budget permits. The requirement must specify a level of importance.
9. Verifiable The implementation of the requirement can be determined through basic possible

methods: inspection, demonstration, test (instrumented) or analysis (to include validated mod-
elling & simulation).

4.3.2 Application to this Research
Specifically, the Requirement Analysis Process of RE is found useful to formulate the cognitive activities
identified above into the cognitive requirements we are looking for, since that activity in Requirement
Engineering deals with the formulation of requirements. More specifically, the these requirements can
be seen as a refinement of the cognition skills, contextualised to the corresponding task and difficult
cognitive activity. Notice that the elicitation of the requirements is already taken into account in the
previous section by eliciting the cognitive activities of tasks in a business process.
The syntax and guidelines provided by the ISO 15288 is used here. Figure 4.4 shows the general
syntax formats of a syntactically sound requirement. These formats will be used for the formulation
of requirements, is illustrated by means of the case study previously used.
[Condition] [Subject] [Action] [Object] [Constraint]

EXAMPLE: When signal x is received [Condition], the system [Subject] shall set [Action] the signal x
received bit [Object] within 2 seconds [Constraint].
Or

[Condition] [Action or Constraint] [Value]

EXAMPLE: At sea state 1 [Condition], the Radar System shall detect targets at ranges out to [Action or
Constraint] 100 nautical miles [Value].

Or
[Subject] [Action] [Value]

EXAMPLE: The Invoice System [Subject], shall display pending customer invoices [Action] in ascending
order [Value] in which invoices are to be paid.
Figure 4.4: Types of syntax of requirements, adopted from Institute of Electrical and Electronics

Engineers (IEEE) (2011)

4.3.3 Example by Means of Illustrative Case

For each cognitive skill identified and presented in the Cognitive Demand Table as drafted in Sec-
tion 4.2, a corresponding requirement is formulated according to (one of) the syntax formats of Fig-
ure 4.4. These Cognitive Requirements are displayed in Figure 4.5.

52

Cognition Enhanced Business Processes

Step Difficult Cognitive activity Cognitive skills Cognitive Requirement Syntax used

Perception: percept the facts in the information

When the claim information is received, all claim related facts in the
claim information shall be identified

[Condition] [Subject] [Action]
[Object]

Recognition: recognize the facts contained in
the information

When the claim information is received, all claim related facts in the
claim information shall be categorised

[Condition] [Subject] [Action]
[Object]

Reasoning: reason which facts are relevant for
the judgement of the claim

When the facts in the claim information are categorised, the relevance
of every fact individual to the assessment of the claim shall be noted

[Condition] [Subject] [Action]
[Object]

Reasoning: reason which facts are valuable for
the judgement of the claim

When the claim information has been annotated, the cognitive system
shall state the value of every fact individual to the assessment of the
claim

[Condition] [Subject] [Action]
[Object]

Judgement: determine which facts are relevant
to come to a legit judgement of the claim

When the facts in the claim information are annotated, the relevance of
each fact shall be noted if it is high enough for a legitimate
assessment, in descending order

[Condition] [Subject] [Action]
[Object] [Constraint]

Judgement: determine which facts are valuable
to come to a legit judgement of the claim

When the facts in the claim information are annotated, the value of
each fact shall be noted if it is high enough for a legitimate
assessment, in descending order

[Condition] [Subject] [Action]
[Object] [Constraint]

Indicate which facts relevant for
the line of reasoning are missing
Find the necessary extra facts

Reasoning: reason which of the characteristics
of the claim are comparable to a existing case

When the facts are extracted from the claim information, the facts of
the claim that appear in previous assessed claims shall be noted

[Condition] [Subject] [Action]
[Object]

Recognition: recognise that this claim has come
up before

When the facts are extracted from the claim information, the previous
assessed claim(s) which are similar to this claim shall be noted

[Condition] [Subject] [Action]
[Object]

Meta-cognition - Reasoning: reason if the similar
claim has been treated already → pattern
recognition

A claim that is already assess before shall be presented if it comprises
similar characteristics

[Subject] [Action] [Value]

Reasoning: reason which historic claim has
comparable characteristics (= similar case)

When the facts in the claim information are related to previous claims,
which previous claims have the same properties as the current claim
shall be determined

[Condition] [Subject] [Action]
[Object]

Recognition: recognise the line of reasoning of a
claim

When the facts in the claim information are annoteted, which
previously assessed claims have a similar line of argumentation shall
be indicated

[Condition] [Subject] [Action]
[Object]

Determine that a case can be
treated in the same way as a
similar claim

Judgement: determine that standardisation is
possible without loss of quality

When this is possible, a standardised protocol to assess future claims,
maintaining assessment quality levels shall be suggested

[Condition] [Subject] [Action]
[Object]

Know that a claim can be treated in
the same way as a similar claim

Learning: learn when a case can be treated in
the same way as a similar claim

When this is possible, the current claim shall be assessed using the
line of argumentation of a previous claim

[Subject] [Action] [Value]

When the facts in the claim information are annotated, the relevant
facts that are not present in the claim information shall be suggested,
considering the claim assessment

[Condition] [Subject] [Action]
[Object]

The judgement of a claim by
establishing a line of reasoning
(Meso)

Know that a similar claim already
has been treated

Extract and list relevant and
valuable facts

Indicate the facts within the
information

Indicate which facts are relevant
for the line of reasoning

Reasoning: reason which facts are missing

Figure 4.5: Formulation of Cognitive Requirements for the cognitive activities found in Section 4.2

53

Chapter 4. Cognitive Requirements of Business Processes
Every requirement is constructed using the general notion of the cognitive skill, the cognitive activity
it is expressed in and the step of the task. The cognitive requirement is a refinement of the cognitive
skill, contextualised in its corresponding cognitive activity and step.
For example, the step Extract and list relevant and valuable facts, the cognitive activity Report the
facts in the information and the cognitive skill Perception: perception of the facts in the information
result in the cognitive requirement When the claim information is received, all claim related facts in
the claim information shall be identified. For constructing this requirement, one should formulate the
cognitive skill as it is expressed in the cognitive activity and corresponding step. This contextualises
the cognitive requirement. Furthermore, one should phrase the cognitive requirement as a condition
that something or someone can fulfil by providing the right functionality. This makes the requirement
sound like an obligation that is achieved or not.
4.3.4 Intermediate Conclusion
Briefly summarising, this section provided the means to refine and formulate the cognitive activities
from the Cognitive Task Analysis into cognitive requirements. This process is from here referred to as
the design step E. Cognitive activities to Cognitive requirements. These cognitive requirements denote
the cognitive skills in a particular activity that are needed for proper execution of the task. This is
done in such a way that they are ready to be matched with the capabilities of AI software.
4.4 Stage 1 of Conceptual Design Process
Concluding, the steps described in this section can be merged in the first part of the design process,
addressing the cognitive requirements of a business process. Figure 4.6 present the five design steps
of the CRDP. This chapter described the part of the design process that is related to the first and
second step in the conceptual design phase of Dym and Little (2010), and entailed the extraction of
functions (cognitive activities that have to be performed for executing a task) and specification of the
cognitive requirements of the tasks present in a business process. Hence, it delivers the CRDP of the
conceptual design process as envisioned in this Chapter 1 and 2. This chapter concludes by providing
a description of the CRDP and assess it on the criteria stated in Chapter 2.
4.4.1 Description of the Cognitive Requirements Design Process
The first design step of the CRDP comprises the decomposition of a business process to its tasks
and roles (in Figure 4.6 as A). This step involves the use of Business Process Modelling and the
formulation of stakeholder requirements, as elaborated in Section 4.1. After a qualitative assessment
of the identified tasks, the tasks that seem to be essential for the business process and are driven by
cognition are selected for further examination in the conceptual design process. In the second design
step of the CRDP, preliminary domain knowledge used in the selected tasks is collected (B). This
step involves the identification of experts, the analysis of documents and conduction of interviews, all
relevant to the tasks under review. The next design step of the CRDP comprises the identification of
knowledge representations (C). This step involves the decomposition to sub-tasks and identification of
all relevant knowledge and information that is needed for the execution of the task. This information is
a prelude to the fourth design step in the CRDP, which comprises the extraction of cognitive activities
from the tasks under review (D). This step involves the three subsequent interviews described by the
54

Cognition Enhanced Business Processes

E. Cognitive activities to
Cognitive requirements
Utilise the Requirements Analysis
Process of Requirement Engineering
to refine the cognitive demands of the
tasks and formulate them as
requirements one by one
:
 Select an appropriate requirement
syntax considering the level of depth
available in the CDT  Refine and formulate a cognitive
requirement for every cognitive skill
in the CDT

D. Task to Cognitive
activities
1. Apply the Applied Cognitive Tasks
Analysis
(ACTA) to elucidate required
knowledge for executing the tasks
:
 Construct a Task Diagram to elicit
the cognitive steps of each sub
-task
 Execute a Knowledge Audit to
identify the most important cognitive
elements of the task  Conduct a Simulation Interview to
understand the cognitive processes
.
2. Analyse and verify data acquired 3. Format results in a Cognitive
Demands Table
(CDT
)

A. Business Process to Tasks 1. Create an overview of the business
process under review
:
 Decompose the process in sub
-
processes and tasks and construct a
process model
(BPMN
)
 Identify roles and stakeholders 2. Formulate the requirements of
stakeholdersfulfilling a role in the
business process
:
 Elicitstakeholder requirements  Define stakeholder requirement  Analyse and maintain stakeholder
requirements throughout the design
process

C. Knowledge representa
-
tions identification
For each task, identify the sub
-tasks
and types of knowledge required to
perform it
:
 Identify sub
-tasks of each task
 Construct flow charts to capture
procedural knowledge  Construct a concept map to
capture declarative knowledge  Execute a Learning hierarchy
analysis

B. Preliminary knowledge
collection
For the business process under
review, collect preliminary domain
knowledge
:
 Identify experts to participate in
the knowledge elicitation process
.
 Execute Document Analysis  Observe the process while being
executed  Conduct Unstructured interviews

Figure 4.6: Schematic representation of the design process to formulate cognitive requirements, using the concepts from Chapter 4

55

Chapter 4. Cognitive Requirements of Business Processes
Applied Cognitive Task Analysis, presenting its results in a Cognitive Demands Table. Design process
steps B, C and D are elaborated in Section 4.2 of this chapter. Finally, the cognitive activities identified
in D are refined and reformulated to cognitive requirements in the fifth and last design step of the
CRDP (E). This design step entails the use of the rules for the formulation of system requirements
from Requirement Engineering, which is elaborated in Section 4.3.
4.4.2 Assessment of the Cognitive Requirements Design Process
To assess the suitability of the CRDP constructed and described in this chapter, the criteria stated in
Section 2.2 are elaborated below regarding the CRDP.
Universally applicable Both Business Process Modelling and Cognitive Task Analysis as concepts
used in he CRDP (step A respectively D) have the characteristic to be object independent. That
is, they can both be used to analyse respectively all business processes and all tasks. Requirement
Engineering (step E), the third concept used in the CRDP, is a general design concept that can
be used in every design exercise, domain or object independent. Hence, the CRDP is considered
to be universally applicable to business processes.
Methodical in essence The CRDP entails a number of steps to be followed by the designer. They
are constructed such that they together form a systematic, methodical procedure to formulate

cognitive requirements of the business process under review. These steps are ought to be for-
mulated in such a way that it is understandable, providing a clear procedure for the designer

to follow. This is the very reason why in the CRDP is chosen for the Applied Cognitive Task
Analysis-method (as argued before in Section 4.2).
However, the CRDP produces to some extent different outcomes, i.e. designs, when executed by
different people. That is, the resulting design will change if the CRDP is not executed by the

same person. One can expect changes in the part of the CRDP where the Cognitive Task Anal-
ysis is performed (step D), since this concept has a relatively strong personal bias. It depends

on the questions the designer asks, the perspective that the designer has towards the tasks and
cognitive activities, the persons to be interviewed, et cetera, that influence the outcome of the
ACTA. Efforts to reduce this variability in the design caused by the personal bias of the ACTA
are therefore advisable, such as conducting the ACTA with more than one designer. Formulating
the cognitive requirements has less possible bias when executed by different people, due to the
syntax that is provided with the method itself (step E). However, still some variability can be
present in the quality of the formulated requirements, which is for a part due to the knowledge
level of AI technology. Designers with more AI knowledge can formulate the requirements in
such a way that they match the cognitive capabilities later on in the design exercise more easily
than designers with less knowledge about AI. The variation that here can occur is thus in essence

more about fitness of the cognitive requirements to future cognitive capabilities, and not experi-
enced until the matching sub-design process of the CDP (see Chapter 6). The Business Process

Modelling is, in contrast to the other two concepts used in the CRDP, considered to deliver
relative similar results when conducted by different persons. This is because of its formalised

56

Cognition Enhanced Business Processes
language and due to the fact that its outcomes are used in the CRDP not in a very detailed way,
in contrast to the ACTA and RE.
Flexible but robust In reality, experienced in the case study used to construct the CRDP, a business
process does not always operate in the way it is thought of. The CRDP accounts for this variation
on the work flow, since the ACTA provides room to manoeuvre to extract the cognitive activities
from a business process (step D), if the outcomes of the Business Process Modelling exercise
would be of less quality (step A). This is due to the designer’s gut feeling to indicate what is
most important.
On the other hand, all steps (A to E) can still be executed when disruptions in the design
process occur, and not jam. One can think of that a member of the design team can be replaced
by someone else, without damaging the design process or design. Knowledge gained can be
passed on due to the fact that the steps produce proper documentation (written text, figures and
schematics, tables, et cetera). The CRDP is therefore considered to be robust to external effects.
Context-aware In general, executing projects within organisations that demand change and affect

people have to deal with resistance towards this change (de Bruijn and Heuvelhof, 2012). Fur-
thermore, as mentioned in Chapter 1, people are hesitant about the rapid development of AI

technology. A CDP that is accommodated to these phenomena is considered to be less affected

by their negative effects. For example, if people are resisting themselves to the project automat-
ing parts of their work flow by implementing AI, they will not contribute to the design process

in a productive way. This can lead to designs of. Thus, regarding the context-awareness of the
CDP, the CRDP is ought to be constructed such that it accommodated to this context. The
design steps A, B, C and D of the CRDP is the part that satisfies to this criteria, since these
steps actively pursue collaboration with stakeholders in the business process and let them be
part of the design process. For example, in step A, the designer interviews stakeholders to elicit
their ‘stakeholder requirements’, where in step D the ACTA is grafted on extracting the cognitive
activities executed in the tasks in the process thought the people that actually perform them,
interviewing them personally. Instead, the CRDP could have also consists of methods that are

less stakeholder-oriented. Hence, the CRDP is considered to be context-aware in a process man-
agement or expectation management way.

Next to this, the availability of data is in executing the CRDP is considered as an important
issue in the case study. Namely, the CRDP is part of the CDP that provides the first pointers
how the eventual design is going to look like. Thereby, it is thus also indicates what data can be
used in this (conceptual) design. Especially the fact if relevant data is available in the business
process or organisation is important, and to what extent it is complete. If this data is not present
and/or incomplete to large extent, the designer(s) are ought to be hindered in the the design
process in moving towards that particular design. Thus, data – and especially data availability
– are in the case study performed here considered as a constraint in the CRDP.
This chapter provides an answer to the second research issue (RI2) raised Chapter 1 in Figure 1.4:
Establish requirements to enhance the cognition of business processes. The Cognitive Requirement
Design Process (CRDP) created is thereby the first component of the conceptual design process as
57

Chapter 4. Cognitive Requirements of Business Processes
envisioned in Section 2.2. It provides a way to formulate cognitive requirements from a given business
process. As described above, the CRDP thereby recognises the values (specified in criteria) elaborated
in Section 2.2 to a certain extent. To what extent precisely is not researched here, and therefore subject
for further research (see Chapter 8).

58

5 | Cognitive Capabilities of Artificial In-
telligence Software

This chapter presents the research that is conducted on the third research issue (RI3) raised Chapter 1
in Figure 1.4: Establish the functions to enhance the cognition of business processes. That is, it describes
how the cognitive capabilities (CC) of AI software can be formulated. Thereby, it creates the second
part of the conceptual design process, the Cognitive Capabilities Design Process (CCDP) (see
Section 2.2). This chapter shows a way to elicit the cognitive capabilities of AI software, by sequentially
describing how to elicit these cognitive capabilities, illustrating the procedure by applying it to the
case study introduced in Section 3.1.
Firstly, Section 5.1 posts some general notions on these Cognitive Capabilities of software, and
especially on the role of AI software in this story. Next, Section 5.2 elaborates on how these Cognitive
Capabilities can be extracted and formulated from AI software and illustrates this by means of the case
study. This chapter then presents and describes the second part of the conceptual design process in
Section 5.3. This last section furthermore assesses and discusses the constructed CCDP on the criteria
stated in Chapter 2, which thereby concludes this fifth chapter.
5.1 Extracting Cognitive Capabilities of AI software
The conceptual design process that is drafted in this research is meant for the design of computerised
software systems. As argued in Chapter 2 and Chapter 3, this research takes a more human-centric
perspective on the design of ‘intelligent’ computer systems by embracing the cognitive systems theory.
Therefore, when talking about this software, it does not consider the capabilities of software systems
right away, since the human cognitive capabilities are taken into account also. This research focuses
on the enhancement of human cognition by automation, delivered by computer systems. The question
that rises is what software can deliver to this enhancement.
Currently, AI software has gained great attention by the media and business and is developing rapidly.
Artificial Intelligence is considered to be the field of study that today delivers the most sophisticated
intelligent systems. Therefore, AI software holds strong ties to this research. AI software rises from

a (purely) machine-intelligence perspective. Hence, AI software does not fully match with the per-
spective taken by this research. However, this research recognises that AI software comes closest to

the suppositions regarding the design of cognitive software systems. This is mainly because no other
clear field of study developing intelligent software is currently present, i.e. insisting its applicability to

59

Chapter 5. Cognitive Capabilities of Artificial Intelligence Software
research and practice.
This chapter tries to bridge the gap between the cognitive requirements of business processes and what
is currently possible designing and developing automated cognition, in the existence of AI software.
This type of software is able to power automation, which is recognised and appreciated by this research.
On the other hand, the software is not able to deliver high-level cognitive functionalities, as referred
to in Chapter 2. It mainly focuses on algorithmic problems.
This is considered as a big limitation to the design of cognitive enhanced business processes as
studied here. However, in the light of this research, it is not perceived as a problem, since its structured
approach provides the possibility to pin point these mismatches between the cognitive requirements
of business processes and the cognitive capabilities of AI software. It is thus possible that certain
cognitive requirements cannot be met by the cognitive capabilities delivered by AI software today.
Although, if technology develops over time, the conceptual design process will be able to deal with this
change. That is, it can still be used unconcerned about such possible changes in technology, since the
process is not adjusted to a level of technology, i.e. the process is technology independent.
Above is explained why AI software is of interest for this research. In the remaining of this chapter,
the process of extracting and formulating the cognitive capabilities of AI software are examined and
elaborated.
5.2 Analysis of IBM Watson
To extract the capabilities of software to achieve a particular goal, one first needs to select the software
that will be focused on. Such a software package often consists of multiple services, which on their
turn deliver multiple functionalities. Each of these functionalities can be employed to contribute to
achieve a particular (business) objective. A description of services and the functionalities is therefore
desirable. The first step of the CCDP thus entails the selection of software, and the identification and
description of its services. From here, this design step is referred to as A. Selection of AI software.

When talking about AI software, these functionalities are considered to result in intelligent be-
haviour of the software system. However, this research aims to leverage the functionalities of AI

software to support, i.e. enhance the cognitive capabilities of a human being, thereby relying on the
(composition) of a cognitive system. Hence, the cognitive capabilities of AI software can be identified
according to the cognitive systems functions as described in Chapter 3. That is, the functionalities
of an AI software package can be translated into cognitive capabilities by typifying each functionality
according to the cognitive functions of a cognitive system.
From the process of designing these cognitive capabilities of AI software, the research found that by
first determining a specific related field of study of each functionality of the software, the translation
into cognitive capabilities is more easy and also more logical. Furthermore, to make sure the cognitive
capabilities can be properly matched against the cognitive requirements extracted in Chapter 4, it

60

Cognition Enhanced Business Processes

Entity Extraction The identification and classification of entities within a text. Exam-
ples of entities are people, places and companies: US President Barack Obama[person],

Google[company], The Netherlands[country].
Keyword Extraction The extraction of topic keywords within a text. Examples of keywords
are US President Barack Obama, Google, The Netherlands.
Concept Tagging The classification of concepts / topics relating to the overall text, which do
not necessarily have to be mentioned. An example is a text mentioning BMW and Audi is
about ‘the automotive industry’.
Taxonomy Classification The identification of one or more taxonomic categories related to
the overall text. An example here is ‘automotive/brands/german/...’.
Relationship Extraction The identification of Subject-Action-Object relations within a text.

An example of relationship extraction is ‘Lars[subject] is a student[action] of Delft Univer-
sity[object]’.

Speech to Text The recognition and conversion of spoken language into text.
Visual Recognition The identification of objects in a visual image.

Trade-off Analytics The ability to visually balance multiple objectives such that it can sup-
port decision making.

Knowledge Studio The ability to train a machine learning model, such that it can be used in
a particular setting.
Figure 5.1: Selection of IBM Watson’s services and a description of their functionality

is desirable to formulate the cognitive capabilities similarly to these cognitive requirements. Conse-
quently, Requirement Engineering seems again valuable in this design exercise.

In the claim-assessment process case, the goal is to develop a software system that is able to support or
enhance the cognitive functions performed to execute the task in the process. In this case study, the AI
software IBM Watson is selected to deliver these capabilities and is therefore examined in this section.
Besides that this software package is actually used in a development project at a health insurance
company, IBM’s software Watson is considered to be the leading AI business software service at this
moment (Hof, 2016). Therefore, executing this design exercise with IBM Watson as subject is a logical
choice.
First, the software is decomposed in the different modules or software services it consists of.
IBM Watson provides twenty unique services companies can utilise. These services are divided in five
categories: Language, Speech, Vision, Data Insights and Knowledge Studio. Each service is briefly
described in Appendix C. Figure 5.1 presents a list of a couple of these services and a description of
their functionality.
The functionalities as illustrated above can be categorised into distinct sub-fields of study within the

Artificial Intelligence field of study. The following list in Figure 5.2 expands the list above, by indicat-
ing these categories. This is the second step of the CCDP, further referred to as B. Identification of

Methods and Techniques.

61

Chapter 5. Cognitive Capabilities of Artificial Intelligence Software

Functions
(according to IBM) Example Formal concept(s) ..is a sub-field of.. ..is a sub-field of.. ..is a sub-field of.. Underlying techniques

Language
Entity Extraction Identifying people, companies, cities,
geographic features and other typed

entities from your HTML, text or web-
based content

Lars Reeker = Person Entity linking Information Retrieval Information Extraction Natural Language Processing Classification, Conditional
random fields (CRFs), Rigid
designator

Keyword Extraction Extracting keywords from text that can
be used to index content, generate tag
clouds, and more

Lars, student, Delft, University of
Technology

Keyword Extraction Information Retrieval Information Extraction Natural Language Processing Count

Concept Tagging Abstraction, understanding how
concepts relate and tagging
accordingly

Education, Technology,
Profession

Latent-semantic indexing Information Retrieval Information Extraction Natural Language Processing Latent-semantic indexing
(SA), singular value
decomposition (SVD),
Clustering

Taxonomy Classification Assigning the most likely topic
category (baseball, mobile phones,
etc.) to a text

this article corresponses to the
taxonomy
education /academic/university

Classification Multivariate statistics

- Natural Language Processing Classification

Relationship Extraction Linguistic analysis of input text Lars [is a] student of Delft
University of Technology

Relationship extraction Information Retrieval Information Extraction Natural Language Processing Classification

Speech
Speech to Text Convert speech in multiple languages

into text

Lars is a student (speech) to Lars
is a student (text)

Machine translation Computational linguistics

- Natural Language Processing Multiple approaches: rule-
based, statistical, etc.

Vision
Visual Recognition Provides scores for relevant classifiers
representing things such as objects,
events and settings of an image

A picture of a tiger with 99%
confidence

Image processing
Pattern recognition
Statistical Inference

Signal processing
-

- Wide variaty of techniques:
Statistical signal processing,
Functional Analysis, etc.

Data Insights
Tradeoff Analytics Helps people make decisions when
balancing multiple objectives

Visual tradeoff support tool, multi-
critera, filtering and selection

Visual analytics Information visualisation
-

- Multiple: Argument map,
software visualisation

Watson Knowledge Studio Supervised learning environment [Lars

Reeker]person_1/student_at_uni
_1 studies at [TU
[Delft]loc
/uni_1 . [He]person_1 is
working on its graduation thesis.
Entity linking
Keyword extraction
Relationship extraction

Information Retrieval Information Extraction Natural Language Processing Machine learning

AI commercial software
(IBM Watson)

Figure 5.2: IBM Watson’s services, annotated with their related field of study

62

Cognition Enhanced Business Processes
With these categories and the description of their meaning, the services within IBM Watson can be
scrutinised to ascertain the set of ‘cognitive’ functionalities they can provide when leveraged properly

when applied. Utilising the cognitive processes that can be performed by a cognitive system as pre-
sented in Chapter 3, these ‘cognitive’ functionalities of IBM Watson can be tagged to indicate their

potential support to cognition within a business process. Recall the schematic visualisation of a cog-
nitive system form Chapter 3 and presented again in Figure 5.3 for convenience. This figure indicates

the different cognitive function of a cognitive system.

Figure 5.3: Schematic illustration of a System embodied in its Environment, denoting the Cogni-
tive and Physical aspects of control of these two concepts – from Heylighen and Joslyn (2001)

With the knowledge from Chapter 3 about cognitive systems and the cognitive processes such
a system can express, the list in Figure 5.2 can be expended by adding the corresponding cognitive
processes from Figure 5.3 to each of the functionalities identified. The result of this exercise is presented
in Figure 5.4. This is the third step of the CCDP, further referred to as C. Establish Cognitive
Functionalities of Services.

To illustrate this process, the service Entity Extraction is used as an example. This service com-
prises the classification of all entities which are mentioned in the text. It identifies and classifies these

entities, and finally extracts them. Thus, this service can be used to identify the entities that are in
the text in an automated way. Thereby it serves as a step to get insight what the text is about, for
example which persons or organisations play a role in the text. This information can thereby be used
to analyse the text even further. Relating this functionality to the cognitive functions identified in
Chapter 3, this services can be categorised as Perception. To perform this identification process, the
system has to have a knowledge base which consists of entities and their corresponding entity type,
for example IBM: Company. Since the service needs a knowledge base to perform its functionality,
this services can be categorised as Knowledge as well. It thus recognises entities in the text using its
knowledge base and scanning through the text at hand.

Functions
(according to IBM) Example Formal concept(s) ..is a sub-field of.. Underlying techniques Cognitive functionality

Language
Entity Extraction Identifying people, companies, cities,
geographic features and other typed

entities from your HTML, text or web-
based content

Lars Reeker = Person Entity linking Information Retrieval Classification, Conditional
random fields (CRFs), Rigid
designator

Perception, Recognition, Knowledge

Keyword Extraction Extracting keywords from text that can
be used to index content, generate tag
clouds, and more

Lars, student, Delft, University of
Technology

Keyword Extraction Information Retrieval Count Perception

Concept Tagging Abstraction, understanding how
concepts relate and tagging
accordingly

Education, Technology,
Profession

Latent-semantic indexing Information Retrieval Latent-semantic indexing
(SA), singular value
decomposition (SVD),
Clustering

Recognition, Knowledge,
Comprehension

Taxonomy Classification Assigning the most likely topic
category (baseball, mobile phones,
etc.) to a text

this article corresponses to the
taxonomy
education /academic/university

Classification Multivariate statistics Classification Reasoning, Knowledge

Relationship Extraction Linguistic analysis of input text Lars [is a] student of Delft
University of Technology

Relationship extraction Information Retrieval Classification Recognition, Comprehension

Speech
Speech to Text Convert speech in multiple languages

into text

Lars is a student (speech) to Lars
is a student (text)

Machine translation Computational linguistics Multiple approaches: rule-
based, statistical, etc.

Language, knowledge

Vision
Visual Recognition Provides scores for relevant classifiers
representing things such as objects,
events and settings of an image

A picture of a tiger with 99%
confidence

Image processing
Pattern recognition
Statistical Inference

Signal processing Wide variaty of techniques:
Statistical signal processing,
Functional Analysis, etc.

Perception, Recognition

Data Insights
Tradeoff Analytics Helps people make decisions when
balancing multiple objectives

Visual tradeoff support tool, multi-
critera, filtering and selection

Visual analytics Information visualisation Multiple: Argument map,
software visualisation

Goal generation, Judgement

Watson Knowledge Studio Supervised learning environment [Lars

Reeker]person_1/student_at_uni
_1 studies at [TU
[Delft]loc
/uni_1 . [He]person_1 is
working on its graduation thesis.
Entity linking
Keyword extraction
Relationship extraction

Information Retrieval Machine learning Learning

AI commercial software
(IBM Watson)

Figure 5.4: IBM Watson’s services, annotated with related cognitive functions from cognitive systems theory

64

Cognition Enhanced Business Processes
Figure 5.4 provides an overview of the services from IBM Watson and the corresponding cognitive
functionalities. That remains this sub-design process with the step to reformulate these cognitive
functionalities into cognitive capabilities. This is done based on the same principles as the cognitive
activities of a business process are reformulated into cognitive requirements, argued and illustrated in
Chapter 2. The same syntax is used here, therefore recall Figure 4.4 which is adopted form Institute of
Electrical and Electronics Engineers (IEEE) (2011). In contrast to Chapter 4, the subject is here the
selected AI software package. The result of the reformulation of cognitive functionalities into cognitive
capabilities is illustrated in Figure 5.5. Thus, the last step of this design exercise to identify and
formulate the cognitive capabilities of IBM Watson software is hereby executed, referred to as the
design step D. Cognitive Functionalities to Cognitive Capabilities.
To illustrate this process, again the service Entity Extraction is used as an example. As mentioned
before, this service entails the identification and classification of entities in a textual document. Its
functionalities are categorised with the cognitive functions Perception and Knowledge. Formulating
the cognitive capability per service is done by taking into account both cognitive functions. Together
with its brief description, these cognitive functions are formalised in such a way that a system that uses
this service, is able to perform its functionality. In this example, the system will have the functionality
to perceive and recognise the entities in a textual document, using a knowledge base or dictionary to

denote the known entities. Formulating this functionality in one sentence delivers its cognitive capa-
bility, which is worded as ‘The system can denote (all) entities within a textual document’.

65

Chapter 5. Cognitive Capabilities of Artificial Intelligence Software

Functions
(according to IBM) Example Formal concept(s) Underlying techniques Cognitive functionality

Cognitive capability
(The system...)

Language
Entity Extraction Identifying people, companies, cities,
geographic features and other typed

entities from your HTML, text or web-
based content

Lars Reeker = Person Entity linking Classification, Conditional
random fields (CRFs), Rigid
designator

Perception, Recognition, Knowledge ...can denote (all) entities within a
textual document

Keyword Extraction Extracting keywords from text that
can be used to index content,
generate tag clouds, and more

Lars, student, Delft, University of
Technology

Keyword Extraction Count Perception ...can identify all keywords within a
textual document

Concept Tagging Abstraction, understanding how
concepts relate and tagging
accordingly

Education, Technology,
Profession

Latent-semantic indexing Latent-semantic indexing
(SA), singular value
decomposition (SVD),
Clustering

Recognition, Knowledge,
Comprehension

...can denote relevant concepts from a
textual document

Taxonomy Classification Assigning the most likely topic
category (baseball, mobile phones,
etc.) to a text

this article corresponses to the
taxonomy
education /academic/university

Classification Classification Reasoning, Knowledge ...can denote the taxonomy classes a
textual document corresponds to

Relationship Extraction Linguistic analysis of input text Lars [is a] student of Delft
University of Technology

Relationship extraction Classification Recognition, Comprehension ...can recognise relationships between
enitities mentioned in a textual
document

Speech
Speech to Text Convert speech in multiple languages

into text

Lars is a student (speech) to
Lars is a student (text)

Machine translation Multiple approaches: rule-
based, statistical, etc.

Language, knowledge ...can translate a textual document into

another language

Vision
Visual Recognition Provides scores for relevant
classifiers representing things such as
objects, events and settings of an
image

A picture of a tiger with 99%
confidence

Image processing
Pattern recognition
Statistical Inference

Wide variaty of techniques:
Statistical signal processing,
Functional Analysis, etc.

Perception, Recognition ...can recognise objects in a static

image

Data Insights
Tradeoff Analytics Helps people make decisions when
balancing multiple objectives

Visual tradeoff support tool, multi-
critera, filtering and selection

Visual analytics Multiple: Argument map,
software visualisation

Goal generation, Judgement ...can support decision making through
visual mathematical representation
decision criteria

Watson Knowledge Studio Supervised learning environment [Lar

Machine learning Learning ...can learn from previous textual
documents to improve its linguistic
functionalities

AI commercial software
(IBM Watson)

Figure 5.5: IBM Watson’s services and their corresponding cognitive capabilities

66

Cognition Enhanced Business Processes

5.3 Stage 2 of Conceptual Design Process
Summarising the steps illustrated in the previous section, the second part of the envisioned conceptual
design process can be drafted. This part is visualised in Figure 5.6 and thus entails the identification

and specification of the cognitive capabilities of selected AI software, referred to as the CCDP sub-
design process. This chapter concludes by providing a description of the CCDP and assess it on the

criteria stated in Chapter 2.
5.3.1 Description of the Cognitive Capabilities Design Process
The first design step of the CCDP comprises the selection of AI software package for further study,
and its decomposition to all services it offers to the user, i.e. developer of the AI driven system (A in
Figure 5.6). For every service in the software package, the methods and techniques are identified in the
second step (B). This is done to get an idea how the service actually works and to which field of study
its part of. That provides the designer with information what functionality the service can deliver when
implemented. This knowledge is needed in the third design step of the CCDP (C), which comprises
the formulation of the cognitive functionality of the service. This is done based on the principles of
cognitive systems theory elaborated in Chapter 3. The last step of this sub-design process entails the
refinement and formulation of these cognitive functionalities to cognitive capabilities (D). This is done
via a design exercise itself, leveraging the rules for the formulation of requirements in Requirement
Engineering.

67

Chapter 5. Cognitive Capabilities of Artificial Intelligence Software

D. Cognitive Functionalities
to Cognitive Capabilities
Utilise the Requirements Analysis
Process of Requirement Engineering
to refine the cognitive
functionalities found in
C of each
service and formulate them as
capabilities one by one:  Analyse the cognitive
functionalities and formal concept
of each service  Formulate a cognitive capability
for every cognitive functionality

A. Selection of AI Software 1. Through a business focussed
assessment, select the AI software
to consider in this stage
.
2. For the selected AI software
:
 Identify the distinct services of
the software
.
 Describe the distinct services of
the software
.

C. Establish Cognitive
Functionalities of Services
Formulate the distinct
functionalities that comprise some
form and degree of cognition of the
services identified under
A using the

information from

B and cognitive

systems theory
:
 For every service
, analyse the
underlying techniques it utilises  Categorise each service with at
least one cognitive function

B. Identification of Methods
and Techniques
1. For each software service
identified under
A
:
 Identify the methods
(formal
concepts) it utilises  Describe the application of these
methods in the service 2. For each method utilised in the
software services
:

 Identify the techniques it utilises  Describe the application of these
techniques

Figure 5.6: Schematic representation of the design process to formulate cognitive capabilities, using the concepts from Chapter 5

68

Cognition Enhanced Business Processes

5.3.2 Assessment of the Cognitive Capabilities Design Process
To assess the suitability of the CCDP constructed and described in this chapter, the criteria stated in
Section 2.2 are elaborated below regarding the CCDP.
Universally applicable The CCDP extracts the cognitive capabilities of a given AI software package.
In the case study used in this research to construct the CCDP, IBM Watson is the software
considered to deliver AI technologies. In step A, the CCDP specifically focuses on selecting and
identifying AI software to be considered in the design process. In the subsequent steps, the CCDP
formulates cognitive capabilities of this chosen AI software packages by analysing the methods
and techniques it comprises, and thus independently of any other software. Hence, the CCDP is
considered to be universally applicable to AI software suites.
Methodical in essence The CCDP steps are formulated in such a way that a designer can produce
cognitive capabilities of an AI software suite in a systematic way: from the services of the
software, to methods used in the services to the techniques that power these methods, selecting
cognitive functions and formulate cognitive capabilities. Hence, the CCDP is considered to be
methodological in essence.
However, the CRDP produces to some extent different outcomes, i.e. designs, when executed by
different people. That is, the resulting design will change if the CCDP is not executed by the
same person. Step A en B are considered to result in similar outcomes, due to the factuality
of identifying and describing the software’s services, methods and techniques. Knowledge of AI
technology does play a role here, as it will decrease the time needed by the designer to go through
steps A and B. On the contrary, step C is more driven by the designer’s way of thinking and
perspective on cognition. Therefore, this step is considered to be affected by personal bias of the
designer. Formulating the cognitive capabilities in step D has less possible bias when executed by
different people, due to the syntax that is provided with the Requirement Engineering method
that is used here. However, still some variability can be present in the quality of the formulated
requirements, which is for a part due to the knowledge level of AI technology.
Flexible but robust The CCDP is the part of the CDP that elicits the cognitive capabilities of the
AI software selected. Due to the systematic procedure to formulate these cognitive capabilities,
the CCDP allows for changes in the software under review during the design process. Although
the steps in the CCDP are subsequent to each other, the designer can execute the CCDP in an
iterative way. Therefore, the CCDP is considered to be able to embrace the changes in a software
package and adjust its design to.
However, if the software suite will change in such a way that it is not functioning anymore, the
design process will jam. When this happens, the CCDP can be re-executed from start using a
different software package. The progress made using the old software suite can not be re-used.
The CCDP is not robust to such a disturbance.
Context-aware The CCDP is focused on the AI technology that is leveraged to enhance the cognition
of a business process under review. Thereby, the context of the CCDP can be seen as the people
that are related to this software and the resources the software uses to perform. As experienced
in the case study executed to create the CCDP described in this chapter, some people in the
69

Chapter 5. Cognitive Capabilities of Artificial Intelligence Software
organisation often have preliminary knowledge about a software suite already available before a
project is initiated to start leveraging AI technology in a business process. These people often
drive the initiation of the project in the organisation. As AI software companies have big promises
for organisation that use their software, it is important that organisations have a realistic view of
what software can do for them. The CCDP provides insights in the functionalities of the software
under review, by analysing the techniques underlying the services. These insights provide the
organisation knowledge what functionalities the current state-of-the-art can offer.
Next to this, the context in which the CCDP is executed, data is of importance. Where in the
first part of the CDP, the CRDP, data availability is considered to be a constraint for that design
process in the case study, the type of data is in the CCDP is considered to constrain the further
design process. This is because the type of data prevails the choice for particular AI techniques.
The CCDP allowed for the first ‘back-of-the-envelop’ analysis to indicate which AI technologies
in any case can be excluded from further analysis, since they cannot be used for the type of data
at hand. Thus, data – and especially the type of data – is in the case study performed here
considered as a constraint in the CCDP.
This chapter provides an answer to the third research issue (RI3) raised Chapter 1 in Figure 1.4:
Establish the functions to enhance the cognition of business processes. The Cognitive Capability Design
Process (CCDP) created is thereby the second component of the conceptual design process as envisioned
in Section 2.2. It provides a way to formulate cognitive capabilities from a given AI software suite. As
described above, the CRDP thereby recognises the values (specified in criteria) elaborated in Section 2.2
to a certain extent. To what extent precisely is not researched here, and therefore subject for further
research (see Chapter 8).

70

6 | Generating Possibilities for Cognition

Enhancement

In the previous two chapters, 4 and 5, the processes of acquiring the cognitive requirements of business
processes and the cognitive capabilities of software are elaborated and argued. Both chapters account 
for a step in the conceptual design phase of Dym and Little (2010). The next step in this conceptual

design phase is to establish means for functions, i.e. to construct means by matching the generated ca-
pabilities to the requirements, such that it results in an overview of all possibilities to leverage software

capabilities in the business process. In this research they are called Cognitive Possibilities (CP).
This chapter describes how the design possibilities for cognition enhancement of business processes can
be formulated, illustrating the procedure by applying it to the case study introduced in Section 3.1.

The steps described in this chapter result in the third part of the conceptual design process, the Cog-
nitive Possibilities Design Process (CPDP) (see Section 2.2). It thereby presents the research

that is conducted on the fourth research issue (RI4) raised Chapter 1 in Figure 1.4: Match functions
to requirements, generating possibilities for design.
First Section 6.1 kicks off this chapter by synthesising the cognitive requirements from Chapter 4
and the cognitive capabilities from Chapter 5, and describes and argue the theoretical underpinning
of the matching exercise, based on the concepts and theories introduced in Chapter 3. Section 6.2
continues by elaborating on how the matching process can be structured. This matching process is
illustrated in Section 6.3, using the case study used before. The matching exercise is formalised and
presented as the third component of the conceptual design process in Section 6.4. This last section
furthermore assesses and discusses the constructed CPDP on the criteria stated in Chapter 2, which
thereby concludes this chapter.

6.1 Theoretical Underpinning of Matching Capabilities to Require-
ments

Chapter 4 and 5 studied the cognitive elements of respectively business processes and AI software. This
section provides several notions on these cognitive requirements and cognitive capabilities, focused on

the envisioned matching process which is presented in the next section. Furthermore, the Cognitive Sys-
tems theory studied in Chapter 3 is leveraged to provide a framework to support this matching process.

Chapter 3 explained the concept of a Cognitive System. In this research, such a Cognitive System
is regarded as the systemic abstraction of a human being performing tasks supported by computing

71

Chapter 6. Generating Possibilities for Cognition Enhancement
technologies. The envisioned conceptual design process is meant for the design of software systems
that embody these computing technologies. The decomposition of a Cognitive System by Heylighen
(2011) provides the elements of a ‘system’ that is able to express cognition in different forms and
degrees. These elements corresponds to the majority of concepts of the definition of cognition used in
this research. Recall Figure 3.2 in Figure 6.1 from Chapter 3. However, the value of the schematic
visualisation of a Cognitive System in Figure 6.1 goes beyond its textual and visual definition. This
is because this concept is constructed from a cybernetic perspective. It denotes the relations between
the concepts. That is, it indicates how its different elements (i.e. different elements of cognition) form
together a system that is able to utilise cognition to solve problems, that is within a business process,
to execute allocated tasks. Each of these elements influences how such Cognitive System performs its
problem solving exercises, i.e. executes its assigned tasks, and to which level of quality.

Figure 6.1: Schematic illustration of a System embodied in its Environment, denoting the Cogni-
tive and Physical aspects of control of these two concepts – from Heylighen and Joslyn (2001)

As stated in Chapter 1 and 2, it is interesting to know how to raise the cognition of a Cognitive System,
since it can increase the performance of such a system. The control feedback loop of the Cognitive
System is thereby of importance. For example, when the information processing skills of the Cognitive
System are improved, the system will be able to analyse information more efficient and/or effective.
The result of this improvement can be twofold: the Cognitive System is able to come faster to the
decision-making part of the problem-solving cycle (1) and/or the system processes information in a
qualitative better way, which can results in a higher quality of decision-making or following cognitive
functions. Thus, improving a cognitive element of a Cognitive System can lead to an increase in
performance (quantitative and/or qualitative).

In this research, this improvement is caused by the support by computing technologies, enhanc-
ing the cognitive expressions of human beings. Not their own real cognition (one would talk about

human-computer symbiosis), rather the abstraction of cognition used to execute tasks in a business
72

Cognition Enhanced Business Processes
process. Thus, considering what is said above, the cognitive capabilities of software pick up on the
cognitive functions of the Cognitive System to enhance these functions. Hence, the cognition level of
the particular cognitive function of the Cognitive System will be improved. How much the level of
cognition is improved, is not in the scope of this research, as stated in Chapter 1.
Chapter 4 elicited a way to extract and formulate the cognitive requirements of business processes.
These cognitive requirements denote the cognition that is being asked for proper execution of the task
within the business process. This research assumes that, especially in knowledge-intensive business
processes, the cognitive skills that are utilised to execute tasks are not providing the maximum level
of cognition which can be deployed to execute those tasks. That is, there is a difference between the
level of cognition currently used and what level of cognition ultimately can be used (such that it still
results in a performance increase). This is where the cognitive capabilities of software systems kick
in. These cognitive capabilities are namely considered to be able to increase this level of cognition. In
other words, these capabilities are able to change the level of cognition used to execute tasks in the
direction of the ultimate level of cognition, increasing the performance of a system.
It might sound evident that a cognitive capability concerning the (cognitive) perception function

will not increase the cognition level of the system when applied to, for example, the (cognitive) informa-
tion processing function. However, this insight is important for the matching of cognitive capabilities

to cognitive requirements. Hence, cognitive capabilities that are not of the same cognitive function

category as cognitive requirements will probably not be applicable, since they are addressing a fun-
damentally different part of cognition. Furthermore, more than one cognitive capability (of the same

category) can positively influence a cognitive requirement (of the same category), also in synchroneity.
These insights form the groundwork of the matching framework, which is described in the next section.
6.2 Application to this Research
The previous section elaborates on (theoretical) insights how to use the cognitive capabilities of software
systems to improve the level of cognition leveraged in business processes. This section presents a
framework how to match cognitive capabilities to cognitive requirements. From Systems Engineering
theory, ideas from rigorous engineering methods are borrowed how to perform this matching process.
An example of such method is the House of Quality for defining the relationship between customer
desires and the enterprise capabilities (Sage and Armstrong, 2000). To match cognitive capabilities to
cognitive requirements in a complete and structured way, one would try to match each capability to
every requirement. This would however be a very time-consuming exercise.

Based on the insights of Section 6.1, only the capabilities and requirements that belong to a partic-
ular cognitive function category are needed to be tried to match to each other. This results in the first

design step of the CPDP, comprising the categorisation of the cognitive requirements and cognitive
capabilities by cognitive function. This first design step is referred to as design step A. Categorisation
by Cognitive Functions. Furthermore, the to be constructed framework needs to support the matching
of multiple capabilities to one requirements. A similar form of matrix-based framework as the House
of Quality can be generated to support the systematic matching process of cognitive capabilities to
cognitive requirements. Table 6.1 shows a version of this framework. This matching activity can be

Subsequent to this table, a second table can be generated that provides an overview of all matches of
cognitive capabilities to cognitive requirements. Furthermore, an extra column in the table serves as a
placeholder to describe the rationale of each match. This serves both validation purposes (objectivity
of the matching process) and for communication about the matching process to all parties involved
in the project. An example of this table is shown in Table 6.2. The analysis of the matched pairs,
and the formulation of the rationales for each pair together form the third and last design step of this
sub-design process. This design step is referred to as C. Analysis of matched pairs. The next section
illustrates how to go through these three design steps and how both frameworks can be used by means
of the claim-assessment process case study.
Match number Cognitive Requirement Cognitive Capability Rationale for match

Table 6.2: Suggested structure for listing the matches and their rationale

6.3 Example by Means of Illustrative Case
Recall the Cognitive Requirements of the case study from Chapter 4 and the cognitive capabilities of the
case study found in Chapter 5. In this section, the capabilities will be matched onto the requirements
by means of the framework as visualised in Table 6.1. However, first a synthesis step needs to be
conducted since the cognitive requirements and -capabilities are not yet sorted per cognitive function
category. Furthermore, this step entails the realisation of requirements and capabilities that can be
matched onto each other. Since their formulation is conducted independently, it may be that they
are not specified on the same abstraction level. This could result into a difficult matching process.
Therefore, this two steps support some tweaking that may be needed before the actual matching of
the two.

Another important step in the synthesis process is the filtering of categories and their correspond-
ing capabilities and requirements. Through experience, not every cognitive function is found equally

valuable for the claim-assessment process. Hence, not every category is considered to be necessary to
74

Cognition Enhanced Business Processes

Cognitive Requirements belonging to the category Reasoning

R1 When the facts in the claim information are categorised, the cognitive system shall state the
relevance of every fact individual to the assessment of the claim
R2 When the claim information has been annotated, the cognitive system shall state the value
of every fact individual to the assessment of the claim
R3 When the facts in the claim information are annotated, the cognitive system shall suggest

which relevant facts are not present in the claim information, considering the claim assess-
ment

R4 When the facts are extracted from the claim information, the cognitive system shall assess if
and which facts of the claim appear in previous assessed claims / a known case
R5 When the facts are extracted from the claim information, the cognitive system shall indicate
if and which previous assessed claim is similar to this claim
R6 The cognitive system shall be able to present if a similar claim is already assess before,
based on recognition of patterns
R7 When the facts in the claim information are related to previous claims, the cognitive system
shall determine which previous claims have the same properties as the current claim
Figure 6.2: A selection of cognitive requirements belonging to the category reasoning from the

illustrative case, as formulated in Section 4.3.2

Cognitive Capabilities belonging to the category Reasoning


illustrative case, as formulated in Section 5.3

analyse in the matching process. Through deliberation, the cognitive functions perception, recognition,
reasoning/information processing, learning, judgement and comprehension are identified as relevant
cognitive functions for the claim-assessment process, since they cover the essence of an assessment to

process information, analyse it and formulate a decision based on that information. Hence, these cat-
egories are taken into account in the matching process of this case study. The cognitive requirements

formulated in Section 4.3.2, belonging to the cognitive systems category reasoning, are presented for
illustrative purposes in Figure 6.2. From Section 5.3, the cognitive capabilities of the IBM Watson
software package, belonging to the same category (reasoning), are listed in Figure 6.3.

The following step is to fill in all the cognitive requirements and cognitive capabilities into the frame-
work as presented in Section 6.2. This is done per category. Table 6.3 shows the framework filled in

75

Chapter 6. Generating Possibilities for Cognition Enhancement
with the requirements and capabilities belonging to the category reasoning of this case study.
Next, the actual matching process takes place. This is done by analysing each cell in the framework
in sequence, indicating all possible matches between the requirements on the one hand, and capabilities
on the other hand. One by one the designer has to consider if a capability has the potential to fulfil,
i.e. meet a requirement. If so, the designer places an ‘x’ in the corresponding cell. If not, an ‘-’ (dash)
is placed. Then, the designer goes on to the next cell and so on and so forth. The matching process
ends when all cells have been subject to consideration.
Table 6.3: Matching the cognitive capabilities to the cognitive requirements of the illustrative case
A few insights can be gathered from Table 6.3. First, one can see that C3 and C9 are capabilities
that can be used often. The other capabilities of IBM Watson software are not applicable in this case.
Secondly, one should keep in mind that the capabilities matched to certain requirements are not the
only capabilities that are needed in order to fully realise the corresponding requirement. That is, other
capabilities will be needed to meet the requirements. These capabilities can be cognitive capabilities
as well as non-cognitive capabilities. The latter type of capabilities are not addressed in this research.
After the actual matching process, a new table can be generated by extracting all pairs of cognitive
requirements and -capabilities which are in the framework indicated with an ‘x ’, indicating a potential
software cognitive capability that could improve the cognition in a business process. For every pair of
capability-requirement, the software service can be denoted that delivers that particular capability by
looking back into the results of the CCDP. Finally, a table that provides an overview of all matches
and their rationale can be constructed, as elaborated earlier in Section 6.2.
The last steps of the third part of the conceptual design process comprise the generation of design
alternatives, refinement and application of metrics to design alternatives and ultimately choosing a
design. These three steps are not addressed in this case study, since they do not differ from other
design exercises: the main focus and scientific potential is in the first three steps of the conceptual
design process. This ends the illustration of the third step of the conceptual design phase. Next,
the insights generated from theory and this case study deliver the third component of the envisioned
conceptual design process, which is presented in the next section.
6.4 Stage 3 of Conceptual Design Process
Summarising the steps illustrated in this chapter, the third part of the envisioned conceptual design
76

Cognition Enhanced Business Processes
process can be drafted: the Cognitive Possibilities Design Process. This part is visualised in Figure 6.4
and thus entails the matching of the cognitive capabilities of selected AI software to the cognitive
requirements of the tasks within a particular business process. This chapter concludes by providing a
description of the CPDP and assess it on the criteria stated in Chapter 2.
6.4.1 Description of the Cognitive Possibilities Design Process
The CPDP consists of three steps. First, the cognitive requirements from the CRDP and the cognitive
capabilities from the CCDP are synthesised and categorised according to the cognitive function they
relate to (A). In the second design step, the cognitive requirements and cognitive capabilities are
put opposite to each other in a table (Table 6.1). For every cognitive function, such a table will be
created. This table structures the matching process, as each cell corresponds to a potential match,
indicating a pair. The designer should go through each of the cells, assessing if the selected capability
can potentially comply to the selected requirement. Each cell will be marked with an ‘x ’ if this is
the case (B). When this matching process is completed, the third and last design step of the CPDP
will extract all pairs of capabilities and requirements matched to each other and put them in a new
table (Table 6.2). A more thorough analysis will be conducted to reason if the capability can meet the
requirement when implemented. For each matched pair of capability and requirement, the designer
writes down the rationale behind this match for communication purposes (C). These three design steps
together form the CPDP of the CDP.

A. Categorisation by
Cognitive Functions
Synthesise the outcomes of Stage 1
and Stage 2 and categorise these
results according to cognitive
functions
:
 Categorise the cognitive
requirementsfrom Stage 1  Categorise the cognitive
capabilities from Stage
2

C. Analysis of Matched Pairs
After going through the whole table
:
1. For each marked cell in the table,
execute a more thorough analysis
whether the capability can actually
cause the effect that the
Requirement can be met
:
 Analyse the methods and
techniques of the service  Write for every pair one or two
lines argumentation explaining the
rationale of the match 2. Check if a combination of
capabilities can have a positive effect
such that a requirement can be met

B. Structural Matching
To match the Cognitive Capabilities
to the Cognitive Requirements
:
1. Fill in the template table structure
per category for both Requirements
and Capabilities 2. Based on the description of both
requirements and capabilities
:
 Go through each cell of the table
and assess if the selected capability
can potentially meet to the selected
requirement.  If so, mark this cell. If not, go to
the next cell

Figure 6.4: Schematic representation of the design process to match the cognitive requirements to the cognitive capabilities, using the con-
cepts from Chapter 6


6.4.2 Assessment of the Cognitive Possibilities Design Process
To assess the suitability of the CPDP constructed and described in this chapter, the criteria stated in
Section 2.2 are elaborated below regarding the CPDP.

Universally applicable The CPDP as described above provides a way to match the cognitive capa-
bilities to the cognitive requirements as formulated in the previous sub-design processes of the

CDP. The steps in the CPDP are formulated in such a way that they can be performed to each
set of cognitive capabilities and/or cognitive requirements; the steps are not bounded to certain
specific formulated cognitive capabilities and/or cognitive requirements. Thereby, the CPDP can
be run through irrespective of how its input is formulated, although this input should conform
to the syntax of a requirement. The CPDP is therefore universally applicable to the outcomes
of the CRDP and CCDP.
Methodical in essence The steps of the CPDP together form a systematic way to create cognitive
possibilities from the cognitive requirements of the business process and the cognitive capabilities
from the AI software under review; each step contributes in its own way to formulate these
possibilities for AI technology to power the cognition of a business process. The CPDP is therefore
considered methodical in essence.
The CPDP is considered rigid, since its procedure to generate the cognitive possibilities is strict
and self-explaining, offering one structural way to generate these possibilities. This strictness is
considered the main strength of the CPDP, ensuring that this sub-design process of the CDP
where the design space is created is executed in an uniform way when performed by different
people. However, the matching step (B) itself comprises room for manoeuvre for the designer.
Different sets of input (cognitive capabilities versus cognitive requirements) therefore result in a
different set of cognitive possibilities, i.e. design space. On the other hand, the CPDP ensures
that the same sets of input in principle result in the same set of cognitive possibilities when
executed by different designers, thanks to its strictness. Step A is considered to have little
variation in its outcome if performed by different people. This is also considered for step C,
despite variations in the rationales of the matches; people can have different ideas why a cognitive
capability meets a certain cognitive requirement, and/or formulate the rationale of a match in a
different way but still meaning the same.

Flexible but robust If the set of cognitive requirements and/or cognitive capabilities changes en-
tirely during the execution of the CPDP, the CPDP can be re-executed using the new set of

capabilities and/or requirements. This will require the designer to start from the first step (A) of
the CPDP, categorising this new set. The progress it already made is however no longer useful,
since it is generated for a different set of input. If the set changes only a bit, for example only a
few cognitive requirements and/or cognitive capabilities are added to or removed from the set,
the matching table created and used in step B can be changed by the designer, and thereby offers
the possibility to extend or limit the design space. This characteristic of the CPDP denotes the
flexibility of the CPDP.

On the other hand, all steps of the CPDP ensure robustness to the CPDP to external disrup-
tions, such as the removal of a designer (in a design team). Each step in the CPDP requires
documentation of the outcomes of the step; a list of cognitive requirements and -capabilities per
cognition function, a completely filled matching table and a textual description of the matching
rationales. This documentation maintains the progress made in the design process, and gives the
CPDP robustness to unexpected disruptions in the design process.
Context-aware The documentation generated in the CPDP serves furthermore one other purpose,

which is related to the context in with the CPDP is executed. The documentation, and espe-
cially the written description of the matching rationales, provide explicit argumentation why a

particular match is made. When a capability is considered not to fulfil a requirement, this is
also indicated by the matching table. This allows stakeholders in the project to understand why

certain design choices are made in the design process. This contributes to more realistic expec-
tations by the stakeholder regarding possible design alternatives created in the next sub-design

process of the conceptual design phase. With that, the CPDP is aware of its environment when
questions are raised regarding its outcomes.
Just as the CRDP and CCDP, data is considered to be of importance when executing the CPDP.
The CPDP is meant to generate design possibilities. These design possibilities indicate which AI
software service can meet the cognitive requirements of the business process with its cognitive
capability(-ies). Since these AI technologies are driven by data, the data to power these cognitive
capabilities need to be of sufficient quality. If the data is of bad quality, the AI software cannot
provide the cognitive capabilities it is developed for. Where data availability, completeness and
type are important for the CRDP and CCDP, the other data quality aspects are important for
the CPDP: consistency, timeliness, accuracy and precision. Therefore, these data quality aspects
are ought to be analysed in step C of the CPDP where a thorough analysis is performed whether
the cognitive capability can actually cause the effect such that the cognitive requirements can
be met. Thus, data available in organisation partly determines the possibilities for design. The
CPDP is thereby aware of the context it is used in, i.e. the environment it generates design
possibilities in.
This chapter provides an answer to the fourth research issue (RI4) raised Chapter 1 in Figure 1.4:
Match functions to requirements, generating possibilities for design. The Cognitive Possibilities Design
Process (CPDP) created is thereby the third component of the conceptual design process as envisioned
in Section 2.2. It provides a way to formulate cognitive possibilities from the cognitive requirements
and capabilities from previous sub-design processes of the CDP. As described above, the CPDP thereby
recognises the values (specified in criteria) elaborated in Section 2.2 to a certain extent. Again, to extent precisely is not researched here, and therefore subject for further research (see Chapter 8).

This chapter presents the research that is conducted on the fifth and last research issue (RI5) raised
Chapter 1 in Figure 1.4: Establish a conceptual design process of cognition enhanced business processes.
That is, it describes how the three design processes created in Chapter 3 to 6 can be combined to form a
coherent whole. This chapter delivers the overall Conceptual Design Process (CDP) as envisioned
in this research.
Section 7.1 describes the effort of constructing the CDP from the CRDP, CCDP and CPDP.
Furthermore, it argues that besides these components two other components are necessary for the sake
of completeness. Section 7.2 and Section 7.3 elaborate on these two other components of the CDP.
Section 7.2 presents and elaborates on the theoretical concept of Design Process Co-ordination, which
coordinates the other components of the design process such that its goal are met. It furthermore
discusses how this concept of Design Process Co-ordination can be used in the conceptual design
process envisioned in this research. Next, Section 7.3 introduces and describes the way in which data
is present and formalised in the overall conceptual design process. Adding these two last components
of the conceptual design process results in the last part of this research creating a CDP for cognition
enhanced business processes. Section 7.4 therefore presents the final conceptual design process, which
is validated in Section 7.5 by a round table discussion with experts and assessment on the criteria
introduced in Chapter 2, and by applying the conceptual design process to the second case study in
this research. Section 7.6 presents the conclusion of this chapter.
7.1 Constructing the Conceptual Design Process
So far, Chapter 4 presented the CRDP which extracts and formulates the required cognitive activities
of a business process, by decomposing a business process to tasks and elicit the cognitive activities
that are performed when executing each task. Next, Chapter 5 presented the CCDP, which formulates
the cognitive capabilities of AI software, by defining the different services provided by the software
and by specifying its functionalities in the context of the business process under review. This part of
the design process is also driven by a design exercise. The previous chapter, Chapter 6, described how
the outcomes of the CRDP and CCDP can be matched to each other supported by a categorisation
leveraging the concepts addressed in Chapter 3. This matching process generates insights in the
possibilities of utilising AI software in the business process under review (such that ultimately design
alternatives can be constructed), referred to as the CPDP.
81

Chapter 7. Constructing & Validating the Conceptual Design Process
These three design processes are viewed as components of the overall CPD how to enhance the
cognition of (the tasks in) a business process. As formulated by (Dym and Little, 2010), the conceptual
design phase consists of six steps, of which three are covered by the CRDP, CCDP and CPDP. However,
the last three steps of the conceptual design phase – Generate design alternatives, Refine and apply
metrics to design alternatives and Choose a design – are not covered yet. These steps together are
referred to as the Design Alternatives Generation & Selection Process (DAGSP). However,
as already mentioned and argued in the introduction of this thesis, the DAGSP is not studied in this
research.
To form the overall conceptual design process, the question is how the components are connected to
each other. Are they used purely sequentially? Do these components transfer information or other
resources to each other? If so, what information do these components transfer to each other? The
answer to this question is rather difficult. Expert opinions from design scientists state that, because
every design exercise is different (Rittel and Webber, 1973), their design strategy used in the project is
never the same (van Langen, 2015b). Therefore, it is not worthwhile to formulate a fixed sequence for
the execution of the four components of the design process. Furthermore, the transfer of information
between the four components will be different in each design project. Thus, within the design process,
another component is needed that is responsible for the the strategy that guides the design process
towards its goal: to design a cognition enhanced business process.
The CDP is, in all its essence, ought to support the conceptual design exercise as mentioned above.
Thereby, the CDP has to cover the important design activities that are related to this design exercise.
The CRDP, CCDP and CPDP are the components responsible for this. However, next to the important
design activities, other relevant and important concepts and/or factors are ought to be covered by the
CDP, since they would play an significant role in the design process or affect this design process.
As noticed while exploring the possible ways to form the final conceptual design process, the CDP
components (CRDP, CCDP and CPDP) all are affected by data related factors. As already mentioned
in the introduction of this thesis, AI technology cannot be leveraged without data that fuels these
technologies. If data is not available or considered to be of insufficient quality, the number of possible
designs decreases. Data quality aspects of importance to the CRDP, CCDP and CPDP are discussed
in Chapter 4 to 6. Thus, data quality aspects are important during the whole CDP as investigated in
this research. Hence, the concept of data, and especially data quality, has to be included in the CDP
in a certain way.
Figure 7.1 schematically denotes the two envisioned components that have to be found, in order for the
design process to be useful and sound. The dark grey blocks indicate the still missing components of
the CDP. The striped grey block represents the DAGSP component of the CPD, which is not further
addressed in this research. This chapter presents the last two components of the design process, and
integrates them in the CDP together with the CRDP, CCDP and CPDP from respectively Chapter 4,
5 and 6

components still unknown
7.2 Strategy in the Design Process
A designer who is responsible for the design of a (new) cognition enhanced business process can leverage
the three design processes to receive the guidance that these processes provide for conducting the design
task at hand. The question is how to utilise these three processes in such a design exercise? More
specifically, the following unknowns are present regarding the use of these three design processes (i.e.
interaction between these design processes):
• The order of use (task control): with which design process do you start? Do you use it purely
subsequently? Or do you switch between the design processes (iterative character)? How often
do you then switch?
• The information exchange between the design processes: does information flow from one design
process to the other? How often? And what information precisely?
Two objects in the design process that are of relevance to these two unknowns are the specification of
the system under design, and the (design) requirements to which the system’s design should comply
(van Langen, 2002). The changes by the designer to the specification of the design and the design
requirements influence the order in which design activities are performed and what information between
which design activities is exchanged. The process of these manipulations is driven by the objectives
of the design process, which manifest themselves in the form of a strategy that provides the design
process with a course of action.
7.2.1 Design Process Co-ordination
As firstly coined by van Langen (2002), Design Process Co-ordination (DPC) is the concept that
controls a design process in accordance with given design process objectives. Hence, it is responsible

Chapter 7. Constructing & Validating the Conceptual Design Process
for the overall design strategy of the design exercise. Design Process Co-ordination is furthermore
concerned with the use of requirements, working towards a certain (design) solution. Besides indicating
which design activity should be performed at a given moment, it suggests how to properly allocate
resources such as information, time and stakeholders in the design process.
DPC is thus situated on a more strategical level of a design process, where the other components
of a design process have a more tactical nature within the design process. Just as other components
in the design process, the context of the design is taken into account in DPC. Because DPC can be
considered as the ‘glue’ between all components in a design process, it embodies the learning character
of the design process, which the other components do not deliver on themselves.
Figure 7.2 schematically visualises the input and output of Design Process Co-ordination. The input
of DPC are the objectives of the design process. An example of a design process objective is the
goal to finish the design exercise within three months. The output of DPC are evaluations of the
Design Process. These evaluations are for example statements if and to what extent the design process
objectives are met, throughout the design process and at the end of the design exercise. The connection
of DPC with other components of the design process is twofold. First, it entails the communication of
the strategy formulated by DPC to the other components. An example of this is when and which steps
should be executed by that part of the design process. The second part is the evaluation of this first
part, which are evaluation statements how well that part of the design process is controlled. Every
other part of the design process still has its own inputs and outputs. These components mutually
communicate and transfer information when necessary to perform their design activities.

7.2.2 Application to this Research
Interpreting the notions above in the light of this research, DPC is considered to be of value in the
envisioned conceptual design process since it delivers the strategy throughout the design process that
is needed to steer the process towards its objectives. The Design Process Co-ordination process should
not be considered to be similar to the three design processes constructed in this research, since it
serves a different purpose in the conceptual design process. Namely, where the three components can
be typified as tactical deciding how the design activities should be performed, DPC can be considered
here as strategical, since it decides on when certain design activities should be performed in the overall
design process. Figure 7.3 illustrates the structure and relationships between the DPC and the three
design processes. For the sake of clarity and simplicity, the DAGSP is not visualised in Figure 7.3 since
84

Cognition Enhanced Business Processes
it is not specifically addressed in this research. As described above, DAGSP is part of the total CDP
and thus has a link to DPC as well. Its place in the CDP is illustrated in the overview of the total
CDP.

Figure 7.3: Design Process Coordination applied to the CRDP, CCDP and CPDP of the CDP
In the process of designing a software system that enhances the cognition used in a business process,
DPC is thus the responsible component that determines the strategy of the design exercise. It executes
this strategy by determining which design activities should be performed of which part of the design
process (CRDP, CCDP, CPDP) at a given moment in time. For example, when entering the CPDP,
it could be that some requirements (or capabilities) are not formulated detailed enough in order to be
properly used in the matching process. DPC then determines to go back to the CRDP (or CCDP) to
re-execute the design activity that is responsible for this formulation. DPC is thus a guiding component
of the conceptual design process.

When utilising the conceptual design process, first the design process objectives have to be spec-
ified. These objectives form the input for the DPC, and are used for the formulation of the design

strategy. Each time a design activity is performed, for example in a design step of the CRDP, the

strategy is evaluated if it still meets the predefined design process objectives. Furthermore, through-
out the design processes CRDP, CCDP and CPDP, the DPC gathers information about the need for

resources from the sub-design processes and facilitates this. DPC fulfils thereby an essential role in
the CDP.

Figure 7.4 shows the components of the conceptual design process, adding the Design Process Co-
ordination component to the four other known components. This leaves a component ensuring the

design to fit to the data still unknown. The following section elaborates on this last unknown component
of the CDP.
Figure 7.4: Visual of the components of the conceptual design process, indicating in dark grey the

component still missing

7.3 Ensuring Fitness to Data
As assumed in Chapter 1, data is of importance when using AI technology in a particular system. As
turned out in the assessments of the design processes applied to the case study regarding the criteria
drafted in Chapter 2, the CRDP, CCDP and CPDP can indeed be affected by data. More specifically,
the quality of the data is considered to be especially relevant for the design of AI powered systems to
create cognition enhanced business processes.
Data quality is thus of significant importance to the whole CDP. If the conceptual design under
creation does not fit to the data (quality) available, the conceptual design will in the end not suit
the environment in which the design exists in, i.e., the solution will not suit the problem. Thus, data
can then be considered as a constraining factor in the design process. Therefore, in each sub-design
process of the CDP, an assessment if data quality is considered as a issue or not should be performed
frequently. Hence, a sixth component of the CDP is proposed, that is responsible to make sure that
the system under design will in the end be accommodated to the relevant available data.
7.3.1 Assessment of Data Availability & Quality
To what extent data fits to a particular design is considered to be determined by the availability of
the data and the quality of the data. As fitness of the system under design to data is important to
keep track of, Data Availability & Quality Assessments (DAQA) have to be executed in the
conceptual design process. In essence, an assessment of the data availability & data quality entails the
determination of whether the data – of the quality required by a particular design – can be available,
such that it can be used when the design is going to be developed after the design phase of the system.
If the required data is not available, data quality is non-existing; if the required data is to some extent
available, data quality is of importance.

Cognition Enhanced Business Processes
Literature on data quality identifies multiple attributes of data quality, although no consensus
seems to exist on this topic. An example of a set of data quality aspects is found in Wang and Strong
(1996), which analysed multiple studies on data quality. They drafted a conceptual framework of
data quality differentiating fifteen data quality dimensions into They drafted a conceptual framework of
data quality differentiating fifteen data quality dimensions into four categories. These four categories
are Intrinsic Data Quality (1), Contextual Data Quality (2), Representational Data Quality (3) and
Accessibility Data Quality (4). Figure 7.5 shows these four categories and the data quality dimensions
they consists of.

Data Quality

Intrinsic Data
Quality

Believability
Accuracy
Objectivity
Reputation

Contextual Data
Quality

Representational
Data Quality

Accesibility Data
Quality

Value-added
Relevancy
Timeliness
Completeness
Appropriate amount of data

Interpretability
Ease of understanding
Representational consistency
Concise representation

Accessibility
Access security

Figure 7.5: A conceptual framework of data quality, from Wang and Strong (1996)

The data quality dimensions provided by the framework visualised in Figure 7.5 are an example of a
set of data quality aspects that can be used to identify data issues along the way of designing. Based
on such a set of data quality dimensions, a data quality assessment can be executed.
7.3.2 Application to this Research
Interpreting the notions above in the light of this research, DAQA is considered to be of value in
the envisioned conceptual design process, since it provides a way to frequently check if the solution
under design will fit to the data. If data is not taken into account in the conceptual design process, a
implication is that the resulting (conceptual) design will not fit to its purpose, due to the applicability
of the ‘Garbage-in, Garbage-out’ principle. With fitness of a design to its purpose is meant the extent
the design is similar to its most desired state. The same holds when the quality of the data is not
taken into account; low data quality results in low fitness of the design to its purpose. Featuring data
availability and data quality in the conceptual design ensures a more appropriate design. By frequently
assessing the data availability and data quality in the CDP, the designer can minimise the risk that
the design does not fit to the data.
As described in Section 4.4.2, data availability and data completeness are considered important to the
formulation of the cognitive requirements in the case study used. They determine if a certain direction
of the design process is in the first place considerable. The type of data available is of importance to
87

Chapter 7. Constructing & Validating the Conceptual Design Process
the cognitive capabilities of an AI software suite, indicating if the AI techniques available are suitable
to deal with the data that is present (see Section 5.3.2). Besides, the appropriate amount of data
is considered to be important for the services of the AI software to be used in the case study. The
matching of the cognitive capabilities to the cognitive requirements in the case study can be affected
by data quality factors such as the accuracy, consistency and timeliness of the data available to the
design process (see Section 6.4.2).
Although the important data quality dimensions are found for the case study used in this research,
these are not necessarily the (most) important data quality dimensions for each part of the CDP or the
CDP as a whole. No guidelines are considered to exist which tell when which data quality dimension
is important during each sub-design process of the CDP. A designer can choose to use a conceptual

framework such as the one presented in Figure 7.5 or other reference material on data quality dimen-
sions to supports the data assessment. Next to ensuring fitness to data, this component can be used

to indicate to what extent an increase in data quality can change the possibilities in design. When the
designer or other stakeholders of the design process finds a design possibility particular interesting, but
the data quality is not sufficient enough for this, a data quality project can be initiated to increase the
quality of the data available.
The DAQA comes to play when certain design activities are performed and their outcomes can be
assessed. The DAQA provides information to the design processes in the form of an assessment to
what extent a particular design fits the data. Figure 7.6 indicates the relationship of the DAQA with
the CRDP, CCDP and CPDP, which is in essence an exchange of information on whether the data
quality aspects are relevant to the conceptual design process component (assessment). The CRDP,
CCDP and CPDP share this information between themselves to support the execution of their design
activities, which is then typified as a resource.

CRDP
Business Process to
Cognitive
Requirements

CCDP
AI software to
Cognitive Capabilities

CPDP
Mapping of
Capabilities to
Requirements

assessment assessment assessment

DAQA
Ensuring Design fitness to Data
resources resources

resources

Figure 7.6: The Data Availability & Quality Assessment-component applied to the CRDP, CCDP

and CPDP of the CDP

88

Cognition Enhanced Business Processes
In contrast to the other components in the CDP, the DAQA is not considered to be situated on

the more strategic level of the CDP such as the DPC (steering the design process to meet its objec-
tives), neither the more tactical level where the of the CRDP, CCDP and CPDP (specify which design

activities should be executed). However, since the the DAQA comprises the verification if certain
design elements and -possibilities can exist or not, the DAQA is considered to be on a different, more
operational level, in the CDP. It thus supports the CRDP, CCDP and CPDP in executing the design
activities they propose.
To illustrate the DAQA component in a conceptual design process, let us take an example in the CCDP
where the AI software under review only consists of capabilities to extract entities of an input text
and translate a text into another language. If the data available to the design process relevant to its
objective are mainly databases and other data sources that are not suitable to these AI capabilities,
the DAQA will indicate this mismatch. It thereby indicates that or an other AI software suite should
be used with different capabilities, or that suitable data should be gathered in the organisation, which
often comprises in a data collection project internally in the organisation.
Another example to illustrate the DAQA in the CDP is the following. In matching the cognitive
capabilities of the AI software to the cognitive requirements of the business process, data availability
can be important. If for a particular design possibility (a match of a cognitive capability to a cognitive

requirement) the specific required data is not available, the design possibility will never come to de-
velopment. The DAQA provides this information to the CPDP by assessing which data is specifically

needed by the AI software from the CCDP and which data is available in the business process or
organisation from the CRDP.

7.4 Final Conceptual Design Process
Summarising the steps illustrated in this chapter, the envisioned conceptual design process can be
created. This part is visualised in Figure 7.7 and thus entails the CRDP, CCDP, CPDP and DAGSP,
connected to each other by the DPC and supported by the DAQA as explained in this chapter. For
simplicity, only the interactions from and to the Design Process Co-ordination-component and Data
Quality Assessment-component are visualised.
Figure 7.7 thus provides the overview of the final CDP as studied and constructed is this research. It
provides sub-design process and corresponding design steps to develop in the end a conceptual design
of a cognition enhanced business process. Furthermore, it specifies what the design activities within
these design steps are, and how they should be performed and paid attention to.
The conceptual design process can serve as a structural approach to identify the possibilities for AI
software in business processes, but also the problems currently present regarding the use of cognition.
It forces to describe these opportunities and problems in an explicit way, instead of talking about it in
a general way. Its scope does not entail what software should be doing in a business process (software
engineering view), but where it can fit in, and be of value to the business process (systems engineering
view). The conceptual design it addresses adds value to the design of AI systems within business

89

Chapter 7. Constructing & Validating the Conceptual Design Process

Design Alternatives
Generation & Selection
Process

Cognitive Capabilities
Design Process

Cognitive Requirements
Design Process

Design Process
Co-ordination

Cognitive Possibilities
Design Process
Conceptual Design Process

Data Availability & Quality
Assessment

Legend
Component
Flow of resources

Figure 7.7: Schematic of the elements of the conceptual design process constructed in this re-
search, indicating in light grey striped the component not addressed in this research

processes, by making explicit why particular AI services and corresponding techniques should be used.
The design process is built upon the theory of Cognitive Systems theory as elaborated in Chapter 3,
using the concept of cognition to enhance a business process, thereby contributing to the overall goal
set by the organisation. By going through the CDP, answers can be provided to questions such as:
what is our objective regarding the use of AI in the business process? And what needs to be improved
exactly? And which feasible AI solutions can therefore be used?
To make sure that this CDP can be used in design projects, the validity of the CDP is assessed
in two ways: validating the conceptual design process and a conceptual design it produces. The next
section describes this validation exercise. The assessment of the final CDP on the criteria stated in
Chapter 2 is a part of the validation and is addressed in Section 7.5.1.4.
7.5 Validation of Research
The validation of the conceptual design process is performed in twofold. Process-wise, the CDP is
validated with Jibes Data Analytics B.V. who employs experienced AI system developers which belong
to the target group of the CDP, since they design, develop and implement these types of systems. This
validation shows how usable the conceptual design process is for them, and how they think about its
value for the to be designed systems in the end. To assess the validity of the CDP, a full blown empirical
validation study is desirable. However, due to time constraints and due to the fact that this research
uses a RtD approach, such a validation study is not performed. Instead, a round table discussion was

90

Cognition Enhanced Business Processes
organised and conducted with three AI-solution experts of Jibes, which also serves the purpose of this
research in a proper way. Content-wise, the CDP was validated by applying the conceptual design
process onto a new case study. This shows whether if the CPD actually generates meaningful and
useful design solutions in another case study. Respectively Section 7.5.1 and Section 7.5.2 present and
elaborate on these two validations performed in this research.
7.5.1 Validating the Conceptual Design Process
For the validation of the Conceptual Design Process, I hosted a round table discussion at Jibes Data
Analytics B.V with three of its AI-business solutions experts. In this round table session, I first briefly
presented the goal of this validation step, setup of the meeting and the background of my research. The
latter explained why I developed a CDP and what perspective I took to scrutinise the opportunities
of AI in business processes. The main part of the meeting comprised of a discussion about the five
components studied in this research of the CDP to challenge their existence and content. Here, I asked
the participants to speak out their thoughts and beliefs about the usability and usefulness of the CDP.
Furthermore, the discussion entailed also the broader topic of AI solutions for business processes, not
limiting to the conceptual design phase of such solutions. Therefore, together with the actual design
process of the CDP, these validation steps can be seen as results of this research, and therefore as a
prelude to the conclusions of this research.
To describe the main points from this round table validation discussion, the notions are presented
per sub-design process of the CDP. At last, this section assesses the final CDP on the criteria stated
in Section 2.2.

Validity of
Conceptual Design
Process (CDP)

Assess usability of
CDP

Assess usefulness
of CDP

Implicit added
value for design

Explicit added
value for design Ease of use Learnability

Assess CDP on
criteria

Figure 7.8: Schematic outline of validation of the conceptual design process

7.5.1.1 Validation of CRDP
The discussion about the CRDP touched upon each step of this design sub-process (recall Figure 4.6)
in Chapter 4. Overall, the participants of the round table stated that it is desirable to think about the
requirements of the solution independently from the technological possibilities. Not until the CCDP in
the CDP, the CCDP, the technology is considered for the possibilities it can deliver. Because of this,
91

Chapter 7. Constructing & Validating the Conceptual Design Process
a designer is able to analyse the possibilities for the business process under review as objectively as
possible regarding the goals it wants to achieve. The participants agreed on the fact that the CRDP
allows for this objective review.
Secondly, the experts indicated that human-centric view of the CRDP is a strength of the design
process. They described that, to date, projects concerning AI solutions in business processes are often
viewed from a purely machine-centric perspective, mainly due to the hype regarding automation by
AI. The human-centric approach of the CRDP forces stakeholders to think about the environment
the solutions is going to play a role in, namely the business process, in which human beings are not
neglectable. In the end, the AI solutions have to function and perform well in the business process
such that it is accepted in the whole organisation.
Furthermore, they have expressed that the detailed analysis of the business process is another
strong point of the CDP, and even necessary for a proper design of a solution. They refer here to the
decomposition of the business process, by specifying the boundaries of the business process (system)
under review, its break-down to sub-processes and tasks.
Next to that, the Cognitive Task Analysis is found to be interesting by the experts and useful for the
extraction and analysis of both explicit and implicit knowledge used within the business process, as
well as other cognitive skills expressed and utilised in the business process. They indicated that the
new glance, the cognitive systems perspective on the business process, and especially on its tasks to
be refreshing and to some extent valuable. However, they also expressed some concerns regarding the
(still) general character of the Applied Cognitive Task Analysis for its use in projects developing AI
solutions in business processes. To use this CDP, the Cognitive Task Analysis step is suggested to be
researched more in detail, resulting in a more specific, more rigorous and less elaborate version of this
analysis.
Furthermore, they indicated that the ACTA has to deal with a bias in its analysis process, due to
the fact that information is extracted from other persons. These interviewees could be biased in certain
ways, resulting in less usable and objective results. For example, they can be uncooperative, provide
false information, lack a necessary degree of self-reflection, and/or are not able to pronounce their
thoughts well enough. When executing the ACTA in this CRDP, the designer has to acknowledge this
potential bias and take into account that the raw results of the analysis have to be carefully examined
and reflected on.
At last, the experts reported their slight concern of the approach of the CRDP of the CDP, since
it seems that the CDP focuses only on optimising a business process within the established frameworks,
i.e. the boundaries of the current state of the business process. Therefore, potential solutions will not
be designed outside these boundaries, or question those boundaries, which could result in ignoring even
more valuable solutions. Indeed, the CDP does not cover this, since it formulates the requirements for
the cognition in the business process, in its current situation / state. The cognitive perspective taken
in the design of this CDP is thus the reason for this approach.
7.5.1.2 Validation of CCDP
In the discussion about the CCDP, all steps were touched upon as presented in Figure 5.6 in Chapter 5.
92

Cognition Enhanced Business Processes
The participants of the round table discussion expressed that they value the result of the CCDP, a table
which describes the AI software under review on several criteria, as useful for themselves and colleagues
as a reference for the design exercises they perform. Furthermore, it can serve also as a communication
vehicle to clients and other stakeholders, describing relevant and valuable information about the AI
software services such as their functionalities, examples of their use, and relation to technical fields of
study. It thereby fosters the design process of the solution. It could be helpful for the selection process
of the right software service, from for example a financial perspective and/or functionality perspective.
Furthermore, an expert indicated the usefulness of the CCDP to extract information about which
data is needed for proper use of a particular software service. One could think about which business
data is necessary, but also which dictionaries should be available for proper use of natural language
processing services.
The experts also indicated that the CCDP of the CDP forces the designer to formulate and specify
the functionalities of the software package under review in a clear way, which they think is a good
thing and valuable for a solid and rigour design process. It can then improve communication about the
software with both colleagues, clients as well as other relevant stakeholders (think about the financial
department which has to agree on the software investment). Once created, updating the overview is
less work and also desirable, since functionalities of software packages change over time (during the
period of this research, one service of IBM Watson is taken offline and not available anymore for use).
For a consulting company as Jibes, it is also desirable that its consultants have this knowledge about
the software availability, to be able to quickly identify possible solutions for AI in a business process.
Just as the critique on the CRDP, the experts question the approach of the CCDP, since it extracts
and formulates the capabilities of the software first. Instead, one can also reason more freely about the
functionalities the to-be designed systems should have. Then, the designer can search for means that
fulfil these functions. This is indeed a good point and a weakness of the CDP. Again, the cognitive
systems perspective taken in the design of this CDP seems to be the reason that the CDP is constructed
as it is now. It forced the design process of the CDP to identify and examine the cognition that is
currently used in the business process.
7.5.1.3 Validation of CPDP
After the CRDP and CCDP, the design activities of the CPDP in Figure 6.4 in Chapter 6 are discussed
in this round table meeting. Just as the CCDP, this sub-design process of the CDP is found useful
as a communication vehicle to colleagues, as well as clients and/or other relevant stakeholders for the
same main reason: it fosters the process of properly designing the (AI) solution. Furthermore, the
extensive character of this design sub-process makes sure that no solutions possibilities are forgotten.
This is also indicated as a strength of the CPDP by experts. The categorisation that is applied to the
potential matches in the form of the cognitive skills from theory, is considered as logical and valuable
for the matching process.
In addition to that, some experts expressed some critique on this extensiveness and rigorous character
of the matching process. They question if this formal way of examination of each possibility is necessary
93

Chapter 7. Constructing & Validating the Conceptual Design Process
in practice, since the designer’s gut feeling and creativity will focus on the possibilities that he or she
thinks are viable. This research does not contradict this concern regarding this matching process.
When the designer is experienced, he or she will indeed be able to recognise the potential solutions for
design faster than going through all possible matches in the CPDP. In other words, when conducting the
CDP in all its detail, it could take away some of the creativity of the designer to develop out-of-the-box
solutions.
At last, the experts identified a limitation of the matching process, comprising the performance
(quality and robustness) of a software service. The matching process as presented in Section 7.3 only
identifies whether a particular capability is usable to fulfil a particular requirement. That is, it only
takes into account the usefulness of the software service, but not how well this service is actually
capable of meeting that requirement. The matching process can thus be improved by adding a scale
of performance of the capability, being able to fulfil the requirement. The experts stated that the
software services that they used (of IBM Watson) are not all of the same quality or robustness. The
maturity levels of these software services can thus differ.
7.5.1.4 Assessment of the Conceptual Design Process

Besides these notions of the experts on the distinct components of the CDP, the round table discus-
sion also resulted in some general remarks about the CDP as a whole (see Figure 7.7 for the total

structure of the CDP). The experts agreed on the fact that the CDP provides end-to-end insights in

the possibilities of AI software in a business process. The CDP supports this exercise with its struc-
tural character, identifying the potential of AI objectively. For developers of AI solutions in business

processes, it decreases the freewheeling behaviour which is present in the design process without ex-
plicitly formulated guidelines. In addition, according to the experts, the CDP in the end contributes

to help client organisations to identify the potential added value of AI for their business processes, in
a methodological structured way, before investing (heavily) in software and/or projects that are not
feasible. Furthermore, the methodical character of the CDP ensures that the conceptual design is made
explicit. The documentation of the executed conceptual design process thereby serves as justification
of the design choices made during the design process to the client.
The explicit details and structural character of the CDP is being praised by the experts, although
it is also recognised as a potential fallback. This is due to the fact that in practice, it is an extensive
process to go through. The question that is asked is if this much detail significantly increases the
added value of the CDP for use in real projects. The experts therefore recommend to assess in further
research if a light version of the CDP as presented here is of about the same value. If so, a light version
of the CDP can be constructed from the detailed version of the CDP as presented here. At least, and
backed by the participants of the round table meeting, the structure and level of detail of the CDP
makes sure that a designer will not be able to miss a step in the design process. Although the designer
can choose to skip a design step or design activity, the CDP compels the designer to at least think
about it.
According to the experts, the CDP components together form a logical and consistent whole. To
their knowledge, the design activities in each sub-design process seem to be clear and usable in different
cases. This accounts for the universal applicability of the CDP. They found it particularly useful that
94

Cognition Enhanced Business Processes
the CDP indicates how much design activities exist and have to be conducted. It thereby provides also
knowledge about when a particular sub-design process is completed, as well as the final conceptual
design of a solution. Hereby, the CDP is considered to be methodical in its essence.
Another strong point of the CDP acknowledged by the experts is the fact that the CDP is able to
grasp and explicitly formulate the implicit and explicit knowledge about how to perform tasks and
processes, present in a business process and particular tasks in the business process. Until know, the
experts found it hard to identify and specify the implicit knowledge that is present in an organisation,
embodied by its people. This also counts for the implicit knowledge about potential solutions, which
is in the CDP covered by the DPC. This denotes the context-awareness of the CDP.
As stated earlier in the validation of the CRDP, the experts state the human-centric perspective taken
by the CDP as valuable, next to its focus on cognition from a systems perspective. However, due to this
view on the design of a solution, the CDP constructs solutions from within certain solution boundaries,
set by the situation of the business process as it is in know. It does not take a to-be perspective, and
from there reasons back to the means that are available to fulfil the requirements set by the envisioned
state of the solution. This is considered by some experts as a fallback of the constructed CDP.
As encountered in executing the case study, people working in the claim process interviewed for the
design process did not feel threatened by the content and form of the analysis of the cognitive activities
they performed. Thereby the CDP is on this point considered to be aware of the context it is performed
in. Besides, data is a factor in the design process that is considered by the experts to ‘make or break’
possible design possibilities and thereby final (conceptual) designs along the way of designing cognition
enhanced business processes. They acknowledge the importance of a component of the DAQA in the
design process. This also accounts for the context-awareness of the CDP.
As explained in Chapter 3 to 6, changes in the design do occur when the CRDP, CCDP and CPDP
are executed by different designers. Besides this, variety in the result of CDP can also be caused by the
DPC and/or DAQA. For example, different designers interpret design objective and control process
evaluations in a different way, resulting in different design strategies. The DPC provides the designer
space to executes the CDP in the way that he or she thinks is best, shaping the solution in a particular
way. This also holds for the DAQA, where different designers can focus on different data quality aspects
that he or she thinks is of importance. In these ways, the flexibility of the CDP is also covered in the
DPC and DAQA components.
7.5.2 Validation of Conceptual Design
To validate the (first steps of the) conceptual design such as it results from the CDP, a second case
study is used. This second case study also entails a claim assessment process of a health insurance

company. In this case study, the three design sub-processes CRDP, CCDP and CPDP have been com-
pleted, resulting in a list of cognitive possibilities for that particular business process. This validation

therefore presents its findings according to these three sub-design processes, concluding with some
general notions on the validity of the process. As mentioned before, the DAGSP is not in scope in this
95

Chapter 7. Constructing & Validating the Conceptual Design Process
research, and therefore also not addressed in this validation.

Validity of results of
CDP (Conceptual
Design Possibilities)

Assess objectivity
of results

Assess added
value for the
business

Reproducibility Design
independent

Implicit added
value for design

Explicit added
value for design

Before continuing to the results of the validation, a short description of the case study is provided
here. In general, this case study is similar to the characteristics of case study 1 in that it also is
a claim-assessment process, assessing health insurance claims if they should be reimbursed or not.
However, the two case studies are different in the way that they exist in two different health insurance
companies, and therefore comprise different people, systems and processes. Hence, this case study can
be used to validate the CDP if it indeed results in valuable outcomes for the business.
The validation is conducted with three persons of the insurance company and one business-AI
analyst of the consultancy company Jibes. Of the insurance company, two interviewees hold positions
in the Medical Assessment department of the organisation, responsible for claims that comprise health
support tools: a claim assessor and an advanced assessor. The claim assessor (CP) is part of the
processing team, actually assessing claims from start to finish. The advanced assessor (AP) advises
the processing team with medical substantive matters, and is responsible for the periodical checks of
claims, maintaining and updating protocols, e.g. work instructions, and the higher level assessment
of cases (claims that belong to the same case). The third person that is been interviewed to assess
the validity of the conceptual design is an internal adviser (IA) on strategy and development of the
business, currently participating in (internal) projects to determine the value of Artificial Intelligence
for the business. From the consultancy company, a business-AI analyst (BA) that participated in
a project to build an AI solution for the insurance company is also used as a source of information
regarding the validity of the Conceptual Design deliverables.
7.5.2.1 Validation of CRDP
In Appendix D, the three deliverables of the Cognitive Requirements Design Process are documented:
the Business Process Model of the claim assessment process, the Cognitive Demands Table of the
Cognitive Task Analysis, and the Cognitive Requirements table as the end result of the CRDP. The
main question here is to assess the validity of these deliverables of the CRDP executed in this case
96

Cognition Enhanced Business Processes
study. That is, are these deliverables established objectively, and do they result in added value to the
business and the CDP as a whole in the end.
The Business Process Model is constructed based on an interview with the IA and validated afterwards
with the BA of the consultancy company. Since Business Process Modelling is a mainstream practice
in the business community as well as the scientific community, this deliverable is regarded as relatively
straight forward in the CRDP. This step is furthermore not of high importance for the resulting
Cognitive Requirements of the CRDP, it mainly provides the designer with a sound overview and
decomposition of the business process at hand, providing relevant general information about the process
and indicate about which concepts substantive knowledge is required such that the next steps of the
CRDP can be executed properly. The final version of this deliverable is checked by the IA and BA
and judged to be objective. Furthermore, they stated that it is a valuable step to execute, because it
provides an overview of the possible tasks for further study in the CDP.
The Cognitive Task Analysis is conducted with the CP and the AP. Relative to the previous
addressed Business Process Model, the Cognitive Task Analysis is a less straightforward method to
perform. Some prior study on how such an analysis should be conducted is necessary. There should be
mentioned that this analysis inherently comprises subjective judgement of the designer. However, after
validation with the IA of the insurance company, the Cognitive Task Analysis is considered to result
in interesting insights in the cognition that is required for executing the task(s) of the business process
at hand. Furthermore, the BA stated that this analysis does result in insights that are objectively
satisfying.
The last step of the CRDP entails the refinement of the outcomes of the Cognitive Task Analysis
into the cognitive requirements. This step is meant to more precisely formulate these requirements of
the cognition necessary to perform the task(s) in the business process. Therefore, this step actually
helps to improve the degree of objectivity of the cognitive requirements. The BA recognises the value of
this step to refine the results of the Cognitive Task Analysis, mainly because it will serve their purpose
in a better way: the matching of the cognitive requirements to the cognitive capabilities, formed in the
next design sub-process.
Overall, the CP, AP and IA acknowledged the correctness of the content of the deliverables, which
serves a quick verification. Furthermore, the IA stated the value to the business of the whole CRDP,
since it provides insights in the cognitive aspects of the business process under study. However, the IA
indicated that the Cognitive Task Analysis would result in more valuable insights if the analysis would
be refined and adjusted more specifically to the type of process it is conducted on. This adjustment
exercise of the Cognitive Task Analysis is not performed in this research, but seems as a valuable thing
to do regarding the goals of the CDP. It is therefore discussed in the further research section of this
thesis (see Chapter 8).
7.5.2.2 Validation of CCDP
In this case study, IBM Watson software is selected on forehand in the project to construct a proper AI
system for the claim assessment process. This case study is therefore the same as the case study used
97

Chapter 7. Constructing & Validating the Conceptual Design Process
in this research for illustrative purposes. For the elaborate description of the extraction of cognitive
functions from IBM Watson software services, the same information is therefore used. This information
can be found in Appendix C. The main question here is to assess the validity of the decomposition of
IBM Watson services to its cognitive capabilities as is performed in the Cognitive Capability Design
Process. That is, if this decomposition is performed objectively, and resulted in added value to the
business and the CDP as a whole.
For this part of the validation, the BA is questioned about his thoughts concerning the results of
this exercise. The decomposition of the AI software is experienced as a valuable exercise, since it
provides information about the functional capabilities of the software services, examples how it will
function, identifies the formal (scientific) fields of which the services, and the cognitive aspects that
each individual service can deliver, i.e. its cognitive capabilities. These insights are not yet found in
other sources of information as specific as the CCDP for business processes. It is therefore considered
to be valuable for the business. Because the CCDP holds a crucial position in the CDP, namely the
supply of the capabilities of the to be designed (software) system, it is found also valuable for the CDP
as a whole.
7.5.2.3 Validation of CPDP

The Cognitive Possibilities Design Process entails the matching of the cognitive capabilities, the out-
comes of the CCDP, to the cognitive requirements, which are the outcomes of the CRDP. The CPDP

is important to the overall CDP, since the CPDP delivers the (final) outcomes of the whole Conceptual
Design Process as addressed in this research. The CPDP provides the outcomes of the first three steps
of the conceptual design phase which are addressed in this research, and thereby the final outcomes of
the CDP as constructed in this research. However, they are thus not the final outcome of the formal
conceptual design phase. The main question here is to assess the validity of this matching process of
the CPDP by indicating its objectivity and value to the business.
The objectivity of the results of the CPDP is to assess by means of questioning the BA. This person
stated that the results of the CPDP are to some extent objective, although the character of this
matching exercise is rather subjective. Proper documentation and discussion of the (intermediate)
results with peers support the objectivity of the CPDP. To be clear, the CPDP as presented in this
research is not foolproof, since it does not provide a detailed road map that fully voids improvisation,
creativity, etc., i.e. the design skill(s) that a designer is expected to have.
The value to the business of the CPDP is to assess through questioning the BA. The BA stated
that the cognitive possibilities resulting from the CPDP are of value, if the matching of the cognitive
requirements to the cognitive capabilities is properly done. More specifically, the results are of value for
the business, in that they comprise the possibilities of AI software to possibly and ultimately increase
the efficiency and consistency of the claim assessment and decrease the number of errors made in this
assessment. However, the assessment if these possibilities indeed result in these envisioned business
goals is outside the scope of this study.

98

Cognition Enhanced Business Processes

7.5.2.4 General Remarks
Besides the validation of the three components of the resulting Conceptual Design from the CDP, some
general remarks are presented here. The BA of the health insurance company characterised the focus of
the CDP as human-centric, since it specifically addresses the cognition used in the business process by
people. Therefore, he recognises the potential supportive role of designed, developed and implemented
(cognitive) systems to people executing tasks in the process at the moments where it is really needed.
Furthermore, the BA acknowledged that the support or (partial) automation of activities within a
task has the potential to decrease the number of moments where cognitive skills are performed while
being unnecessary. In both ways, the BA thus states that utilising the CDP generates an overview of
the possibilities for an enhancement of the cognition of the business process as a whole by means of
integrating AI software to a (still) unknown extent. Recall that the level of cognition enhancement of
a business process is outside the scope of this research.

However, the BA also states some criticising notions. Namely, AI is not always the solution to op-
portunities to improve the business process. For example, in many ways the business process benefits

more from very strict rule-based solutions, which have the advantage above more fuzzy solutions such
as machine learning models. The criteria of reproducibility and traceability play an important role
here. Furthermore, he questions the as-is oriented analysis performed in the CRDP, since it describes
the current situation of the business process. However, in the first place, it could be that the process
itself can be improved by optimisation projects, resulting in a different kind of process. Extending on
this, the to be designed, developed and implemented system as envisioned, will also require a different
business process. That is because the way of working (due to this new system) will change. These two
points are however outside the scope of this research and thus not addressed.
7.6 Chapter Summary
To conclude, the CRDP, CCDP, CPDP and DAGSP can be merged such that they form the conceptual
design process envisioned in Chapter 1. The first design process addresses the whole process of analysing
a Business Process and formulating its Cognitive Requirements. The CCDP addresses the analysis of
the (chosen) AI software and formulating its Cognitive Capabilities. The outcomes of these first two
design processes can be matched to each other, such that the opportunities for AI software in the
considered Business Process can be identified and described. The last design process, DAGSP, finally
generates and selects a feasible design alternative, but is not covered by this research.
These four design processes are overseen and managed by a fifth of the Conceptual Design Process,
which is Design Process Co-ordination. DPC determines and controls the strategy of the design process,
making sure that the design objectives will be met by managing the design activities of the conceptual
design sub-processes. At last, the sixth component of the CDP is the Data Availability & Quality
Assessment component. The DAQA ensures that, during the design process, the design fits to the data
available and its quality.
When this final design is developed and implemented in a proper way, the cognition utilised in
the business process will (partly and to some degree) be enhanced. However, the development and
implementation phases are not part of this research and thus not addressed. The value of the CDP

Chapter 7. Constructing & Validating the Conceptual Design Process
and its usefulness for system designers are assessed by means of a case study and expert workshop.
Based on these validations, the conceptual design process is found to be effective, useful and usable to
a large extent.
2.0 ANATOMY OF A NEURON
The basic unit of the nervous system is the neuron (nerve cell). A neuron is the name
given to the nerve cell and all its processes (Snell et al., 2010). Neurons are found in ganglia, in
the brain and spinal cord (Snell et al., 2010). They receive input from other neurons, and are
excitable cells specialized for the reception of stimuli and the conduction of nerve impulse (Snell
et al., 2010).
Neurons vary considerably in size and shape, but each possesses a cell body from whose
surface project one or more processes called neurites (Snell et al., 2010). The neurites, also
known as dendrites, are responsible for receiving and conducting information towards the cell
body (Snell et al., 2010). The term nerve fiber is used to refer to the dendrites and axons which
conduct impulses away from the cell body (Snell et al., 2010).

Fig. 1.0 Structure of a Neuron (Papadourakis, 2014)

- 6 -

The figure (Fig. 1.0) above shows the structure of a biological neuron with various parts.
They are:
(a) The Dendrites
(b) The Cell body or Soma
(c) The Axon
THE DENDRITES
The dendrites are the branched projections of a neuron that helps to conduct the
electrochemical stimulation received from other neural cells; by upstream neurons via synapses
which are located at various points throughout the dendritic arbor, to the cell body of the neuron
from which the dendrites project (Mujeeb, 2012). Dendrites are important in integrating synaptic
inputs and in determining the extent to which action potentials are produced by the neuron
(Mujeeb, 2012). More so, recent research has found that dendrites can support action potentials
and release neurotransmitters, a property believed to be specific to axons (Mujeeb, 2012).
THE SOMA
This is where the signals from the dendrites are joined and passed on. The soma serves to
maintain the cell and keep the neuron functional, but does not play an active role in the
transmission of the neural signal (Mujeeb, 2012).
THE AXON
The axon is also known as a nerve fiber. It is a long, slender projection of a nerve cell or
neuron that typically conducts electrical impulses away from the neuron’s cell body or soma
(Mujeeb, 2012). Axons are distinguished from dendrites by several features, including shape,
length, and function. Some types of neurons have no axon and transmit signals from their

- 7 -

dendrites (Mujeeb, 2012). No neuron ever has more than one axon, as most axons branch in
some cases very profusely (Mujeeb, 2012).

2.1 BIOLOGICAL AND ARTIFICIAL NEURONS
The types of neurons that interconnect to form neural networks are:
1. Biological Neurons (Nerve cells)
2. Artificial Neurons
The biological neurons constitute the basic building blocks of the human brain. Artificial
neurons on the other hand, forms artificial neural networks (ANNs) which are computer
algorithms inspired by a neuron and modeled after brains, to perform specific computational
tasks (Mano, 2014).

Fig 1.1 Artificial Neuron Model .

(Stergiou and Siganos, 2014).
The above figure (Fig. 1.1) shows a model of an artificial neuron described in 1943 by
McCulloch and Pitts. It consists of inputs with an electrical impulse represented as (1) or with no
electrical impulse represented as (0) (Reed, nd.). Each input has a weight associated with it and

- 8 -

the activation function multiplies each input value by its weight. If the sum of the weighted
inputs is greater than or equal to , then the neuron fires and returns 1, if not, it does not fire but
returns 0 (Reed, nd.).
Each neuron basically has an activation threshold, and series of weighted connections to
other neurons (Mano, 2014). When the aggregate activation that a neuron receives from the
neurons connected to it exceeds its activation threshold, the neuron fires and relays its activation
to the neurons connected to it. The weights associated with these connections can be modified by
training the network to perform certain task, and this modification accounts for learning (Mano,
2014). Artificial neuron forms the basis of artificial neural networks (ANNs). In the 1950’s, it
became a focus of Computer Science research, because at that time, it was said that humans lack
the speed and memory of computers, yet are capable of complex action and reasoning.
2.2 NEURAL NETWORKS
Neural networks are formed by interconnecting neurons. There are two types of neural
network: Biological Neural Network (BNN) and Artificial Neural Network (ANN).
2.3 BIOLOGICAL NEURAL NETWORKS
The neural network in the brain is an interconnected web of biological neurons
transmitting elaborate patterns of electrical signals. The human brain can anatomically be
distinguished into several divisions like; the cortex, brainstem, cerebellum e.t.c. It can further be
subdivided into several areas and regions according to the functions performed by them, and the
anatomical structure of the neural networks within it (Mano, 2014).

- 9 -


Fig 2.0 Anatomical areas of the Human Brain (Willamette, 2014)
The figure (Fig. 2.0) shows the different anatomical areas of the human brain. These
areas consist of numerous biologically interconnected neurons, which works in a very complex
and elaborate way.
In biological neural network, dendrites receive input signals which fires an output signal
based on the input signals. The overall pattern of bundles of neural connections (projections)
between areas is extremely complex, and only partially known.
(Shiffman, 2014)
The figure (Fig. 2.1) shows a biological neural network formed by an interconnected
nerve cell. Apart from forming long-range connections with neighboring neurons, they also link
up with many thousands of their neighbors, to form very dense, complex local networks.
2.3.1 GENERAL ARCHITECTURE OF NEURAL NETWORKS IN THE BRAIN
The general brain architecture contains many networks formed by interconnected
neurons. Each neuron is a cell that uses biochemical reactions to receive process and transmit
information. The terminal button of each neuron is connected to other neurons across a small gap
called a synapse.
with the dendrites of another neuron. The dendrite serves as the input device which receives
electrical signals or impulses from other neurons. The cell body or soma gives a summation of
inputs from the dendrites which causes excitation or inhibition. When the summation of inputs
from the dendrites exceeds some threshold, the neuron fires an output along axon (Mujeeb,
2012).
A neuron typically receives input from other neurons via dendrites as seen in figure 2.3.
Its dendritic tree is connected to a thousand neighboring neurons. When a neurons fires, a
positive or negative charge is received by one of the dendrites. The strengths of all the received
charges are added together through the processes of spatial and temporal summation. These
inputs sums approximately, and once the summed input exceeds a critical level, the neuron
discharges an electrical pulse that travels from the body, down the axon, to the next neuron(s) or

- 12 -

receptor(s). This event leads to depolarization, followed by a refractory period, during which the
neuron is unable to fire.
The axon endings of a neuron form the output zone. The output zones almost touch the
dendrites or cell body of the next neuron. Transmission of an electrical signal from one neuron to
the next is effected by chemicals called neurotransmitters, which are released from the first
neuron and which bind to receptors in the second neuron. This forms a linkage between the two
neurons, and is called a synapse (Mujeeb, 2012).
THE SYNAPSE

(Tewari, 2012)

Fig. 2.4 Synapse
A synapse is a structure that permits a neuron to pass an electrical or chemical signal to
another neuron. The word “synapse” comes from ”synaptein”, which Sir Charles Scott
Sherrington and colleagues coined from the Greek ”syn-” (together) and ”haptein” (to clasp).
Synapses are essential to neuronal function and are the means by which neurons pass
signals to individual target cells. At a synapse, the plasma membrane of the presynaptic neuron

- 13 -

comes into close apposition with the membrane of the postsynaptic cell. Both the presynaptic
and postsynaptic sites contain extensive arrays of molecular machinery that link the two
membranes together and carry out the signaling process (Mujeeb, 2012).
In many synapses, the presynaptic part is located on an axon, but some presynaptic sites
are located on a dendrite or soma. There are two fundamentally different types of synapses:
1. The chemical synapse and
2. The electrical synapse.
In a chemical synapse, “the presynaptic neuron releases a chemical called a
neurotransmitter that binds to receptors located in the postsynaptic cell, usually embedded in the
plasma membrane” (Mujeeb, 2012). The neurotransmitter sometimes initiates an electrical
response or a second messenger pathway that may excite or inhibit the postsynaptic neuron.
In an electrical synapse, the presynaptic and postsynaptic cell membranes are connected
by gap junctions that are capable of passing electrical current, and causing voltage changes in the
presynaptic cell to induce voltage changes in the postsynaptic cell (Mujeeb, 2012). Electrical
synapses rapidly transfers signals from one cell to the next, which is a major advantage
The extent to which the signal from one neuron is passed on to the next depends on many
factors, such as:
(a) The amount of neurotransmitter available,
(b) The number and arrangement of receptors,
(c) Amount of neurotransmitter reabsorbed, etcetera.

- 14 -
2.4 ARTIFICIAL NEURAL NETWORKS
An artificial neural network (ANN) is a computational system, where information is
processed collectively, in parallel throughout a network of nodes (neuron) (Shiffman, 2014). In
ANN the individual elements of the network, the neurons (nodes), read an input, process it, and
generate an output. With ANN, a network of many neurons, can exhibit incredibly rich and
intelligent behaviors (Shiffman, 2014).
The ANN can also be described as an information processing pattern that is inspired by
the way biological nervous systems process information (Stergiou and Siganos, 2014). The key
element of this pattern is the novel structure of the information processing system. Analysis of
the artificial neural network (ANN) shows that it is a method of data analysis, which imitates the
human brain’s way of working. It is composed of a large number of highly interconnected
processing elements known as neurons, working in unison to solve specific problems.
As a part of Artificial Intelligence, Artificial Neural Networks (ANNs) attempt to bring
computers a little closer to the brain's capabilities by imitating certain aspects of information
processing in the brain, in a highly simplified way (Willamette, 2014). An artificial neural
network (ANN) is a programmed computational model that aims to replicate the neural structure
or the architecture and functioning of the human brain. It consists of an interconnected structure
of artificially produced neurons that function as pathways for data transfer. Artificial neural
networks are flexible and adaptive, learning and adjusting with each different internal or external
stimulus.
The power of Artificial Neural Networks (ANNs) has been successfully used over the
years in many types of problems with different degrees of complexity and in different fields of
application. Neural networks represent the way in which arrays of neurons probably function in

- 15 -

biological learning and memory. These networks are known as the computational models with
particular characteristics such as the ability to learn or adapt, to organize or to generalize data.
The learning of ANNs takes place by training with examples, in a process that uses a
training algorithm to iteratively adjust the connection weights between neurons to produce the
desired input–output relationships. This has been widely used in optimization, calibration,
modeling and pattern recognition. ANNs is very useful in medical and pharmaceutical sciences,
and in diagnosis of diseases. They have shown a good potential in calculation of physic-chemical
and biological properties of drugs with more attention to pharmaceutical and chemical areas . In

recent years, the pharmaceutical applications of ANN have been reviewed by Agatonovic-
Kustrin and Beresford, and have been used to calculate aqueous solubility of drugs employing a

number of molecular descriptors.
It is proposed that by using artificial neural networks and by designing and testing the
appropriate ANN, it could allow the prediction of binding energy of drugs on basis of structural
descriptors describing the structure of selected basic drugs.
2.4.1 HISTORICAL BACKGROUND OF ANNS
Artificial Neural Network (ANN) or Neural network simulations appear to be a recent
development. Historically, this field was established before the advent of computers. The first
artificial neuron was produced in 1943 by the neurophysiologist Warren McCulloch and the
logician Walter Pitts (Stergiou and Siganos, 2014). Warren McCulloch and Walter Pitts could
not achieve much at that time as a result of the technology available at that time.
The use of inexpensive computer emulations enabled important advances in neural
network simulations. There were periods of excitement initially, followed by periods of
frustration when funding and professional support was minimal, as further vital advances were

made by relatively few researchers (Stergiou and Siganos, 2014). In 1969, Minsky and Papert,
published a book, summing up a general feeling of frustration against neural networks among
researchers. However, other pioneers of neural network simulations were able to develop
convincing technology which surpassed the limitations identified by Minsky and Papert, which
was accepted by many without further analysis (Stergiou and Siganos, 2014). As of this time,
the neural network field currently enjoys a resurgence of interest and a corresponding increase in
funding.
2.5 GENERAL ARCHITECTURE OF ARTIFICIAL NEURAL NETWORK (ANN)
An artificial neuron is the basic building block of an artificial neural network which
consists of many interconnected neurons, each working in parallel, with no central control.
Neurons in artificial neural networks are often organized into layers, where the neurons in one
layer are only connected to those of adjacent layers. Each neuron-to-neuron connection has an
associated weight, and learning within the network is accomplished by updating these weights
(Mano, 2014).
An ANN usually takes one or more inputs, and produces one or more outputs, based on
the strength of the connections within, and the way the connections change the input signals.
Each neuron receives a signal. If this signal exceeds a certain threshold, it is modified and
propagated to connected neurons. The output layers of neurons, those which do not propagate
their signals to other neurons, produce the output calculated by the whole network (Mano, 2014).
An Artificial Neural Networks (ANN) involves nodes that are known as neurons. The neurons
are structured into a sequence of layers and connected to each other by using variable connection
weights. Each layer can have a number of different neurons with various transfer functions.

Like the human brain, Artificial Neural Networks, learn by example. From what is known
of neuronal structures, the human brain learns by altering the strengths of connections between
neurons, and by adding or deleting connections between neurons (Willamette, 2014). An ANN
on the other hand, is configured for a specific application, such as pattern recognition or data
classification, through a learning process. Learning in biological systems is similar to learning in
ANNs, as it involves adjustments to the synaptic connections that exist between the neurons.
The structure and computation of artificial neural networks (ANNs) are often organized
into layers, with each layer receiving input from one adjacent layer, and sending it to another.
The Layers are categorized as:
1. Input layers
2. Output layers, and
3. Hidden layers.
The input layer is initialized to a certain set of values, and the computations performed by
the hidden layers update the values of the output layers, which comprise the output of the whole
network (Russel et al, 2002).
2.6 USES OF ARTIFICIAL NEURAL NETWORKS
Artificial neural networks have been used for a variety of tasks. These include:
(i) Artificial Intelligence: ANN has been used as a form of weak artificial intelligence, to study
how the brain works. Certain types of brain damage can be modeled by removing nodes and
connections from an appropriately trained network. They can also be used to estimate
mathematical functions, and extract features from images for optical character recognition. An
artificial neural network, the Autonomous Land Vehicle in a Neural Network, was used by
Carnegie Mellon University's NAVLAB to extract road features for navigating an unmanned

vehicle. Neural networks have also been used for voice recognition, game playing and email
spam filtering.
(ii) Learning: Learning in neural networks can be supervised or unsupervised. It is
accomplished by updating the weights between connected neurons. The most common
method for training neural networks is back propagation, a statistical method for updating
weights based on how far their output is from the desired output. To search for the
optimal set of weights, various algorithms can be used. The most common is gradient
descent, which is an optimization method that, at each step, searches in the direction that
appears to come nearest to the goal (Heidelberg, 2005).
(iii) Adaptive learning: An ability to learn how to do tasks based on the data given for
training or initial experience.
(iv) Self-Organization: An ANN can create its own organization or representation of the
information it receives during learning time.
(v) Real Time Operation: ANN computations may be carried out in parallel, and special
hardware devices are being designed and manufactured which take advantage of this
capability (Heidelberg, 2005).
(vi) Fault Tolerance via Redundant Information Coding: Partial destruction of a network
leads to the corresponding degradation of performance. However, some network
capabilities may be retained even with major network damage (Heidelberg, 2005).
2.7 REASONS FOR THE STUDY OF ANNs IN NEUROSCIENCE
Neural networks are a popular target representation for learning. Neural networks are
inspired by the neurons in the brain. Artificial neural networks typically contain many fewer than

- 19 -

the approximately 100 billion neurons that are in the human brain, and are much simpler than
their biological counterparts (Poole, 2010).
Artificial neural networks are interesting to study in neuroscience, because it is employed
to understand real neural systems. Researchers in the field are simulating the neural systems of
simple animals to lead to an understanding about which aspects of neural systems are necessary
to explain the behavior of these animals (Poole, 2010).
One hypothesis states that the only way to build the functionality of the brain is by
using the mechanism of the brain by attempting to build intelligence using the mechanism of
the brain, as well as without using the mechanism of the brain.
ANNs is interesting to study because the brain inspires a new way to think about
computation that contrasts with currently available computers, because it simulates the
functionality of a biological neuron. The brain consists of a huge number of asynchronous
distributed processes, all running concurrently with no master controller, unlike current
computers that have a few processors, and a large but essentially inert memory (Poole, 2010).

2.8 APPLICATION OF ANNs IN THE FIELD OF MEDICINE
Artificial Neural Networks (ANN) is currently an interesting research area in medicine,
and it is believed that in few years they will receive extensive application to biomedical systems.
Currently, the research is mostly on modeling parts of the human body and recognizing diseases
from various scans e.g. cardiograms, CAT scans, ultrasonic scans, etc (Stergiou and Siganos,
2014).
According to Stergiou and Siganos (2014) artificial neural networks are ideal in
recognizing diseases using scans since there is no need to provide a specific algorithm on how to

- 20 -

identify the disease. Neural networks learn by example so the details of how to recognize the
disease are not needed. What is needed is a set of examples that are representative of all the
variations of the disease. The examples need to be selected very carefully if the system is to
perform reliably and efficiently. In the field of medicine, ANN can be applied in the following:
(a) Diagnosing the Cardiovascular System via Neural Modeling
Neural Networks are used experimentally to model the human cardiovascular system.
Diagnosis can be achieved by building a model of the cardiovascular system of an individual and
comparing it with the real time physiological measurements taken from the patient (Stergiou and
Siganos, 2014). If this routine is carried out regularly, potential harmful medical conditions can
be detected at an early stage and thus make the process of combating the disease much easier.
A model of an individual's cardiovascular system must mimic the relationship among
physiological variables i.e., heart rate, systolic and diastolic blood pressures, and breathing rate
at different physical activity levels. If a model is adapted to an individual, then it becomes a
model of the physical condition of that individual (Stergiou and Siganos, 2014). The simulator
will have to be able to adapt to the features of any individual without the supervision of an
expert. This calls for a neural network.
Another reason that justifies the use of ANN technology is the ability of ANNs to provide
sensor fusion which is the combining of values from several different sensors. Sensor fusion
enables the ANNs to learn complex relationships among the individual sensor values, which
would otherwise be lost if the values were individually analyzed. In medical modeling and
diagnosis, this implies that even though each sensor in a set may be sensitive only to a specific
physiological variable, ANNs are capable of detecting complex medical conditions by fusing the
data from the individual biomedical sensors (Stergiou and Siganos, 2014).

- 21 -

(b) Electronic noses
ANNs are used experimentally to implement electronic noses. Electronic noses have
several potential applications in telemedicine. Telemedicine is the practice of medicine over long
distances via a communication link. The electronic nose would identify odours in the remote
surgical environment. These identified odours would then be electronically transmitted to
another site where an odour generation system would recreate them. Because the sense of smell
can be an important sense to the surgeon, telesmell would enhance telepresent surgery (Stergiou
and Siganos, 2014).
(c) Instant Physician

An application developed in the mid-1980s called the instant physician trained an auto-
associative memory neural network to store a large number of medical records, each of which

includes information on symptoms, diagnosis, and treatment for a particular case. After training,
the net can be presented with input consisting of a set of symptoms; it will then find the full
stored pattern that represents the best diagnosis and treatment (Stergiou and Siganos, 2014).
Another key element of an artificial neural network is its ability to learn. A neural
network is a complex and adaptive system, which can change its internal structure based on the
information flowing through it. Typically, this is achieved through the adjustment of weights.
Each connection between neuron has a weight, which controls the signal between the two
neurons. The ability of a neural network to learn, to make adjustments to its structure over time,
is what makes it so useful in the field of artificial intelligence. Artificial Neural Networks have
other diverse application in:
1. Pattern Recognition: This is a common application of artificial neural networks
used in facial recognition, optical character recognition, etc (Shiffman, 2014).

- 22 -

2. Time Series Prediction: Artificial Neural networks can be used to make predictions
e.g. predictions on rise and fall in the stock market, predictions on whether.
3. Signal Processing: Cochlear implants and hearing aids need to filter out
unnecessary noise and amplify the important sounds. Neural networks can be
trained to process an audio signal and filter it appropriately (Shiffman, 2014).
4. Control: you may have read about recent research advances in self-driving cars.
Neural networks are often used to manage steering decisions of physical vehicles,
or simulated ones (Shiffman, 2014).
5. Soft Sensors: A soft sensor refers to the process of analyzing a collection of many
measurements. Artificial Neural networks can be employed to process the input
data from many individual sensors and evaluate them as a whole. For example, a
thermometer can give us information about the temperature of the air. With
artificial neural networks, we can also get additional information on humidity,
barometric pressure, dew point, air quality, air density, etc (Shiffman, 2014).
6. Anomaly Detection: because neural networks are so good at recognizing patterns,
they can also be trained to generate an output when something occurs that doesn’t
fit the pattern. For example, a neural network monitoring a person’s daily routine
over a long period of time could alert one when something goes wrong, after
learning the patterns of such behavior (Shiffman, 2014)
Artificial Intelligence (AI) can be defined as a subfield of computer science closely tied
with biology and cognitive science. It is concerned with computing techniques and models that
simulate and investigate intelligent behavior. Research into artificial intelligence builds upon our
understanding of the brain, its evolutionary development, and provides insights into the way the
brain works, as well as the larger process of biological evolution.
Artificial Intelligence (AI) can also be simply described as a collection of hard problems
which can be solved by humans and other living things, but for which the algorithm for solving
them is not available (Zhang, 2011). Although a subfield of computer science, Artificial
Intelligence (AI) has had its fair share from the field of neuroscience, particularly in the study of
the brain.
How the brain enables human beings to think has remained a mystery until the present
day. However, significant ventures in the field of Artificial Intelligence have enabled scientists to
come close to the nature of thought processes inside a brain.
3.1 RESEARCH AREAS AND APROACHES IN AI
Two major research areas in Artificial Intelligence (AI) are:
(a) Artificial Neural Networks: This area of research involves building a model of the brain
and training that model to recognize certain types of patterns.
(b) Genetic Algorithms: This area of research deals with evolving solutions to complex
problems that is hard to control or deal with using other methods.

- 24 -

Artificial Neural Networks (ANN) are an important part of Artificial Intelligence (AI), an
area of computer science concerned with making computers behave more intelligently (Agarwal,
2014). ANN is modeled on the brain where neurons are connected in complex patterns to process
data from the senses, establish memories and control the body (Agarwal, 2014). ANN process
data and exhibit some intelligent behaviors like learning, generalization and pattern recognition.
3.2 BENEFITS OF ANNs IN ARTIFICIAL INTELLIGENCE
ANNs forms an important aspect in the field of Artificial intelligence and are very much
involved in so many exciting applications of AI. The benefit of ANNs in Artificial Intelligence
includes:
1. Backpropagation Nets
Backpropagation nets learn to generalize and classify patterns. When they are presented
with a pattern, the interconnections between the artificial neurons are adjusted until they give a
correct response. Backpropagation nets are the most common kind of ANN (Mano, 2014). The
basic topology is that layers of neurons are connected to each other. Patterns cause information
to flow in one direction, then the errors "backpropagate" in the other direction, changing the
strength of the interconnections between layers (Mano, 2014).
A successful example of backpropagation nets is NetTalk. NetTalk was invented by Terry
Sejnowski, professor and head of the Computational Neurobiology Laboratory at the Salk
Institute in La Jolla, California (Mano, 2014). This net learns to read English or any other
language and is used all over the world to read to blind people. Basically with backpropagation
nets, after sufficient training with a number of patterns, they will give the correct response to a
pattern they have never seen (Mano, 2014).

2. Hopfield Nets
John Hopfield, a Nobel Prize-winning physicist at California Institute of Technology
(Caltech), invented Hopfield nets. The basic topology is that every artificial neuron is connected
to every other artificial neuron. These nets memorize collections of patterns (Mano, 2014). When
given a part of one of the patterns or a badly distorted pattern, the net delivers the complete
pattern. Hopfield nets have been applied in fingerprint recognition. Given a partial print or a
smudged print, the Hopfield net can deliver the complete fingerprint (Mano, 2014). NASA uses
Hopfield nets to orient deep-space craft by visual star fields. When the craft looks at a picture of
the stars, a Hopfield net can match the view with the known pictures of the stars to orient the
craft (Mano, 2014).
3. Self-Organizing Maps
Finnish professor Teuvo Kohonen invented self-organizing maps, also known as
Kohonen nets. The basic topology is that each artificial neuron is connected only to its neighbors
(Mano, 2014). Kohonen nets reduce the complexity of data--especially experimentally obtained
data. Repeatedly "training" a Kohonen net with an n-dimensional data set can produce a lower
dimensional data set that captures the essential nature of the n-dimensional data set in a much
simpler form (Mano, 2014). A major application of self-organizing maps is in the several
projects that are looking for a simpler way to understand the Internet. Kohonen nets are regularly
used as a preprocessor for other types of ANN (Mano, 2014).

- 26 -
3.3 APPLICATION OF ARTIFICIAL INTELLIGENCE
According to Zhang (2011) artificial intelligence can be applied in the following areas which
include:
1. Intelligent Agents
2. Information Retrieval
3. Electronic Commerce
4. Data Mining
5. Bioinformatics
6. Natural Language
7. Expert Systems
3.3.1 APPLICATION OF AI IN EXPERTS SYSTEMS
In expert systems Artificial Intelligence (AI) can be applied to do intelligent task. For
example, an experts system helps ford mechanics track down and fix engine problems as seen in
the diagram below.

(Zhang, 2011)
Fig. 3.1 Expert system used by ford mechanics.

Not only that, an airline scheduling program produced with the aid of an expert system,
offers graphical user interface to help solve complex airport scheduling problems. For example,
it can show graphics of planes circling the airport, the number of planes approaching the airport,
gate information, and two concourses with planes at their gate (Zhang, 2011). The airline
scheduling program can be seen below in figure 3.2.

4.0 BENEFITS OF NEURAL NETWORKS IN NEUROSCIENCE
The benefit of neural networks in neurobiology is quite enormous. Humans have not fully
understood the complex nature of the brain neural networks. In neurobiology analysis, the
understanding of neural networks is used in applications in medical science, psychological
science, behavioral analysis and treatment of diseases and defects of the nervous system.
Artificial neural networks on the other hand, helps as research tools in developing the
understanding of neural networks by simulating those networks. The use of artificial neural
networks models in research have led to significant developments in the field of neuroscience.
Neural networks are fault tolerant (Mano, 2014). This can be seen in Biological Neural
Networks that are inherently fault tolerant, which is seen in frequent cases of partial nervous
system or brain damage without disruption of life itself. Artificial neural networks (ANN), also
exhibit a similar high level of fault tolerance because of their highly distributed and modular
nature. In neural networks, if one particular component or a group of components fails, certain
functions cannot be performed (Mano, 2014). However, the capabilities of the intact components
are retained, and the networks do not completely fail. This makes them fault tolerant.
4.1 SCIENTISTS CONCERNED WITH NEURAL NETWORKS
Research into the use, benefits and applications of neural networks covers a wide range of
topics ranging from theoretical neurobiology to statistical physics and machine learning. It
encompasses so many fields such as neuroscience, computer science, engineering, statistics,
cognitive science, physics, biology and philosophy. Scientists from these disciplines are
concerned about the exciting and complex nature of neural networks.

- 29 -

The computer scientist wants to find out about the properties of non-symbolic
information processing with neural nets and about learning systems in general. Statisticians use
them as flexible, nonlinear regression and classification models (Heskes and Barber, 2014).
Engineers make use of the capabilities of neural networks in areas, such as signal processing and
automatic control. As for Cognitive scientists, they are trying to exploit neural networks as a
possible apparatus to describe models of thinking and consciousness relating to High-level brain
function (Heskes and Barber, 2014).
In the field of neuroscience, neuroscientists use neural networks to describe and explore
medium-level brain function such as memory, sensory systems, etcetera (Heskes and Barber,
2014). Physicists and Biologist also use neural networks. While Physicists use neural networks
to model phenomena in statistical mechanics and other tasks, Biologists use them to interpret
nucleotide sequences (Heskes and Barber, 2014).
Philosophers and some other academicians may also be interested in Neural Networks for
various reasons (Heskes and Barber, 2014).
4.2 THE COMPUTER AND THE HUMAN BRAIN
The brain's network of neurons forms a massively parallel information processing
system. This contrasts with conventional computers, in which a single processor executes a
single series of instructions. According to Willamette (2014), the similarities and contrast
between the brain and the computer is based on the following:
(a) Processing Element: While the brain has 1014 synapses, a computer has 1018 transistors as
their processing element.

- 30 -

(b) Processing Speed: For the human brain, it is composed of about 10 billion neurons while a
computer have less than 1 million processors (Zhang, 2011). While the human brain has a
processing speed of 100 Hz, that of a computer is 109 Hz.
(c) Style of Computation: The human brain performs massively parallel computations
extremely efficiently. For example, complex visual perception occurs within less than 100 ms,
that is, 10 processing steps; while the computer performs serial centralized computations (Zhang,
2011).
(d) Fault Tolerance: The human brain is fault tolerant. This means that partial recovery from
damage is possible if healthy units can learn to take over the functions previously carried out by
the damaged areas. This is not true with computers (Zhang, 2011).
(e) Intelligence and Consciousness: The brain supports our intelligence and self-awareness.
Conventional computers have not yet been able to do this.
(f) Learning: The human brain can learn to reorganize itself from experience unlike in
conventional computer, where little learning occurs (Zhang, 2011).

4.3 NEURAL NETWORKS VERSUS CONVENTIONAL COMPUTERS
Neural networks take a different approach to problem solving than that of conventional
computers. Conventional computers follow a set of instructions in order to solve a problem
(algorithmic approach). If the specific steps that the computer needs to follow are not known, the
computer cannot solve the problem (Stergiou and Siganos, 2014). That restricts the problem
solving capability of conventional computers to problems that we already understand and know
how to solve (Stergiou and Siganos, 2014).

- 31 -

Neural networks process information in a similar way the human brain does. The network
is composed of a large number of highly interconnected processing elements (neurons) working
in parallel to solve a specific problem. Neural networks learn by example, so they cannot be
programmed to perform a specific task (Stergiou and Siganos, 2014). The examples must be
selected carefully otherwise useful time is wasted or even worse the network might be
functioning incorrectly. A major disadvantage is that, because the network finds out how to solve
the problem by itself, its operation can be unpredictable (Stergiou and Siganos, 2014).
On the other hand, conventional computers use a cognitive approach to problem solving.
The way the problem is to be solved must be known and stated in small unambiguous
instructions. These instructions are then converted to a high level language program and then
into machine code that the computer can understand. These machines are totally predictable; if
anything goes wrong it will be due to a software or hardware fault.
Despite their many differences, neural networks and conventional algorithmic computers
complement each other. This is because, there are some tasks that are more suited to an
algorithmic approach like arithmetic operations and tasks that are more suited to neural networks
(Stergiou and Siganos, 2014).
4.4 WHY WE NEED BRAIN-LIKE INTELLIGENCE
Scientists have spent a lot of time researching and implementing complex solutions
involving brain-like intelligence. It is obvious that there are problems that are incredibly simple
for a computer to solve, but difficult for humans. If a computer is to find the square root of
864,900, for example, an algorithm of a quick line of code produces the value 930, computed in
less than a millisecond as the answer. For humans, this will prove a difficult task and will
require more time (Sawicki, 2014). On the other hand, some task that are simple for humans, are

- 32 -

not so easy for a computer. If a human is shown a picture of a mice or an African giant rat, they
will be able to say very quickly which one is which as humans do not need a machine to perform
this tasks.
One of the reasons why we need brain like intelligence is to perform task that are easy for
a human but difficult for a machine. An example of this task is pattern recognition, which is a

common application of neural networks in today’s computing. Applications that require brain-
like intelligence range from optical character recognition such as turning printed or handwritten

scans into digital text, to facial recognition. These neural network applications use artificial
intelligence algorithms.
Another reason is that a computerized neural network performs better than the brain in
terms of speed. The brain cannot process information nearly as quickly as a computer, due to
some physical limitations such as concentrating long enough to perform a task. More so, the
human brain does not have a programmer in any easily definable sense, as it programs itself in
response to input from the person's senses (Sawicki, 2014). When there is a large number of an
input variable, the task becomes very difficult to visualize, although the brain has the potential to
be more than sufficient to make a neural network that could eventually solve this kind of
problem.
Naturally unassuming, nothing in the physical world prepares a person for the task of
creating a twelve-dimensional boundary that divides a thirteen-dimensional space into regions
that are characterized by different likelihoods of events being signal and background as our
brains have learned from birth how to help us perform tasks in a three-dimensional environment,
so anything more is a struggle to learn. Therefore, brain-like information processing is needed to
achieve true human-level intelligence (Zhang, 2011).
In accordance with this paper, I was given the task of creating my own artificial
intelligence. Due to the limited time, I decided against creating one from scratch and instead
I based my AI on the award-winning AI, A.L.I.C.E.
A.L.I.C.E. (Artificial Linguistic Internet Computer Entity) is an open-source AI
developed in 1995 using AIML. I chose to base my own intelligence off of A.L.I.C.E. due its’
acclaimed success in the field and winning the Loebner Prize 3 times.
Having the AI based on A.L.I.C.E means to take the core structure of the program and
then tweak areas that I feel I want to be different, essentially changing its’ personality. In
order to use the AIML files provided by A.L.I.C.E., I needed an AIML interpreter, a program
that matches input to statements within the AIML code. For my project I chose Program-O.
10.1 AIML
10.1.1 Basic Construct (Bush, 2001 (rev 2006))
The Artificial Intelligence Markup Language (AIML) describes AIML objects and it is
based on the Extensible Markup Language (XML). AIML objects contain "Topics" and
"Categories". These can be read and output by an AIML interpreter, such as Program-O.

At first glance, the AIML document may remind you of HTML, but do not be fooled.
While the quotations and symbols are similar, the document is made and structured
differently. An AIML document is contained within the following tags:
where ‘xxx’ is replaced with the version you are using. There are only two tags which are
allowed to be directly under the AIML tags:
 <category> </category>

 <topic name =‘xxx’> </topic>
These are called Top-Level commands, due to the fact that they are the first to be accessed.
A Topic may contain multiple categories, but it is, for the most part, optional. Note that
when a category is placed outside of a Topic, the AIML interpreter must assume it is within a
Topic named ‘*’, which is the wildcard.
Within the Top – Level command (or in the event that the Category is within a Topic, a
second-Level command) Category there are, once again, two underlying tags;
Each category must contain no more and no less than one of each of these tags and the
Pattern must always come first within a Category. The Pattern is a kind of trigger to tell the
AIML interpreter what to answer to. Looking at an example from A.L.I.C.Es AIML files:
The Pattern contains only the word CAPRICORN. It means that if a user were to
mention CAPRICORN one of the possible Match Paths (A logical connection between
patterns and input made by the AIML interpreter) would lead here. The response is
then written in the Template. In this case the script sets Sign equal to Capricorn meaning
that if a user were to mention signs again in the same session, the AIML interpreter
would remember that the sign they talked about last was Capricorn.

So how are Match Paths created and how does the Interpreter know which path
to take?
A Match Path has 3 major elements: a Pattern, a That-Element and a Topic. The
Topic is not written within the document meaning that the Interpreter must either find
an implied topic or the topic is the name of the document. The That-Element, in this case
a Pattern-Side That-Element, is a special element used for context matching. It is
optional, but in the case that it is missing the Interpreter must replace it with a Wildcard.
The Pattern-Side That-Element is simply a means to further specify in what scenario the
following should occur.
On to the actual method of constructing a Match Path, we see that there happen to
be multiple ways to accomplish this. So let’s create a simple Topic/Category/Pattern
construction.

Let’s say our Topic name is Food
<topic name =’FOOD’>(Important: the name of the Document is food. This
is not always written within the document (in the case that there are
multiple topics in one document it is written into the document) also this
is only a representation of how a topic would look like)
Within our Category we have a Pattern for chicken.
In this case scenario we do not want chicken, rather we want chicken that the bot
has previously said to be warm.
After this comes the Template. In this case we will simply have it contain ‘xxx’ as
it is not relevant to this example.
Now the final code looks like this:
<topic name=’FOOD’>
So the code now resembles something one can find within an AIML document.
The Match Path construction is as follows: Chicken –Warm - Food.
Simply put, it means that the Interpreter searches from the bottom. When a user
mentions Chicken, the Interpreter searches all Patterns that contain chicken.
Afterwards it checks for That-Elements. In this case, the bot must have previously said
the word ‘warm’. Only then does it check to make sure the user is talking about chicken
as a food and not as an animal. This type of Match Path making is slow, but for the most
part quite robust.
In the scenario that there is no <that>, the That-Element must be made into a
Wildcard by the Interpreter. The Match Path in our example would look like this:
Chicken - * - Food. This means that this is a viable option as long as the user is talking
about chicken as a food (The Wildcard is allowed to be anything).
The same is true when you have no Topic. The Match Path in case of a missing
Topic and That-Element would simply be Chicken - * - *, meaning that as long as the user
mentions chicken, this statement can be said.
AIML Interpreter cannot simply build the Match Paths to match user input. The
statements made by users must first be ‘normalized’ by the Interpreter.

There are 3 Types of normalization that can be performed. The only one of these
normalizations that is mandatory is the Pattern-Fitting normalization. The other 2,
Sentence-Splitting and Substitution normalizations are decided upon by the user or
maker of the bot and are therefore optional.
If it is performed, the Substitution normalization is the first to be executed.
During the Substitution process, the Interpreter filters things out that may mess up its
understanding of a sentence. Abbreviations are a prime example of things that may need
to be substituted by the Interpreter. If we were to have the sentence:

I met Mrs. Jones at a café last week.
The Interpreter would need to do something about the abbreviation Mrs., as if it
were to leave it as-is, it might end up deciding that ‘I met Mrs’ was one sentence and
‘Jones at a café last week’ was another. During the Substitution normalization Mrs.
would be exchanged for misses. This process also takes care of web-addresses and file
extensions in a similar way. ‘https://www.google.at/’ becomes ‘https://google dot at’
and ‘.txt’ would simply be made to ‘ txt’.
The next normalization to occur is the Sentence-Splitting normalization. During
the process the Interpreter tries to break down the user input into separate sentences.
While further rules can be applied by the maker of the bot, the only rule which works
with any certainty is to break sentences at periods, making the Substitution
normalization important if you were to perform this one.
Lastly, we have the Pattern-Fitting normalization. This process turns all
lowercase letters into uppercase letters and all non-letter characters into spaces, as the
only input accepted by the interpreter must be made up of uppercase letters.
Based on these three normalizations, if you were to input, “Hi. How was your
day? ”, the bot would be receiving, “HI HOW WAS YOUR DAY”, where “HI” and “HOW
WAS YOUR DAY” are on separate lines.

Now that we have gone over the process of normalization and input, we can go
into how the bot decides which Match Paths to take.

This process is learned and explained best by example. Let us go ahead and
assume the following chat log:
User: “I like fruit.”
Bot: “What is your favourite fruit?”
User: “I like bananas.”
Bot: “I enjoy bananas, too.”
In this chat the user initiated with a conversation by mentioning fruit. The match
path used to find the bots answer would look along the lines of this:

LIKE FRUIT<that> * </that> <topic>FOOD
It finds the pattern LIKE FRUIT. Since the conversation just started, there is
currently no <that> statement, meaning the interpreter fills it in with a Wildcard. Lastly,
the bot finds itself within the Topic FOOD. Within the Template for LIKE FRUIT with any
That-element the bot finds the answer “What is your favourite fruit?”
After the user answers the bots question, the bot must once again decide which of
the Match Paths it is to take. The Match Path for the users’ statement would look
something like this:

BANANAS <that>FAVOURITE FRUIT</that> <topic>FOOD
The That-Element is taken from the last thing mentioned by the bot. In this
scenario the bot asked for the users’ favourite fruit, hence the That-Element. The user
answered with bananas and the bot now searches for a suitable answer to bananas
being a favourite fruit. The bot may also save the information that the users’ favourite
fruit is banana and use it in the conversation later.
In reality the bot searches each input word-for-word and sentence-for-sentence
and matches them with the patterns. After the match is found the Interpreter constructs
the contents of the template into a formulated, grammatically correct, output.

10.1.2 Other important concepts and constructs

Now that we have figured out the basics of AIML, we can go into the more complex
concepts and constructs within the language.
Reductionism (Wallace, 2000)
 the amount of text that needs to be written by rerouting all
of these inputs to one pattern.

Predicates (Bush, 2001 (rev 2006))

A predicate is an Item which can be declared at any point in the AIML file and whose
value can be manipulated, in this way it act much like a variable would in other languages.
Predicates have to be set using the <set name=’xxx’> tag, where xxx stands for the variable
name.
Predicates can be quite flexible. ‘Return-name-when-set’, although not an actual
function, is a way to set 2 strings to correspond to each other.

Why do you think <set name = ‘she’><star/></set> made you do it?
<get name = ‘she’> is a reasonable woman.
In this scenario the computer sets the value ‘she’ to equal to the name of the person, which
is received after the question ‘Who?’ The conversation would then be as follows:
User: She said I had to.
Bot: Who?
User: Janice
Bot: Why did she make you do it? Janice is a reasonable woman.
This allows for the bot to know what is referenced when the user says ‘she’.

10.2 Program-O

My AIML Interpreter of choice for this project is Program-O. Program-O is an open
source server-side Interpreter that uses MySQL and PHP to store information and display it.
Not much work needs to be done in order to make a functioning chat bot, thanks to
Program-O’s easy setup and user-friendly controls. One must only create a MySQL database
and a new user to install the program and run it in your browser. Afterwards, you need only
to fill in the blanks within Program-O, either in the browser or in MySQL.
To add new personality-associated information into your bot you just add to the
‘botpersonality’ database, which was created upon installing Programm-O. Adding
information goes as follows: (`id`, `bot_id`, `name`, `value`)
(NULL, JASON, 'wear', t-shirts'),

In this case we just added that our bot, Jason, wears a t-shirt, so if he were to be
asked what he wears he could answer t-shirt. The first value is NULL, because we only have 1
chatterbot and therefore do not need to ID them. Information like this is left out of the AIML
code and filled in Interpreter side in order to make it easier for people to personalize a
chatterbot, both for private and business oriented use.
10.2.0 Installation Process of Program –O(Installation)

Installing program-O is a rather simple process. After downloading program-O, the
first step is to extract the data to your local host or webserver. Afterwards, you have to
create a MySQL user as follows:
CREATE USER 'myUser'@'localhost' IDENTIFIED BY '***';

Then create a program-O database:
CREATE DATABASE program_O;
And then grant the user you created all privileges regarding the database:
GRANT USAGE ON * . * TO 'myUser'@'localhost' IDENTIFIED BY '***' WITH
MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0
MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0 ;

GRANT ALL PRIVILEGES ON `program_o` . * TO 'myUser'@'localhost';

Lastly open your browser to the directory where you extracted program-O and go through
their step-by-step installation process.
In order to get a bot to function, import AIML files into program-O through the program-O
menu.

11 Results

Using this knowledge of the AIML language I decided to further the A.L.I.C.E. source
code to better fit my purposes. One of the things that I wanted to do was to have the bot
talk about some of the things that I like. One of these things is the game Smite. To start, I
made a file for the categories, which I named, Smite. Then I made a series of categories that,
when mentioned, would set the topic to Smite, such as:
<category>
<pattern>
HAVE YOU HEARD OF SMIT
Notice the <srai> tag being used to reference a different category. I made a few
categories that look similar, but have different phrasing to make sure to get every possible
variant of asking whether or not the bot knows of the game. The <srai> tag then leads to the
following category
I love Smite! What is your favourite god?

</category>
The bot answers to any form of bringing up Smite with the phrase ‘I love Smite! What
is your favourite god?’ The <think> tag makes sure nothing inside of the tag gets printed
onto the screen. I then set ‘it’ and the ‘topic’ to SMITE, which means the program knows
that we are talking about Smite and it defaults the word ‘it’ to refer to Smite.
Once the user answers with a name of a ‘god’ present in the game, each of which has
their own category that leads to the category FAVOURITE GOD, the bot goes into this
category:
The bot in this situation has a multitude of possible answers, which, I decided, it
chose at random with the use of the <random> tag. The tag functions by randomly selecting
one of the options inside the tags, which are enclosed by <li> tags. I also use the <input> tag
to print the users last input onto the screen and set that input to ‘user god’ in order to make
note of that being the users’ favourite god. I then set the random answer chosen by the bot
to ‘god’ to make sure the bot uses the same god when referring to its favourite. The output
at this part in the script can take on a few forms. The bot can print out ‘I dislike ’ + user
input+ ‘. My favourite is Agni’, as well as ‘I love ’+user input + ‘. My favourite is He Bo’. I
made use of <random> to give the bot a feeling of diversity.
The conversation regarding the topic Smite goes on further in a similar fashion.
2.2.2 What Does Meaning Mean?
To address this question, this section briefly discusses Peirce and
Wittgenstein’s theories of understanding and meaning. Wilks et al.
(1996a) survey the “history of meaning” including views of other
thinkers. Nirenburg & Raskin (2004) discuss the evolution of formal
representations for semantics and ontologies.
Besides understanding natural language, Peirce also considered
understanding of phenomena in general, e.g. developing and using
explanations of how (by what cause) and why (for what purpose)
something happens or is done. Peirce discussed language as a system of
signs, and wrote: 14
“A sign, or representamen, is something which stands to
somebody for something in some respect or capacity. It

14 Reprinted by permission of the publisher from Collected Papers of
Charles Sanders Peirce: Volume II, Elements of Logic edited by Charles
Hartshorne and Paul Weiss, p.135, Cambridge, Mass.: The Belknap
Press of Harvard University Press, Copyright © 1932, 1960 by the
President and Fellows of Harvard College.

Natural Language

33
addresses somebody, that is, creates in the mind of that person
an equivalent sign, or perhaps a more developed sign. That sign
which it creates I call the interpretant of the first sign. The sign
stands for something, its object.” (Peirce, CP 2.228)
This may be taken as a general description of the process of
understanding something. The initial sign (thing to be understood) is
what Peirce calls the representamen – It is typically something external
in the environment. It may be a symbol printed on paper (such as a
Chinese symbol for “lamp” ), or smoke perceived at a distance, or to
use Atkin’s (2010) example a molehill in one’s lawn, or a natural
language utterance (such as “the cat is on the mat”), or anything else
that is a perception from the environment.
The process of understanding the representamen leads the mind to
conclude it stands for (or represents, or suggests the existence of)
something, called the object. The object of the Chinese symbol might be
a real lamp, the object of the smoke might be a fire that produces it, the
object suggested by the molehill could be a mole that created it, the
object of the natural language utterance could be a cat on a mat, etc.
So, from Peirce’s perspective, the process of understanding a sign or
representamen involves developing an explanation for the meaning or
cause of the sign. Peirce used the term abduction to refer to reasoning
that develops explanations:
“The surprising fact, C, is observed;
But if A were true, C would be a matter of course,
Hence, there is reason to suspect that A is true.”
(Peirce, CP 5.189) 15
Understanding involves developing explanations for what is
observed. This applies to both understanding natural language, and
understanding in general for human intelligence (cf. Hobbs et al., 1993;
Bunt & Black, 2000).
According to Peirce, the mind does not go directly from the

15 Reprinted by permission of the publisher from Collected Papers of
Charles Sanders Peirce: Volume V, Pragmatism and Pragmaticism edited by
Charles Hartshorne and Paul Weiss, p.117, Cambridge, Mass.: The
Belknap Press of Harvard University Press, Copyright © 1934, 1961 by
the President and Fellows of Harvard College.

Subject Review: Human-Level AI & Natural Language

34
representamen to the object in developing an explanation for what is
observed. The mind internally creates another sign, called the
interpretant, which it uses to refer to the object. 16 Vogt (2000 et seq.) has
used computer simulation of the Peircean triad, in studies of symbol
grounding and language learning; also see Vogel & Woods (2006).
We do not have to know precisely how this internal sign is
represented in the brain, to believe some pattern of physical information
must exist in the brain to represent an internal sign, providing a link
between the external representamen and the external object.
Importantly, we do not have to believe there is just one kind of physical
information pattern used to represent all internal meanings – the brain
could use a variety of different physical information media and patterns
for representing meanings.
Though Wittgenstein (1922) presented a purely logical description of

the relationship between language and reality in Tractatus Logico-
Philosophicus, he later restated much of his philosophy about language

in Philosophical Investigations. A central focus of Investigations was the
idea that the meaning of words depends on how they are used, and that
words in general do not have a single, precisely defined meaning. As an
example, Wittgenstein considered the word “game” and showed it has
many different, related meanings. What matters is that people are able
to use the word successfully in communication about many different
things. Wittgenstein introduced the concept of a “language game” as an
activity in which words are given meanings according to the roles that
words perform in interactions between people. 17
It does not appear there is any fundamental contradiction between
Wittgenstein and Peirce. Rather, what Wittgenstein emphasized was
that an external representamen may stand for many different external
objects. From a Peircean perspective this implies the representamen
may have many different internal signs, or interpretants, corresponding
to different external meanings in different situations. A Peircean
understanding process needs to support disambiguation (via abductive

16 Viz. Atkin’s (2010) discussion of how Peirce’s theory of signs
evolved throughout his lifetime.
17 Vogt (2005) showed that perceptually grounded language games
can lead to the emergence of compositional syntax in language
evolution. Also see Bachwerk & Vogel (2011) regarding language
evolution for coordination of tasks.

Natural Language

35
inference) of different interpretants to understand what a usage of an
external sign means in a particular context.
These considerations can be summarized by saying that just as a
word like “game” can have many different meanings, the word
meaning itself can in principle have many different meanings. Hence
the TalaMind architecture is open to many different ways of
representing meanings at the three conceptual levels, for example:
o Linguistic Level: Linguistic Concept Structures
• Concepts represented as sentences in a language of
thought
• Semantic domains – Collections of sentences about a
topic.
• Mental spaces, conceptual blends
• Scenarios for simulation of hypothetical contexts
• Grammatical constructions for translation and
disambiguation of linguistic meanings
• Executable concepts for representing and developing
complex behaviors
• Finite state automata for representing simple behavioral
systems
• Formal logic representations, e.g. predicate calculus or
conceptual graphs.

o Archetype Level: Cognitive Concept Structures
• Idealized Cognitive Models (Lakoff, 1987)
• Conceptual Spaces as topological or metric structures
(e.g. convex regions) in multiple quality dimensions
(Gärdenfors, 2000), with support for prototype effects,
similarity detection, etc.
• Radial Categories (Lakoff, 1987)
• Image Schemas (Johnson, 1987; Talmy, 2000)
• Semantic Frames (Fillmore, 1975 et seq.) and Conceptual
Domains (Lakoff & Johnson, 1980; Langacker, 1987)
• Perceptual Symbols (Barsalou, 1993 et seq.)
o Associative Level: Associations & Data Analysis
• Neural networks (e.g. Hinton, 2006)
• Expressions or data structures induced via machine
learning algorithms (e.g. Valiant, 2013)
• Bayesian networks (e.g. Pearl, 1988 et seq.)

Subject Review: Human-Level AI & Natural Language

36
This is just an illustrative, not exhaustive list of different ways to
represent meanings at different conceptual levels, which will be
discussed in greater detail as needed in the following chapters. Note
that a representation of meaning may span levels and forms of
representation, e.g. a linguistic concept structure may reference a
cognitive concept structure. Also, some authors may disagree with this
placement at different levels. Thus, Fauconnier and Turner might argue
mental spaces and conceptual blends should be at the archetype level.
While conditional probabilities fit the associative level, Bayesian
networks may represent semantics of sentences at the linguistic level, in
future research. Within the scope of this thesis, precisely how concepts
are represented in the archetype and associative levels is not crucial. A
Tala agent may not need to include all the different forms of concept
representation listed above, particularly at the archetype level, since
these overlap in representing concepts. Ways to unify representations
within or across the three levels may be a worthwhile topic for future
research.
2.2.3 Does Human-Level AI Require Embodiment?
Though the TalaMind approach focuses on the linguistic level of
conceptual processing, a Tala agent also includes environment
interaction systems with lower levels of conceptual processing, as
discussed in §1.5 and shown in Figure 1-1. Consequently a Tala agent
can in principle be embodied in a physical environment. So, to the
extent that understanding natural language requires embodiment, the
TalaMind approach supports this.
However, embodiment does not require that an intelligent system
must have physical capabilities exactly matching those of human
beings. This would imply human-level intelligence requires the human
physical body, and could only be possessed by people. Yet we know
people have human-level intelligence even when born without senses
like sight or hearing. Also, the unexplained features of human-level
intelligence, and in particular the higher-level mentalities, can be
described in terms that are essentially independent of the human body
(viz. §2.1.2).18

So, there should be no reason in principle why human-


18 Perhaps the only exception would be the first-person, subjective
experience of consciousness. Yet the possibility that other species might
possess human-level intelligence suggests that human-level intelligence

Natural Language

37
level artificial intelligence must require human physical embodiment.
And we should note that embodiment for humans is not what
people normally think it to be: We do not have direct knowledge of
external reality, nor even direct knowledge of our bodies. Instead, we
have an internal, projected reality (Jackendoff, 1983) constructed from our
perceptions of external reality, and our perceptions of our bodies. This
can be appreciated by considering various illusions, both in our
perceptions of external reality, and in our perceptions of our bodies (e.g.
virtual body illusions). Such illusions show our perceptions are internal
constructs that indirectly represent external reality and our bodies,
sometimes incompletely, inaccurately or paradoxically. It is only
because our perceptions generally track reality very closely, that we
normally think we directly perceive reality.
The TalaMind approach accepts that a language of thought must be
embodied by reference to perceptions of an environment, yet that such
perceptions are generally incomplete and potentially inaccurate.
Understanding of concepts related to the environment, one’s body, or
physical systems in general, can be achieved indirectly by representing
knowledge of physical systems, and reasoning within and about such
representations. Such reasoning may amount to a mental simulation. A
variety of different kinds of representations may be useful, e.g. image
schemas, finite state automata for representing behaviors of simple
systems, mental spaces, conceptual simulation, etc. These
representations may exist within a Tala agent’s projected reality or
elsewhere in its conceptual framework.
In these pages, this idea is called virtual embodiment. It allows an
intelligent system to understand and reason about physical reality, and
to transcend the limitations of its physical body (or lack thereof) in
reasoning about the environment – perhaps in the same way a person
blind from birth may reason about sight, without direct experience or
memory of sight. The projected reality of a TalaMind conceptual
framework will be virtual and indirect, though it could in principle be

does not require the subjective experience of what it is like to have a
human body. Thus it’s clear other species (e.g. dolphins, whales, octopi,
elephants, ...) have substantial intelligence and yet have very different
physical senses and embodiment. And it’s at least conceivable that
extraterrestrial intelligence may exist comparable or superior to
humans, yet with different physical bodies from humans.

Subject Review: Human-Level AI & Natural Language

38
interfaced with physical reality (viz. §4.2.2.4).
To appreciate how limited our perceptions are of reality, consider
that the frequency of visible light ranges from about 400 to 800 trillion
cycles per second, while our nerve cells can only transmit about 1000
pulses per second. So, the reality we see visually is producing waves
that oscillate hundreds of billions of times faster than we can perceive.
The processing of information by 140 million neurons in each
hemisphere’s visual cortex, as well as many more neurons elsewhere,
enables a 100 billion neuron human brain to perceive a visual projected
reality with great complexity. Yet what we perceive is only a miniscule
fraction of the complexity of events around us, happening at different
scales of space and time within external reality.
Also, what we perceive of reality is qualitatively different from what
actually exists. For example, we now know that what we perceive as
solid objects are in reality almost entirely empty space pervaded by
force fields and subatomic particles (Close, 2009). So, our human
projected reality is inaccurate at the lowest levels of physics, though it is
pragmatically very accurate at our day-to-day level of existence.
Our ability to have this knowledge, and to transcend our projected
reality, is an example of our own virtual embodiment: It is only by
applying human-level intelligence that after generations of thought and
experiment we have been able to find ways to virtually perceive aspects
of reality that are either too small or too fast for us to perceive in
projected reality (such as viruses, microbes, molecules, atoms,
subatomic particles, the speed of light, etc.) or too large or too slow for
our projected reality (such as Earth’s precession about its axis, evolution
of species, continental drift, the Sun’s lifecycle, the size and age of the
universe, etc.)
2.3 Relation of Thesis Approach to Previous Research
2.3.1 Formal, Logical Approaches
As noted in §1.1, one broad stream of research related to
understanding intelligence has focused on formal logical approaches to
representation and processing. If one accepts the physical symbol
systems hypothesis (§1.4.4), then one may argue these approaches have
in principle the ability to support intelligent systems, based on their
generality for representing symbolic systems. So this thesis accepts the
potential value of formal theoretic approaches, and acknowledges much
has been accomplished with them. Further, the TalaMind architecture is

Relation of Thesis Approach to Previous Research

39
open to use of formal, logical systems within it, including systems based
on predicate calculus, conceptual graphs, etc.
Thus, we note in particular the work of Hobbs et al. (1993 et seq.)
regarding “Interpretation as Abduction” in understanding natural
language using first order predicate calculus; the work of Sowa (1984 et
seq.) and others on conceptual graph structures; and McCarthy’s papers
on artificial intelligence cited in the Bibliography, as research directions
to consider in future extensions of the TalaMind approach.
However, the formal, logical approaches do not seem to easily
provide the broad range of representations we express with natural

language, e.g. features of natural language like self-reference, meta-
expressions, metaphor, mental spaces, conceptual blends, idioms, modal

verbs, verb aspect and tense, de dicto and de re expressions, metonymy,
anaphora, mutual knowledge, etc. – though in principle each of these
features should be possible to represent within formal, logical
approaches, and many of them have been investigated. For instance,
Vogel (2011) discusses a formal model of first-order belief revision to
represent dynamic semantics for metaphors and generic statements.
Doyle (1980) described a formal logic approach to reflection and
deliberation, discussed further in §2.3.5.
In discussing abduction in natural language understanding, Hobbs
(2004) lists the following requirements for a language of thought to
support commonsense reasoning:
“Conjunction: There is an additive effect (P ˄ Q) of two distinct
concepts (P and Q) being activated at the same time.
“Modus Ponens: The activation of one concept (P) triggers the
activation of another concept (Q) because of the existence of
some structural relation between them (P → Q).
“Recognition of Obvious Contradictions: The recognition of
contradictions in general is undecidable, but we have no trouble
with the easy ones, for example, that cats aren't dogs.
“Predicate-Argument Relations: Concepts can be related to
other concepts in several different ways. For example, we can
distinguish between a dog biting a man (bite(D, M)) and a man
biting a dog (bite(M, D)).
“Universal Instantiation (or Variable Binding): We can keep
separate our knowledge of general (universal) principles (“All

Subject Review: Human-Level AI & Natural Language

40
men are mortal”) and our knowledge of their instantiations for
particular individuals (“Socrates is a man” and “Socrates is
mortal”).
Hobbs writes “Any plausible proposal for a language of thought
must have at least these features, and once you have these features you
have first-order logic”. He notes higher-order languages can be recast
into first-order logic using reification (Hobbs, 1985). In addition, he says
the logic for a language of thought must be nonmonotonic: It must be
possible for us to change what we believe to be the truth value of a
statement, if we gain more information. Hobbs (2004) advocates
abduction (reasoning to determine best explanations) to support
commonsense, nonmonotonic reasoning for a language of thought.
Hobbs et al. (1993) discuss how abduction with first order logic can be
used to solve a variety of problems in natural language understanding,
including reference resolution, ambiguity resolution, metonymy
resolution, and recognizing discourse structure. Hobbs (2004) discusses
how it can be used to recognize a speaker’s plan or intentions.
However, Hobbs (2004) only claimed a language of thought must
have at least the features he listed. While these features are necessary, it
does not appear they are sufficient in general for a language of thought.
Chapter 3 will present reasons why a language of thought should be a
higher level language than first order logic, though it may sometimes be
worthwhile to translate into first order logic.
Wilks et al. (1996b) criticize Hobbs’ approach and contend that
abduction as a form of logical proof is not adequate for semantic
interpretation. They note that given a false premise, one can prove
anything, so abduction needs to filter out false hypotheses. Abduction
needs to be guided by meta-knowledge and meta-reasoning, to
determine which hypotheses are most relevant. In general, their
criticisms show the importance of viewing abduction as providing
explanations, rather than just logical proofs – a perspective consistent
with Peirce’s view of abduction, and with Wittgenstein’s view of
meaning as involving explanations (viz. §2.2.2).
2.3.2 Cognitive Approaches
If formal, logical approaches are one broad stream of research
related to understanding intelligence and natural language semantics,
then cognitive approaches may be considered as ‘everything else’. This
includes a wide variety of approaches by researchers in Artificial

Relation of Thesis Approach to Previous Research

41
Intelligence, Linguistics, Anthropology, Psychology, Neuroscience,
Philosophy, and Education.19


In AI research, cognitive approaches include Minsky’s society of
mind architecture, Wilks’ work on preference semantics, Schank’s work
on narrative case-based dynamic memory structures, Sloman’s (1971 et
seq.) research, Sowa’s cognitive architecture, and work by many
researchers on neural networks.
Outside of AI, linguists, psychologists, philosophers, neuroscientists
and researchers in other fields have developed approaches to
understanding intelligence and natural language. Many of these
researchers would not agree their approaches can be replicated by
computers: There is no general agreement among cognitive scientists
that human-level AI is possible. Perhaps the only general agreement
within cognitive science is that what happens within the human brain
cannot be explained simply by observing external behavior, i.e.
behaviorist psychology is not sufficient, and one must consider internal
information and processes in the brain, to understand the mind.
The TalaMind approach is consistent in many respects with
cognitive linguistics research on “embodied construction grammars”,
such as the ECG work of Feldman, Bergen, Chang et al. (2002 et seq.) or
the research of Steels, de Beule et al. (2005 et seq.) on Fluid Construction
Grammar. ECG provides a computable approach to construction
grammar, with embodiment represented via simulation of discrete
events. ECG also has grounding in a connectionist, neural theory of
language. ECG is relevant to this thesis by providing an existence proof
that a computational approach may be considered “embodied”. Fluid
Construction Grammar research has focused on demonstrating the
evolution and emergence of language, using constraint processing for
identification and matching in embodied systems, which is an
interesting topic for future research in the TalaMind approach, outside
the scope of this thesis.
One difference of the TalaMind approach appears to be that previous
approaches do not provide constructions for an internal language of
thought. Rather, they provide constructions for external natural
language parsing and generation, with internal representations of
semantics that in general have been somewhat restricted, and

19 Fields included within Cognitive Science, by the Cognitive Science
Society.

Subject Review: Human-Level AI & Natural Language

42
apparently not described as languages of thought.
Many researchers in cognitive linguistics have not supported a
language of thought hypothesis, but have developed multiple other
descriptions for the nature of internal conceptualizations. 20 Some
cognitive linguists have expressly rejected a computational language of
thought hypothesis. Thus, Lakoff (1987, p. 343) presented arguments
against the viability of an artificial mentalese, concluding: “[T]hose who
really believe in the mind-as-machine paradigm, in computational
realism, in objectivist AI, and in the idea that the symbols in a computer
language really can be ‘internal representations of external reality’... are
simply wrong”. However, Lakoff leaves the door open that “something
like the experientialist approach...to what AI researchers call
representations and what I have called cognitive models will mesh
better with empirically responsible AI research.”
Evans (2009) presents a cognitive linguistics account of meaning
construction in natural language called Lexical Concepts and Cognitive
Models (LCCM) theory, which is consistent with the TalaMind
approach. He writes:
“LCCM Theory suggests that rather than the semantic
representation encoded by language being equated with
conceptual structure, semantic structure takes a distinct form.
Specifically, semantic structure, unlike conceptual structure, is
directly encoded in language, and takes a specialized and
highly elaborate form: what I refer to as lexical concepts. While
lexical concepts are concepts, they encode a highly schematic
form of semantic representation, one that is specialized for
being directly encoded in and externalized via language. In
contrast, conceptual structure takes a qualitatively distinct form,
which I model in terms of the theoretical construct of the
cognitive model.”
Evans describes lexical concepts as being based on construction
grammar, so that by extension it appears his semantic structures can
include multi-word expressions, e.g. sentences. He describes LCMM

20 Thus the terms mentalese and language of thought are not mentioned
in either of the comprehensive texts on cognitive linguistics by Evans &
Green (2006) or by Croft & Cruse (2004).

Relation of Thesis Approach to Previous Research

43
cognitive models as being similar to Barsalou’s (1999) description of
simulators and perceptual symbols (§4.2.2.4), and encompassing frames
and simulations. Thus, Evans’ lexical concepts correspond to the
linguistic level of Figure 1-1, and his cognitive models for conceptual
structure correspond to elements of the archetype level (although not
identical to Lakoff’s idealized cognitive models).
LCCM theory is consistent with the TalaMind approach in using
conceptual structures based on natural language at the linguistic level,
interacting with an archetype level. LCCM theory is different from the
TalaMind approach in several respects. For instance, LCCM is not a
theory of how to achieve human-level AI; it does not describe a
conceptual framework at the linguistic level; it does not include

Hypotheses I and III of this thesis; it does not discuss support of higher-
level mentalities.

2.3.3 Approaches to Human-Level Artificial Intelligence
This section gives some further discussion of research toward
human-level AI, augmenting the brief discussion in §§1.1 and 1.2.
2.3.3.1 Sloman
Sloman (1978) published a high-level “sketch of an intelligent
mechanism” to describe “the overall architecture of a computing system
which could cope with a variety of domains of knowledge in a flexible
and creative way, so that, like people, it can use available information,
skills and procedures in order to solve new problems, or take decisions
in new situations, in ways which were not explicitly foreseen or planned
for by the programmer.” He wrote that to achieve artificial intelligence
comparable to an adult human, it would be necessary to “produce a
baby mind with the ability to absorb a culture through years of
interaction with others.” In general, Sloman’s (1978) discussion and his
subsequent work have been in a similar direction to this thesis, though
with different focus. Sloman’s (2008) discussion of “Generalised
Languages” for innate representation (cf. §2.2.1) is similar though not
identical to the TalaMind natural language mentalese hypothesis.
2.3.3.2 Minsky
2.3.3.2.1 The Society of Mind Paradigm
As noted in §1.5, the TalaMind hypotheses do not require a ‘society
of mind’ architecture, but it is consistent with the hypotheses and
natural to implement a society of mind at the linguistic level. Since

Subject Review: Human-Level AI & Natural Language

44
Minsky (1986) described the society of mind as a theory of human-level
intelligence, this section provides a brief discussion of his ideas and of
similarities and contrasts with the TalaMind approach.
Minsky wrote: 21
"I'll call 'Society of Mind' this scheme in which each mind is
made of many smaller processes. These we'll call agents.
22 Each
mental agent by itself can only do some simple thing that needs
no mind or thought at all. Yet when we join these agents in
societies - in certain very special ways - this leads to true
intelligence."
Singh (2003) gave an overview of the history and details of Minsky's
theory, noting that Minsky and Papert began work on this idea in the
early 1970’s. Minsky’s description and choice of the term ‘society of
mind’ were evocative, inspiring research on cognitive architectures
more broadly than he described, to the point that the idea may be
considered a paradigm for research. Thus, the term may be used in
either of two senses:
1. The society of mind as proposed by Minsky, including a
specific set of methods for organizing mental agents and
communicating information, i.e. K-lines, connection lines,
nomes, nemes, frames, frame-arrays, transframes, etc.
2. A society of mind as a multiagent system, open to methods
for organizing agents and communication between agents,
other than the methods specified by Minsky, e.g. including
languages of thought.
This thesis uses ‘society of mind’ with the second, broader sense,
though not precluding future research on use of Minsky’s proposed
methods for organization and communication within TalaMind
architectures.
To give a few examples of the second perspective, Doyle (1983)
described a mathematical framework for specifying the structure of

21 Quotations in this section are used by permission of Simon &
Schuster Publishing Group from The Society of Mind by Marvin L.
Minsky. Copyright © 1985, 1986 by Marvin Minsky. All rights reserved.
22 In other sections, the term ‘subagent’ will refer to an agent in a
society of mind within a Tala agent (§1.5).

Relation of Thesis Approach to Previous Research

45
societies of mind having alternative languages of thought. 23 More
recently, Wright (2000) discussed the need for an economy of mind in
an adaptive, multiagent society of mind. Bosse & Treur (2006) gave a
formal logic discussion of the extent to which collective processes in a
multiagent society can be interpreted as single agent processes. Shoham
& Leyton-Brown (2008) provide an extensive text on multiagent
systems, including a chapter on communication between agents. Sowa
(2011) describes communication of conceptual graphs between
heterogeneous agents in a framework inspired by Minsky’s society of
mind.
Minsky described a society of mind as an organization of diverse
processes and representations, rejecting the idea that there is a single,
uniform process or representation that can achieve human-level
intelligence:
“What magical trick makes us intelligent? The trick is that there
is no trick. The power of intelligence stems from our vast
diversity, not from any single, perfect principle. “ (Minsky,
1986, p.308)
This thesis is compatible with Minsky’s tenet – the TalaMind
architecture is envisioned to enable integration of diverse processes and
representations.
However, issues related to a language of thought are an area of
difference between the TalaMind approach and Minsky’s theory. He
considered that because agents would be simple and diverse, in general
they would not be able to understand a common language. Agents
would need different representations and languages, which would tend
to be very specialized and limited. He wrote (1986, pp.66-67):
“If agent P asked any question of an unrelated agent Q, how
could Q sense what was asked, or P understand its reply? Most
pairs of agents can’t communicate at all. ...The smaller two
languages are, the harder it will be to translate between them.
This is not because there are too many meanings, but because
there are too few. The fewer things an agent does, the less likely

23 As example languages, Doyle discussed logic (FOL – Weyhrauch,
1980), list structures and rational algebraic functions (CONLAN –
Sussman & Steele, 1980), and nodes and links (NETL – Fahlman, 1979).

Subject Review: Human-Level AI & Natural Language

46
that what another agent does will correspond to any of these
things. And if two agents have nothing in common, no
translation is conceivable.”
Thus, Minsky did not describe agents in a society of mind sharing an
interlingua. He described other, lower-level ways for agents to partially
communicate, which he called K-lines and connection lines. To

exchange more complex descriptions, Minsky proposed an ‘inverse-
grammar-tactic’ mechanism for communication by re-constructing

frame representations (viz. Singh, 2003).
In contrast, the TalaMind approach enables agents in a society of
mind to share a language of thought based on the syntax of a natural
language. 24 Two agents can communicate to the extent that they can
process concepts using common words, and can share pointers to
referents and senses of the words. Pattern-matching can be used to
enable an agent to recognize concepts it can process, that were created
by other agents. This will be discussed and illustrated further in
Chapters 3, 5, and 6. An agent in a society of mind may reason directly
with concepts expressed in the Tala mentalese, or it may translate to and
from other representations and languages, if needed.
Chapter 3 will also discuss how the Tala mentalese can support
representing and reasoning with underspecification in natural language.
This is compatible with Minsky’s discussion of ambiguity in thought
within a society of mind:
“We often find it hard to ‘express our thoughts’—to summarize
our mental states or put our ideas into words. It is tempting to
blame this on the ambiguity of words, but the problem is
deeper than that. Thoughts themselves are ambiguous! ...We can
tolerate the ambiguity of words because we are already so
competent at coping with the ambiguity of thoughts.” (Minsky,
1986, p.207)
Although Minsky attributed the ambiguity of thought to the act of


24 This corresponds somewhat to the idea of a “network of question-
answerers” described in Jackson (1974, p.328) which suggested a form

of emergence for such systems, in the potential for a network of
question-answerers to answer a question that could not be answered by
a single agent in the system.

Relation of Thesis Approach to Previous Research

47
expression being a process that simplifies descriptions of mental states,
the TalaMind approach allows individual thoughts to be ambiguous,
just as natural language sentences can be. For instance, in the TalaMind
approach the agents in a society of mind could communicate and
process the thought “In most countries most politicians can fool most people
on almost every issue most of the time” (Hobbs, 1983) without needing to
consider all the sentence’s different logical interpretations, and without
needing to consider nonsensical interpretations (viz. §3.6.3.7).
Per §1.6, a society of mind will only be developed in this thesis to a
limited extent, as needed to illustrate the thesis approach.
2.3.3.2.2 Theoretical Issues for Baby Machines
Minsky (2006, p. 178-182) is not optimistic about the prospects for
the ‘baby machine’ approach to human-level AI. He cites several
previous research efforts toward general-purpose learning systems and
says: 25
“... each such system made progress at first but eventually
stopped extending itself. I suspect that this usually happened
because these programs 26 failed to develop good new ways to
represent knowledge. Indeed, inventing good ways to represent
knowledge has long been a major goal in computer science.
“... each human child makes extensive use of ... high-level
structures to develop our uniquely human ways to represent
new kinds of knowledge and processes. It seems clear to me
that this is why the attempts to make baby-machines have led to
unimpressive results: you cannot learn things that you cannot
represent.

“... I do not mean to dismiss all prospects of building a baby-
machine, but I suspect that any such system would develop too


25 Quotations in this section are used by permission of Simon &
Schuster Publishing Group from The Emotion Machine by Marvin L.
Minsky. Copyright © 2006 by Marvin Minsky. All rights reserved.
26 The previous research efforts cited by Minsky appear to all be
experiments in general-purpose learning systems based on lower-level
representation and search methods. None appear to consider
representation of concepts using a conceptual language with syntax
based on natural language.

Subject Review: Human-Level AI & Natural Language

48
slowly unless (or until) it was equipped with adequate ways to
represent knowledge.”
The Tala mentalese will have the same extensibility as natural
language, for developing new representations of concepts. So at least in
principle, a Tala agent should be able to represent and discover any
concepts that can be expressed via natural language, once this approach
has been fully developed and implemented.
In addition and related to the knowledge representation problem,
Minsky (2006) cites other problems for baby-machines and general
learning systems:
“The Optimization Paradox: The better a system already works,
the more likely each change will make it worse – so it gets more
difficult for it to find more ways to improve itself.
“The Investment Principle: The better a certain process works, the
more we will tend to rely on it, and the less we will be further
inclined to develop new alternatives – especially when a new
technique won’t yield good results until you become proficient
with it.
“The Complexity Barrier: The more that the parts of a system
interact, the more likely each change will have unexpected side
effects.”
These are important issues, addressable with an appropriate
language for concept representation, and a suitable conceptual
framework and conceptual processes. Indeed, in discussing these
problems Minsky (2006, p.181) notes the importance of “language-like
systems” that enable societies of “higher animals” to overcome such
learning problems more quickly than genetic evolution can. We’ll return
to these issues later, in §§3.7.2.3 and 4.2.6.
2.3.3.3 McCarthy
Two papers by McCarthy (2007, 2008) considered the general
problem of how to achieve human-level artificial intelligence. The first
gave a general discussion of how to go “from here to human-level AI”.
He wrote “The key to reaching human-level AI is making systems that operate
successfully in the common sense informatic situation”, which he defined as
the situation in which: known facts are incomplete; there are no a priori
limits on what facts are relevant; it cannot be decided in advance what

Relation of Thesis Approach to Previous Research

49
phenomena are to be considered; concepts and theories are approximate
and cannot be fully defined; nonmonotonic reasoning is need to reach
conclusions; introspection may be needed about the system’s mental
state.
Though McCarthy supported extending mathematical logic
formalisms to operate in common sense informatic situations, he noted
some other approach might work:
“Since it seems clear that humans don't use logic as a basic
internal representation formalism, maybe something else will
work better for AI. Researchers have been trying to find this
something else since the 1950s but still haven't succeeded in
getting anything that is ready to be applied to the common
sense informatic situation. Maybe they will eventually succeed.
However, I think the problems listed [below] will apply to any
approach to human-level AI.”
McCarthy (2007) identified the following problems for any approach to
human-level AI:
• Representation of common sense knowledge of the world, in
particular the effects of actions and other events;
• Epistemologically adequate languages that can be used to
express what a person or robot can learn about the world;
• Elaboration tolerance – the ability to add information without
starting over in the representation of previous information;
• Nonmonotonic reasoning – the ability to reason with partial
information, where additional information may change one’s
conclusions;
• Contexts as objects – the ability to reason about contexts “from
the outside” about contexts as well as internally within a
context; the ability to transcend the current context of thinking
and reason about it;
• Introspection – the ability for a system to reason about its
mental state and processes;
• Action – reasoning about strategies of action, considering
multiple actors, concurrent and continuous events;
• Heuristics – the ability to give programs domain and problem
dependent heuristic advice.
Humans have historically used natural language to describe and

Subject Review: Human-Level AI & Natural Language

50
help solve the above problems, and natural language already possesses
syntax to represent their semantics. Hence these problems may be
plausibly represented and solved within a human-level AI, using a
mentalese with the expressive scope of natural language, as proposed in
this thesis.
McCarthy (2008) discussed the design of a baby-machine approach
to human-level artificial intelligence. In general, his discussion is
consistent with the approach of this thesis, which would agree the
system needs to have an initial set of concepts corresponding to innate
knowledge about the world. He lists several kinds of innate conceptual
knowledge the system should have, which in general could be
supported in the TalaMind architecture. It appears the major difference
between McCarthy’s perspective and this thesis is regarding the nature
of the language of thought that a well-designed baby-machine should
have. McCarthy wrote that grammar is secondary, that the language of
thought should be based on logic, and not on natural language.
Responses to his objections will be presented in Chapter 4, along with
discussion of other theoretical objections to the TalaMind approach.
2.3.3.4 Reverse-Engineering the Brain
Markram (2006) describes the ongoing Blue Brain project, for which
the long-term goal is to perform detailed, biologically accurate
computer simulations of a human brain’s neural processing. This
approach, reverse-engineering the brain, appears to have the potential
to achieve human-level AI. Arguably, the physical processes used by
the brain to achieve intelligence could be simulated by computers –
especially since if needed, emerging technologies for computation could
be applied, e.g. nanotechnology, quantum computation, etc. However, it
is beyond the scope of this thesis to discuss the technical feasibility of
this approach. At minimum the Blue Brain project, and related research,
should yield insights into human brain function, and could also help
support other research toward human-level AI. For example, such
research may identify computational neural modules that could be
simulated in the associative level of a Tala agent, perhaps supporting
Barsalou’s perceptual symbols (§4.2.2.4).
2.3.3.5 Cognitive Architectures & AGI
Several authors have conducted research into cognitive architectures
and/or ‘artificial general intelligence’ (AGI). This includes research
discussed in Albus & Meystel (2001), Cassimatis (2002 et seq.), Doyle

Relation of Thesis Approach to Previous Research

51
(1980 et seq.), Forbus & Hinrichs (2006), Goertzel & Pennachin (2007),
Laird (2008), Langley (2006), Lenat (1995), Pollock (1990 et seq.),
Schlenoff et al. (2006), Schmidhuber (1987 et seq.), Sowa (2011), Swartout
et al. (2006), and Wang & Goertzel (2012). In general, these efforts do not
discuss research in the same direction as the TalaMind approach, i.e. an
intelligence kernel using a language of thought based on the syntax of
natural language.
Yudkowsky (2007) advocates levels of organization in “deliberative
general intelligence” (DGI) as a direction for future research in AGI. The
DGI paper proposes a research direction somewhat similar to the
TalaMind approach, although the DGI and TalaMind approaches were
developed independently. The DGI paper does not present a prototype
design or demonstration of its proposed approach. It includes a
proposal for “Seed AI” that is similar to the TalaMind intelligence
kernel hypothesis (§§1.4.1, 2.3.5). DGI’s five levels of organization map
into the three levels of conceptual processing discussed in §1.5. In
particular, the archetype level corresponds to DGI’s layer for
“concepts”, and the linguistic level includes DGI’s layers for “thoughts”
and “deliberation”. Yudkowsky’s description of the “thoughts” layer
(2007, p.407) is similar to the TalaMind natural language mentalese
hypothesis (§1.4.2), and similar to Evans’ LCCM theory (§2.3.2).
However, it appears Yudkowsky (2007, pp.458-461) does not expect that
DGI thoughts will (at least initially) be represented as sentences in a
natural language mentalese, nor propose representing thoughts in
structures corresponding to parse-trees of natural language expressions,
as this thesis discusses in §§3.3, 3.4, 3.5. Also, DGI focuses on mental
images for reasoning. To contrast, this thesis focuses on linguistic
reasoning, with spatial reasoning and visualization left as topics for
future research.
To the extent that DGI envisions internal use of concept structures
different from the Tala natural language mentalese, its proposed
research direction appears similar to that investigated by Sloman
(§2.3.3.1), or to that implemented independently by Sowa’s (2011)
VivoMind Cognitive Architecture (VCA). Sowa describes VCA as using
conceptual graphs for communication within a society of mind
architecture (§2.3.3.2.1), and as a scalable, efficient system supporting
applications that include natural language processing.

Subject Review: Human-Level AI & Natural Language

52
2.3.3.6 Other Influences for Thesis Approach
The approach proposed in this thesis has been influenced by several
previous research efforts related to the analysis of human-level
intelligence, including: Gärdenfors’ (2000) discussion of conceptual
spaces; Gelernter’s (1994) discussion of focus of attention and the
‘cognitive pulse’; Hofstadter’s (1995) discussions of fluid concepts and
analogies; Mandler’s (1988 et seq.) study of how babies develop an
extensible representation system with conceptual primitives.
2.3.4 Approaches to Artificial Consciousness
Blackmore (2011, pp.286-301) gives an overview of research on
artificial consciousness. Much of this research has derived from work in
robotics, and has focused on the associative level of conceptual
processing (viz. Figure 1-1). Following is a brief summary of research:
Aleksander (1996) writes that in 1991 he began investigating artificial
consciousness based on neural nets. He and Morton (2007) propose five
“axioms of being conscious”, using introspective statements:
1. I feel as if I am at the focus of an ‘out there’ world.
2. I can recall and imagine experiences of feeling in an out there
world.
3. My experiences in 2 are dictated by attention, and attention is
involved in recall.
4. I can imagine several ways of acting in the future.
5. I can evaluate emotionally ways of acting into the future in
order to act in some purposive way. 27
Aleksander uses first person statements to address Chalmers’ (1995)
“Hard Problem” of explaining the subjective experience of
consciousness, by asking “What do I need to have in my head to believe
that I am conscious?” and “What does a machine need to have for it to
believe it is conscious?” (Aleksander 1996, p.31).
This approach does not fully address the Hard Problem though it
does support one answer, namely that if the subjective, first person
aspect of consciousness is an illusion, then in principle machines could
also have this illusion (viz. Blackmore 2011, p. 285). Of course, we are
not interested in machines simply giving canned responses saying they

27 Earlier versions of these axioms were given by Aleksander &
Dunmall (2003) and Aleksander (2005).

Relation of Thesis Approach to Previous Research

53
believe they are conscious: We want to be able to point to the internal
design of the machine and the processing within it, that supports a
machine have perceptions of itself, developing beliefs and acting as if it
believes it is conscious (viz. §2.1.2.8). Section 4.2.7 will discuss the
relationship of the Hard Problem to the TalaMind approach.
Aleksander & Morton’s five axioms may be taken as theoretical
requirements for the TalaMind architecture to demonstrate aspects of
consciousness, discussed further in §§3.7.6 and 4.2.7, though this thesis
intentionally omits discussion of emotion in relation to consciousness,
and does not focus on attention in recall – these are topics for future
research. In addition, reflective observation is included in the list of
theoretical requirements for TalaMind to demonstrate consciousness,
which seems to be implicit in Aleksander’s discussions.
Aleksander & Morton (2007) discuss “depictive architectures” to
satisfy these axioms, focusing on the “kernel architecture” proposed by
Aleksander (2005). They define a depiction as “a state in a system S that
represents as accurately as required by the purposes of S the world,
from a virtual point of view within S” and describe kernel architectures
in terms of neural state machines. This is analogous to the TalaMind
approach, which §3.7.6 discusses at the linguistic concept level, while
depictive architectures are discussed at the associative concept level
(viz. Figure 1-1).
Aleksander (1996, 2001) accepts Searle’s arguments against symbolic
AI, and does not appear to allow his approach to go beyond the
associative level of concept processing. This thesis leverages
Gärdenfors’ (1994) discussion of three levels of inductive inference
(§1.5), and does not accept Searle’s arguments, in agreement with
Chalmers as well as many AI researchers (viz. §4.2.4).
Sun (1997 et seq.) describes research on learning and artificial
consciousness, representing explicit knowledge via symbolic rules and
implicit knowledge via neural networks. Symbolic rules can be
extracted from neural networks, and selected via hypothesis testing, to
support learning. He gives experimental results on performance of the
approach in learning tasks such as the Tower of Hanoi, artificial
grammar learning, process control, and minefield navigation.
Chella et al. (1997 et seq.) discuss the integration of three levels of
concept representation to support artificial consciousness, including
symbolic concepts expressed as semantic networks (“in the tradition of
KL-ONE”) and cognitive concepts represented via conceptual spaces

Subject Review: Human-Level AI & Natural Language

54
(Gärdenfors, 2000), with expectations at the linguistic level helping to
guide recognition at lower levels. This is consistent with the TalaMind
approach.
Rosenthal’s (2005) theory of consciousness in terms of “higher-order
thoughts” is synergistic with the TalaMind approach, though he
discounts the value of using natural language as a representation for
internal thoughts, claiming “Our thoughts, by contrast, seldom need to
respect the fine-grained semantic distinctions inherent in natural
language.” The use of natural language syntax in the Tala conceptual
language greatly facilitates expression of higher-order thoughts, since it
allows Tala conceptual sentences to include other sentences, nested to
an arbitrary degree. The use of the reserved variable ?self in
TalaMind appears equivalent to Rosenthal’s discussion of the need for a
first-person indexical in higher-order thoughts. Investigation of
Rosenthal’s theory within the TalaMind approach would be an
interesting topic for future work.
2.3.5 Approaches to Reflection and Self-Programming
Another perspective on artificial intelligence, related to artificial

consciousness, is given by research on the topics of reflective and self-
programming systems. It is an old, but as yet unrealized and still largely

unexplored idea that computer programs should be able to extend and
modify themselves, to achieve human-level AI.
In this thesis, self-programming is proposed by the intelligence
kernel hypothesis (§1.4.1), which is a variant of Newell & Simon’s (1976)
Physical Symbol System Hypothesis (§1.4.4). Other authors have

proposed similar ideas: Schmidhuber (1987 et seq.) has investigated self-
referential, self-improving systems. Nilsson (2005) 28 proposed that

human-level AI may need to be developed as a “core” system able to
extend itself when immersed in an appropriate environment, and wrote
that similar approaches were suggested by Wegbreit, Brooks (1997),
McCarthy, and Hawkins & Blakeslee (2004). Yudkowsky (2007)

proposed creating “seed AI”, i.e. “an AI designed for self-
understanding, self-modification, and recursive self-improvement.” In

2011, papers by Goertzel, Hall, Leijnen, Pissanetzky, Skaba, and Wang

28 Nilsson cited a private communication from Ben Wegbreit ca. 1998,
and the 1999 version of McCarthy’s The well-designed child, cited here as
McCarthy (2008).

Relation of Thesis Approach to Previous Research

55
were presented at a workshop on self-programming in AGI systems.
Thórisson (2012) discusses a “constructivist AI” approach toward
developing self-organizing architectures and self-generated code.
Coincidentally, the prototype TalaMind demonstration system
illustrates some of the architectural principles Thórisson advocates (e.g.

temporal grounding, self-modeling, pan-architectural pattern-
matching), at least to a limited degree (§§5.4.14, 5.4.9, 5.5.3).

Doyle (1980) discussed how a system could defeasibly perform
causal and purposive reasoning, to reflectively modify its actions and
reasoning. He described a conceptual language based on a variant of
predicate calculus, in which theories could refer to theories as objects,
and in which some concepts could be interpreted as programs. Doyle
noted the use of predicate calculus was “inessential”, but did not
discuss a language of thought based on the syntax of a natural
language. His thesis did not include a prototype demonstration, though
elements of the approach were partially implemented by himself and
others. He expected the approach would require “machines a thousand
times larger (and perhaps faster)” than computers available in 1980. The
TalaMind approach is compatible with Doyle’s thesis, and following
chapters explore similar ideas to a limited extent, as a subset of the
TalaMind architecture.
Smith’s (1982) doctoral thesis studied “how a computational system
can be constructed to ‘reason’ effectively and consequentially about its
own inference processes”. Though he focused on a limited aspect of this
problem (procedural reflection, allowing programs to access and
manipulate descriptions of their operations and structures), he gave
remarks relevant to human-level AI. He stated the following “Knowledge
Representation Hypothesis”, as descriptive of most AI research at the time:
“Any mechanically embodied intelligent process will be
comprised of structural ingredients that a) we as external
observers naturally take to represent a propositional account of
the knowledge that the overall process exhibits, and b)
independent of such external semantical attribution, play a
formal but causal and essential role in engendering the behavior
that manifests that knowledge.”
This may be considered as a variant of PSSH (§1.4.4), and describes
much AI research up to the present. It is consistent with Hypothesis I of
this thesis, to the extent that concepts are considered as symbolic

Subject Review: Human-Level AI & Natural Language

56
structures (expressions) that represent knowledge. Though in the
TalaMind approach, conceptual structures may also represent
questions, hypotheses, procedures, etc., each of these may be considered
a form of knowledge. Thus, a question may be considered as a
statement that something is not known.
Smith provided the following “Reflection Hypothesis” as a statement
guiding his research into self-reflective systems:
“In as much as a computational process can be constructed to
reason about an external world in virtue of comprising an
ingredient process (interpreter) formally manipulating
representations of that world, so too a computational process
could be made to reason about itself in virtue of comprising an
ingredient process (interpreter) manipulating representations of
its own operations and structures.”
This is also consistent with PSSH, and with Hypothesis I of this thesis.
Thus, Hypothesis I may be seen as combining Smith’s two hypotheses
into a single statement.
Smith gave general remarks about reflection and representation,
which are consistent with the TalaMind approach and architecture.
More specifically, he wrote:
“The successful development of a general reflective calculus
based on the knowledge representation hypothesis will depend
on the prior solution of three problems:
1. The provision of a computationally tractable and
epistemologically adequate descriptive language,
2. The formulation of a unified theory of computation and
representation, and
3. The demonstration of how a computational system can
reason effectively and consequentially about its own
inference processes.”

Smith did not pursue the first problem, “ in part because it is so ill-
constrained.” This thesis adopts Hypothesis II, within the TalaMind

architecture, to investigate the first problem.
Regarding the second problem, Smith wrote that “every
representation system proposed to date exemplifies what we may call a
dual-calculus approach: a procedural calculus...is conjoined with a

Relation of Thesis Approach to Previous Research

57
declarative formalism (an encoding of predicate logic, frames, etc.).” He
suggested “this dual-calculus style is unnecessary and indicative of
serious shortcomings in our conception of the representational
endeavor.” However, he wrote “this issue too will be largely ignored”
in his thesis.
In developing Hypotheses I and II within the TalaMind architecture,
this thesis investigates a unified approach to the second problem: The
Tala conceptual language provides a representation for both declarative
and procedural knowledge, based on the syntax of a natural language.
Smith’s thesis focused on the third problem he identified, discussing
a limited aspect of this problem. He translated the higher-level problem

of how a system could reason about its inference processes into a lower-
level problem, i.e. how a programming language could support

procedural reflection, allowing programs to access and manipulate
descriptions of their operations and control structures, dynamically
affecting their interpretation at runtime. This implicitly connects
procedural reflection with a form of self-programming. Smith showed
how procedural reflection could be incorporated into a variant of Lisp,
to support continuations with a variable number of arguments, improve
support of macros, etc. Coven (1991) gave further discussion of
reflection within functional programming languages, toward support of
systems that could in principle reflect on their own reasoning processes
and learning algorithms.
Effective reflection and self-programming in human-level AI require
computers to have what Smith called ‘semantic originality’ (in other
literature called ‘original intentionality’), i.e. to be able to attribute
meaning to symbols and processes independently of human
observation. Smith (1982) noted that computers could not yet attribute
meaning to what they do, but suggested the possibility they could do so
in principle. Haugeland (1985) discussed the topic and its philosophical
history at some length, and left open the possibility computers could in
principle attribute meaning. Dretske (1985) discussed the “entrance
requirements” for computers to ascribe meaning. Dennett (1987) argued
that humans have no greater semantic originality than computers do in
principle, because we are biomolecular machines constructed by
evolution. Searle (1992) argued that computers cannot in principle
attribute semantics – Chalmers (1996) refutes Searle’s argument.
Regarding arguments that the meanings of computational systems are
intrinsically derivative or attributed by humans, Smith (1996, p.10) said

Subject Review: Human-Level AI & Natural Language

58
he was “dubious about the ultimate utility (and sharpness) of this
distinction, and also about its applicability to computers...” Section
3.7.2.2 explains how Tala agents can have semantic originality.
2.4 Summary
This chapter discussed the relation of the TalaMind hypotheses to
previous research, and presented the approach of this thesis to verifying
whether a system achieves human-level AI. This approach (design
inspection for support of higher-level mentalities) is different from
previous research focused on behavioristic comparisons, e.g. via the
Turing Test. It is also different from research that seeks to achieve
human-level AI through general methods without specifically
addressing higher-level mentalities. This chapter’s review of previous
research has not found an equivalent discussion of the TalaMind
hypotheses as a combined approach.


