This term paper describes “Artificial Intelligence” as a branch of computer science and the development of 
computer programs which are self-decisive and intelligent as humans.

Every day we use smart phones but have you ever thought the mobile phones of the last decade are not called 
“Smartphones”. Isn’t there a massive difference in accuracy between the search engines of the past and now? 
Well aren’t social networking sites more relevant and connected as in the present? All this is possible only 
due to the application of Artificial Intelligence in the field of computers and technology. In the near future
we can expect AI applications such as highly efficient self-driven cars, programs which can predict our future, 
applications which can convert any speech from one language to another and the next generation face recognition 
applications which can identify you even in the dark. All this will be a reality with the development of AI as a 
core science and engineering field.

With AI gaining more importance, many nations have significantly invested in this field and the result was they 
have progressed both economically and financially. This technology has helped in fully automation and profit 
maximization in the industrial field with fully intelligent machines which can make decisions more accurate than us.

Technological advances in the field of three-dimensional (3D) technologies have recently led to a thorough revision 
of the postulates of architectural representation. Recent 3D techniques have allowed for accurate representation of 
real environments and the generation of virtual spaces, by blurring the limits between ‘real’ and ‘non-real’ spaces.

Some of the latest developments include 3D ‘Light Imaging Detection and Ranging’ (LIDAR) scanners that have allowed 
3D scanning of real environments, which have later been processed in a computer. Other recent developments are 3D 
printing devices that have allowed the printing of three dimensional objects from a 3D virtual model.

LIDAR scanner technologies use a high-precision laser beam (within 1 mm precision range), able to analyze a real 
object or environment taking data from its geometry, color and texture. These data can be used to reconstruct the 
scans into 3D models, with a large variety of uses and applications. This technology allows for the compilation of 
accurate data within a very reasonable timeframe. The format used by the scanners is typically referred as a Point 
cloud, consisting of a database of vertex referred to a coordinate system. The vertex is defined by its coordinates 
(X, Y, Z) that represent the external surfaces of the scanned objects. The points typically also contain other 
information, such as color, reflectivity index and illumination levels.

This technology allows for a broad range of applications in architecture, such as the reproduction of architectural 
details, topographic definition of buildings or civil works, 3D cataloguing of historical heritage and 
representation of public spaces. Other applications involve the use of data for their manipulation into new 
designs, historical renovations or morphogenesis processes.

The advantages of 3D LIDAR scanning are currently limited by the size of the scans, which typically are difficult 
or impossible to process by current personalcomputers, jeopardizing its applications by common users and in the 
professional world. Currently, new point-based computation techniques are under development; these new procedures 
already allow for the visualization of millions of points captured by a LIDAR scan. The further development of 
point-based techniques opens up new possibilities for the use of LIDAR computational capabilities. Furthermore, 
points are an ideal format for the application of artificial intelligence (AI) based algorithms, such as 
self-organization, morphogenesis and evolutionary processes. The use of point geometry in combination with AI 
opens up the possibility for new architectonical methodologies that could lead to major changes in the 
computational methodological postulates of architectural design and representation, by further blurring the 
boundaries between the ‘real’ and 3D ‘artificially generated’ world.

The goal of this thesis is to conduct a preliminary analysis of transparency and openness in the development of 
Artificial Intelligence (AI) as well as explore the possibility of the declaration of AI as a commons. A commons 
has several characteristics and one of the first question would be if AI fulfills these traits. Other questions 
include: What would happen if everyone gained access to AI? How can we distribute software equally? If there are 
no monopolies and no owners, who would regulate it and who would allocate the wealth generated by AI? How can we 
ensure transparency about means and purposes in the making of AI? What other challenges might society face? What 
opportunities emerge for society?

Definitions of several technical terms have been examined and established at the begin-ning. Furthermore, AI experts 
in the United States (U.S.) were interviewed and the results have been analyzed and summarized in this paper.

Artificial Intelligence is a unique technology in that sophisticated AI can pose an existen-tial risk to human life 
(Bostrom 2016, p. 9). A so-called “superintelligence” is one that would exceed human intelligence and therefore 
pose a potential threat to human existence by surpassing human capabilities. A human-level (or above) Artificial 
Intelligence could develop its own will and it could be in conflict with the will of humans. Autonomous robots – 
AI paired with robotics – could be a great danger, too. Autonomous weapons, disruption of financial markets and 
oppression are some of the dangers humanity might face as soon as a sophisticated AI has been developed, according 
to Hawking (Fuller 2016). As Russell and Norvig (1995, p. 3) state

“Although no one can predict the future in detail, it is clear that computers with human-level intelligence 
(or better) would have a huge impact on our everyday lives and on the future course of civilization.”

We are not there yet – a lot of research and development still needs to be done. However, experts give warnings, 
hold talks, get inquired by governments, and form committees to explore these risks.

Presently, AI can already program machine-learning software which can executes tasks. As such, the barrier of 
the prerequisite of expertise to program AI software has greatly reduced while the pace of AI development has 
increased (Simonite 2017). Soon there could be AI which can program its own code. Recursive self-improvement is 
risky as human developers may have problems understanding the code. Experts even predict that these sophisticated 
AIs will be able to communicate in languages not understandable by ordinary citizens with the consequence of 
exclusion. It is unforeseeable but predictable that superhuman intelligence, or superintelligence, will first 
be created by computers within the AI space and not by enhancing human intelligence somehow (Yudkowsky 2012, p. 32).
 An exponential technological growth that is beyond human control is called “technological singularity” by 
futurologists (Bostrom 2016, p. 9).

Having an understanding of background processes as well as capabilities, and knowing the means and purposes of AI 
could keep humans in control. Knowledge especially plays a role in transparency and openness. Only informed people 
can make informed choices, and in order to be informed, it is important to have access to information 
(Bostrom 2016, p. 13). But leading AI companies like Google and Facebook have built monopolies around the software 
and data including property rights and expert accumulation. And ex-pertise seems like the trading good these days 
because big AI companies are fighting over talented software engineers (Metz 2017). With a lot of power often comes 
a lot of respon-sibility. Bill Hibbard (2016, p. 3) even declares the responsibility of AI developers and announces 
them as “representatives for the future of humanity”.

There are initiatives trying to bring more transparency and balance into the market sur-rounding this field of 
science. For example the OpenAI organization has “openness ex-plicitly built into its brand identity” 
(Bostrom 2016, p. 2). It pushes openly accessible and modifiable AI. Indeed, the “Open AI Movement” can be 
observed worldwide with more and more developers as well as companies’ open-sourcing their AI’s.

This thesis tries to combine the phenomena of AI as an existential risk, “Open AI” as a multiplicator of 
transparency and openness, and the idea of the commons as a stepping stone for participation and inclusion.

It begins by rendering a definition, history, methods, and other technical terms of AI. In the third chapter, 
commons will be explained and the theories around human participation and involvement in software development 
and common distribution. The fourth chapter will combine both phenomena, AI and commons, explore open-source AI 
and the “Open AI Movement,” represent and summarize the AI expert interviews, outlining opportuni-ties and 
challenges facing society as well as the legal placement of AI. The fifth chapter will outline an assessment on 
competition within the AI market and finally the last chapter will conclude the findings of this thesis.

Flooding is a natural disaster that can have extreme consequences on
communities and the people within them. In the UK, there is a risk of flooding
from rivers, the sea, groundwater, reservoirs and surface water (Environment
Agency, 2009), which can all have a devastating impact on individuals and
communities. The UK has experienced widespread flooding in recent years, most
notably 2007, 2009, 2012 and 2014 (Centre for Ecology & Hydrology, 2014; JBA
Risk Management Limited and Met Office UK, 2012; Marsh and Hannaford, 2007;
Smythe, 2013).

Climate change, ageing flood-prevention infrastructure and methodologies, and
socio-economic circumstances have all played a part in these events and how
they have impacted the UK. Additionally, the majority of the UK drainage
networks are still combined systems, handling both waste water and storm water
(Marsalek et al., 1998). This leads to massive additional pressure on drainage
systems during rainfall events. Looking to the future, and taking into account the
trend across the time-scale of this research, it seems likely that more extreme
weather events will become a fact of life and the UK, therefore, must adopt a proactive
approach to improving the way in which it manages the risks of flooding.

One of the key areas of flood risk management is the identification of intervention
strategies that can be applied to drainage networks, in order to reduce the flood
risk associated with those networks. Inevitably, the bodies concerned with
applying these intervention strategies to the networks will be interested in
ensuring that they maximise the return on their investment or in other words,
achieve the maximum reduction of risk of flooding for the amount of money they
are investing. Additionally, they will be keen to ensure that they make informed
and justifiable choices in terms of identifying the optimal investment point to
target.

The main method of achieving these goals to date has been with the aid of human
engineers who invest a great deal of time into providing information to decision
makers and making sure that they can have confidence in the information they
have provided. The problem solving approach can involve computational
modelling (Marsalek et al., 1998), examination of previous approaches used in
situations that are related in some way, and a considerable amount of
engineering experience and knowledge being applied. This thesis aims to
advance the development of flood risk intervention strategies for urban drainage
networks.

As computers lack human intuition for what makes a “good” or “bad” solution, the
only way to truly solve a problem for computers is to perform an exhaustive
analysis of every possible solution. In situations where this is not possible (due
to time or computing power constraints), heuristic algorithms are generally used,
which are aimed at producing a “good enough” solutions. One such type of
heuristic algorithm is a “genetic algorithm” (see section 2.3.2).

In order to solve these problems, this thesis has utilised multi-objective
optimisation techniques. These techniques develop a Pareto front allowing an
expert user to give guidance, as to prime investment points, and the improvement
in (reduction of) flood risk potentially available at each investment point. In order
to allow these techniques to complete within a reasonable period, it has been
necessary to investigate heuristic methods of decreasing the computational cost
of the necessary objective functions. A problem with these methods is the
reduction in accuracy that is necessarily a part of the way they operate. Care
must be taken in their application to ensure that as little as possible accuracy is
lost in the process.

In order to gain the benefits that these technologies can supply, this thesis
describes and demonstrates the application of an object oriented software
solution utilising these methods, which could guide engineers in the process of
developing investment options and advising their clients/decision-makers as to
the appropriate investment points available to them.

The main aim of this thesis is to describe the development of a new methodology
for drainage system flood risk management, which will aid in the identification of
optimal investment points, as well as the maximum potential reduction in risk for
a given investment.

Urban flooding is estimated to cost two hundred and seventy million pounds per
year in England and Wales combined, with eighty thousand homes at risk
(Parliamentary Office of Science and Technology, 2007). It is usually caused by
rainfall overwhelming combined drainage systems, rivers overflowing due to
excessive surface run off, or a combination of the two. In addition to these
monetary costs, flooding presents various health risks (Fewtrell and Kay, 2008),
which strongly affect the quality of life of individuals even after the flooding event
has itself passed. Looking at these facts, it is clear that there is a need to develop
improved methods for identifying the most suitable intervention strategies for
flood risk reduction in given urban areas.

1Metallic interfaces are a key ingredient in controlling the strength, ductility,
reliability and lifetime properties of metal-based structural and functional materials
and devices. This holds for bulk materials, where many properties are
controlled by the behaviour at and through physical boundaries (precipitate
hardening, Hall-Petch effect, etc.), and is even more so for small metallic components,
like those used in Microelectromechanical systems (MEMS) where the
mechanical properties are largely dominated by the metallic interfaces present
(e.g. Grain Boundaries (GBs) and oxide layers). Likewise, solder interconnects
are prone to failure along interfaces.

Studies of GBs, dislocations and their interaction, inevitably, have to be done
at the atomic level, where the dynamics of individual atoms are expected to
provide the answers that will help resolve the unknowns at larger scales.
Within the framework of the MuMIM (Multiscale Metallic Interface Modelling)
project, which aims at constructing a generic multiscale technique for
the quantitative description of metallic interfaces, this thesis aims at generating
a framework that allows the simulation and understanding of GBs, dislocations
and their interaction at the atomic scale.

A method that allows to deal with atomistic systems that are big enough to
capture the behaviour of GBs and dislocations is Molecular Dynamics/Statics
(MD/MS). Here, in order to maintain a reasonably good representation of the
properties of the material, the interaction of the atoms is defined via interatomic
potentials based on the Embedded Atom Method (EAM).
MS allows to model the behaviour of atomistic systems in quasi-static conditions.
but it has the mayor drawback that, since a full relaxation of the system
has to be done after each deformation step, the calculations can become computationally
expensive. Currently, there exist a variety of methods to perform
this relaxation, but not much attention has been paid in the literature to the selection
of the optimal choice for treating atomistic systems containing plasticity
elements.

To tackle this issue, in Part i an analysis and comparison of different minimization
algorithms that can be used for molecular static simulations is presented.
The objective is to find an appropriate and efficient method for modelling GBs
and dislocations at the atomic scale.

The study of single dislocations via MD/MS starts by the generation of an
atomistic system containing such a defect. Once a dislocation is inserted, MD/MS
can be used to calculate its static and dynamic properties, among which is perhaps
the most fundamental characteristic of dislocations: the Peierls stress.
In the specific case of aluminium, available values of the Peierls stress in the
literature show a spread of around two orders of magnitude. To provide some
clarity in this topic, Part ii deals with the generation and the properties of single
edge dislocations. Here a detailed description of a simple method for inserting
dislocations into face-centred cubic crystals is introduced and a study of the
motion of partial dislocations and its relation with the Peierls stress is carried
out.

Following the line of thought used in the case of dislocations, the simulation
of GBs also requires as an initial step the generation of a system containing
the desired GB. It turns out that, due to the complex order of the atoms in
interfaces, the generation of realistic GBs is more delicate than just putting two
single crystals together; GBs with equal orientations can have different energies.
To shed some light in this topic, Part iii treats the subject of GBs at the atomic
scale. First, two different methods for the generation of GBs in atomistic simulations
are shown, one of which is based on genetic algorithms. And second,
artificial neural networks are used for the prediction of GB energies as a function
of their misorientation.

A few attempts have been made to address the interaction between dislocations
and a GB, but mainly on a phenomenological basis. This plasticityinterface
interaction takes place at the level of individual dislocations, which
can be blocked by, absorbed in, transmitted through or nucleated from GBs.
Part iv gathers the results obtained and the methods developed in the previous
parts, and combines them to perform an analysis of the interaction between
a GB and a single dislocation. This analysis is focused on extracting quantitative
information that is useful for larger scale simulation methods

The human brain has been undergoing serious investigations by many researchers in the field of neuroscience. 
In time past, there have been considerable investigations of the structure of the brain (anatomy of the brain), 
but studies on the functional operation of its complex neural network, paraded all sorts of fantasies as knowledge 
for many centuries (Sundal et al., 2014).

Around the middle of the 18th century, a functional understanding of the human brain began to take shape. At 
that time, studies on the brain revealed that nerve signals formerly thought of as “animal spirits” are actually 
electric signals not very different from the currents that flow in an electrical circuit (Sundal et al., 2014). 
Not only that, advancements in microscope and neuroscience revealed the morphology of neurons, and presented a 
vision of the brain as a network of neurons; unraveling how neurons interact among themselves using chemical 
signals.

The adult human brain is made up of about 100 billion neurons, each with about 1,000 - 10,000 connections, 
making a total 1014-1015 connections in the brain (Sundal et al., 2014). It is perhaps the most complex system, 
more complex than the entire mobile network of the world, with its neurons making and breaking connections at a 
time-scale that can be as short as a few tens of seconds (Sundal et al., 2014). How the brain enables human beings 
to think has remained a mystery until the present day. Significant ventures in the field of Artificial Intelligence 
have enabled scientists to come close to the nature of thought processes inside a brain (Zhang, 2011). In the area 
of artificial intelligence, Artificial Neural Network (ANN) is employed as computational tools to model a 
biological brain (Willamette, 2014). Artificial intelligence seeks to answer questions like “how network of neurons 
in the visual processing areas of the brain transduce the optical image that falls on the retina and how they can 
be simulated to make intelligent device?” Answers to questions like this are best described in the language of 
mathematics, which is the primary preoccupation of science in computational neuroscience (Sundal et al., 2014).

In the same vein, computation in the brain involves understanding human and animal brains using computational 
models (computational neurobiology); and the process of simulating and building a machine to emulate the real 
brain (neural computing). All these and other diverse neural network models are examined in the emerging field 
of computational neuroscience.

For several years software companies have promised to develop systems that use cognitive principles like
humans have. An example is IBM, that commercialises Artificial Intelligence technologies in its ‘cognitive
computing’ platform called Watson, that can be used by organisations to leverage the promised
power of these technologies (IBM Corporation, 2016; Olavsrud, 2014). This and other commercial
‘intelligent’ software claim to provide businesses with the ability to ingest and analyse (large amounts
of) data, and support intelligent decision-making. The scientific field of Artificial Intelligence (AI),
formally studying the creation of machines that express intelligent behaviour (Russell and Norvig,
2003), is a major driver behind these software technologies. AI has given birth to technologies such
as machine learning, natural language processing, speech recognition and computer vision (Schatsky
et al., 2014).

Applications of AI software can be distinguished in three main categories: product, process, and/or
general insights (Schatsky et al., 2015). Product applications of AI software provide benefits to the
product and/or service, and thus in the end the end-customer. Process applications of AI technologies
on the other hand, deliver value to a work-flow of an organisation, automating or improving business
operations. Finally, AI technologies could be exploited to generate more general insight for a business:
knowledge that can inform those, e.g. managers, to help them in operational and strategic decision
making across an organisation.

Of these three applications of AI driven software systems, their application to business processes
is considered to have a lot of potential (Power, 2015). For example, business processes can perform
more efficient and/or more accurate when they leverage AI in a proper way, reshaping workforce needs
and reducing costs (KPMG International, 2015). As a result, the quality of its output can increase
significantly. Therefore, this research particularly focuses on business processes within organisations.
According to Schatsky et al. (2015); Verhoeff (2016); Goel and Davies (2011), business processes that
have to deal with a lot of information and knowledge seem to be most viable as a subject for application
for AI technologies, since these technologies are driven by (a lot of) data. When the performance of a
business process is strongly related to its ability to properly handle a large amount of data, computer
machines are considered to be of great value when their (AI) computing power is leveraged properly,
thereby creating a competitive edge (Microsoft Corporation, 2012).

Some organisations have initiated projects to discover the possibilities of AI for their organisation
However, these organisations have experienced these (transformational) projects as very challenging
(van der Hulst, 2015; The Economist, 2015b). A survey among people working with AI software such
as IBM Watson shows for example that no shared understanding is present amongst stakeholders –
system designers, end-users, process managers, et cetera – to communicate effectively and efficiently
about how these new business-AI technology concepts can be utilised in favour of the business (van der
Hulst, 2015). Furthermore, companies often find Artificial Intelligence and its derived technologies
difficult to integrate into business solutions, especially with the technology moving so rapidly (Power,
2015).

AI software services like IBM Watson’s ‘cognitive computing’ thus raise some issues. At this
moment, AI is gaining momentum in the business world by attracting the attention of CIO’s, CTO’s
and IT specialists in organisations. AI does not remain a topic only related to the field of business.
Because of its developments and (media) attention past years, some people do have concerns that these
technologies will affect people’s life drastically in the future. For example, Silicon Valley entrepreneur
Elon Musk is investing in AI “to keep an eye on it” – has said it is potentially “more dangerous than
nukes” (Schatsky et al., 2014; Hern, 2015). Scholars at the University of Oxford published a study
estimating that 47 percent of total US employment is “at risk” due to the automation of human-cognitive
tasks (Schatsky et al., 2015).

The attention for AI in the business context is clearly visible in recent investment decisions made
by companies. For example, on November 6st, 2015, the Japanese car manufacturer Toyota announced
an one billion dollar investment over five years in the research and development of artificial intelligence
applications in its business (Markoff, 2015). Google recently open sourced its Artificial Intelligence
engine for anyone to use, to stimulate and foster the development of Artificial Intelligence and its
applications (Lewis, 2015). IBM has committed $1 billion to commercialise AI such that businesses
will be able to leverage these technologies. These snippets indicate a lot of movement in the field of
AI business applications in the past year, and more will probably follow. As a result, the list of business
application examples of AI is continuously becoming larger. However, with all these applications
available, many businesses ask themselves where they should start, what they need to do in advance,
and how to execute a (transformational) project to start leveraging AI technologies in their business
(van der Hulst, 2015).

Although this attention to AI and its development of intelligent systems by the business world is
quite recent, scholars have been studying how to design and develop such systems already for decades
(Ashby, 1962; Russell and Norvig, 2003). Intelligence is the capability to analyse, interpret and understand
data to generate relevant and valuable information and to learn from it. Human beings are the
example of entities that strongly exhibit intelligence. Intelligent behaviour of human beings is studied
in literature extensively. This intelligent behaviour arises from the mental processes of humans, such
as perception, reasoning, decision-making and learning. These mental processes are designated as cognition.
Put differently, a more general component of cognitive competence, in the sense that it does
not require specific knowledge, is what is conventionally known as intelligence, i.e. the general ability
to solve problems (Heylighen, 2011).

Recent experiments show that computer intelligence has become more powerful in the past years (MIT
Technology Review, 2015). Very recently, in March 2016, a computer system powered by AI technology
achieved another milestone in Computer Science history, by being victorious in the game Go over the
world champion. Because the ruleset of this game is very small, the rules give rise to a lot of complexity
– the reason why this game is considered to be “one of the great intellectual mind sports of the world”
(Byford, 2016). With these technologies improving so fast, the opportunities for organisations increase.
Human cognition has the ability to analyse and interpret data fairly well, but there are limitations,
such as in our capabilities to analyse huge amounts of data (The Economist, 2015a). In contrast to
humans, computers are able to process a vast amount of data, but are still not able to structurally
translate these data from observation to understanding and ultimately decision-making autonomously.
Furthermore, AI’s ability to learn from behaviour and subsequent consequences (internal learning), and
especially from outside its context (external learning) is not near human performance (Chang, 2016).
In other words, computers have not yet the “fluid ability to infer, judge and decide that is associated
with intelligence in the conventional human sense” (The Economist, 2015b). Many authorities in the
field of AI therefore do not think that AI will replace humans in organisations in the foreseeing future
(The Economist, 2015b; Gordon, 2016).

In 1950, Turing’s paper on Computing Machinery and Intelligence
challenged scientists to achieve human-level artificial intelligence,
though the term ‘artificial intelligence’ was not officially coined until
1955, in the Dartmouth summer research project proposal by McCarthy,
Minsky, Rochester, and Shannon.
In considering the question “Can machines think?” Turing suggested
scientists could say a computer thinks if it cannot be reliably
distinguished from a human being in an “imitation game”, which is
now known as a Turing Test. He suggested programming a computer to
learn like a human child, calling such a system a “child machine”, and
noted the learning process could change some of the child machine’s
operating rules. Understanding natural language would be important
for human-level AI, since it would be required to educate a child
machine, and would be needed to play the imitation game.
McCarthy et al. proposed research “to proceed on the basis of the
conjecture that every aspect of learning or any other feature of
intelligence can in principle be so precisely described that a machine can
be made to simulate it.” They proposed to investigate “how to make
machines use language, form abstractions and concepts, solve kinds of
problems now reserved for humans, and improve themselves” and to
study topics such as neural nets, computational complexity,
randomness and creativity, invention and discovery.

McCarthy proposed that his research in the Dartmouth summer
project would focus on “the relation of language to intelligence”. Noting
that “The English language has a number of properties which every
formal language described so far lacks”, such as “The user of English
can refer to himself in it and formulate statements regarding his
progress in solving the problem he is working on”, he wrote:
“It therefore seems to be desirable to attempt to construct an
artificial language which a computer can be programmed to use
on problems requiring conjecture and self-reference. It should
correspond to English in the sense that short English statements
about the given subject matter should have short
correspondents in the language and so should short arguments
or conjectural arguments. I hope to try to formulate a language
having these properties and in addition to contain the notions of
physical object, event, etc., with the hope that using this
language it will be possible to program a machine to learn to
play games well and do other tasks.”
Turing’s 1950 paper concluded:
“We may hope that machines will eventually compete with men
in all purely intellectual fields. But which are the best ones to
start with? Even this is a difficult decision. Many people think
that a very abstract activity, like the playing of chess, would be
best. It can also be maintained that it is best to provide the
machine with the best sense organs that money can buy, and
then teach it to understand and speak English. This process
could follow the normal teaching of a child. Things would be
pointed out and named, etc. Again I do not know what the right
answer is, but I think both approaches should be tried. We can
only see a short distance ahead, but we can see plenty there that
needs to be done.”

The first approach, playing chess, was successfully undertaken by AI
researchers, culminating in the 1997 victory of Deep Blue over the world
chess champion Gary Kasparov. We4 now know this approach only
scratches the surface of human-level intelligence. It is clear that
understanding natural language is far more challenging: No computer
yet understands natural language as well as an average five year old
human child. No computer can yet replicate the ability to learn and
understand language demonstrated by an average toddler.

Though Turing’s paper and the Dartmouth proposal both stated the
long-term research goal to achieve human-level AI, for several decades
there were few direct efforts toward achieving this goal. Rather, there
was research on foundational problems in a variety of areas such as
problem-solving, theorem-proving, game-playing, machine learning,
language processing, etc. This was perhaps all that could be expected,
given the emerging state of scientific knowledge about these topics, and
about intelligence in general, during these decades.
There have been many approaches at least indirectly toward the
long-term goal. One broad stream of research to understanding
intelligence has focused on logical, truth-conditional, model theoretic
approaches to representation and processing, via predicate calculus,
conceptual graphs, description logics, modal logics, type-logical
semantics, and other frameworks.

A second stream of research has taken a bottom-up approach,
studying how aspects of intelligence (including consciousness and
language understanding) may emerge from robotics, connectionist
systems, etc., even without an initial, specific design for representations
in such systems. A third, overlapping stream of research has focused on
‘artificial general intelligence’, machine learning approaches toward
achieving fully general artificial intelligence.
Parallel to AI research, researchers in cognitive linguistics have
developed multiple descriptions for the nature of semantics and concept
representation, including image schemas, semantic frames, idealized
cognitive models, conceptual metaphor theory, radial categories, mental
spaces, and conceptual blends. These researchers have studied the need
for embodiment to support natural language understanding, and
developed construction grammars to flexibly represent how natural
language forms are related to meanings.

While people do informally speak of machines thinking, it is widely
understood that computers do not yet really think or learn with the
generality and flexibility of humans. While an average person might
confuse a computer with a human in a typewritten Turing Test lasting
only five minutes, there is no doubt that within five to ten minutes of
dialog using speech recognition and generation (successes of AI
research), it would be clear a computer does not have human-level
intelligence.

Since the challenges are great, and progress has been much slower
than early researchers such as Turing and McCarthy expected, there are
good reasons to reconsider the approaches that have been tried and to
consider whether another, somewhat different approach may be more
viable. In doing so, there are good reasons to reconsider Turing’s and
McCarthy’s original suggestions.

To begin, this thesis will reconsider Turing’s suggestion of the
imitation test for recognizing intelligence. While a Turing Test can
facilitate recognizing human-level AI if it is created, it does not serve as
a good definition of the goal we are trying to achieve, for three reasons:
First, as a behaviorist test it does not ensure the system being tested
actually performs internal processing we would call intelligent. Second,
the Turing Test is subjective: A behavior one observer calls intelligent
may not be called intelligent by another observer, or even by the same
observer at a different time. Third, it conflates human-level intelligence
with human-identical intelligence. These issues are further discussed in
§2.1.1. This thesis will propose an alternative approach, augmenting the
Turing test, which involves inspecting the internal design and operation
of any proposed system, to see if it can in principle support human-level
intelligence. This alternative defines human-level intelligence by
identifying and describing certain capabilities not yet achieved by any
AI system, in particular capabilities this thesis will call higher-level
mentalities, which include natural language understanding, higher-level
forms of learning and reasoning, imagination, and consciousness.

It is not the case that people have been trying and failing to build
baby machines for the past sixty years. Rather, as noted above, most AI
research over the past sixty years has been on lower-level, foundational
problems in a variety of areas such as problem-solving, theoremproving,
game-playing, machine learning, etc. Such research has made
it clear that any attempts to build baby machines with the lower-level
techniques would fail, because of the representational problems Minsky
identifies.

What we may draw from this is that the baby machine approach has
not yet been adequately explored, and that more attention needs to be
given to the architecture and design of a child or baby machine, and in
particular to the representation of thought and knowledge. This
provides motivation for Hypothesis I of this thesis (stated in §1.4 below)
which describes a form of the baby machine approach. This thesis will
discuss an architecture for systems to support this hypothesis, and make
some limited progress in investigation of the baby machine approach.
Chapters 3 and 4 will analyze theoretical topics related to this
architecture, and discuss how the approach of this thesis addresses the
representational issues Minsky identified for baby machines.

Next, this thesis will reconsider approaches toward understanding
natural language, because both Turing and McCarthy indicated the
importance of natural language in relation to intelligence, and because it
is clear this remains a major unsolved problem for human-level AI.
Indeed, this problem is related to Minsky’s representational problems
for baby machines, since the thoughts and knowledge that a humanlevel
AI must be able to represent, and a baby machine must be able to
learn, include thoughts

Software quality measurement encompasses varieties of techniques. As new types of software are emerging 
newer measurement techniques might be essential. Initial attempt was to adopt the well established techniques 
from hardware area. Measurement techniques are mainly based on measurement of defects or faults. In software 
area this is commonly called as bugs. Due to defects or bugs failures may take place. To estimate defects 
failure rates can be studied. 

There are different views about what type of software can be put in the AI category. W. J. Rapaport 
made quite an exhaustive compilation of definitions of AI software [2]. It is surprising to note that 
most of the definitions indicate that, this is new and emerging type of software with not enough quality 
measurement metrics, control and assurance. One such definition is: “AI is a collective name for problems 
which we do not yet know how to solve properly by computer. Once we do know how to solve them, they are no 
longer AI”. If we accept this definition quality measurement is definitely a challenge or might be impossible 
in some cases. However, to focus on a particular type of software we follow the first definition listed in this 
document: “The goal of work in artificial intelligence is to build machines that perform tasks normally requiring 
human intelligence”. We follow the dictionary meaning of intelligence as: “The ability to acquire and apply 
knowledge and skills”. Newborns are usually very intelligent. Though they do not have any knowledge or skill 
they have the capability to acquire and apply these. AI software just after implementation can be compared to 
a newborn’s brain. In the learning phase it may learn face recognition and becomes very knowledgeable. In the 
testing phase it applies its knowledge to recognize a face and take a suitable decision e.g. opening a door to a 
authenticated person. Prior to learning it did not have any knowledge but had the ability to learn and become 
knowledgeable. So it was intelligent.

As a kid I’ve always been fascinated by robots. I loved the idea of robots and the
possibilities that they opened for day-to-day life. I chose to write my paper on artificial
intelligence (AI) because the idea of creating life or life-like things is one that excites me and
many others. Artificial intelligence is a topic that many people quickly disregard as science
fiction and it is in my interests to show just how real AI can be as well as show how a
chatterbot can be made.

In a world that is becoming more and more technology centric, it is only appropriate to
consider the ramifications of creating various forms from that technology. One form that has
long been a dream of scientists and Science Fiction enthusiasts, such as me, is artificial
intelligence. From almost as early as when I first truly started to read for enjoyment, I have loved
stories about robots and computers with the capability to think and feel. One of my favorite
authors has always been Isaac Asimov, who has written numerous stories and books concerning
thinking robots.

In one of his first short stories concerning robots, “Runaround” (Asimov), Isaac Asimov
mentions the ‘Three Laws of Robotics’, which were his first iteration of rules concerning how an
artificial construct would act and react to various stimuli. Asimov uses the three laws to create a
situation in which two of the laws conflict, thus causing all kinds of trouble for the humans
involved. This conflict, in a broad sense, is the same conflict that is of concern even today to
those who desire to create, or stop the creation of, artificial intelligence. Ethics are one of the
major questions because there are so many different ways that an ‘unethical’ construct could
hurt, or even kill, humans. How can ethics be codified so that a construct can accomplish its job,
while keeping it from doing something wrong or ‘unethical?’

In the movie ‘I, Robot’ (Vintar), the premise of which is based on Isaac Asimov’s series
of the same name, the Three Laws of Robotics become a critical factor in the progression of the
storyline. When a murder is committed, the only possible culprit is a robot, but the robot is
suppose to be unable to harm a human. The story unfolds, as the main character, a detective
played by Will Smith, must discover how this could happen against all evidence to the contrary.
In the end, the detective discovers that this one robot does not have the laws installed, and was
ordered to commit murder by the very man who was killed!