Gated Recurrent Unit (GRU) is a recently-developed variation of the
long short-term memory (LSTM) unit, both of which are variants
of recurrent neural network (RNN). Through empirical evidence,
both models have been proven to be effective in a wide variety of
machine learning tasks such as natural language processing[28],
speech recognition[4], and text classification[29]. Conventionally,

like most neural networks, both of the aforementioned RNN vari-
ants employ the Softmax function as its final output layer for its

prediction, and the cross-entropy function for computing its loss.

In this paper, we present an amendment to this norm by introduc-
ing linear support vector machine (SVM) as the replacement for

Softmax in the final output layer of a GRU model. Furthermore,
the cross-entropy function shall be replaced with a margin-based
function. While there have been similar studies[2, 27], this proposal

is primarily intended for binary classification on intrusion detec-
tion using the 2013 network traffic data from the honeypot systems

of Kyoto University. Results show that the GRU-SVM model per-
forms relatively higher than the conventional GRU-Softmax model.

The proposed model reached a training accuracy of ˜81.54% and
a testing accuracy of ˜84.15%, while the latter was able to reach a
training accuracy of ˜63.07% and a testing accuracy of ˜70.75%. In
addition, the juxtaposition of these two final output layers indicate
that the SVM would outperform Softmax in prediction time - a
theoretical implication which was supported by the actual training
and testing time in the study.
Information security is a challenging issue with the high growth rate of internet.
Cryptography, Steganography and Digital watermarking techniques are widely
used for information security for different purposes. Cryptography is used mainly

for secure communication with the existence of encrypted message. Steganogra-
phy is also used for secure communication but existence of the message is hidden.

Digital watermarking is a specific type of steganography in which we hide the in-
formation to claim the ownership of digital media, to control the copy of digital

media and the illegal distribution of multimedia data. Now a day’s more chal-
lenges we are facing in distribution of illegal copy of multimedia data, copyright

infringement and illegal ownership. Many times multimedia data i.e. images, au-
dio, video etc. are illegally circulated and violating intellectual property rights and

hence incurred losses to the owner of data. In order to overcome this problem,

watermarking is the most effective way suggested in literature for copyright pro-
tection, copy protection and ownership assertion. Watermarking is the procedure

by which information pertaining to the owner and/ or copyright i.e. watermark
is embedded into the host data that may be visible or invisible but it must not
be perceptually degraded the host image. The main concern in embedding the
watermark is the not to degrade the host image and it is recoverable from the
cover image by the owner not otherwise even after any image processing attacks.
The amount of information related to watermark which is to be inserted in host
image is also of our concern.

The main intention of the research work presented in the thesis is to design imper-
ceptible, robust and secure digital watermarking schemes using machine learning

algorithms for copyright protection, illegal copy protection and ownership asser-
tion. Digital watermarking schemes are divided into two category based upon do-
main i.e. spatial domain and frequency domain schemes. Our focus is to develop

v

frequency domain schemes with machine learning algorithms to increase impercep-
tibility, robustness beside various image processing operations. In the watermark-
ing applications such as copyright protection, copy protection and ownership as-
sertion, high degree of robustness is the basic requirement. The proposed schemes

in this thesis are based on discrete Cosine transform (DCT), Lifting wavelet trans-
form (LWT), Integer wavelet transform (IWT) and QR decomposition that can be

applied successfully to extract the features of selected image blocks. The blocks
of the cover object to be altered are chosen based on the fuzzy entropy in order to
achieve good visual quality. After extracting the features of each selected image
block, use of machine learning algorithms such as Lagrangian twin support vector
regression (LTSVR) and genetic algorithm (GA) is made to learn the non-linearity

features of the domain. The work presented in this thesis discusses the develop-
ment of watermarking schemes which involve a trade-off between imperceptibility

and robustness as well as to enhance the security of watermark through Arnold
transformation.
The research work carried out in the thesis is divided into two phases. In first

phase, imperceptibility and robustness is achieved by extracting the robust fea-
ture in DCT domain and embedding the watermark in the selected block of the

image with an optimal watermark strength using GA and trained LTSVR.
In the second phase, lifting wavelet transform and integer transform transform
are used to extract the image features and robustness is enhanced by the high
generalization ability of LTSVR. A constant scaling factor is used as a watermark
strength obtained after repeating a number of experiments.
Recurrent neural networks (RNNs) are powerful sequence models. Using their internal
memory they are able to model temporal dependencies of unspecified duration between
the inputs and the desired outputs. This property makes them good candidates for time
series modeling.
The aim of this thesis is to apply RNNs to univariate and multivariate time series.
Initially, we examine if the order of the time series should be taken into account in
the structure of the network for one step ahead time series prediction. Then, we
explore the use of RNNs in the setting of multivariate time series, proposing also a
new way of training, called Multiple training. Finally, we present a new approach
to recover the Granger-causal relations in multivariate time series using RNNs with
Group lasso. Experimental results are presented for generated linear and non-linear time
series, demonstrating the performance of recurrent networks on time series prediction and
Granger-causality structure recovery.
Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on
a variety of pattern-recognition tasks, most notably visual classification problems. Given
that DNNs are now able to classify objects in images with near-human-level performance,
questions naturally arise as to what differences remain between computer and human vision.
A recent study [1] revealed that changing an image (e.g. of a lion) in a way imperceptible
to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling
a lion a library). Here we show a related result: it is easy to produce images that are
completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable
objects with 99.99% confidence (e.g. labeling with certainty that white noise static
is a lion). Specifically, we take convolutional neural networks trained to perform well on
6
either the ImageNet or MNIST datasets and then find images with evolutionary algorithms
or gradient ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs believe
with near certainty are familiar objects, which we call “fooling images” (more generally,
fooling examples). Our results shed light on interesting differences between human vision
and current DNNs, and raise questions about the generality of DNN computer vision.