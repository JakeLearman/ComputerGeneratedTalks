The main objective is the development of a Sales Forecasting Tool (SFT) for an
English company. This main objective is subdivided into three specific objectives
which are:
1. Find relationships in the data in order to create mathematical models that
accurately predict the general sales and sales by category of a range of
product categories.
2. Given the relationships and the models, interpret the results and detail the
business insights and the implications of the knowledge gathered during the
modelling process.
3. Given the models, implement an information tool that can be actively used by
the company in order to optimize the stock and support supplier purchase
decisions.
The main requirements of the company for this task are the ease of implementation
and the interpretability. If the use of the models is shown to be of value for the
company the next step is the integration (and automatization) of the modelling
process to a software package used by the company.
The models have to consider both the internal variables (like the stock levels and the
visits to the website) as well as the external variables (like economic indicators and
weather data) to make forecasts. Given that the sales apparently follow seasonal
trends special emphasis will be made to time variables and their modelling
implications.
3.2 Data Description
The data consists of sales and purchase records from April 2007 to December 2013.
Each record has the date, product details and volume sold. Although the goal is to
forecast the sales, the purchase orders were given in order to estimate stock levels
over time and take them into account when forecasting particular product categories.
For the development of the models only the main product categories or “Main
Categories” are considered, there are 38 Main Categories.

36

Each one of the products in these categories can then be subcategorized in 3 ways
depending on their use, their material and their processing details.
After building an Overall Sales Model, additional models will be built for each one of
the Main Categories and subcategories while their business relevance is high and the
existence of sufficient data records make it practical.
Added to the sales and purchase data, the visits to the company’s website where
provided for the same time period.
For the external variables monthly weather data and monthly economic indicators in
the UK were gathered.
The weather data consists of 4 indicators: Mean Daily Maximum C°, Mean Daily
Minimum C°, Total Rainfall in millimetres (mm) and Total Sunshine duration in hours.
The data was recorded in a Station close to the company (Met Office, 2014).
The economic data consist of 8 indicators including Consumer Confidence, Consumer
Spending, Personal Savings, Retail Sales and other market-specific indicators.
(Trading Economics, 2014)
3.3 Exploratory Analysis and Modelling Approach
For the exploratory analysis the total volume of sales for each one of the main
categories was calculated. This was done in order to identify the categories with the
most sales, and therefore, the most relevant when building the models. A correlation
matrix was built comparing the yearly sales across product categories. (Appendix 1).
Two things were found in this analysis, the first one is that most of the sales are
generated by few of the Main Categories; 11 of the categories with the most sales
represent over 80% of the total sales while 17 categories represent over 90% of the
total sales.
The second finding is that the records are highly correlated (both positively and
negatively) to the Total Sales and across product categories. Given an accurate
model to forecast the total sales (or one significant category) this property can be
leveraged in order to build simpler and more accurate models for the correlated
categories and subcategories.

37

Another relevant finding is that for some specific product categories and
subcategories have few data records. This makes the analysis of such categories
difficult since the model is likely to overfit and show erratic estimates when adding
time-dependent variables or stock data to the analysis.
Taking these findings into consideration, the modelling approach for the SFT is the
following:
• Create an Overall Sales Model:
Given the high correlations across categories, the seasonal pattern of the sales
and its possible dependency to external variables an accurate estimate of the
Overall Sales is required in order to facilitate modelling specific categories.
This model will put light on the intrinsic behaviour of the sales, like the extent
of the dependency to the time of the year, and the optimal time windows to
analyse and the relationship with external variables.
• Create Main Category Models:
Based on the Overall Sales Model and the knowledge extracted from it.
Individual forecasting models for each Main Category will be built.
The degree in which these more specific observations are related to: the
Overall Sales, the stock levels of other product categories and to the external
variables will be analysed.
• Build subcategory-specific sales estimators:
Given the Main Category models, simpler sale estimators will be chosen in
order to forecast even more detailed products subcategories.
The complexity of these estimators will vary depending on the number of
records. Simple estimators can be generated by applying weights to the upper
categories’ forecasts. While more complex estimators may require the
training of independent models.
Because of the less technical approach to the task of this final step it will not
be detailed in this document and will be assessed directly on the
implementation phase.

account. These variables include the month as a categorical variable (January-
December), year, sales of the two previous periods, web visits, weather indicators

and economic indicators.
In order to determine the complexity of the relationships of the sales to the variables.
New features where engineered (transformed) to represent interactions between
the variables and their effect on sales.
For example, an engineered feature captures the specific effect of a rainy March or a
sunny December. Another feature shows to which extent the sales of May have
effect on the sales of June.
In order to get a good estimate of the real performance of the model a meticulous
crossvalidation procedure was performed. A total of 47 variables where evaluated.
A leave-one-out crossvalidation was performed for each one of the possible practical
combinations of features (2

). The lowest error was recorded for each amount of

variables in the model. The results of this procedure are shown on Graph 1.
The models were trained using a Linear Regression that minimizes square errors.

39

As expected, as the number or variables grow the performance on the training data
improves, while the crossvalidation error increases. Based on this analysis it was
found that sales can be explained in a very low dimensional space (none of the
engineered features reduced the crossvalidation error).

THE BEST FIT TO THE DATA WAS ACHIEVED BY DIRECTLY MODELLING THE MONTH,
THE SALES OF THE PREVIOUS PERIOD AND A SPECIFIC SET OF WEATHER AND
ECONOMIC INDICATORS.

3.4.2 Overall Sales Model: Optimal Time Windows
While it is known that the best fit to the data is achieved without using engineered
features. The next step is to find the best time period to forecast.
For this purpose, the 14 variables where used to build 4 models that forecast the
Overall Sales in different time windows. These time periods are the Monthly,
Fortnightly (15 days), 10 days and 1 week periods.
As before, a leave-one-out cross validation was performed for each combination of
features (2

:) for all the 4 models. The forecast of each model was summed to get
the respective monthly forecast; the results of this analysis are shown on Graph 2.

IT WAS FOUND THAT THE OPTIMAL TIME WINDOW TO MAKE FORECASTS IS EVERY 15
DAYS (MONTHLY DATA DIVIDED IN TO HALVES BEFORE AND AFTER THE 15TH DAY OF
EACH MONTH).

The best model for this time period happened to be quite simple. Virtually only four
variables were used: the month (categorical), sales of the previous period, one
general economic indicator and one market-specific economic indicator.

3.4.3 Data Smoothing: Exponential smoothing and Fuzzy logic
Because of the time dependent nature of the sales; the use of data smoothing
techniques were considered in order to improve the robustness of the model.
Based on the 15 days sales data, two smoothing factor where applied to the data:
The first smoothing factor (alpha) was applied to the sales by using an exponential
moving average (EMA). In theory this was done to remove of some of the variance
before training the model.
A second smoothing factor (beta) was applied to the input data of the months.
Instead of representing the month categories as binary variables (either 0 or 1). They
were represented as a continuous variable between 0 and 1 (fuzzy logic).
In this smoothing technique, the first period of the current month’s coefficient plus
the previous month’s coefficient have to add to 1; for the second period the sum of
the current month’s coefficient plus the succeeding month’s coefficient have to also
add to 1.
For example, if the smoothing factor beta is 0.1 and the period to forecast was the
first 15 days of February. The input coefficients would be 0.1 for January, 0.9 for
February, while0 for the rest of the months. This allowed the model to make two
different forecasts during the same month without increasing the number of
variables.
The performance of the model with different values of alpha and beta was evaluated
using a uniform search heuristic (21x21 trials). The results of this are shown on Graph 3.

Graph 3. Error surface plot of smoothing factors

41

THE USE OF A SMOOTHING FACTOR ALPHA (ON SALES) DOES NOT IMPROVE THE
PERFORMANCE OR THE ROBUSTNESS OF THE MODEL; IN CONTRAST, THE SMOOTHING
FACTOR BETA (ON MONTHS) HAS A SIGNIFICANT EFFECT ON THE MODEL.
By adding the smoothing factor beta three things were achieved:
1. Accuracy: the error of the model measured by the MAD is decreased by 3%
without increasing the complexity of the model.
2. Robustness: the forecast presents less spikes and the transition from one
period to the next is smoother since the variance is fitted among two month
coefficients instead of just one.
3. Interpretability: The regression coefficients now can be interpreted directly
for both the 15 days forecasts (by doing a weighted average using beta
depending on the time of the month), and the monthly forecast (by
multiplying the month coefficient times 2).
3.4.4 Main Product Categories Models
The analysis and modelling for the product categories was done with the same
approach.
From the 38 categories only 2 were analysed in this paper. These two categories are
the Category 1 (the category with the most sales and a high positive correlation with
Overall Sales), and the Category 8 (a category with relatively high sales but with very
low correlation with the rest of the categories and with the Overall Sales).
The analysis of these two distinctive categories will shed light on which variables are
the best and to what extent using the Overall Sales as input is of help.

Feature Selection for MCMs
Added to the 4 variables used in the OSM. The Overall Sales Volume was used as
input for both categories’ models. Now with 5 variables, the different possible
combinations of features (2
;
) were crossvalidated in order to find the best set of

variables to build the MCMs. The results of this are shown on Table 1.

42

The Overall sales are the most relevant variable to predict sales of specific product
categories. This holds true even when the category did not show a strong correlation
with the overall sales to begin with.
THIS INDICATES THAT SALES, AT ALL LEVELS, ARE SIGNIFICANTLY TIME-DEPENDENT
AND ARE SUBJECT TO THE GENERAL MARKET POSITION OF THE COMPANY. USING
ADDITIONAL VARIABLES TO BUILD THE FORECASTS IS OF LITTLE HELP.
After the Overall sales, the best predictor for the MCMs is the month as a “fuzzy”
categorical variable (applying the Beta smoothing factor explained in the previous
section).
For the implementation of the MCMs, it is best to have a mixed approach; three very
similar models are suggested to be implemented at once:
1. MCM to Total: The first forecasting model is based only on the output of the
OSM and a constant variable. This model is presumably the most accurate,
and represents the sales of the category as a direct proportion of the Overall
sales.
2. MCM to Month: This model is based only on the month as a fuzzy categorical
variable. The purpose of this model is to capture the trend of each product
category for interpretation purposes.
3. Mixed MCM: This model uses both the Month variable and the OSM output.
Its purpose is to interpret to which extent a specific product category is more
reliant than to its own seasonality or to the Overall sales’.
The final forecast will be a weighted average of the three models putting more
weight on the MCM to Total model. The complete analysis is on the Appendix 2.
Table 1. MCMs Feature comparison (Category 1)

43
3.4.5 Stock levels relevance analysis
Another analysis was to discover the relationships between the stock levels of
product categories and the sales of a specific category. This analysis was done
because it was suspected that when the stock level of a specific product was low, it
pushed up the sales of other products.
For this analysis the Category 1 and Category 8’s sales were forecasted using the
Overall Sales and a constant as input. Added to this, the 37 stock levels of the
remaining categories were calculated and used as input.
The stocks were Forward-selected (added to the model one at the time) and the
crossvalidated performance was recorded in each case. An example of the output of
this analysis is shown on Graph 4.

CONSIDERING SOME STOCKS IMPROVED THE PERFORMANCE OF THE MODEL BUT SHOWED
LITTLE STATISTICAL SIGNIFICANCE. THE MINIMAL INCREASE IN ACCURACY AND A FURTHER
ANALYSIS CONCLUDED THAT SUCH STOCKS ARE NOT RELEVANT FOR THE FINAL MODEL.
A further analysis was made on the models that considered the stock level of
another product. In the three analysed cases, the sales of the selected product
where very low (less than 1% of total sales) and had a discontinuous pattern.
Arguably, such models found fit to the data in the very specific cases of a restocking
(products in the same lot were received and sold together in the same time period)
causing a “false fit” to the data.
Modelling using stock levels would bring high risks of overfitting (variance) and the
forecast would be subject to Inventory Management Interventions.
Chapter 4: Analysis and Results
4.1 Business and Management Implications
Model Complexity
The best fit to the sales data was found using only the linear relationships between
few variables. In addition to this, the modelling method (Linear Regression) is widely
used and has already been integrated in most of the commercial spreadsheet
applications.
Therefore, the interpretation and model update tasks can be performed by end user
without much trouble.

Robustness and self-adjusted forecasts
The final model is robust; the same modelling principles are applied for the Overall
Sales Model and the Main Category Models. The only difference between modelling
broader and narrower categories are the variables that are used for making the
forecast.
This robustness is leveraged in the spreadsheet implementation with the ability to
forecast different products and categories with very few configuration changes.
It was found that some of the models rely on the previous period’s sales; this makes
the forecast sensitive to sudden changes in demand.
If the demand of a specific product changes (because of consumer preference or
marketing efforts of the company), the forecast for the consecutive months will
adapt by reflecting the increase or decrease of demand using a fixed ratio found in
the regression.
Economy driven growth
The company’s general increase and decrease of sales is not correlated to the years
of operation but to economic indicators. This means that the growth of the company
is more dependent on the state of the market rather than on the company’s efforts
and strategy.


Main Findings
• The sales are highly dependent on the period of the year (measured as months)
• The best time windows to make forecasts are 15 days periods.
• For some specific product categories there is not enough data to build a reliable
model
• The growth of the company is more correlated to economic indicators than to the
years of operations
• Increasing the number of variables or the complexity of the model does not
increase the real accuracy (crossvalidation performance) of the model
• There is a weak correlation between sales and weather (rain and sunshine)
• Adding the stock levels to the model does not improve its performance
• The Overall Sales are a good predictor for forecasting narrower product
categories
4.2 Model Evaluation
The Overfitting problem
The sales data is prone for overfitting, adding variables, transformations or more
complex methods to the analysis may seem to increase the accuracy of the forecast,
but this would also increase the variance of the estimates which may result in very
volatile forecasts when trying to predict unseen data.
If in the future there is the need to shift from the models proposed here, special
emphasis has to be made on the crossvalidation procedure in order to guarantee a
real improvement over the proposed models.
The Multicollinearity problem
During the modelling process, Multicollinearity was found in the data, this means
that some features where highly correlated to each other. For example, the month
and rain variables where highly correlated and both where good predictors of sales.
In this situation the coefficient estimates of the regression may change erratically in
response to small changes in the model or the data.

47

Further improvements
• Use of Generalization methods:
Since one of the main issues is the overfitting caused by the lack of sufficient data
points, adding Generalization methods (like regularizers) may help extract more
information from the data even if the data sets are small.
• Alternative approaches:
In this paper only, the use of Linear and Polynomial Regressions was explored;
the use of alternative (simple) data mining method for regression may be
effective for this specific task. The suggested next approach is the use of decision
trees.
• Question-specific models:
Because of the Multicolinearity problem, some variables were not considered for
the final model. However, these unused variables still have predictive power and
its interpretation may be relevant for the business.
The use of Question-specific models is suggested (using Linear Regression or
Partial Regression) in order to analyse the specific effects of these variables.
4.3 Implementation
4.3.1 Forecasting Process
For the forecasting process a spreadsheet model was developed. The model is
divided in three parts: the modelling part, the Analysis of Variance Report and the
Stockout Analysis Report.
When it is required to build a model for a new product or update the coefficients of
a current model the spreadsheet is given records with the sales data and the
external indicators.
The model automatically builds regression equations based on the input. For
comparison purposes the model outputs three regression equations: a regression
using only the month as input, a regression using both the month and the external
variables as input and the third one is a partial regression that uses the external
variables to explain the residuals of the month model.
Each model is presented along with the standard error of its coefficients (range) and
their overall performance.
4.3.2 Analysis of Variance Report
To complement the decision making the spreadsheet model also contains and
Analysis of Variance (ANOVA) Report. In this report the significance of each variable
is shown.
This report also contains a graph of the Fit Plot of the predictions made by the model
against the actual sales. The average error per month was calculated in order to
estimate the range of the prediction for each month (the error is heteroscedastic).
A screenshot of the model can be seen on the Appendix 4.
4.3.3 Stockout Risk Analysis
The stockout analysis section is used for the final decision on how much to order.
Based on the given input the model outputs the respective expected error of the
forecast and uses it to build a Probability Density Function (PDF) of the future sales.
The PDF is then used to estimate the best order size based on a Confidence indicator
and the Probability of a stockout. A screenshot of this can be seen on the Appendix 5.


Application: Neural Networks for Regression
Problem Description
A dataset containing different features of houses and their respective prices in the
city of Guadalajara in México was gathered. The data consists of 113 observations
and 9 features to be used in the model.
The task of this model is the building of a Housing Price Predictor (HPP) that will put
light on the features that have the most value in the market.
The requirements of this model are interpretability and accuracy over model
simplicity. Given only a few observations and the prejudicial consequences of
overestimating or underestimating the prices. The model has to be able to perform
well when predicting unseen data (must be well generalized).
Modelling
For this task some of the features where discretized. For example, instead of
representing the number of bathrooms as a single continuous variable. It was
represented with two or more binary variables, which allowed the model to capture
the special effect of having additional bathrooms.
An Artificial Neural Network with One hidden layer was used to fit the data, three
crossvalidation procedures where used. For the three procedures a crossvalidation
dataset with 25% or the records was chosen using the hold-out method.
The ANN’s hidden layer is activated using the sigmoid function and the final forecast
is left unaltered (no activations are applied to the output layer).
The first crossvalidation method was used to choose how many nodes (neurons) to
use in the hidden layer. The more nodes the more parameters the model uses.
The second method was the truncation of the input matrix X so the propagated
values would not overestimate in the case of an anomalous input record (all the data
was normalized before these procedures).
The last method was the truncation of the errors to be backpropagated at every
iteration of the gradient descent. This was done in order to prevent the algorithm to
try to forecast anomalous prices in the training set (overfitting).

52

The results of these procedures are shown on Figure 7.

The final model was picked training an ANN with 2 nodes, an Input truncation of 1.5
and an Error backpropagation truncation of 3. Even though the accuracy of the
model could be increased using more nodes. The improvement in performance was
not considered significant given the increase in complexity.
The coefficients of the final model were found by running the gradient descent
algorithm multiple times with different initializations in order to prevent getting
stuck with low quality local optimums.
Analysis and Findings
A multiple linear regression was used on the training data in order to get a
benchmark to compare the performance of the ANN. The results of the two models
are the following:

Looking at the coefficients it seems that Node2 captured the main relationships in the
data. Node2’s coefficients share the same signs with the linear regression’s coefficients;
this node is likely to output values close to 1 (constantly activated) and given its
outgoing weight it is the most relevant node when build the final prediction.
Node1’s coefficients have the opposite signs and the output is likely to be close to 0
(constantly inactivated). The low values outputted by this node apparently correct the
overestimations made by Node2.
In order to analyse the model even further, 3 values were calculated. These are the
“Unbiased node coefficients” (Incoming weight * |Outgoing weight|), the node
“Confliction” (1- |Node1 Coefficient/Node2 Coefficient|) and “Predictive Power” (MAX
[|Node1 Coefficient|, |Node2 Coefficient|]). The table is on Appendix 3.
With these indicators, we can get a better understanding of the data, for example, the
Unit size is the most relevant variable when trying to predict the prices of the houses
(highest predictive power).While it seems that the variable Months on sale can be
dropped out from the analysis (lowest predictive power).
Using the Confliction indicator, we can get a higher level of knowledge from the data.
For example, looking at the confliction (90%) and predictive power (0.27) we can
interpret that having an additional bathroom increases the value of the property, but
not always. Similar interpretations can be drawn from the Floors (2) and Bathrooms (3)
variables.
When the confliction is close to 1, the activation of a node is not followed by the
deactivation of parallel nods. This means that the contribution of the variable to the
final prediction is subject to the activations caused by the remaining variables.
In contrast, when confliction is close to 0 the activation of a node for that variable is
followed by the deactivation of parallel nodes. This makes the contribution to the
prediction more likely to stay unaltered.
When predicting the prices of specific houses, the levels of the activations through the
network can be tracked in order to see which variable contributed the most to the value
of a specific house.
Even though the ANN has over twice the number of parameters compared to a linear
regression (21 versus 9). A higher level of abstraction was achieved while maintaining
better accuracy and robustness on unseen data.

Application: Neural Boosting for Model Augmentation
Problem Description
For the case study described before; given that the accuracy of the Overall Sales
forecast is of high importance. In order to make better forecasts across product
categories. A further analysis was made in order to improve on the proposed
multiple linear regression models.
For this, the variables used to build the Overall Sales Model plus the Rain variable
were analysed along with the proposed model to build a “stronger” predictor.
The new model has to maintain interpretability while increasing the accuracy and
the knowledge discovery from the data. For this task model simplicity was not
relevant.
Modelling
To solve this problem, a Model Boosting approach was used. Given the errors of a
former predictive model, a new model was trained in order to “correct” or
“compensate” for the mistakes made by the former model.
For this, an ANN with a slightly different architecture was used. The input layer remains
unaltered while the first (and only) hidden layer is extended by adding the predictions
made by the former model; this prediction is passed unaltered (weight of 1) along with
the activations of the nodes in the hidden layer to generate the final prediction.

NN Coefficients
Using the prediction of the Linear Regression or “LR Prediction” as direct input for
the final forecast exhausters the predictive power of the linear relationships and
forces the neural network to find non-linear dynamics between the variables in order
to increase the accuracy of the final forecast.
Since the former model’s coefficients are not changed, their interpretability and
output can still be used. Neural Boosting provides additional insight from the data
when following the activations of the nodes without necessarily using a different
model in the implementation.
Analysis and Findings
The final model was trained using 2 nodes in the hidden layer and truncating the
input variables (error truncation did not improve the performance on unseen data).
In order to interpret the coefficients of the neural networks, the
predictive power of each variable was tracked back. Through this,
it was found that the month of June (the month with the most
sales) still has predictive power that was not captured by the
linear regression (sales during June may be higher than estimated
by the former model).
For the rest of the variables it seems that the second economic
indicator and the sales in the previous period still have predictive
power when interacting with other variables using this more
complex modelling approach.
As mentioned before, it seems that building question-specific
models as partial regressions or even building models for every
month of the year may improve the accuracy of the forecast.
The final model performs better on unseen data than a linear regression. Using this
approach the accuracy of the model increased very slightly. The results suggests this
approach should only be used as insight for further (simpler) improvements.

Between 2003 and 2008, I contributed to a research project, financed by Belgian universities,
which aimed to help small and medium Ecuadorian software-development companies to
improve their processes and to become aware of the new trends in the software engineering
field. Within this R&D project, an inventory of companies was built to determine their size,
the target market, the process they used to develop software, their beliefs and desire of
obtaining certifications (ISO, CMMI), and so on. In addition, two workshops were organized
with participants from industry and academia to develop strategies for reaching the project
research objectives.
This led to changes in the curriculum of the software engineering courses taught at ESPOL
University (Ecuador) at the undergraduate and graduate level, introducing subjects such as
SPI (Software process improvement) and measurement techniques. Moreover, over that
period, annual Software Engineering Conferences were held in Ecuador with the sponsorship
of the Belgian universities and the IEEE. In 2008, Dr. Pierre Bourque visited ESPOL -as one
of the invited speakers and talked about software measurement, including the COSMIC
method and the ISBSG repository.
Several months later, officials from the Ecuadorian government who had attended the
conference indicated to ESPOL’s faculty their concerns about the lack of software
development companies in Ecuador measuring the size of their products. The latter was
intended to be a government requisite in order to make a proper estimation of prices of
software applications. This governmental concern along with the desire of software
companies to obtain CMMi certifications were one important motivation for the teaching of
software measurement at the undergraduate level.

24

2.2 Research goal
The long term goal of this research work is to facilitate the implementation of measurement
programs in software development organizations. We believe that the first step to reach this
goal is the enhancement of education in software measurement at the undergraduate level, the
undergraduates being the majority of the workforce in the software industry
2.3 Research objectives
To address the research goal, the following research objectives have been selected:
1) To determine the state of the art of software measurement in higher education.
2) To identify the priorities of software measurement in higher education at the
undergraduate level.
3) To develop a framework for education in software measurement at software engineering
undergraduate programs based on:
• the related bodies of knowledge;
• the results of the surveys and the Delphi study (see chapter 3);
• the Bloom's taxonomy regarding the levels of learning (revised version (Anderson et
al., 2001)); and
• the constructivist approach.
2.4 Originality of the proposed research
As in any engineering discipline, measurement should be playing an important role in the
software field. The connection between measurement and software process improvement has
been emphasized, not only in the existing or upcoming standards, models and bodies of
knowledge (Abran, Bourque and Dupuis, 2004; Bourque et al., 2008; IEEE ACM, 2004;
Integrated Software & Systems Engineering Curriculum (iSSEc) Project, 2009b; Trienekens
et al., 2007; Weber and Layman, 2002), but also in several studies conducted within the
industrial software sector (Gopal et al., 2002; Iversen and Ngwenyama, 2006; Rainer and
Hall, 2003; Staples and Niazi, 2008). Moreover, the curriculum guidelines for undergraduate

25

and graduate software engineering programs include knowledge areas in which measurement
topics are explicitly considered (IEEE ACM, 2004; Integrated Software & Systems
Engineering Curriculum (iSSEc) Project, 2009b).
Despite the extensive research work in software measurement and process improvement,
little has been done in academic settings to tackle the lack of guidelines to address the
teaching of software measurement in undergraduate programs. The reported efforts in
addressing the education of software measurement are mostly focused in using specific
methods, techniques or learning objects for gathering data from experimental studies
conducted with students. Indeed, the shortcoming in providing proper education of software
measurement in universities has been evidenced in a number of publications (Gresse von
Wangenheim, Thiry and Kochanski, 2009; Jones, 2008; Zuse, 1998). Therefore, there is a
need to consolidate the existing software measurement knowledge so that university students
can learn and reach the expected levels of learning.
The present doctoral research work contributes to fill this gap by researching, designing and
providing an educational framework to facilitate the teaching and learning process of
software measurement topics considered as priorities in higher education at the
undergraduate level.
By following the proposed framework, students (learners) are expected to become familiar
with key software measurement topics as well as to be aware and able to support software
measurement programs as part of software process improvement initiatives within
organizations hiring them.
2.5 Target audiences of this research
The audiences targeted in this research are the following:
1) University teachers or instructors:
• who want to include software measurement topics in their courses.

26

• who want to improve their current courses in which software measurement topics are
included.
• who are interested in knowing how to apply active learning approaches in their
classrooms and promoting meaningful learning among students.
2) Researchers looking for educational proposals that can be applied in higher education.
3) Software engineering practitioners or university students who want to learn software
measurement by themselves and are looking for related theory and examples.
4) Software measurement practitioners, consultants or organizations looking for new
examples of software measurement or new ideas for training.
5) Bodies of knowledge searching new publications and research works in their field of
knowledge.
2.6 Overview of the research methodology
The methodology proposed to achieve the objectives of this doctoral thesis is divided into
four phases:
• Phase 1: Literature review and design of surveys related to software measurement
educational issues.
• Phase 2: Identification of priorities of software measurement for undergraduates.
• Phase 3: Design of an educational framework.
• Phase 4: Evaluation of the educational framework.

Figure 2.1 presents an overview of the research methodology, in which the inputs, phases,
outputs and outcomes are visible. The corresponding chapters (sections, sub-sections) and
appendices containing the details about the methodology appear in parenthesis.
The phase 1 includes the identification of software measurement topics taught at universities
as well as the level of learning reached by students. The findings on this phase are based on

27

the reported studies published since the year 2000 (section 3.1) and on a new web survey
designed and conducted among university teachers worldwide (section 3.2.1).
The phase 2 includes three activities to identify priorities in the education of software
measurement: a web survey with practitioners (section 3.2.2), interviews with experienced
software measurement teachers (section 3.4); and a Delphi study to reach consensus about
the priorities (section 3.3).
The phase 3 includes: the definition of the educational framework; the design of its structure;
and the filling-up of the framework. This latter consists of examples to show the applicability
of the framework (chapter 4 and Appendix XV). The structure of the framework presents the
connection of the required components to reach the expected levels of learning associated to
the topics considered in the framework.
The phase 4 presents the evaluation of the framework performed by university students and
teachers. The former were used to identify flaws in the understandability of examples and
tasks assigned to students (sections 5.3.1 and 5.4.1); whereas the latter were important to
determine the level of adoption of the proposed framework among teachers. The evaluation
with teachers was done through a model adapted from (Gopal et al., 2002) -sections 5.2,
5.3.2 and 5.4.2).

28

Figure 2.1 Overview of the research methodology

CHAPTER 3

STATE OF THE ART OF SOFTWARE MEASUREMENT IN HIGHER
EDUCATION AND IDENTIFICATION OF PRIORITIES

This chapter presents the results obtained in the phases 1 and 2 of this research work: the
literature survey and the research step taken for the identification of priorities of software
measurement for undergraduates. The findings of these results will be used for the
development of the educational framework explained in chapter 4.
3.1 Literature survey
3.1.1 The objective of the literature survey

The objective of this literature survey was to gain insights into how software measurement is
taught at universities. To accomplish this objective, publications related to software
measurement in academia were reviewed.

3.1.2 The selection of papers

The detailed information related to this literature survey (related work, methodology and
results) is available in our first article “Software Measurement in Software Engineering
Education: A Comparative Analysis” presented in Stuttgart, Germany in November 2010
(see Appendix XVIII) (Villavicencio and Abran, 2010).
This section briefly describes how we searched publications related to software measurement
in an academic environment. This survey looked for studies conducted in universities where
students – undergraduate or graduate - were performing measurement activities as part of
their assignments in software measurement related courses (software engineering, software
measurement, software quality, software project management, etc).

30

The databases used for this purpose were Compendex and Inspec. The searching criteria
employed for executing the queries included a number of keywords, as follows:

Keywords:
• Software engineering;
• Measurement OR metrics;
• Education OR teaching;
• Experiment OR empirical;
• CMM OR PSP OR TSP OR GQM OR GQMI OR functional size OR COCOMO OR
function points OR COSMIC OR McCabe OR estimation OR quality control.
• Undergraduate OR Graduate OR students
• Year of publication: from 2000 to 2010
In mid-2010, when this study was conducted, 18 articles met the search criteria. A complete
description of the methodology used for this study can be found in section 3 of our article
(see Appendix XVIII).

3.1.3 Results

The results published in (Villavicencio and Abran, 2010) can be summarized as follows:
• The majority of articles reporting experiments where students performed measurement
activities (67%) were found in conference proceedings. From them, (28%) were
published in the Software Engineering Education and Training Conferences – SEET.
• These experimental studies used mostly undergraduate (67%) rather than graduate
students (33%).
• The experiments were mostly performed with students enrolled in computer sciences
programs: Undergraduate level (67%) and graduate level (100%).
• For the experiments, students mostly worked with toy projects (undergraduate 75%,
graduate 67%).

31

• The students participating in the studies were mostly enrolled in their last year of studies
(undergraduate 67%, graduate 100%); and were taking a mandatory course related to
software measurement (67% both undergraduates and graduates).
• In those experiments, students generally worked in teams (undergraduate 42%, graduate
67%).
Only few articles mentioned the learning objectives related to software measurement
(undergraduate 17%, graduate 33%).
• The topics most commonly covered according to the publications reviewed were:
• Measurement techniques, mainly Goal Question Metric (GQM) and Personal
Software Process (PSP) (undergraduate 50%, graduate 67%).
• Measures by life cycle phase, especially effort, size and defects (undergraduate 42%,
graduate 33%).
• The teaching approach most commonly used was lectures (50% both - undergraduates
and graduates).
• Among the approaches used for assessing students’ learning, written exams were
explicitly referred (undergraduate 25%, graduate 17%).
• The level of learning expected to be accomplished by students was rarely specified (no
more than 25% in both cases: undergraduates and graduates).
More details about the results are available in section 4 of Appendix XVIII. After the

publication of our article, in 2011 and 2012, two new articles were identified (Cuadrado-
Gallego et al., 2012; Cuadrado-Gallego, Borja Martin and Rodriguez Soria, 2011).

3.2 Web survey
From January to May of 2011, two web surveys were designed and conducted - one for
teachers and the other for practitioners.
The survey administered to teachers was designed to build the state of the art of software
measurement education from primary sources - the university teachers. On the other hand,

32

the survey administered to practitioners was intended to know issues related to software
process improvement (SPI) initiatives and measurement programs in software organizations,
and to get preliminary insights of the software measurement topics that should be the focus
of university programs, according to practitioners.
The list of potential respondents was obtained from several sources, including: software
measurement associations (i.e. GUFPI-ISMA); software measurement conferences
organizers (i.e. UKSMA, IWSM-MENSURA); software engineering research group mailing
lists (i.e. Competisoft), and digital libraries (i.e. engineering village and IEEE Xplore). A
total of 159 respondents worldwide (107 teachers and 52 practitioners) answered the
questionnaires.
Detailed information related to both surveys is included in Appendix XXII, which contains
the article "Software Measurement in Higher Education" submitted to the International
Journal of Software Engineering and Knowledge Engineering.
3.2.1 Web survey for teachers

3.2.1.1 Objectives of this survey

This survey aimed to identify:
• what software measurement topics are being taught in universities programs and their
expected levels of learning.
• what practices are currently used at universities for teaching software measurement.

3.2.1.2 Methodology

The methodology for both surveys is explained in sections 3.1 to 3.5 of appendix XXII. The
methodology mainly considers the design of the instrument (questionnaire) and the selection
of the sample.

33

To design the instrument, we:
• created a list of software measurement topics based on the ACM and IEEE software
engineering curriculum guidelines for undergraduate programs (IEEE ACM, 2004) and
the Software Measurement Body of Knowledge (Abran, April and Buglione, 2010;
Bourque et al., 2008).
• used the revised version of Bloom’s taxonomy (Anderson et al., 2001) to propose a range
of learning outcomes that are in relation with the six levels of learning of the taxonomy
(remember, understand, apply, analyze, evaluate, and create).
• used concepts related to teaching and assessment approaches to write the questions.
To select the sample, we looked for university teachers who were teaching software
engineering, software measurement, or any course in which software measurement topics are
taught. The name and email of teachers were obtained from the sources mentioned in the

section 3.2. From these, a total of 107 teachers - representing universities from 27 countries-
answered the survey.

3.2.1.3 Results

The results from this sample are described in the Appendix XXII - section 4.1.
A brief summary of those results is presented next:
• The most referred courses in which software measurement topics are covered are:
software engineering (48.6%), software quality (11.1%), software measurement (9.7%),
and software project management (9%).
• The majority of the measurement-related courses (52.8%) is mandatory and taught to
undergraduates.
• The measurement-related courses are generally offered during the third or fourth year of
study for undergraduates (65.3%) and during the first year for graduates (77.6%).

34

• The software measurement topics most commonly covered at universities are: 1) basic
concepts; 2) measures related to quality; 3) techniques and tools; and 4) the measurement
process.
• In the Software Engineering courses, which represent the majority of courses where
software measurement is taught, the main focus related to measurement relies on basic
concepts and techniques and tools regardless the level in which they are taught
(undergraduate or graduate).
• The first three levels of learning of the Bloom's taxonomy (remember, understand, apply)
are expected to be achieved by all students, regardless of the educational level of the
program (undergraduate or graduate).
• Graduate students are expected to achieve higher levels of learning (analyze, evaluate,
and create) than undergraduates.
• Around 60% of teachers combine theory and exercises in class.
• Students work in groups to measure size, effort, and defects. They commonly measure
toy software projects (64.5%).
• The instructional approaches most commonly used for teaching software measurement
are: lectures (88.2%), case studies (64.6%) and collaborative activities (59.2%).
• The level of learning of students is commonly assessed through written exams (69.7%),
term projects (67.1%) and presentations in class of assignments/projects (63.2%).
3.2.2 Web survey for practitioners

3.2.2.1 The objective of this survey

With this survey, we wanted to determine:
• the level of importance perceived by organizations on software measurement.
• how organizations appreciate software measurement knowledge acquired by graduating
students when they become their employees.

35

• what specific software measurement topics should be emphasized in software engineering
education from the practitioners point of view.

3.2.2.2 Methodology

The methodology applied to carry out this survey is similar to the one used for the web
survey with university teachers (see section 3.2.1.2). However, for designing the
questionnaire we took into consideration aspects related to software process improvement
(SPI) initiatives and measurement programs in organizations. To create the questionnaire,
some ideas were taken from articles related to surveys performed in the software industry
such as (Bush and Russell, 1992; Chudnovsky, López and Melitsko, 2001; Salazar et al.,
2004; Trienekens et al., 2007; Yazbek, 2010).
Regarding the sample, this was composed by practitioners working on SPI programs, as well
as software measurement specialists from private or public organizations. A total of 52
practitioners from 18 countries answered the questionnaire.

3.2.2.3 Results

Partial results of this survey were presented and published in the 25th IEEE Canadian
Conference on Electrical and Computer Engineering held in Montreal (see Appendix XXI).
In addition, an extended version of this publication is included in the article "Software
Measurement in Higher Education" submitted to the International Journal of Software
Engineering and Knowledge Engineering (Appendix XXII - section 4.2).
A brief summary of the results is presented next:
• The majority of organizations represented in the sample had a Software Process
Improvement (SPI) program (96%) at the time the survey was conducted.

36

• Among the certified organizations, 85% had ISO 9001 certification and 45% had CMMI.
Certified organizations employ more people with Master’s and Phd. degrees than non
certified ones.
• From the set of organizations that had measurement programs, 89% of them used their
own definitions of software measures, 22% used the ISO 20926 IFPUG 4.1 method, and
11% used the ISO 19761 COSMIC functional size method.
• The software measurement tools used by organizations are spreadsheets (41% - for
registering their measures) and Microsoft Project for planning and tracking (26%).
According to the totality of this sample, there is an agreement in considering three software
measurement topics as essential to be taught in university courses: basic concepts; the
measurement process; and techniques and tools. For the rest of the topics, there were
differences in opinions between respondents depending on the type of organization they work
for (certified, not certified, with or without a measurement program). Respondents from
certified organizations considered that measurement standards should be emphasized in
university courses, while those from non certified organizations preferred software
engineering management measures. In addition, certified organizations and those that had
measurement programs gave greater importance to the topic measures for the requirements
phase than non certified organizations and those without measurement programs.
3.3 The Delphi study to identify priorities
The Delphi method is used to reach consensus among experts regarding an issue that needs to
be investigated or solved. For reaching consensus, several rounds are performed via a
structured communication process (Amos and Pearse, 2008; Bourque et al., 2002; Gatchell,
Linsenmeier and Harris, 2004; Howze and Dalrymple, 2004; Hung, Altschuld and Lee, 2008;
Okoli and Pawlowski, 2004).
Delphi studies are generally used in educational research projects and are helpful for
determining learning goals (Suskie, 2009).

37

Our Delphi study started in summer 2011 (preparation phase) and ended in fall 2012
(verification phase). The methodology and the results of the pilot test of this study are
available in Appendix XIII, which corresponds to the article "Software Measurement in
Software Engineering Education: A Delphi Study to Develop a List of Teaching Topics and
Related Levels of Learning" presented in the 38th Euromicro Conference on Software
Engineering and Advanced Applications - SEAA 2012.

3.3.1 The objective of the Delphi study

The objective of this study was to identify the software measurement topics that should be
emphasized at the undergraduate level in software engineering programs, and the levels of
learning that students should reach according to Bloom’s taxonomy.

3.3.2 Methodology

As previously mentioned, the methodology is covered in Appendix XXIII (section III -
Research Methodology). Figure 3.1 summarizes the steps followed to perform this Delphi
study.

38

Figure 3.1 General view of the Delphi study - adapted from (Okoli and Pawlowski, 2004)

The Delphi study had two panels of software measurement experts: university teachers and
practitioners. The profile of each type of participant is as follows:
Practitioners
• With five or more years of professional experience in software measurement by working
on SPI programs, and/or software measurement programs as a team member or specialist.
• Members of a software measurement committee or a software measurement association
(not mandatory, but preferable).
• With publications in software measurement related to professional and research
experience in the field.
• With post secondary education.
Literature
Revi

University teachers
• With five or more years of teaching experience in specialized software measurement
courses, or related software engineering courses in which software measurement topics
are covered.
• With publications in software measurement related areas.
• Members of a software measurement committee or a software measurement association
(not mandatory, but preferable).
A list of potential participants who met the expert profiles was developed by searching
articles related to software measurement in digital libraries (e.g. IEEE Xplore, Engineering
Village) and by looking for experienced practitioners in LinkedIn specialized software
measurement groups.
Our Delphi study was performed in three rounds:
1) In the first round, the experts had to choose five software measurement topics that they
considered as essential to be taught at the undergraduate level; they also selected the
expected levels of learning per each of the topics; and the set of skills required for
undergraduate students to complement their education in software measurement.
In the second round, the order of importance of the software measurement topics, skills and
levels of learning was determined by the participants.
In the third round, participants had to manifest whether or not they agreed with the ranking of
the priorities defined in round 2 or provide new rankings.
Table 3.1 shows the number of participants who were invited per panel and the number of
participants per round.


After each round, a summary of the results was sent to the participants along with the
invitation to participate in the next round.
After the three rounds, the results were verified by two means: 1) conducting a survey among
people attending software engineering international conferences (practitioners and teachers);
and 2) interviewing recognized experts in the software measurement field.
In the case of the participants of the survey for the verification step after the three Dephi
rounds, they did not have to meet selection criteria to fill out the questionnaire: the
participation in this verification process was completely voluntary. A total of 50 people
answered the verification questionnaire (26 teachers and 24 practitioners).
Regarding the recognized experts, the profile defined was:
• A person with more than 10 years of experience in the field by leading software
measurement communities, associations, or enterprises specialized in software
measurement.
• With relevant publications in software measurement such as books and/or articles in
renowned journals.
By searching software measurement books in Amazon.com, eleven recognized experts were
identified. Four of them were interviewed in a software measurement conference held in Italy
in October 2012.


Software measurement topics
During the first round, the participants of each panel were asked to select among a list of
software measurement topics from Phase 1 the five most important to be taught to
undergraduate students. Each topic included in the list had examples in order to avoid
misunderstandings or confusion among participants.
With the data of each panel, we used the following criteria to select the five most important
topics:

• More than 50% of the experts in both panels (university teachers and practitioners) chose
the topic.
• More than 50% of one expert‘s panel chose the topic (university teachers or
practitioners).
• The topic did not reach the 50% acceptance level but it was still rated among the 5 most
important topics in both panels.

42

Both panels agreed on four topics. However, each panel selected a different fifth topic, given
a total of six topics that met any of the above criteria, as follows:
1) Basic concepts in software measurement (both panels)
2) The measurement process (both panels)
3) Techniques and tools for software measurement (both panels)
4) Software management measures (both panels)
5) Measures for the requirements phase (practitioner's panels)
6) Measures for the design phase (teachers' panel)

43

In round 2 of the teacher's panel, the order of the first four positions of the rankings was
identified. Notwithstanding, positions 5 and 6 were not clear. A third round was required to
confirm the four first positions and to identify positions 5 and 6.
In round 2 of the practitioners' panel, the positions 1 and 4 to 6 were identified but not the
positions 2 and 3. The round 3 was needed to identify those positions with low level of
agreement.
Table 3.2 presents the final rankings of the software measurement topics obtained in round 3.
More information about these results, including the explanations provided by participants, is
available in Appendix XXIX.

In the first round, the participants had to select from a list of levels of learning, the ones that -
according to them - should be reached by undergraduate students in the five most important
software measurement topics.
A web application was developed in such a way that once the participants had selected the
five most important topics, their corresponding levels of learning appeared in the screen to be
selected. Every topic had between 4 to 6 levels of learning related with the levels of the
Bloom's taxonomy. This means that every expected level of learning per topic had a
corresponding level of the Bloom's taxonomy. The participants did not know the relationship
of the levels of learning shown in the screen with the Bloom's taxonomy. In the analysis, we
did use the relationship to match the levels of learning selected by participants with the
Bloom's taxonomy.
Once the participants had selected the levels of learning per topic (the five most important),
the following criteria were used for identifying the most preferred:
• More than 50% of the experts in both panels chose the level of learning.
• More than 50% of one expert‘s panel chose the level of learning.
• The level of learning did not reach more than 50% of acceptance, however it was
considered as the most important in both panels.
For the second round, we asked again the participants to choose the levels of learning per
topic. In this round we got a clear consensus about the preference of the participants (strong,
very strong, extremely strong), so we did not include this question in round 3.
The criteria used to categorize the preferences of the participants regarding the levels of
learning were

Levels of Learning per software measurement topic

47

In rounds 2 and 3, the participants were asked to rank the skills. The criteria that we used to
determine the ranking were the same used for the topics.
In round 2 of the teacher's panel, only the positions 1 and 2 of the ranking were identified.
So, we needed the third round to identify the remaining positions.
In round 2 of the practitioners' panel, the positions 3 and 4 were identified but not the first
two of the ranking. The third round was also needed.
The table 3.4 presents the final rankings of the skills obtained in round 3. Appendix XXIX
contains complementary information.

Table 3.4 Ranking of skills needed to complement education in software measurement

Survey among people attending software engineering international conferences
The main objective of the survey was to verify the results of the Delphi study. To accomplish
this objective this survey was conducted among teachers and practitioners.
The survey was promoted among the attendees of the following conferences:

48

• 38th Euromicro Conference on Software Engineering and Advanced Applications.
September 5-8, 2012. Cesme, Izmir, Turkey.
• The Joint Conference of the 22nd International Workshop on Software Measurement
(IWSM) and the 7th International Conference on Software Process and Product
Measurement (Mensura), October 17-19 2012, Assisi, Italy.
• The IX Ibero American Conference on Software Engineering and Knowledge
Engineering, November 28-30, 2012, Lima, Peru.
Fifty people from 18 countries showed interested in our research work and voluntarily
answered the questionnaire: 26 teachers and 24 practitioners.
For the survey, similar questionnaires were used with both types of participants. The
questionnaires for teachers and practitioners had a general information section and three
sections to verify the results of the Delphi study (software measurement topics, levels of
learning; and skills to complement the education in software measurement) - See Appendix
XIII. The questionnaire for teachers included an additional section to gather information
about educational aspects related to constructivism and active learning. This additional
section and the information collected during the interviews with teachers provided relevant
inputs to develop the framework presented in chapter 4.
For the three sections used for the verification of the Delphi study, the respondents had to
indicate their level of agreement with the results obtained by using a five points Likert scale
from strongly agree to strongly disagree. The results presented here and the following pages
correspond to the counts and percentages obtained for the alternative strongly agree.
As it can be noticed from table 3.5, the topics techniques and tools and measures for the
requirement phase obtained less percentages of strongly agree among practitioners.
However, in the case of techniques and tools, the majority of practitioners chose somewhat
agree (45.8%); giving an agreement of 75% (strongly agree + somewhat agree) in the
ranking of this topic. In the case of measures for the requirement phase, 33.3% of

49

practitioners chose somewhat agree (i.e. strongly agree + somewhat agree = 75%). The
reasons given by practitioners to justify their selection are listed below:
• In real projects, the requirements phase has the strongest impact in project success. IT
students should be more convinced to study this topic than techniques and tools.
• You first need to know what the specific "measures" are prior to selecting tools.
Measures tell you "what" while tools show you "how".
• Measures for design (#6) are specific; they are suitable for master degree. There are
several methods for the measure #5, explaining all of the function point counting methods
would last too long. It would be suitable in master degree not for undergraduates.
• The position #5 (measures for the requirements phase) should be #1 because all the
problems start with poor requirements.
• The position #5 should be #3 because it is better to know these concepts before
introducing the #4 (software management measures).
• The measures in the requirements phase are more relevant than management measures.
• The ranking reflects a theoretical approach in software measurement. In my experience, it
is better to give an objective first with "why we need measures" by following a pragmatic
approach. So I would rather start with examples of measures and why we need them and
how they can be used and then the final stage I would go for the theory and fundamentals
of measurement. My ranking would be 4, 5, 6, 3, 1, 2.
Table 3.6 shows the results of the preference of levels of learning chosen by teachers and
practitioners. For simplification purposes, this table only includes the levels of learning that
reached 50% or higher percentages in the category strongly agree.

50

Table 3.5 Verification of the ranking of the software measurement topics

Ranking Topics

Strongly agree
Teachers
(N1=26)

critical thinking and team work can be easily improved. So, I would work on the "low-
hanging fruit" first.

51
e

The second question of this section aimed at identifying the resources that teachers consider
as the most valuable to facilitate the implementation of active learning in their courses that
include software measurement topics. Table 3.9 shows that three resources are considered
very valuable for teachers: examples of software measurement; guidelines for applying active
learning; and suggested activities to promote active learning in their courses.

53

Table 3.9 Resources for teaching software measurement

Valuable resources for teaching software


Finally, the third question asked teachers about the perceived impediments or barriers for
adopting an active learning approach in their courses that include software measurement
topics. Fifty percent of teachers consider that there are no impediments. The other fifty
percent believes that the limited time they have to cover the course content, along with their
own time constraints for preparing suitable activities as well as the lack of support and
resources from universities are impediments (see table 3.10).
Interviews with recognized experts
Four software-measurement book authors were interviewed in October 2012. During the
interview - 40 minutes in average, they were asked about their opinions with regard to the
results obtained in the Delphi study. They had the freedom to make comments and give
reasons that supported their agreement or disagreement with the Delphi's results.
To meet the ethics policies at ETS, an information and consent form was signed by the
interviewees to assure them the confidentiality of the data collected (written notes and audio
recordings).

54

Table 3.10 Impediments for adopting an active learning approach for teaching software

measuremen.

55

Interviews with recognized experts: Ranking of software measurement topics
The software measurement topics kept the ranking order equal to the one obtained in the
Delphi study (Basic Concepts 100% - i.e. the four experts agreed, Measurement process 75%
- 3 experts agreed, Techniques and tools 100%, Software management measures 50%,
measures for the requirements phase 75%, measures for the design phase 75%). One expert
mentioned that software management measures should be taught after the specific measures
(requirements and design measures). He commented that the current ranking of topics is
logically ordered for trainees who work in organizations that already have historical data for
performing estimations. He said that this is not the case of undergraduate students because
they are in the process of learning software measurement, so they should learn first the
measures for the requirements phase. Another expert ranked the topic software management
measures as second since he considers that students need to know first why we need specific
measures before performing estimations. Finally, another expert said that he agreed with the
topics selected as priorities except for the measures for the design phase because he thinks it
should not be a priority. According to him, the first five topics in the ranking are mandatory
for university students.
Two experts suggested that the importance of software measurement should be strongly
emphasized. One of them also recommended linking the importance of measurement with
having clear objectives when measuring the software and the consequences of not doing that
(measurement). In other words, students should be aware of potential lost of not estimating
well for the lack of measurement. He suggested academia to look for ways of motivating
students to measure.

Interviews with recognized experts: Ranking of skills
Among the four interviewees, two of the skills - presented for being ranked - reached
consensus: Critical thinking (#1 - 100%) and written communication (#3 - 75%). The other

56

two skills did not reach a clear consensus. The explanations for making the ranking of the
skills are presented next.
One expert said that there were 2 ways of ranking the skills. One is from an academic
perspective and the other is in practical terms. According to him, from an academic view (for
university students), the ranking should be: critical thinking, communication (oral and
written) and team work. The practical ranking (for practitioners) may consider other skills to
rank: leadership, problems resolution, ethical aspects, etc. Another expert mentioned that he
would split critical thinking into 2 skills: critical thinking per se and problem solving. These
two skills should be the firsts in the ranking, followed by oral communication, written
communication and team work. Another opinion from an expert was to rank the skills taking
into account that software measurement demands to work carefully. Therefore, he proposed
the ranking as follows: critical thinking, writing skills, team work and oral skills. The last
expert said that all those 4 skills selected by participants of the Delphi study are important.
He suggested using the term interpersonal skills rather than team work. According to him,
interpersonal skills are needed for convincing people about the value of measurement for
organizations. Notwithstanding, he believes that the development of skills is difficult for
young undergraduates.
Interviews with recognized experts: Levels of learning per topic
The opinions of the experts regarding the level of learning are somehow similar to the results
obtained in the Delphi study and the surveys conducted at the verification phase. This means
that the experts mostly chose the levels of learning that correspond to the first levels of the
Bloom's taxonomy (remember, understand and apply). However, the following differences
were observed:

We describe this thesis research methodology through the Stol and Fitzgerald
(2013)
4 Research Path Schema, which indicates the possible emphasis and order
given to the main elements present on any research methodology. The Research Path
Schema is based on the Validity Path Schema from Brinberg and McGrath (1985) who
argue that any research design involves at least three elements: (a) some content of
interest, (b) some ideas that give meaning to that content, and (c) some techniques or
procedures by means of which those ideas and content can be studied.
In Research Path Schema, these three elements are classified in substantive,
conceptual and methodological domains, respectively. Depending on how these
domains are combined different paths are established. In this thesis, we follow the
method-driven study design path. In the study design path, the goal is to build a study
design based on the conceptual and methodological domains, and use it on one or
more elements of the substantive domain (Stol and Fitzgerald, 2015). If the primary

interest is on the methodological domain, then the study design path is called method-
driven study design path. Still according to Stol and Fitzgerald (2015), a common

scenario found in SE research that follows the method-driven study design path is
when a conceptual model or framework is taken as basis to develop a method,
technique or tool, which is the primary research interest. In this scenario, the
conceptual domain works as a lens through which the methodological domain is
developed and addressed. As a result, the substantive domain is relatively less
important since the implementation is the result, which serves as an initial validation of
the researcher’s proposed idea.



3 http://www.merriam-webster.com/dictionary/evidence.
4 Later extended in Stol and Fitzgerald (2015).

11
As shown in Figure 2, the primary domain investigated in this thesis is the
methodological domain. This primary focus of the research methodology is aligned with
the research questions previously defined given their design nature. In Easterbrook et
al. (2008), design questions are put within the non-empirical research as they
concerned with designing better ways to do something (e.g., research or software
engineering). This definition is also aligned with the absence of empirical studies on
most research methodologies and paradigms as discussed in the previous section.

Figure 2 - Research methodology path and the topics addressed (underlined items are

original research presented in this thesis)

Although the methodological domain is the primary domain of this thesis, the first
research step begun in the conceptual domain. At the initial phase of the research, it
was necessary to delimit the conceptual domain scope from which the methodological
aim could be developed. Three conceptual domain topics were explored in order to
define the research synthesis method proposed in this Thesis (denominated the
Structured Synthesis Method) and design the computational infrastructure to support it
(denominated Evidence Factory). Scientific theories form the basis for the elaboration
of the diagrammatic representation for theories in SE (Sjøberg et al., 2008), which we
formalize in this work as a representation for evidence. The Mathematical Theory of
Evidence, also known as belief functions, was selected for evidence aggregation since
it does not distinguish qualitative and quantitative evidence and can combine evidence
in any order (i.e., accumulate incrementally) (Shafer, 1976). Knowledge Engineering

12
topic (Studer et al., 1998) was also an important foundation upon which we could guide
the representation formalization and the computational infrastructure design. Some
definitions were also taken from the methodological domain itself, as is the case of
research synthesis methods (Dixon-Woods et al., 2005) used to compose Structured
Synthesis Method.
Apart from the two main contributions of this thesis in the methodological domain,
we also have concentrated efforts on better delineating in what consists Knowledge
Engineering in the case of scientific knowledge. We have conducted a literature review
on Knowledge Engineering works focusing on scientific knowledge. The idea was also
to have a comparison baseline with which the Structured Synthesis Method and the
computational infrastructure could be contrasted. Clearly, many other research gaps
could be addressed in the conceptual domain such as a comprehensive investigation
of other diagrammatic representations for evidence than the used in this research or a
thorough analysis of uncertainty formalisms to be used for evidence aggregation.
However, we tried to keep attention to the aim of this research in the methodological
domain, which is to contribute to identify and justify the necessary building blocks of an
alternative view for knowledge translation.
As expected by following the chosen research path, the remaining contributions are
empirical investigations to demonstrate the usefulness of the proposal in the
substantive domain. The first is an experimental study where subjects were asked to
use the Structured Synthesis Method to aggregate evidence from four papers in the
Test-Driven Development topic. This experimental evaluation was followed by two
worked examples. One related to Usage-Based Reading inspection and the other
about Software Reference Architecture. The worked examples were also basis for a
preliminary evaluation of the Evidence Factory tool.
1.6 Thesis organization
The thesis is organized following the research methodology described in the
previous section. The first three chapters introduces de conceptual domain. Then,
chapters four, five and seven are related to the methodological domain. The
substantive domain is detailed on chapters six and eight.
It is possible to notice the intersection between chapters related to the
methodological and substantive domains. This is because we decided to describe the
experimental study immediately after describing the method, then present the Evidence
Factory tool which support the method, and only later detail the working examples.
Thus, if readers want to focus on a specific subject, we suggest two reading paths
according to the main topics addressed in this work. For the ‘research synthesis path’,

14
2 Theory building in Software Engineering

In this chapter, we explore the theory notion indicating why it has
interesting features to be thought out as a representation for
evidence. More specifically, a proposal for theory representation is
detailed, which will be later used as a formal model for evidence.

2.1 Introduction
Theory building is a research subject concerned with how scientific knowledge is
organized and represented to use it to explain real world phenomena. Amongst the
most relevant characteristics for investigating theories as a tool for evidence
representation, we can cite:
• Generalist nature: theories are regarded as a device used to organize a
complex empirical world (Bacharach, 1989). In most scientific disciplines,
theories support scientists in providing an organized description about some
real world phenomenon in a way that part of the real phenomenon complexity is
reduced in the representation. For this reason, some researchers (e.g., Suppe,
2000) understand theories as a collection of models for describing the world;

• Epistemological grounding: a formal representation depends on a well-
defined conceptual basis to be developed. In that regards, it is not difficult to

see that the notion of theories is widely discussed in the technical literature.
There are entire books (e.g., Reynolds, 1971, L’Abate, 2012) dedicated to the
topic on what constitutes a theory, what its main elements are and what
paradigms and epistemological views can be used for its construction. The
importance of this topic is apparent on several works that use epistemological
grounding as an approach to develop knowledge representations in scientific
domain (e.g., Hars, 2001, Travassos et al., 2008, Lopes and Travassos, 2009).
This rationale led us to expect theories as an alternative for representation
aligned with our research goal;
• Knowledge tool: ‘Theories are practical because they allow knowledge to be
accumulated in a systematic manner and this accumulated knowledge
enlightens professional practice’ (Gregor, 2006, apud Stol and Fitzgerald,
2015);

15
• Empirical heterogeneity manageability: Given its generalist nature, theories
can be instantiated within different levels of abstraction (Yang, 2002), which we
believe may help linking different findings and putting them in a coherent
perspective. This is particularly important in SE where research studies are
more heterogeneous with a wide range of research approaches, methods and
techniques, both quantitative and qualitative.
It is important to say at this point that, although our purpose is to use the notion of
theories as evidence representation, the theory concept is not going to be applied to its
full extension. Given its comprehensiveness, there are definitions and usages of
theories that are different from the used here, from a wide scope defining it as a tool for
describing general laws of nature (Reynolds, 1971) to the specificity of using it to test
hypotheses within the positivist quantitative paradigm (Pawar, 2009). Thus, our aim will
be more directed to the theoretical structure itself. That is, to the concepts and relations
forming a theory. As it will be discussed in this chapter, these are the basic elements of
any theory, so we are not making any deviation from its definition, just taking the
perspective aligned to the defined goals. The idea of focusing on the theoretical
structures is to have evidence organized in a way that aggregation is facilitated. For
this reason, we will use the term ‘theoretical structure’ on any occasion that emphasis
is more on the representation than on the ‘complete’ theory conception. This does not
mean, however, that other topics, besides the related to theory structure and
representation, will not be discussed in this chapter.
On the next section, we define the theory concept. Then, in Section 2.3 we
enumerate and describe theories’ main features. A proposal for building theories in SE
is presented in Section 2.4. The proposal defines a diagrammatic representation for
theories in SE, which we will use for evidence aggregation. To clearly set our view
about theories, we discuss different epistemological perspectives in Section 2.5 and
examine related works about theories in SE in Section 2.6. Section 2.7 concludes this
chapter.
2.2 Definition
It is not trivial to define the necessary and sufficient conditions for what constitutes a
theory. Still, despite this difficulty in precisely delimit what forms a theory, in most
mature sciences the use of theory tends to be taken for granted and discussions tend
to focus on how, rather than whether, to use theory (Hannay et al., 2007). This is
because theories tend to facilitate the communication of ideas as they offer a
conceptual framework upon which knowledge can be structured in a precise and
concise way (Reynolds, 1971).

16
The concept of theories is broadly used in several scientific disciplines, even though
with different formulations (Reynolds, 1971). For that reason, this theme is frequently
object of analysis and discussions in Philosophy of Science aiming at the definition of
its own terminology, the delineation of the basic conditions for characterizing a theory,
and the investigation of how to evolve theories along the time (Suppe, 1977). Some
formulations are more abstract (ignoring temporal or contextual aspects) while others
are more realist (depending on a more detailed description), but a rather general
definition is found in (Gioia e Pitre, 1990, apud Lynham, 2002): ‘a coherent description,
explanation and representation of observed or experienced phenomena’.
This definition, although useful for a basic understanding of what can be considered
a theory, is excessively generic and do not offer support for practical application.
Nevertheless, even if there is not a consensus about theory characteristics in a detailed
level, discussions about this topic commonly involves the following issues (Reynolds,
1971, Bacharach, 1989, Lynham, 2002, Walker and Avant, 2004, Gregor, 2006,
Sjøberg et al., 2008, Pawar, 2009): (i) its utility and purpose, (ii) its basic elements, (iii)
its development levels along its formation, and (iv) how it is evaluated. The next section
briefly address each of these issues – more comprehensive discussions can be
obtained in the referenced authors.
2.3 Essential theory features
2.3.1 Utility
The utility of theories is associated to the purposes of a systematically organized
body of knowledge. According to Reynolds (1971), scientists are usually interested in a
piece of theory-based knowledge that provides at least one of the following aspects:
• Typologies: also known as taxonomies or ontologies, they offer a form of
classification or general organization of a domain allowing to form an initial
understanding of a phenomenon or matter. In comparison to other purposes,
typologies are usually more easily to be obtained as it basically depends on
observation. The difficulty lies in providing a classification that is complete (all
‘things’ are classified) and mutually exclusive (absence of ambiguity on the
classification of each ‘thing’) (Reynolds, 1971);
• Prediction and explanation: it is possible to say that except from a temporal
perspective, predicting future events or explaining what happened on the past
represent essentially the same notion. Both perspectives are concerned with
the justification of an event. An interesting aspect of predictions and
explanations is that a reason can be provided for an (past or future) event
without necessarily explaining why this is observed or expected (Gregor, 2006).

17
Theories combining these two features of predicting and explaining are
considered the ‘standard’ conception of theories (Sjøberg et al., 2008);
• Sense of understanding: is present in theories that, regardless used for
prediction or explanation, completely describes the mechanisms that links the
changes in one or more concepts with changes in other concepts. The causal
links unveils the causal process that both can explain an event but also can
predict it. To that end, it is also necessary the given description to contain the
concepts used to organize and classify the phenomenon of interest. Thus, to
provide of a sense of understanding theories must also have characteristics of
typologies and prediction/explanation. When all these elements are found
together in a theory, it can be said that it provides a ‘sense of understanding’
(Reynolds, 1971).
2.3.2 Basic elements
The basic elements that form a theory are (i) a system of constructs linked by
propositions and (ii) a scope defining its applicability (Bacharach, 1989).
Sjøberg et al. (2008) put the theories’ basic elements into a perspective on how they
are used. According to them, theories should support the description of what (defining
constructs to represent observable or non-observable entities), how (determining
propositions to indicate the relationships between entities), why (providing explanations
to describe why the propositions hold), and where/when (establishing the conditions
under which the theory constructs and propositions are supposedly applicable).
A more detailed description of what elements form a theory can obtained in Pawar
(2009). Mentioning Dubing (1969, 1976), Kerlinger (1988), Bacharach (1989), and
Whetten (1989), he decomposes a theory into the following elements: domain,
concepts, variables, definitional statements, assumptions, propositions and hypothesis.
The first observation from the enumerated elements is the division of scope into
domain and assumptions. This brings the attention to the importance of not only
describing the real domain where the phenomenon manifests itself, but also defining
the expected requirements or preconditions about the nature of the phenomenon and
the way that the entities and their relations can be observed and measured.
Another interesting association is established between propositions and hypotheses.
In Pawar (2009), hypotheses are defined as conjectures derived from the observations
that can be conceived based on propositions. Taking this interpretation to its
conceptual limit, some researchers use propositions and hypotheses interchangeably,
even though with a preference to use propositions for more comprehensive
generalizations while associating hypotheses to relatively more specific statements. In

18
addition, hypotheses can also be generated in situations where concepts are
operationalized by variables that were not used to measure these concepts.
The relation and differentiation between constructs, concepts, and variables is also
essential for understanding the theories’ basic elements. Constructs and variables are
clearly on different abstraction levels, and usually it is said that a variable
operationalizes a construct (Yang, 2002) – that is, it measures or is used to observe a
construct. This differentiation is necessary to distinguish terms representing abstract
entities or ideas, which cannot be directly measured (e.g., software quality), and terms
related to features of the empirical world (e.g., software number of defects). Apart from
the more evident distinction between constructs and variables, some authors also try to
establish a distinction between constructs and concepts as a gradual spectrum of
abstraction levels from construct, to concepts and to variables. Still, according to Pawar
(2009), it is not uncommon to see the same term labelled as both constructs and
concepts, and even as variables. It seems this usually happens when scientists are
more interested in using theories as a knowledge tool to organize ideas and structure
the phenomenon description than using it as part of the research methodological
explanation. Thus, in this work, as attention will be given to knowledge representation
and how it can be used to represent evidence, this distinction will not be regarded as
imperative and the usage of construct and concept will be considered interchangeable.
The last basic element enumerated by Pawar (2009) is definitional statements. As
the name implies, definitional statements are simply the collection of definitions and
descriptions about the theories’ concepts, variables and propositions. This element is
commonly absent from the theory definitions found in the technical literature, since it is
implicitly assumed to be present in any theory description.
2.3.3 Development
It seems to be consensual among scientists that theories can have different
development levels. According to Bacharach (1989), ‘implied in the notion of
generalizability are different levels on which one can theorize. This implicit continuum
stretches from empirical generalizations (rich in detail but strictly bounded in space
and/or time) to grand theoretical statements (abstract, lacking in observational detail,
but relatively unbounded in space and/or time)’. Therefore, these levels can represent
not only milestones in the theory building process, but also ‘full’ theories by themselves
depending on their utility (Sjøberg et al., 2008). Analyzing these development levels,
Walker and Avant (2004) distinguish among four levels.
In the first level (level 1) are the theories with strong focus on practical utility, which
are also denominated as small-range theories. This kind of theories have concrete
relationships, which are directly based on observations, and usually have a causal

19
nature, but do not provide a sense of understanding (Jacox, 1974 apud Walker and
Avant, 2004). To summarize, the definitive feature of the small-range theories is their
specificity to a particular situation (Higgins and Shirley, 2000). The next level (level 2) is
formed by the middle-range theories. For Lens et al. (1995), middle-range theories are
those sufficiently specific to guide practice and research, and still generic enough for
different populations so it can explain similar phenomena. The third level (level 3)
categorizes the so-called grand theories, which are highly abstract and have concepts
and propositions transcending specific populations or events. And in the last level we
find the meta-theories, which focus on methodological and philosophical issues and do
not represent theories themselves.
When constructed from the beginning, theories are usually created based on only
one initial observation (i.e., evidence), from which its preliminary constructs,
propositions and explanations are extracted. From that point, theories that advance
throughout the different levels undergo over a continuous refinement process, which
involves reiterated development cycles of definition or adaptation of the theory
elements based on new evidence obtained in the field (Lynham, 2002). During this
process, with the incorporation of new evidence, the confidence on the theories’
explanations and predictions increases while, at the same time, the theories become
more independent from specific context and population. As a result, theories’
explanation coverage is extended over different phenomena, which is precisely the
most discernible aspect of theory evolution over the different development levels.
The link between evidence and theories is so important that Higgins and Shirley
(2000) try to suggest specific evidence types for each theory development level. For
instance, they indicate middle-range theories are usually associated with replications in
controlled conditions (e.g., controlled experiment), while small-range theories would be
associated with non-systematic observations. This categorization could also point that
a possible source of level 1 theories would be practitioners trying to describe their
experiences or researchers in initial exploratory studies, while level 2 and 3 would be
preferably produced by researchers in more advanced stage of their investigations.
The development process described until now is similar to what Reynolds (1971)
denominated as research-then-theory approach. As it is possible to presume, the
complementary approach is called theory-then-research, which consists in the situation
where new ideas or hypotheses are analytically described with a theory and
subsequently evaluated through observations or controlled studies. Theories
constructed in this way are denominated hypothetical theories – at least initially while
they are not supported by evidence.

20
Regardless the used approach, research-then-theory or theory-then-research, the
theory development process must be guided by strategies that are suitable to the
technical literature type and available data. Walker and Avant (2004) define three
strategies for theories development levels summarized here:
• Synthesis: used to extract and accumulate constructs and propositions from a
data set or pieces of evidence. Synthesis allows researchers to combine
isolated pieces of information that are yet theoretically unconnected. Synthesis
can work well when a researcher is collecting data or trying to interpret data
without an explicit theoretical framework;
• Derivation: used in the cases where a scientist wants to transpose and
redefine a concept, statement, or theory from one context or field to another.
Can be especially useful when there is not theoretical basis in the field for that
matter, bringing a theoretical soundness to the explanations. A possible result
from these situations is that new understandings or theories can be developed
from that point;
• Analysis: this strategy allows the understanding of the whole phenomenon with
the analysis of its constituent parts. This strategy is usually applied to the cases
where there is a solid theoretical basis in the field so that their constructs and
propositions can be reformulated to represent the new understanding about the
investigated phenomenon.
It is interesting to add here that these strategies are not only used to support the
definition of the theories’ parts, mainly its constructs and propositions, but also the
theory as a whole. In fact, Walker and Avant (2004) describe separately in detail how
each of these strategies are used to each theory elements and the theory itself. For
instance, in the case of the synthesis strategy, they show how qualitative methods,
such as Grounded Theory, can be used to identify concepts and how quantitative
methods, such as correlation analysis, to characterize propositions based on a
dataset. In the theory level, still related to the synthesis strategy, they indicate the use
of graphical representation to facilitate the understanding when making explicit the
relationships among the concepts. In SE context, the importance of theory
representation can also be found in Sjøberg et al. (2008), where the authors mention
how it helps to put all evidence related to the theory into the same perspective.
2.3.4 Evaluation Criteria
Theory evaluation is an important mechanism to assess its ‘quality level’ or how
well-formed we estimate it is. There is a wide discussion about how to evaluate a
theory and what criteria can be used to that end. Sjøberg et al. (2008), citing the works

21
of Bunge (1967), Dubin (1978) and Cohen (1989), enumerate six criteria they consider
most relevant to evaluate theories in SE:
• Testability: degree to which a theory can be refuted;
• Experimental support: degree to which a theory is supported by evidence
confirming its validity;
• Explanatory power: degree to which a theory explain or predict known
observations in a specified scope;
• Parsimony: degree to which a theory is economically constructed using the
minimum set of constructs and propositions;
• Generality: capture the breadth of scope and the degree to which a theory is
independent of specific settings;
• Utility: degree to which the theory support relevant areas in the state of the
practice.
The evaluation criteria have a correspondence with the theories’ development levels
described in the previous section. For instance, a theory with a high explanatory power,
parsimony and generality are usually classified in the third development level. Another
example is the hypothetical theories, which have no experimental support. In summary,
the evaluation criteria are essential to compare candidate theories to explain the same
phenomenon, indicating the strongest given a specific research or practice goal.
2.4 The adopted representation for theories
The proposal for evidence aggregation presented in this thesis has as one of its
main parts the diagrammatic theory representation from Sjøberg et al. (2008). We
choose to use this theory conceptualization as it is already tailored to SE and defines a
visual representation with specific notational semantics. Moreover, it is regarded as
well suited for mid-range generalizations and it is appropriate to make explicit the
underlying phenomenon mechanisms, which are decisive properties for the
development of SE theories (Wieringa et. al, 2011).
Figure 3 shows an example of a theory represented using the diagrammatic notation
proposed by Sjøberg et al. (2008). This theory was extracted from a real action
research study regarding the use of source code refactoring in a medium-to-large scale
Web software project (Santos and Travassos, 2011). The notational semantics are
partly based on UML.

22
Figure 3 – Refactoring theory diagrammatical representation from (Santos and

Travassos, 2011)

A concept (or construct) is represented as a class or class attribute. A class is
represented by a box with its name written at the top such as, for instance, ‘Distributed
Project’. A class can have a subclass (using the same generalization notation as in
UML) or a component class (drawn as a box inside another box such as, for instance,
‘Source Code’). Usually, if the concept represents a particular variable value, then the
concept is modelled as a subclass or component class (e.g., ‘Large Scale Web
Systems’). Otherwise, if the focus concerns the values variations, then the concept is a
variable modelled as a class attribute, such as ‘Effort’. An attribute is placed at the
bottom of a class box (below the horizontal line).
A relation representing a proposition is drawn with an arrow. An arrow from concept
A to B denotes that A affects B (i.e., A is a cause to B), where A is a class and B is a
property. In addition, B can also be a relationship itself. In this case, A is called
moderator, as in the case of ‘Experience’ concept. A moderator represents that A
affects the direction and/or the intensity of B relationship.
As described, the representation has just ten semantic constructs, which is one of
the reasons for which we believe in its relative simplicity, but it also makes us aware
about potential limiting factors in its capacity of representing different aspects of
theories (and evidence). The ten semantic constructs are separated into five
relationships and five concepts types.
Concept types are named: archetype, contextual aspect, cause, effect and
moderator. The first three types are called value concepts as they represent a
particular variable value, usually an independent variable. Archetypes, the root of each
hierarchy, are fixed in four concepts (activity, actor, technology and system) in an
attempt to capture the typical situation in SE described as an actor applying a

23
technology to perform activities in a software system. Contextual aspects are used to
describe the necessary or at least the important conditions under which the stated
propositions are expected to occur or were observed. Sjøberg et al. (2008) do not give
a specific name for this type of concept, but we assign it a name because it delimits the
theory scope. Causes can be thought as contextual aspect subtype as it one addition
element present in an observed context, but they are directly responsible for the stated
propositions. The last two value types, on the other hand, are called variable concepts
as they are associated to the aspects that are assumed to have value variations,

usually dependent variables. Effects are the counterpart of causes, and the pair cause-
effect are usually propositions that researchers are most interested in. Moderators, as

the name suggests, are used to define concepts (i.e., moderators) that are understood
to influence cause-effect relationships.
Relationship types are denominated: is a, part of, property of, cause-effect, and
moderation. The relationship types can be separated into two categories: structural
relationships and influence relationships. Structural relationships (is a, part of, and
property of) are used to describe how concepts are ‘statically’ arranged. For this
reason, except for the property relationship, which can be used for both value and
variable concepts, they are mainly used to organize the contextual aspects concepts.

Complementing the relationship types, the two influence relationships types, cause-
effect and moderation, are the two available alternatives to indicate how concepts can

affect each other.
As it can be observed in Figure 3 cause concept is central since it is what defines
the hypothesis: ‘given the element X (cause) in a given context (value concepts) this is
expected (effects and moderators)’. This will be important to establish the aggregation
criteria when we define how the representation will be used to that end. Most
investigations in SE tries to observe how technologies affects software development
actors (persons, teams and organizations), activities (full lifecycles or specific phases),
and system (software products, components or other artifacts). Thus, the cause will
usually be a type of technology archetype. Nevertheless, it is possible to have any
archetype as super type of a cause. For instance, there are investigations concerned
with observing how developers’ (actor) personality types (cause) affects pair
programming (technology) characteristics (effects) (Sfetsos et al., 2008). Still, for being
one of the most investigated ‘causes’ for software development, the technology term is
used with different meanings. Sjøberg et al. (2008) do not discuss this issue in depth
leaving room for different interpretations. In this work, we adopt the definition given by
Falbo et al. (1998) in their ontology for software development process, which states
that software development activities use resources (i.e., tool support) or adopt

24
procedures (i.e., method, technique or guideline). For this reason, we will consider
tools, methods, techniques and guidelines as technologies.
Apart from making explicit cause-effect relationships and their moderators, a
significant portion of the context in which they are expected to be observed is also
described with the representation. The context modeling is performed with the
structural relationships where activity, actor, system, and technology are specified
(using is a relationships) and detailed (using part of and property of relationships). This
is aligned with Thagard and Nowak (1990) who state that these types of relationships
are essential to any concept network in science as any scientific revolution consists in,
apart from creating and excluding concepts, changing their relationships, particularly, is
a and part of relationships.
To supplement the diagrammatic representation, some textual description must also
be provided. It is necessary to textually define each concept, describe each
proposition, and provide the explanations of why they can be sustained. These
definitions are indexed using the labels ‘P’ (proposition) and ‘E’ (explanation) next to
the relationships arrows (Figure 3). Some examples of the textual descriptions are
given in Table 1. It is important to see that the proposition values are defined in
qualitative terms (e.g., Code Refactoring positively affects Maintainability). A
complete textual description can be seen in Santos and Travassos (2011).
Table 1 – Some textual descriptions for theory elements from Santos and Travassos

Concepts
C1 Code Refactoring (development practice the act of modifying software
structure without changing its observable behaviour)
C2 Source Code Structure (structural properties perceptible in the source code,
ex.: readability, algorithm structure)
Propositions
P5 Code refactoring positively influences code structure
Explanations
E5 Source code structure improves:
• It becomes more homogeneous throughout the entire software
project, according to the previous knowledge of the developers.
• Its size and complexity is reduced.

As it is recommended to any knowledge representation, a set of activities should be
defined in order to researchers be able to model theories using it. Sjøberg et al. (2008)
present heuristics for theory building using the representation separated into five steps:
(i) concepts definition, (ii) propositions identification, (iii) explanations specification, (iv)
scope determination, and (v) experimental evaluation. The detailing of these heuristics
is not given here, since they are more focused on the notion of theories while in this

25
work we will be more focused in the knowledge representation and research synthesis.
This discussion will be focused when we present the proposal of this thesis.
2.5 Epistemological perspectives
There are several views about how scientific knowledge is acquired to and from
described by theories. Besides presenting a brief discussion about theories’
epistemological perspectives, the idea is to situate Sjøberg et al. (2008) proposal
among them. This has not been done by Sjøberg et al. (2008), but it is examined here
as a mean to make some of its limitations and orientations explicit in comparison to
other views.
Magnusçl (2012) distinguish among five theory perspectives, two of which are
considered the most well-developed by philosophers of science:
• Statement (or structural): a theory is considered structural whether it is
possible to express it in logical language. Thus, in this view, laws of nature are
axioms in a deductive system represented by universal generalizations of the
form ‘for all x in scope S, φ(x) is true’ (Wieringa et al., 2011). Furthermore,
explanations are theorems in the system, provable from the laws of nature as
axioms. Although this view is still being adopted by many scholars, its
opponents usually cite the following issues: (i) the representation of theories as
linguistic entities, using first order logic, artificially turns the problem of
comparing theories into a translation problem; and (ii) logical formulations
requires, in most cases, a significant amount of idealized suppositions, which
make researchers consider many structural theories distant from real world
phenomena, with limited practical applicability. On the other hand, these
characteristics, considered by many negative aspects, are exactly the same
which make them convenient to design simulation models or arrange controlled
studies, as most part of the knowledge necessary to plan these kind of studies
is already formalized;
• Semantic: this view was proposed as a reaction to the structural perspective.
According to Bickle (1993), two were the main motivations for the emergence of
the semantic view: (i) scientific theories should not be conceived by the means
of language entities (i.e., a set of logical sentences), but rather be described by
models; and (ii) the proper tool to formalize scientific theories are not first order
logic, but mathematics. As it can be seen in these motivations, theories in the
semantic view are defined as a collection of models (Suppe, 2000). Even
though emphasis was initially given to mathematical models, currently the
semantic perspective accepts other types of models such as the mechanistic

26
(Glennan, 2005) and the propositional (Thomson-Jones, 2006) – a taxonomy
about different types of models can be found in Thomson-Jones (2006). Apart
from each type of model specificities, a model in a semantic view is understood
as a representation (abstract or physical) of reality. Therefore, differently from
the structural perspective where theories are linguistic entities and, thus, have
its sentences evaluated as true or false, models in semantic view are assessed
for their degree of similarity to or amount of richness about the modeled
phenomenon (Clarke and Primo, 2007). A question that may be raised at this
point is what is the role of theories if models are already assessed comparing
them to the real world phenomenon? According to McKelvey (1999), the role of
theories in the semantic view is to try to explain the models behavior. Thus, a
theory is always associated with and tested through a model, and do not try to
explain the real world phenomenon directly. Rather, it describes the behaviors
of ‘idealized world’ models.
According to Magnus (2012), the other perspectives are not so articulated as the
structural and semantic perspectives. The cognitive view is concerned with theories
relative to the process of theoretical understanding. A theory is what the agent
cognitively grasps, the structure present in the scientist’s mind. Churchland (1992 apud
Magnus, 2012), one of the first scholars to think about theories in this way, used the
connectionist framework that treats brains as neural networks. In his proposal, he
suggests that the codification of theories’ content is embedded in the patterns of neural
network middle layers. There are other proposals of theory representation, but all share
the notion of theory as the content in the scientists’ minds. The toolbox perspective
rejects the view of theories as general explanations of world phenomena. Models are
the primary unit of analysis, and different theories are put together as tools to construct
models (abstract or physical) – Magnus (2012) gives an example of hybrid physical
models with parts based on classical and others on quantum or relativistic physics. The
historical perspective is defined as a distinct period of an existing research tradition.
Magnus (2012) cites Kitcher (1984) to describe how researchers think about ‘historical
theories’: ‘Classical genetics persists as a single theory with different versions at
different times in the sense that different practices are linked by a chain of practices
along which there are relatively small modifications in language, in accepted questions,
and in the patterns for answering questions’.
In SE, theory epistemological perspectives is discussed to some extent in Wieringa
et al. (2011). In consonance with Magnus (2012), who put structural and semantic
perspectives as the most important and discussed, Wieringa et al. (2011) only focus on
these two perspectives. Yet, they use a different terminology denominating the

27
structural and semantic perspectives as logical and architectural (or mechanistic),
respectively. Indeed, the term logical is commonly used as synonym to structural, since
the structural view uses first order logic. However, what they call mechanistic view is
not a perspective per se, but rather a specific way to construct models on the semantic
conceptualization (Glennan, 2005), as previously mentioned.
Among the different theory epistemological perspectives, Wieringa et al. (2011)
suggest that the semantic view would be the closest conceptualization to the
framework proposed by Sjøberg et al. (2008), with direct association to mechanistic
models. According to them, this indication is based on the presence of cause-effect
relationships and the possibility to represent moderation of these relationships.
Together, cause-effect and their moderations are the basic elements to represent most
mechanisms among the components of a given system. They allow making explicit that
a mechanism (moderation relationship) can neutralize or reduce the effect of another
mechanism (cause-effect relationship), which is essential to represent non-universal
generalizations (i.e., effects that occur sometimes, but not always). Noting that one of
the semantic view criticisms over the structural view is precisely the universal
generalizations present in the logical form: ‘for all x in scope S, φ(x) is true’.
Even though Wieringa et al. (2011) indication of the semantic view to Sjøberg et al.
(2008) framework seems to be, indeed, correct, it is important to observe that other
interpretations would also be acceptable. Taking, for instance, the model taxonomy
defined in Thomson-Jones (2006), Sjøberg et al. (2008) framework would be
categorized among propositional models. In propositional models, as the name
suggests, a set of propositions are used to represent systems’ characteristics or
behaviors. And as described in the previous section, a set of propositions is one of the
main characteristics of Sjøberg and his colleagues diagrammatic representation. Still,
even if the understanding about which type of model better qualifies Sjøberg et al.
(2008) framework is an important philosophical exercise, this discussion is not in the
scope of this work. Moreover, for the convenience of adopting a classification already
indicated in SE (Wieringa et al., 2011) and for being a view that is unifying researchers
of different methodological traditions – including quantitative/qualitative and
experimental/non-experimental –, we will adopt the conceptualization of mechanistic
models, in the semantic view, as the one which best represent the theory framework
adopted in this work.
Objectively using these epistemological perspectives and philosophical discussions
as a prism to define Sjøberg et al. (2008) framework main characteristics, we point the
following:

28
• One of the main Sjøberg et al. (2008) framework features is the possibility to
represent non-universal generalizations. This is fundamental to form basic,
‘local’ theories, which are fundamental to SE since general laws in the area are
hard to achieve (Hannay et al., 2007);
• As mechanisms are not universal laws, qualifying Sjøberg et al. (2008)
framework as a mechanistic model in the semantic view would mean that only
theories in development level 1 and 2 (see Section 2.3.3) could be represented.
However, in situations where the evidence available to support theories’
propositions is sufficiently large, it is possible to accept the emergence of
theories to level 3. Even if this view is somewhat contrary to the notion of
mechanism ‘locality’, the idea of law as a scientists’ consensus about a specific
theme, after producing enough evidence, meet an overall understanding of
philosophers of science and researchers regarding the differentiation between
laws and theories (Reynolds, 1971, Glennan, 1996). This will be precisely our
understating on the theories development, since theoretical structures will be
formed from research synthesis (i.e., evidence aggregation);
2.6 Discussions about theories in SE
If theory building and theory-based investigations are uncommon in SE (Hannay et
al., 2007), even rarer is the discussion focused strictly on the role and the format of
theories in SE – though it seems to recently have begun to receive more attention as
we will see in this section. Besides Sjøberg et al. (2008) framework presented in
previous section, we cite four works discussing theories in SE at the meta-level (i.e.,
regarding some aspect of its necessity, formalization, organization or structuring):
Hannay et al. (2007), Wieringa et. al (2011), OMG (2013)5

, and Stol and Fitzgerald

(2015).
The first work is a systematic review of theory use in software engineering
experiments (Hannay et al., 2007). Apart from presenting essential theory features,
such as discussed in Section 2.3, the authors report some interesting characteristics
from 103 papers found in the review, such as the theories’ roles in each experiment
(e.g., motivation for the study design, experimental testing or new theory generation),
their distribution among SE disciplines, and means of representation (e.g., text, logic,

5 The citation from Object Management Group dates 2013, but SEMAT initiative started at the
end of 2009 (www.semat.org).

29
references or diagrams). After this, some meta-level discussion is presented
addressing issues like the usefulness of theories, obstacles for using them, and what
constitute a theory in SE. Regarding theories’ usefulness, they indicate that theories
used in SE experiments provide a conceptual framework for explaining observed
phenomena, make investigated research questions more clear and easier to
rationalize, and reveal paths for new research since it is common to have parts of
theories’ underlying mechanisms that have not yet been investigated. Most identified
obstacles stems from the perception that it is necessary to accumulate evidence before
building theories. However, according to the authors, this is just one of many ways to
generate theories. Thus, experiments in SE should begin with theories or, at least,
generate a theory so that the theories can be more easily related to experiments. Then,
these theories or models can be composed or synthesized together to explain or
predict phenomena in different ways that are useful for answering practical questions at
hand. This is precisely what the semantic view accommodates and, in the view of
Hannay et al. (2007), it is the most likely prospect for SE, where smaller units of theory
will evolve and address different aspects of a phenomenon.
The proposal from Software Engineering Method and Theory (SEMAT) is not
directly focused in theory, but it is in fact a domain-specific language to allow specifying
SE methods. Still, the ‘theory’ in its name stems from a common ground defined to help
practitioners to compare methods and make better decisions about their practices. This
common ground is implemented through a definition of a Kernel whose elements forms
a basic vocabulary or a map of SE context (OMG, 2013). The idea is that software
development methods are composed of practices, which are described using Kernel
elements. The Kernel is organized into three discrete areas of concern focusing
different aspects of SE: Customer, Solution and Endeavor (software development
team). For each one of these areas, the Kernel contains a small number of Alphas,
which are representations of the essential things to work with, and Activity Spaces,
which are representations of the essential things to do (OMG, 2013). There are seven
Alphas, including ‘stakeholders’, ‘software system’ and ‘team’, and fifteen Activity
Spaces, including ‘Understanding Stakeholder Needs’, ‘Test the System’, and
‘Coordinate Activity’. With these Kernel elements, practices can be specified using a
combination of Alphas and Activity Spaces. In addition to the Alphas and Activity
Spaces, practices’ steps and guidelines are further detailed through the Alphas’ states
and Activity Spaces’ input, outputs and completion criteria. For instance, in Jacobson et
al. (2013) a requirements elicitation practice is specified using Opportunity,
Stakeholders, and Requirements Alphas, and using ‘Explore Possibilities’,
‘Understanding the Requirements’, and ‘Understand Stakeholder Needs’ Activity

30
Spaces – all these elements are associated with Costumer and Solution areas.
Besides using the Kernel elements ‘as is’, practices can be specified by extending the
seven Alphas and fifteen Activity Spaces.
Wieringa et. al (2011) describe what constitute design theories in SE. The most
fundamental characteristic of design theories is they explain effects of artifacts in
concrete contexts, that is, as part of an engineering cycle. Four logical tasks are
identified as part of any engineering cycle: problem investigation of the stakeholders’
needs and measurable criteria for their goals; treatment design concerned with
solutions designed to operate with stakeholders and artifacts (e.g., devices, software,
techniques); design validation trying to analyze if theoretical predictions of the
treatment design can meet the established goals; and treatment implementation, which
basically consists of transferring the solution to practice. Based on the description of
the engineering cycle, the authors present what should be the structure of design
theories. In their conceptualization, design theories explain why an artifact would
contribute to stakeholder goal achievement. The basic idea is that a design theory
should state what the likely effect E is if an artifact A is used in a context C. It is
interesting to notice how the effect is tied up to the use of an artifact in a specific
context. This is precisely what is represented in Sjøberg et al. (2008) diagrammatic
models, where contextual aspects, cause, and effect elements are used to represent C
 A → E (notation from Wieringa et. al (2011), the arrow represent causation, not
deduction).
Stol and Fitzgerald (2015) present a different view for theories in SE. The proposed
view is based on their diagnosis that, although SE research do not have theories in the
way found in other disciplines, most SE technical literature does provide theory
fragments. To make these fragments explicit, Stol and Fitzgerald (2015) start from the
perception that any research study is a result of a combination of three elements or
domains: some phenomenon or topic of study, a research method or technique, and a
set of concepts or theory. Then, it is presented what they call Research Path Schema,
which, as the name suggests, defines different ‘paths’ for research studies. These
paths are used to establish how ordered, in terms of importance, the elements should
be so that different theoretical focus can be given to each of them. Thus, the argument
is that by making explicit how each of these elements are being addressed and what
are their order of importance in research studies, then the primary interest domain of
the research combined with the supporting domains help researchers identify the
theory fragments used or generated. We used the Research Path Schema in this work
to help pointing out scientific contributions and its main conceptual and theoretical
foundations.
2.7 Conclusion
The notion of theory is pervasive among most scientific disciplines and represent a
major building block upon which scientific knowledge can be built. Despite the
considerable methodological heterogeneity in SE, and consequent diversity in evidence
and scientific knowledge reporting, theory use and generation should not be left aside.
In fact, one of the most interesting characteristics of theories is that they can be used in
almost any kind of research strategy. Moreover, although different views about what
constitutes a theory exist, there is a near consensus regarding its essential features
such as the ones discussed in this chapter: utility, basic elements, development levels,
and evaluation criteria.
As stated in the beginning of this chapter, we will explore the notion of theories to
aggregate evidence in SE precisely for these reasons. The Sjøberg et al. (2008)
framework was chosen to that end as it has a well-defined (diagrammatic)
representation, which is important for aggregation purposes, and is relatively general
accommodating almost any type of research examining causality questions. In addition,
its semantic view characteristics, particularly of representing non-universal laws, is
fundamental in SE where general laws are considered very unlikely, which favor the
emergence of short or middle-range theories. Another important characteristic in the
context of evidence aggregation is how theories can be structured in the format ‘given
the element X (cause) in a given context (value concepts) this is expected (effects)’ or
putting in Wieringa et al. (2011) terms Context  Artifact → Effect.
In the next chapter, we present the uncertainty framework that will be used in
conjunction with the Sjøberg et al. (2008) framework. Although both form a conceptual
ground for the proposition of the research synthesis method, we still opted to split their
presentation in two separate chapters.

32

3 Mathematical theory of evidence

This chapter presents the uncertainty formalism used for evidence
aggregation. Mathematical theory of evidence is a research topic in
its own. New applications of the framework are still being discovered
and it is still being evolved and extended. Our discussion will be
limited to elements necessary for our intended uses, which is
basically around the Dempster’s combination rule described in the
original work of Shafer (1976).

3.1 Introduction
Evidence aggregation process is not only dependent on some kind of representation
to determine their combinability, but also on an uncertainty formalism to identify the
most important trends considering the analyzed studies. It is important to select an
approach that can cope with uncertainty, which represents how likely to be true are the
findings associated with a piece of evidence, and can handle ignorance, which
indicates the lack of knowledge about the results that usually come from more
unsystematic observations. Particularly important in SE, where there is a considerable
heterogeneity of primary studies types, it is critical to use a probability theory
independent of occurrence frequencies (e.g., known distributions).
These features are considered one of several methodological advantages of
Dempster-Shafer Theory (Ruspini et al., 1992), which is also regarded for its
consistency with classical probability theory, its compatibility with Boolean logic and its
manageable computational complexity. The Mathematical Theory of Evidence, also
known as Dempster-Shafer Theory (D-S) or belief-function framework, is a
mathematical formalism used to reason about uncertain events or hypotheses. The
main motivation to its proposition was an intent to release the classical probability
theory or Bayesian framework from the necessity of assigning an uncertain measure to
all hypotheses under consideration (Shafer, 1976). To show the importance of how this
can represent a constraint in many situations, the following example is given: three
hypotheses A, B and C are possible solutions for a given problem. There is a weak
evidence that indicates B as a possible solution (and nothing else). Based on that,
using D-S, a low value could be assigned to B, for instance, 0.1 (in a range of 0-1), 0.0
to A and B, and 0.9 of uncommitted support. Representing this using the classical

probability theory can be problematic as there is not a direct way to manage the
uncommitted support, i.e., ignorance. In these cases, what is usually done under the
classical probability theory is to distribute support uniformly trying to represent
ignorance (Shafer, 1976). The idea is that when hypotheses have close support values,
there is no indication of what is the most probable result, which can represent a kind of
ignorance. In the given example, it could be defined as P(A) = 0.3, P(B) = 0.4 and P(C)
= 0.3. However, although this can in fact translate the notion of ignorance, this
probability distribution could represent a real distribution and not an attempt to
represent ignorance. So this kind of ambiguity should be avoided.
In D-S, aggregation is achieved by the Dempster’s Rule of Combination, which
takes two pieces of evidence and produces new evidence representing the consensus
of the two original pieces. To that end, D-S defines a basic probability assignment
function that allows expressing evidence in terms of belief values assigned to different
hypotheses and is used as input for combination. The set of the possible hypotheses
forms the frame of discernment. The result of the D-S combination of two pieces of
evidence is also expressed using a basic probability assignment function and, by
definition, can be combined with another piece of evidence. To identify the most
probable result, D-S defines the belief function, which computes the amount of belief
that each hypothesis have.
In this chapter, we briefly present D-S theory describing its main constituent parts
cited in the previous paragraph. We follow the order in which these concepts are used
in the process of aggregation using D-S, which involves defining a frame of
discernment (Section 3.2), representing each evidence using the basic probability
assignment function (Section 3.3), combining evidence by the means of Dempster’s
Rule of Combination (Section 3.4), and identifying the most probable hypothesis using
the belief function (Section 3.5). We also describe the discount operation (Section 3.6),
which is used to adjust the confidence in evidence depending on how it was obtained.
After that, a short example is described (Section 3.7) and concludes de chapter in
Section 3.8.
3.2 Frame of discernment
The frame of discernment is a collection of alternatives that evidence can indicate as
acceptable to be true, a solution to a problem, possible results of an event, or any other
representative state of the real world or theoretical conceptualizations. As D-S uses set
theory as its mathematical foundation, the frame of discernment is defined as a
mutually exclusive and collectively exhaustive set of elements, usually named Θ, with
elements {a1, a2, a3, ... an}.

All operations and definitions of D-S depend on determining a proper frame of
discernment. Thus, it is an important step when preparing to use D-S theory as an
uncertainty formalism.
3.3 Basic probability assignment function
The basic probability assignment function (bpa) represents the strength of evidence.
It generalizes the probability function from the classical probability framework, which
assigns a number in the range [0, 1] to every singleton of Θ such that the numbers sum
to 1, by extending the assignment domain to the power set of Θ, denoted by 2
Θ. By
doing this, bpa allows allocating values in the range [0, 1] to every subset of Θ. In other
words, values can be assigned to all singletons, all subsets of two elements, three
elements, and so on, to the entire superset. These values are represented in terms of
m-values, e.g., m(A) = s, where A is a subset of Θ and s a value in [0,1]. Consistent
with the classic probability theory the sum must equals one as well. The m-value for the
empty set is zero, i.e., m(Ø) = 0.
In short, bpa is defined as m: 2Θ → [0,1], where the only restrictions are:

It is important to say that although the ‘probability’ term on the bpa name suggests
m(A) might be a probability, a clear distinction has to be made. First, and most
important, is that probability distribution functions are defined over Θ whereas bpa is
Trying to avoid the misleading about these two different notions, bpa, which was first
named by Shafer (1976), received different denotations in technical literature, such as,
basic belief assignment (Smets and Kennes, 1994), belief structure (Denœux, 1999),
and basic belief mass (Srivastava and Mock, 2002). In this work, we use the original
denomination from Shafer (1976). The ‘probability’ value itself is referred as m-value,
mass, belief value or degree of belief.
The quantity m(A) is a measure of the portion of the total belief committed exactly to
A. This portion cannot be further subdivided among the subsets of A and does not

35
include portions of belief committed to subsets of A. Here, note that we use the term
belief and not probability to refer to the m-values.
A question that could emerge at this point is what represents the allocation of belief
to a set that is not a singleton. The answer to this question is not straightforward, as it
depends on the problem at hand. The general notion is it represents that there is
evidence to believe that one of the hypotheses contained in A is true. Taking a concept
from Boolean logic, it could be roughly said it represents an ‘or’ operator of the A set
elements. Following this rationale, it is interesting to notice that the quantity m(Θ) is a
measure of the portion of the total belief that remains unassigned after commitment of
belief to the various subsets of Θ, as m(Θ) represents the belief assigned to the whole
frame of discernment. For instance, evidence favoring a single subset A need not say
anything about belief in the other subsets. If m(A) = s and m assigns no belief to other
subsets of Θ, then m(Θ) = 1 – s. Thus, the remaining belief is assigned to Θ and not to
the negation of the hypothesis (¬A), as would be assumed in the Bayesian model.
3.4 Dempster’s rule of combination
Given two bpa’s, both with the same frame of discernment, but related to
independent observations, the Dempster’s rule of combination computes a new bpa
representing the combined evidence. Interestingly enough, according to Shafer (1976),
there is ‘no conclusive a priori argument [to the Dempster’s rule of combination]’, but it
still ‘does seem to reflect the pooling of evidence’. Analytically, it is regarded
appropriated to model the narrowing of the hypothesis set with accumulation of
evidence, a process which according to Gordon and Shortliffe (1985) characterizes
expert reasoning in general.
The mathematical formulation of the combination rule is shown below:

The aggregated belief value for each hypothesis C is equal to the sum of the
product of the hypotheses belief values whose intersection between all hypotheses Ai
and Bj of both evidence is C. When the intersection between two hypotheses is an
empty set, we say there is a conflict. Conflicts originate from bpa’s that represent
partially contradictory evidence. When this happen, conflict is redistributed to the
resulting aggregated hypotheses – that is the function of 1 - K in the denominator.

identical functions is not the same function, i.e., m1 + m1 != m1).
3.5 Belief and plausibility functions
Evidential functions6 are applied upon a belief mass distribution to evaluate the
degree of belief associated with a hypothesis. Arguably, from the two most cited
evidential functions (belief and plausibility), the most important is the belief function. It
is the basic definition that computes how much belief is committed to A or any of its
subsets. The idea is to have a measure of the belief that directly supports a given
hypothesis A or a more specific (subset) one. In this way, it forms a lower bound of how
much is possible to believe in A, i.e., how much belief it is already committed to A or its
subsets. Mathematically, a belief
First, we defined the terms to be included in our search string. We se-
lected all SWEBOK knowledge areas [19] to be included as terms, except

for the three knowledge areas on related disciplines (Computing Founda-
tions, Mathematical Foundations and Engineering Foundations). We also

included the term “Software Engineering”, to augment the comprehensive-
ness of the search string. Finally, to reduce the scope of the search string

to studies that report SE taxonomies, we included the term “taxonomy”.
Since some of the knowledge areas are referred by the SE community
through of other terms (synonyms), we also included their synonyms.
Specifically, the following synonyms were included into the search string:
• Requirements – requirements engineering.
• Construction – software development.
• Design – software architecture.
• Management – software project management, software management.
• Process – software process, software life cycle.
• Models and methods – software model, software methods.
• Economics – software economics.

The selected SWEBOK knowledge areas and the term “Software Engi-
neering” were all linked using the operator OR. The term “taxonomy”

was linked with the other terms using the operator AND. The final search
string is shown below.

65

(“software requirements” OR “requirements engineering” OR “software
design” OR “software architecture” OR “software construction” OR

“software development” OR “software testing” OR “software mainte-
nance” OR “software configuration management” OR “software engi-
neering management” OR “software project management” OR “software

management” OR “software engineering process” OR “software process”
OR “software life cycle” OR “software engineering models and methods”
OR “software model” OR “software methods” OR “software quality” OR
“software engineering professional practice” OR “software engineering
economics” OR “software economics” OR “software engineering”) AND
(taxonomy OR taxonomies)

Although SE knowledge classification could be named in different ways,
e.g. taxonomy, ontology [144] and classification scheme [141], we limited

the scope of this paper to taxonomies. Extending our search string to in-
clude the terms “ontology” and “classification scheme” would have led to

an excessive number of search results that would have been infeasible to
handle4
.
Once the search string was designed, we selected the primary sources to
search for relevant studies. Scopus5

, Compendex/Inspec6 and Web of Sci-
ence7 were selected because they cover most of the important SE databases,

such as IEEE, Springer, ACM and Elsevier. In addition, the selected pri-
mary sources are able to handle advanced queries. The search string was

applied on meta data (i.e. title, abstract and author keywords) in August
2014 on the selected data sources. Table 4.1 presents the number of search
results for each data source.


The selection of primary studies was conducted using a two-stage screen-
ing procedure. In the first stage, only the abstracts and titles of the studies

were considered. In the second stage, the full texts were read. Note that we
used in both stages an inclusive approach to avoid premature exclusion
of studies, i.e. if there was doubt about a study, such a study was to be
included.
For the first stage (level-1 screening), the total number of 1371 studies
were equally divided between the two first authors. As a result, 471 studies
were judged as potentially relevant.

To increase the reliability of the level-1 screening result, the fourth au-
thor screened a random sample of 11.37% (78 studies) from the studies

screened by the first author and the third author screened a random sam-
ple of 11.38% (78 studies) from the studies screened by the second author.

It means that 22.75% of the 1371 studies were screened by two different
reviewers. The first and fourth authors had the same judgment for 91%
(71) of the studies. The second and third authors had the same judgment
for 93.6% (73) of the studies.

To evaluate the reliability of the inter-rate agreement between the au-
thors, we calculated the Cohen’s kappa coefficient [45]. The Cohen’s kappa

coefficient between the first and fourth authors was statistically significant
(significance level = 0.05) and equal to 0.801. The Cohen’s kappa coefficient

between the second and third authors was also statistically significant (sig-
nificance level = 0.05) and equal to 0.857. According to Fleiss et al. [45],

Cohen’s kappa coefficient values above 0.75 mean excellent level of agree-
ment.

The level-2 screening (second stage), performed by the first and second
authors, consisted on applying the selection criteria on the full-text of the

studies selected during the level-1 screening. The total number of 471 stud-
ies were equally divided between the first two authors. As a result, 254

studies were judged as relevant.
To increase the reliability of the level-2 screening, a two-step validation
was performed, as follows:

68

4

1. The first author screened 55% (70) of the studies deemed as relevant
by the second author during the level-2 screening (randomly selected)
and vice-versa. No disagreements were found between the authors.

2. Seven studies were randomly selected from each of the two sets allo-
cated to the first two authors for further validation. The third author

applied the study selection process on these 14 studies (about 5%
of 254) for validation purposes. No disagreements were found with
respect to the study selection (i.e. include/exclude) decisions.
During the entire screening process (stages 1 and 2), we tracked the
reason for each exclusion, as presented in Table 4.2.
4.4 Extraction process
The extraction process employed in this work is summarized in Figure 4.4
and consists of four main steps: Define a classification scheme, define an
extraction form, extract data, and validate the extracted data.

Figure 4.4: Extraction process.

The classification scheme was designed following the guidelines by Pe-
tersen [106]. It has the following facets:

• Research type – This facet is used to distinguish between different
types of studies (adapted from Wieringa et al. [151]).

– Evaluation research – A study that reports a taxonomy imple-
mented in practice, i.e. evaluation in a real environment, in gen-
eral by means of the case study method.

69

– Validation research – A study that reports a taxonomy that was
not implemented in practice yet, although it was validated in
laboratory environment, in general by means of experiment.
– Solution proposal – A study that reports a taxonomy that was
neither implemented in practice nor validated although it is
supported by a small example (illustration) or a good line of
argumentation.
– Philosophical paper – A study that reports a new taxonomy that
has no type of evaluation, validation or illustration.
• SE knowledge area – This facet is used to distinguish between the
SE knowledge areas in which taxonomies have been proposed. The

categories of this facet follow the SWEBOK [19]: software require-
ments, software design, software construction, software testing, soft-
ware maintenance, software configuration management, software en-
gineering management, software engineering process, software engi-
neering models and methods, software quality, software engineering

professional practice and software engineering economics.
• Presentation approach – This facet is used to classify the studies
according to the overall approach used to present a taxonomy: textual
and graphical, respectively.
For the data extraction, the relevant studies (254) were equally divided
between the first and second authors. For each paper, data was collected
in a spreadsheet using the data extraction form shown in Table 4.3.
To increase the reliability of the extracted data, a two-step validation was
performed, as follows:
1. The first author independently re-extracted the data of 55% (70) of

the studies originally extracted by the second author (randomly se-
lected) and vice-versa. Five disagreements were identified and all of

them were related to the item “classification structure”.
2. Fourteen studies were randomly selected from the studies originally

extracted by the first and second authors (7 studies from each au-
thor). Those studies were independently re-extracted by the third

author. Twenty one disagreements were identified; 2 on the “taxon-
omy purpose”, 9 on “classification structure”, 2 on “classification

procedure type”, 3 on “classification procedure description” and 5
on “validation approach”.

All disagreements except for “classification structure” were easily re-
solved. We believe that the high level of disagreement on the item “clas-
sification structure” was due to the fact that none of the studies explic-
itly stated and motivated the employed classification structure, which de-
manded the inference of such data from the text in each paper.

70

4

Table 4.3: Data extraction form

Data item(s) Description
Citation
data Title, author(s), year and publication venue
Taxonomy
definition Definition of taxonomy that is used or referred to
Purpose Text that states the purpose for the taxonomy
Purpose
keyword

To improve the reliability of the extracted data, we decided to re-screen
all 254 papers, focusing only on the item “classification structure”. In this
process 52 papers were re-screened together by the three first authors and
202 were re-screened together only by the first and second authors.
First, we discussed classification structures in detail (based on Kwasnik
[79]) to come to a common understanding of the terms. Thus, three of
us did an independent re-assessment of the classification structure of 52
papers. As a result, we reached full agreement on 50 papers (3 identical
results) and partial agreement on 2 papers (2 reviewers agreeing). There
were no primary studies without full or partial agreement.

After a discussion of the results, the remaining 202 studies were re-
assessed by the first and second authors. As a result, the first and second

authors reached agreement on 190 papers. The remaining 12 papers were

independently re-assessed by the third author, who did not know the re-
sults from the other two reviewers. In the end, full agreement was achieved

for 50 studies and partial agreement was achieved for 204 studies.

During the re-assessment of the primary studies, 10 studies were ex-
cluded because they do not present taxonomies, reducing the final number

of primary studies to 2448

(see Table 4.2).

4.5 Analysis process

Figure 4.5 presents the analysis process conducted herein. First, we classi-
fied the extracted data using the scheme defined in Subsection 4.4. This led

to the results detailed in Section 5. We also performed a quantitative anal-
ysis of the extracted data to answer the research questions of this paper.

Finally, the overall result of the data analysis (see Section 5), along with
information from additional literature ([150, 50, 79]), was used to revise
an existing method previously proposed to design SE taxonomies [11], as
detailed in Section 6.
5 results and discussion
In this section, we describe the results of the mapping study reported
herein, which are based on the data extracted from 244 papers reporting
245 taxonomies (one paper presented two taxonomies). The percentages in
Sections 5.1 and 5.7 reflect the number of papers (244). The percentages in
all other subsections reflect the number of taxonomies (245).
Workshop papers total 32

75

(a) Software Construction (b) Software Design

(c) Software Requirements (d) Software Maintenance

(e) Software Testing (f) Software Quality

(g) SE Models and methods (h) SE process
Figure 4.7: Yearly distribution of primary studies by knowledge areas. Horizontal
axes represent the years (starting 1987), while vertical axes denote the
number of taxonomies.
5.2 Classification scheme results
In this section, we present the results corresponding to the three facets
of the classification scheme described in Section 4, i.e. SE knowledge area
(KA), research type and presentation approach.
76

4

The vertical axis in Figure 4.8 depicts the SE knowledge areas in which
taxonomies have been proposed. Construction and design are the leading

SE knowledge areas with 52 (21.22%) and 50 (20.41% ) taxonomies, respec-
tively. These are relatively mature SE fields with a large body of knowledge

and a high number of subareas.

Figure 4.8: Systematic map – knowledge area vs research type.
A high number of taxonomies have also been proposed in the knowledge
areas requirements (38 – 15.51%), maintenance (31 – 12.65%) and testing
(21 – 8.57%). Few taxonomies have been proposed in economics (3 – 1.14%)

77

and professional practice (2 – 0.76%), which are more recent knowledge
areas.
The results show that most SE taxonomies (78%) are proposed in the

knowledge areas requirements, design, construction, testing and mainte-
nance, which correspond to the main activities in a typical software devel-
opment process [110].

The horizontal axis in Figure 4.8 shows the distribution of taxonomies by

research types, according to Wieringa et al. [151]. Most taxonomies are re-
ported in papers that are classified as “solution proposals” (120 – 48.98%),

wherein the authors propose a taxonomy and explain or apply it with the
help of an illustration. Eighty three taxonomies (33.88%) are reported in
“philosophical papers”, wherein authors propose a taxonomy, but do not
provide any kind of validation, evaluation or illustration. Relatively fewer

taxonomies are reported in “evaluation paper” (32 – 13.06%) and “vali-
dation paper” (10 – 4.08%).

Figure 4.8 also depicts the classification of the taxonomies using 2 as-
pects of the classification scheme, i.e. SE knowledge area and research type.

Taxonomies in the knowledge areas construction and design are mostly
reported either as solution proposals (construction – 27; design – 31) or

philosophical papers (construction – 19; design – 16). There are few tax-
onomies in design and construction that are reported as evaluation papers.

Three taxonomies are reported as validation papers in the KA construction,
while no taxonomy is reported as a validation paper.

Taxonomies in the knowledge areas requirements, maintenance and test-
ing are better distributed across different research types, wherein besides

the solution proposal and the philosophical research types, a reasonable
percentage of taxonomies are reported as evaluation or validation papers.
The horizontal axis in Figure 4.9 shows the distribution of taxonomies by
presentation approach. Most taxonomies (57.96%) are presented purely as
text or table, while 42.04% of the taxonomies are presented through some
graphical notation in combination with text.
Figure 4.9 also displays the classification of the identified taxonomies in
terms of SE knowledge area and presentation approach. The results show
2 different trends:
• For knowledge areas such as design, quality, models and methods,

and process, both textual and graphical approaches are used an al-
most equal number of times. This suggests that the taxonomies in the

KAs that involve a lot of modeling might be better presented using
graphical modeling approaches.
• Most taxonomies in construction (35 out of 52), maintenance (22 out
of 31), testing (17 out of 21) and software management (6 out of 6)
are textually presented.

5.3 RQ1 – Taxonomy understanding
We extracted data about the following 2 aspects to answer RQ1:
• Taxonomy definition: We investigated from each study whether or

not the authors made any attempt to communicate their understand-
ing about the concept of taxonomy by citing or presenting any defi-

nition of it.
• Taxonomy purpose: We identified from each study the stated (if any)
main reason for designing a taxonomy.
The results show that only 5.7% (14) of the taxonomies were reported
with a definition for the term “taxonomy”. Out of these 14 taxonomies, 3
use the Cambridge dictionary’s definition (see Section 2), 7 studies do not

provide an explicit source and the remaining 4 have other unique refer-
ences: The American heritage dictionary9

, Carl Linnaeus [84], Whatis10
and the IEEE standard taxonomy for SE standards. For the remaining
94.3% (231) taxonomies, no definition of “taxonomy” was provided.
To identify the purpose of each taxonomy, we extracted the relevant

text, referred here as purpose descriptions, from each of the primary stud-
ies. Using a process similar to open coding [47, 49], we coded these de-
scriptions. These codes are the keywords that the primary studies’ authors

themselves used in purpose descriptions to describe the taxonomy’s pur-
pose.

Table 4.5 lists the taxonomy purpose codes. The results show that 48.57%

of the taxonomies are designed to classify different subject matters, fol-
lowed by categorize (18 – 7.35%), characterize (15 – 6.12%) and identify

(12 – 4.9%) as purposes. All unique keywords (i.e. with frequency equal to
1) are grouped as “Other” in Table 4.5. Some examples for these unique
purpose keywords include support, explore, overview, capture and catalog.
We could not identify purpose descriptions associated with 16 taxonomies
(6.53%).
Discussion
Taxonomy is a non-trivial concept that has a defined meaning. Some of
its definitions are listed in section 2. In SE discipline, a large number of
taxonomies have been proposed, but in only a handful of the cases (5.7%)
the authors make any effort to clarify their understanding and perspective
of the term “taxonomy”.

Going beyond the citation or use of some definition for the term “tax-
onomy”, we also attempted to extract the taxonomy purpose from each

the primary purpose of any taxonomy is to classify a subject matter. Nev-
ertheless, we found that about 56% of the taxonomies are designed to

classify (48.57%) or categorize (7.35%) a subject matter, while remaining
taxonomies (44%) are designed to achieve some other purpose (e.g. define
and understand), although they do classify some subject matter, i.e. they
employ some type of classification structure.
5.4 RQ2 – Subject matter
In total, we identified 240 unique subject matters11 for the 245 taxonomies,
e.g. technical debt, architectural constraints, usability requirements, testing
techniques and process models.
Discussion

The high number of unique subject matters means that almost each taxon-
omy dealt with a unique subject matter. This might be due to the following

reasons:
• Existing taxonomies fit their purpose well. Therefore there is no need
to define competing taxonomies.
11 See https://drive.google.com/open?id=0B2kvKPmJJREDblQwWDBfbkNOeUk for the full list.

81

• The subject matters for existing taxonomies are so narrowly defined
that they are not suitable for usage outside their original context.
New taxonomies are therefore developed constantly.
• SE researchers do not evaluate existing taxonomies whenever there
is need for organizing SE knowledge, but rather propose new ones.
One indicator for taxonomy use is the number of times each primary
study is cited. This analysis is detailed in Subsection 5.7.
The list of subject matters contains mainly technical aspects of SE. Only

few taxonomies deal with people-related subject matters, e.g. stakeholder-
related and privacy-related.

5.5 RQ3 – Validation of taxonomies
Table 4.6 displays the approaches used to validate taxonomies. Illustration
is the most frequently used approach to validate taxonomies (112 – 45.7%).
Illustration includes approaches such as example, scenario and case.

Case studies have also been used to validate 32 taxonomies (13.1%). Ex-
periments have been used to validate 10 taxonomies, while few taxonomies



Discussion
The results related to RQ3 show that very few taxonomies are validated
through methods like case study or experiment, and a large number of

taxonomies (33.9%) have not been validated by any means. We do not be-
lieve that one particular validation approach would be best for all contexts;

however we believe that in most cases would not be enough just to pro-
pose a taxonomy. The researchers should make an effort to select and use

82

4

a suitable approach either to demonstrate the taxonomy with the help of
an illustration or to validate the taxonomy using formal methods, such as
case study.
5.6 RQ4 – Classification structure
To answer RQ4, the following data was gathered: classification structure,

descriptive bases, classification procedure and classification procedure de-
scription.

Table 4.9 presents the classification procedure types for the identified

taxonomies. The majority of the taxonomies employed a qualitative classi-
fication procedure (238 – 97.14%), followed by quantitative (5 – 2.04%) and

Table 4.10 displays the status of the taxonomies’ classification proce-
dure description. The majority of the taxonomies do not have an explicit

description for the classification procedure (122 – 86.53%) and only 33 tax-
onomies (13.47%) have an explicit description.

Table 4.10: Classification procedure descriptions.
Procedure description F %
Not explicitly described 212 86.53
Explicitly described 33 13.47

Discussion

The results clearly indicate that hierarchical classification structures (hierar-
chy, tree and paradigm) are the ones mostly used to design SE taxonomies.

This is not surprising, since taxonomies were originally designed using
hierarchy as classification structure (see Introduction).
Nevertheless, SE is considered as a young discipline that is in constant
and fast evolution. For this reason, the existing knowledge is sometimes
incomplete or unstable. This is probably the reason faceted analysis is also
frequently employed as classification structure to design SE taxonomies;

such classification structure is the one that is most appropriate for knowl-
edge fields with incomplete and unstable knowledge [79]. It was interest-
ing to observe that in almost all studies the choice to employ a specific

classification structure is not explicitly motivated.
The descriptive bases are mandatory for a reasonable understanding of

a taxonomy (see 2). The absence of such element can hinder the adop-
tion of taxonomies. Not surprisingly, the 24 studies that did not provide

descriptive bases are lowly cited, it might indicate the low usefulness of
them.
At first glance, it appears that the main reason qualitative classification
procedures are more popular is the fact that such approach is easier to

84

4

be designed; the classification using such procedure type is performed
using nominal scales. However, such approach is harder to be applied,
because it can leave room for ambiguities; it is not possible to establish the
differences/similarities between classes because distance functions cannot
be used to calculate the difference/similarity degree between classes [150].
Quantitative approaches are harder to be designed, but they are easier to
be applied to classify subjects, since it is possible to use distance functions
to identify the differences/similarities between classes [150].
To enable accurate subject classifications, the classification procedure of
a taxonomy must be described sufficiently. Only 13.47% of the taxonomies

(33) have an explicit description for their classification procedure. The re-
maining 86.53% of the taxonomies (212) are probably harder to be used by

other people than the designers of the taxonomies themselves.
5.7 RQ5 – Taxonomy usage level
As an approximation for the usage level of SE taxonomies, we looked at the
number of citations for each included primary study12. Since a publication
may be cited due to many different reasons, the number of citations can
only be an indication of actual use of the proposed taxonomy. However,
the number of citations show whether there is an interest in a subject.
Sixty one primary studies (25%) have been cited > 50 times (see Table
4.11). There are only 16 studies that were never cited. Out of these 16
primary studies, 7 were published between 2012 and 2014.
Table 4.11: Number of primary studies (Frequency) by number of citations.

Figure 4.10 shows the distribution of average number of citations per
year for the primary studies13. Most studies were cited 1–5 times per year
on average (116 – 47.5%).

Table 4.12 lists the mean and median number of citations for the knowl-
edge areas with at least 10 taxonomies. The data shows that for all knowl-
edge areas, except for process, the mean value is much higher than the

median, which is due to few studies with very high numbers of citations
(outliers); e.g. construction has a study with 1440 citations. Maintenance
has fewer taxonomies (38), as construction (52) and design (50), but it has
the highest median for the number of citations.

Figure 4.10: Distribution of average number of citations per year. The x-axis shows
ranges of average citations per year. The y-axis shows the number of
primary studies.

Discussion

The results show that most studies reporting taxonomies are cited a rea-
sonable number of times with many primary studies that are highly cited

in all knowledge areas. This indicates an interest in taxonomies by the SE
community. The high number of unique subject matters (see Subsection
5.4) might be due to the breadth of the SE field and a need for classifying
new and evolving SE subareas.
We also observed that some recently proposed taxonomies (2012–2014)
were already cited many times. These results highlight that new taxonomies
are proposed continuously and cited frequently. This indicates that the
classification of SE knowledge through taxonomies is a relevant topic.

13 Average is computed by dividing citation count of a study by number of years from publica-
tion date to 2014.

86

4

A more thorough analysis of the actual usage of the cited primary stud-
ies would be necessary though, to understand the contexts and reasons for

taxonomy usage.

6 a revised taxonomy design method
A method to support the design of SE taxonomies was recently proposed
by Bayona-Oré et al. [11], which consists of 5 phases and 24 activities (see

Tables 4.13 and 4.14). Bayona-Oré et al. based their method on an aggrega-
tion of the steps suggested in 9 sources and used it to design a taxonomy

of critical factors for software process deployment.
Bayona-Oré et al.’s method includes many activities that are important
for systematically designing SE taxonomies. However, their method has
some issues, as follows:
• Issue 1 – Six out of nine sources on which Bayona-Oré et al.’s method
is based are either gray literature or are not available in English.
• Issue 2 – The proposed method is a simple aggregation of all the
activities suggested in the nine sources. Some of these activities are
only used in one of the peer-reviewed sources.
• Issue 3 – The scope of some activities of this method goes beyond

taxonomy design. It covers steps related to project planning, recruit-
ment, training and deployment. The method also seems to be tar-
geted towards committee efforts.

of the taxonomy Combination A2 and A4
