In this chapter we presented a novel strategy to combine sequences of multiple runs,

generated by different sensor configurations, to produce a 3D reconstruction of a street-
side scene. The sequences are initially aligned using landmark-feature correspondences

discovered among keyframes. A nonlinear adjustment is then carried out to optimise
the poses of matched keyframes. Once aligned, the 3D data of the scene structure, from
each sequence, is processed and integrated in a consistent coordinate system, resulting
in a comprehensive reconstruction of the street-side scene. The work presented in this thesis can be further improved and extended toward a
number of directions. The unifying VO framework, introduced in Chapter 4, has been
tested using binocular stereo vision as well as monocular-LiDAR configurations. The
model can be extended in a straightforward way to a trinocular setup which provides
some interesting options in terms of feature tracking topology. The third camera also
provides an additional source for verification of the solved egomotion (Chien, Geng &
Klette, 2014). A calibration tool for trinocular setup is readily deployed, as tested in
Appendix A. We noticed that many buildings in reconstructed scenes are not yet complete (i.e.
their 3D surface). Perhaps such missing pieces can be filled by using a drone flying
around such buildings. If this provides an additional monocular sequence in this case,
the sequence aligning pipeline, introduced in Section 5.4, needs to be reworked to
consider the scale of the monocular reconstruction as an unknown factor, meaning that
the solution to the alignment is generalised from a 6-dof Euclidean transformation to a
7-dof similar one.
It is interesting to study the registration of sequences collected from opposite
directions, which remains challenging. In Fig. 6.1, two frames acquired at the same
place but in very different viewing directions, failed to be associated using the BoW
model described in Section 5.3.2. The issue might be addressed using features in 3D
space to establish an initial alignment (Zeng & Klette, 2013), given that the estimated
camera poses are accurate to avoid distorted point clouds integrated over multiple
frames.
Applying the proposed integration technique to more than just two sequences also

poses new issues that are worth studying. For many-to-many alignments, the keyframe-
based search scheme, adopted in Section 5.3.2, would be less efficient. An incremental

computational-efficient approach needs to be developed. Identification and removal of
temporal objects (e.g. parked cars), that are not persistent in multiple sequences, is also
required to achieve consistent street-side reconstruction.

In many applications it is desired to access high-level information from a reconstruc-
tion for better scene understanding (Badino, Franke & Pfeiffer, 2009). In that case, the

scene can be modelled at a higher level, based on aligned point clouds and recovered
surfaces (N. Savinov & Pollefeys, 2016).
Previous WNN model was used to forecast significant wave heights by
only training the observed wave data at a single location, showing lower
accuracy at longer lead times probably due to the lack of consideration of
the relationship between spatially distributed meteorological variables
and waves. In this study, an EOFWNN model was developed to take this
relationship into account by combining the EOF analysis and wavelet
analysis with the neural network and by using the observed wave data at
multiple locations and the past and future meteorological data in the
surrounding area including the wave observation stations. The developed
model is also able to forecast the wave heights at multiple locations
simultaneously, and thus it is more convenient and more accurate than
the previous WNN model that was developed for each location
separately.
The developed model was employed to forecast significant wave
heights for various lead times at eight stations in the coastal waters
around the East/Japan Sea. The model results at Gangneung, Sakata and
Aomori were compared with those of the WNN model depending on

different input conditions: only wave data or both wave and the meteo-
rological data. The WNN model using both data showed bad perfor-
mance, because the ANN cannot find the relationship between the

spatially distributed meteorological variables and waves. When only the
wave data are used, the two models showed similar performance, with
slightly better accuracy by the WNN model. The values of NRMSE for the
WNN model for 24-hr lead time were between 0.140 and 0.289 and those
for the EOFWNN model were between 0.155 and 0.263. Even though all
the wave observation stations are located in the East/Japan Sea, they are
far from each other. Therefore, using the individual wave data at each
station gives better performance than using all the wave data at multiple
locations for longer lead times. On the other hand, the EOFWNN model

using both wave and meteorological data showed much better perfor-
mance than the WNN model using only wave data for all the lead times

regardless of the wavelet decomposition level. Even at the 3rd decom-
position level, the NRMSE values for the EOFWNN model were under

0.226 for longer lead times, while those for the WNN model were under
0.446.
The high accuracy of the EOFWNN model is attributed to considering
the effect of spatially distributed meteorological variables by the EOF
analysis. The EOFWNN model is better than the WNN model in that the
former shows higher accuracy for longer lead times regardless of the

decomposition level and that it forecasts the wave heights at multiple
locations together. In short, the EOFWNN model can be a promising tool
for forecasting the significant wave heights at multiple locations for a
relatively long lead time with high accuracy. Lastly, it should be
mentioned that the model showed lower accuracy at the Aomori station,
which is located inside a bay and thus in which the wave height is small
compared with other stations. It may be desirable to use the model for the
wave stations which are located in open coasts so that the magnitudes of
wave height are similar to one another.
This thesis introduced new real-time visual flow algorithms for robotic applications. The first
example of such algorithms is an optical flow filter capable of running up to 800 Hz at 640×480
image resolution. An important aspect of the algorithm is that of developing predictors for
image and optical flow to propagate current state estimates forward in time. This is achieved by
modeling the evolution of the image and optical flow as systems of partial differential equations
which is then solved numerically on GPU.

Moreover, a new type of visual flow named structure flow is introduced. Intuitively, struc-
ture flow is the three-dimensional vector field describing the velocity of the environment rel-
ative to the camera as seen by the camera. Geometrically, it is the relative 3D velocity of the

environment sampled at each pixel and divided by the inverse distance, and it is measured in
radians per second. Structure flow is a generalization of optical flow as it contains information
about the motion in the normal direction of the camera. Partial differential equations are used
to model the spatio-temporal evolution of the structure flow and associated image brightness
and depth fields. These equations can be used to create predictors to propagate such quantities
forward in time, as well as to use them to estimate the underlying flow from image and depth
measurements.

To implement the structure flow algorithm, the Spherepix was developed to efficiently rep-
resent data on the sphere. Pixels on Spherepix images are arranged such that locally they

approximately satisfy the properties of equidistance and orthogonality. These properties are
fundamental to achieve efficient implementation of low-level image processing routines such
as blurring and gradient computation. Other algorithms such as SIFT feature point extraction
and optical flow can be efficiently implemented on spherepix. The following is a summary of the achievements of this thesis:

• Optical flow filter: An open-source filtering algorithm for the computation of opti-
cal flow has been formulated and developed. The algorithm is formulated as update-
prediction blocks. In particular, the prediction stage of the algorithm uses the partial

differential equations that model the spatio-temporal evolution of image brightness and

optical flow to create predictions of such quantities at future times using current es-
timates. A fast numerical scheme based in finite-difference methods is proposed and implemented on GPU hardware. The algorithm reaches frame rates in the order of 800
Hz for 640 × 480 video sequences (4 pix. max. flow) and 500 Hz for 1016 × 544 image
resolution (4 pix. max. flow).
• Structure flow: This thesis introduced the structure flow field as the three-dimensional
scene flow divided by the depth of the scene. Partial differential equations to model
the spatio-temporal evolution of the structure flow field, image brightness, depth and
inverse depth were formulated on spherical images. These PDEs can be used both for
developing prediction algorithms to compute future values of the fields, or to estimate
the structure flow from brightness and depth measurements.
• Structure flow filter: An algorithm for the computation of structure flow is proposed.
The algorithm follows the same filtering approach as the optical flow filter to develop
an algorithm capable of estimating the structure flow in real-time. The algorithm uses
image brightness and depth measurements to recover the underlying structure flow. The

algorithm is fully implemented on spherical images using the proposed PDEs for struc-
ture flow, image brightness and inverse depth.

• Spherepix data structure: Chapter 3 introduced the Spherepix data structure for effi-

cient implementation of low-level image operations on spherical images. Spherepix con-
sists of a set of regularized grids on which application of low-level image operators such

as Gaussian blurring or gradient computation have the same computational complex-
ity as their implementation on standard perspective images. Spherepix is the numerical

back-end used for the real-time implementation of the structure flow algorithm.