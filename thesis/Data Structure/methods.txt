This chapter presents the fundamental concepts and techniques of data compression
in general and image and video compression in particular. The terminologies and
notations used throughout the thesis are also defined in this chapter. Besides, the
chapter provides a detailed reviewof depthmap coding techniques. More specifically,
the organisation of the chapter is as follows. In Section 2.1, the key components of
state-of-the-art image and video coding frameworks are presented. The concept of
block-based motion compensated predictive coding, the underpinning framework of
current video coding standards, is detailed in Section 2.2. In Section 2.3, lossless image
coding techniques and standards are briefly reviewed. The essential concepts and
techniques of depth map coding are discussed in Section 2.4 and 2.5. A culmination
of the recent development in video texture and depth map coding is 3D-HEVC, the
current standard for 3D and multi-view video coding. In Section 2.6, an overview of
the 3D HEVC architecture is presented. Finally, the chapter concludes in Section 2.7.

Image and video compression technologies have become an integral part of the way
visual information is stored and transmitted. Image and videos are characterised
by high volume of data. For example, an uncompressed colour image of resolution
1920×1080 needs approximately 47Mb of information. One second of an uncompressed
video of resolution 1920×1080 having a frame rate of 30 frames per second
(fps) requires approximately 1.5Gb of storage. Thus, the prohibitive volume of uncompressed
image and video data demands that they must be compressed before
storage and transmission.

The feasibility of image and video compression relies on exploiting various kinds
of redundancies that exist in them. Statistical analysis reveals that there exists strong
correlation among the neighbouring pixels in natural images. Besides the spatial correlation,
video frames are characterised by strong temporal correlation which refers to
the fact that successive video frames have great deal of similarity. While compression
of still images relies on exploiting spatial correlation, for effective video compression
both spatial and temporal correlation must be exploited.
Although, truly lossless compression, where decoded frame is mathematically
identical to the original frame, is desirable, the compression ratio that can be achieved
with lossless compression is extremely limited. Therefore, current image and video
coding standards have been proposed in the lossy framework that achieve significant
compression at the expense of some loss of information. The lossy coding schemes essentially
exploit psychovisual redundancy, i.e., insensitivity of human visual system
to certain loss of spatio-temporal visual information. However, many applications
such as archiving master copies of digital movies and capturing of medical videos,
require truly lossless compression technique to preserve the content with absolute fidelity.
The final stage of any image or video compression scheme is entropy coding. The
essential purpose of entropy coding is to exploit the statistical correlation that exist
after the removal of spatial, temporal, and psychovisual redundancy.

Essentially there are two approaches to spatial redundancy reduction: DPCM and
transform coding.

Differential pulse code modulation (DPCM): DPCM is a simple predictive coding
approach where an image or frame of a video is encoded in a predefined order,
such as, raster scan order. The intensity value of a pixel is predicted based on the previously
encoded neighbouring pixels. Then the prediction error, i.e., the difference e
between the actual value x and the predicted value ˆ x, is encoded. The decoder makes
the same prediction ˆ x and adds the prediction error e to reconstruct the original pixel
intensity as x = ˆ x+e. Prediction de-correlates the data in the sense that it significantly
lowers the zero-order entropy which can be exploited by an entropy coder discussed
in Section 2.1.3. The prediction error e can be quantised by a factor of Q to reduce
the bit rate further at the expense of some loss of the information. A typical DPCM
encoder is depicted in Figure 2.1

Block-based transform coding: In this approach, the image or video frame is first
transformed into spatial frequency domain. After transformation, most of the energy
in natural images are concentrated into a few low frequency components. Since human
eye is less sensitive to distortions in high frequency components, they can be
discarded by coarse quantisation without significant degradation in the visual quality
of the reconstructed images.

The most popular and well-established transform technique is Discrete Cosine
Transformation (DCT) [45]. The DCT is applied as a block based approach usually on
blocks of size 4×4 or 8×8 pixels. For video compression, transform coding strategies
are usually combined with the motion compensated prediction into a hybrid motion
compensated transform approach to achieve very efficient encoding of video, which
is discussed in Section 2.2.

Quantisation is a technique of representing the possible output from a source using
a much smaller set to reduce the bit rate. A quantiser is consists of two mappings:
encoding and decoding. The quantiser divides the range of a source into a finite number
of intervals and associate a codeword and a reconstruction value with each of
the intervals. Given an output from a source, the encoding operation is to map it to
the interval in which it belongs and represent it using the corresponding codeword.
Then, the decoding operation is to find the interval associated with the codeword and
use the corresponding reconstruction value as the output. Since, in quantization an
entire interval is represented with a single value, the process of quantisation is inherently
lossy. While a scalar quanitser maps a scalar value, vector quantiser refers to the
application of quantisation to multi-dimensional input.

The bound on the minimum average code-length for a symbol is presented using
Equation 2.4 and Huffman coding provides an algorithm to achieve it. In [46], Huffman
coding was presented which is a simple algorithmto construct a prefix code for a
given random variable. Huffman’s algorithm is based on two observations regarding
the optimal prefix codes. In an optimal prefix code (a) highly probable symbols will
have shorter codewords than those of less probable symbols; and (b) codewords for
the two least probable symbols will have the same length and will differ only in last
bit. Algorithmic description of the Huffman code is available in [177].

Limitations: Huffman code is only optimal if the source statistics is given. However,
in practice source statistics are rearly known in advance and also change with
time. Variable length coding schemes (i.e., Huffman coding) share the fundamental
disadvantage that assigning a codeword containing an integral number of bits to each
symbol is sub-optimal, since the optimal number of bits for a symbol depends on the
information content and is usually a fractional number. Compression efficiency of
variable length codes is particularly poor for symbols with probabilities greater than
0.5 as the best that can be achieved is to represent these symbolswith a single-bit code.

Run-length encoding (RLE) is a simple and effective method of reducing redundancies
in a sequence. It can be used to efficiently represent long strings of identical values
by grouping them into a single symbol which codes the value and the number of repetitions.
RLE is suited for compressing any type of data regardless of its information
content, but the content of the data will affect the compression ratio achieved by RLE.

Group of pictures (GOP): In video coding, GOP structure specifies the order in
which intra- and inter-frames are arranged. The GOP is a group of successive pictures
within a coded video stream. Each coded video stream consists of successive GOPs.
Slice: Slice are a sequence of blocks that are processed in the order of a raster scan.
Mainly for parallel processing they are considered, as slices are self contained and
independent.

Figure 2.3 outlines the structure of a video sequence. The major components of
block-based video coding (intra prediction, motion estimation, transformation, quantisation,
and entropy coding) are elaborated in following sections.

The strategy for predicting motion of objects in video sequences is vital for achieving
high compression gains. The most established and implemented strategy is the blockbased
motion compensation technique. Block-based motion compensation strategy is
based on motion vector estimation which is outlined in this section. Images are separated
into disjoint blocks of pixels. The motion of a block within a frame is estimated
and described by only one motion vector (MV). It assumes that all pixels have the
same displacement. The higher the precision of the MVs, the better the prediction.
The bit-rate overhead for sending theMV information to the receiver needs to be well
balanced with the gain achieved by the motion compensated prediction.

CAAC is a method of arithmetic coding in which the probability models are updated
based on previous coding statistics and uses local spatial and/or temporal characteristics
to estimate the probability of a symbol to be encoded. It is performed in two
separate phases. The first phase aims to efficiently estimate the source statistics. The
second phase utilises arithmetic coding to represent symbols with high probability
of the occurrence with fewer bits than symbols with low probability of occurrence.
Multimedia content generally shows high amount of correlation between the samples
used for its digital representation, therefore, CAAC is usually applied to encode multimedia
content in combination with other compression methods in order to achieve
high compression ratio.

High Efficiency Video Coding (HEVC) [15, 39], also known as H.265 and MPEG-H
is a video compression standard, one of several potential successors to the widely
used H.264/MPEG-4 AVC. In comparison to AVC, HEVC offers about double the data
compression ratio at the same level of video quality, or substantially improved video
quality at the same bit-rate. The first version of HEVC was published in June 2013
and the second version was published in early 2015. Additional 3D-HEVC extensions
for 3D video were completed in early 2015. HEVC is actually based on the same
hybrid spatial-temporal prediction system as its predecessor H.264/AVC. Currently
extensions of HEVC are in research stage to provide encoding support for videos containing
rendered graphics, text, or animation.

The context adaptive lossless image compression (CALIC) scheme is a lossless image
compression scheme proposed byWu and Memon in 1994 [29,180]. This scheme uses
both context and prediction of the pixel values. The CALIC scheme actually functions
in twomodes, one for grayscale images and another for bi-level images

Although, quantisation of local gradients reduces the number of contexts, the alphabet
size can still significantly contribute to the overall model cost, especially when
context size is large. For a n-bit image, the probabilities of 2n different values in each
context need to be estimated. Thus, even for 8-bit images, the alphabet size is considered
very large. Therefore, JPEG-LS uses a parametric distribution to approximate
the probability distribution of the residuals. More specifically, it uses the two-sided
geometric distribution (TSGD) [186], which can be specified by only two parameters,
to model the distribution of residuals. Thus, instead of estimating the probabilities
of all possible values of the residuals, it needs to estimate the values of only two parameters
to completely specify a particular TSGD [132]. JPEG-LS applies a run mode
where flat or smooth context region characterized by zero gradients is detected and
after encoding run of a symbol, JPEG-LS returns to the regular mode.

A depth map is an 8-bit grayscale image representing the distance information of 3D
points in the scene fromthe camera view-plane. Unlike regular texture images/videos,
depth maps are not directly visible to the viewer. Nevertheless, viewers can experience
the perception of depth thanks to the binocular vision. As their two frontal eyes
are laterally slightly apart, they capture two slightly different images from the 3D
scene where the relative horizontal position of the points on objects vary inversely to
their distance from the viewer. The closer the point from the viewer the wider would
be its horizontal shift in the images captured by two eyes. These relative horizontal
positional differences are referred to as disparities that are processed in the visual
cortex of the brain to effectively fuse the pair of images into a unified view, representing 
the viewpoint of the viewer in the global 3D coordinate system, which yields the
perception of depth

A depth map, therefore, is a digital representation of the 8-bit quantised disparity
values of 3D points projected onto the pixels of a unified view in a binocular vision
system. One of the primary applications of depth map is to create an illusion of depth
in a stereoscopic 3D viewing platform where two eyes receive separate images for
viewing. If the images are renderedwith appropriate disparity, the visual cortex of the
brain can be fooled to believe it is experiencing the perception of depth.

Figure 2.12 presents an example of view synthesis using DIBR. Here 3 views are
generated for 2 given texture and depth views of Newspapermulti-viewtest sequences.
In the figure, changes in views (real and synthesised), specially objects ( 2 persons)
slow movement are outline by red colour. An inherent problem of the described 3D
image warping algorithmis due to the fact that each 3D point is not necessarily visible
in both views. Consequently, due to the sharp discontinuities in the depth data (i.e.,
strong edges), the 3D image warping can expose areas of the scene that are occluded
in the reference view and become visible in the second view. To deal with these disocclusions
(holes), Zhan-wei et al. [142] proposedmerging two warped images fromtwo
different spatial or temporal viewpoints, where the pixels in reference images are processed
in an occlusion-compatible order. For hole-filling, the average filter has been
commonly used [143]. However, the average filter does not preserve edge information
of the interpolated area. Therefore, using an average filter results in obvious artifacts
in highly-textured areas

View synthesis reference software (VSRS): MPEG has developed the view synthesis
reference software (VSRS) [21, 22]. The VSRS allows synthesising any view between
the two input views and features advanced processing algorithms to reduce
synthesis errors due to noise or errors in the depth data. The VSRS uses both forward
and backward warping [19]: first, the depth map of the virtual view is estimated from
the input depth maps using forward warping, then the depth map obtained is used to
synthesise a virtual view from each input view (backward warping), and finally, the
two virtual views are merged and conveniently filled.

Edges in depth maps are extremely sensitive to coding errors, affecting the perceptual
quality of the synthesised views. Therefore, recent depth map coding algorithms
are motivated to preserve edges. The depth map coding techniques in the literature
have employed several techniques to preserve depth edges, either inherently
or explicitly. Moreover, by assuming significant correlation and similarity between
texture and depth maps, some techniques have proposed texture-assisted depth map
coding to utilise texture components (especially edges and motion information) in
depth map coding. To preserve depth edges, some techniques have tried to avoid traditional
transformation by proposing edge adaptive transformation or segmentation
based coding. Wavelet transformation and filtering based techniques have also been
proposed for efficient depth map coding. As depth maps influence the synthesis view
quality, some coding techniques have been proposedwith special focus onminimising
synthetic view distortion and texture-depth joint coding for rate distortion optimisation.
Moreover, to ensure 100% depth quality, efficient lossless compression schemes
have also been proposed. Depth map coding algorithms can be grouped into several
categories: those that (a) exploit the correlation with the corresponding texture signal;
(b) do not rely on inter-component correspondences; (c) preserve depth edges explicitly;
(d) perform lossy compression; (e) encode depth maps using lossless techniques,
etc. The order in which the existing works is discussed in the following sub-sections
is outlined in Figure 2.13.

All the above schemes used luminance and/or chrominance of the corresponding
texture image for edge detection but it is not always the case that an edge in
a texture image also reflects the corresponding depth as the inside of an object can
have different textures and variation which may not always be reflected in the depth
map. Moreover, explicitly preserved edges with straight lines can cause geometric
and shape errors in a synthetic view.

There are several methods [33,89–91] that took the advantage of shared motion information
(generated froma block-basedmotion search) between texture and depth data
in order to improve coding efficiency. Due to changes in lighting conditions, motion
compensation for texture differs fromthat for depth. While searching for a bestmatching
block, texture motion search can result in long motion vectors and large residual
values. On the other hand, a corresponding depth block is more similar to co-located
or neighbouring blocks in the reference frame. Thus, reuse of texture motion vectors
is not guaranteed to achieve optimal rate-distortion performance [2]

Smolic et al. [48] reported that depth information can be compressed very efficiently
when standard video codecs are used, and roughly 10%–20% of the bit-rate necessary
to encode texture video is usually enough to encode depth data of good quality.
However, it had been shown that theH.264MVC is not optimal for the compression of
multi-view depth video in terms of view synthesis performance [49]. Recent studies
show that optimal texture-depth rate partitioning is content-dependent [50] and influenced
by some key characteristics of the sequence, including the baseline distance
and contrast of neighbouring background/foreground regions [51]. Some solutions
for an optimal joint texture-depth bit-rate allocation had been proposed e.g. [52], in
which a synthesised view distortion estimation model was exploited to allocate the
rate properly. In [53], texture-depth motion encoding and an activity based rate allocation
strategy were proposed combinedly. Kim et al. [55, 56] and Zhang et al. [57]
proposed analysing the effects of depth compression on synthesised views in order to
improve depth map coding, and reported promising improvements in performances.
Recently, Yao et al. [171] proposed a depth map down-sampling and coding scheme
that minimises the view synthesis distortion.

Platelet based depth map coding [138,162] was proposed byMorvan et al. as a way of
preserving steep discontinuities in the depth signal. In this scheme, depth maps were
modelled using piecewise-linear functions (platelets) within blocks of variable sizes
defined using a quad-tree decomposition. This scheme was focused mainly on the
piecewise planar properties of depth maps and introduced a scheme to model depth
map with straight lines and regions.

Based on a standard H.264/AVC codec, Zamarin et al. [78] introduced a new intra
mode specifically targeted coding depth macro-blocks containing arbitrarily-shaped
edges. Three intramodeswere proposed to encodemacro-blocks (MB) that had edges,
where using a simple clustering algorithm the MBs were partitioned into two regions
according to the edge structure . Each region was approximated with a constant
value (EDGE Intra mode). Regions were identified through a binary mask that is losslessly
encoded bymeans of context-codingwith adaptive template selection. The intra
modes were able to exploit previously encoded edge MBs to predict the two constant
values (EDGE Inter CV mode) and the edge structure (EDGE Inter full mode) of the
current MB in order to further improve the compression gain. Figure 2.17 presents an
example of intra modes for edge modelling proposed by Zamarin et al. [78]. If a previously
encoded neighbouringMB is encoded with an intra edge mode, its binary mask
can be attached to the one of the current MBs being encoded, and the corresponding
context statistics can be exploited to initialise the context-based binary encoder. However,
the binary mask is defined for only an MB size of 16×16 pixels, so fine depth
edges cannot be preserved using this technique.

Graph based edge adaptive transforms (EATs) for depth map coding were proposed
by Shen et al. [59] for 4 × 4 residual blocks. The idea was to define a graph structure
among pixels (containing residual values) in edge blocks (indicated by an edge map),
so that the pixels on different sides of the edge were not connected. They used this
structure as the base for the transform. Any transformation can be applied to the pixel
values if designed appropriately. Shen et al. used eigenvectors of the Laplacian [155]
matrix on the graph since this provides a spectral decomposition of the signals on
the graph. The EATs avoided filtering across edges in each image block and so did
not create high coefficient values. This new method, when selectively chosen along
with the DCT, helped in decreasing the bit-rate. The edge map and transform mode
(EAT/DCT) were encoded using CABAC [27]. Since depth maps are not precisely
piecewise planar, platelet based representation always has a fixed approximation error.
On the other hand, this transformation has no approximation error as it can represent
signals that are not precisely piecewise planar. The method was subsequently
improved withmore efficient definition and coding of the graph structures (adjacency
matrix) [60]. In Figure 2.19, the new coding technique for an adjacency matrix [60] using
CABAC is elaborated

But the technique proposed by them, needed explicit calculation of eigenvectors which 
was not suitable for fast implementation.

Wavelet based depth map coding was also explored [68], including techniques
based on wavelet transforms on graphs [69]. The technique in [69] transmitted the
binary edge maps of depth images to the decoder and chose filters adaptively by minimising
a cost function. Transform coefficients were encoded by using the compression
method set partitioning in hierarchical trees (SPHIT) [176], and the edge maps by
the bi-level compression scheme JBIG.

The shape adaptive discrete wavelet transform (SA-DWT) [68] extrapolated pixels
on the other side of the edges by symmetric extension, whereas edges were encoded
by a differential Freeman chain code and bit-stream by the SPHIT. The adaptive
wavelet coding [160] applied shot filters around the edges of depth images and
long filters for other areas. In [157], the authors used a shape adaptive wavelet transformand
an explicit encoding of the locations ofmajor edges in order to encode depth
maps. The article presented a novel approach to generate small wavelet coefficients
along edges so as to reduce the bits required to code the depth map. However, those
transformation were applied to the entire image and were not easily amenable to
block-based processing. Kiani et al. [173] used a linear fractional model to capture
non-linearity of depth values in planar regions using geometrical wavelets, specially
for indoor depth images.

Figure 2.20 presents an example of the method [161] based on binary space partitions
tomesh and compress a depthmap simultaneously. Carmo et al. [164] developed
the technique in [161] by adding a depth analysis module and modifying the binary
tree encoding process for handling the split flags and the depth values.
Quad-tree decomposition for depth image coding was proposed in [162] (and already
has been discussed in 2.5.5.1 . Mesh based depth map coding using the hierarchical
decomposition of a depth map [163] decomposed a depth image into a layer
descriptor and disjoint images. Five types of modelling modes with mesh grids were
used to represent the edge regions, and the no-edge-regions were represented by feature
points. The layer descriptors were encoded by the H.264/AVC, and the disjoint
images were then merged together and also compressed by the H.264/AVC.

Since the CABAC [27] was originally designed for lossy texture coding, it was unable
to provide the optimum coding performance for lossless depth map coding. In [110],
Heo and Ho proposed an improved version of the arithmetic encoder of H.264/AVC
specifically targeted at coding depth data efficiently using a significance map where
a 0 and a 1 denoted zero and non-zero values, respectively. Their technique was devised
only for a 4×4 block size. In [28], the authors used large coefficients clipping
and limited codeword length Golomb-Rice to encode a depth map in the near-lossless
manner.

Cheung et al. [61] proposed a completely different approach introducing the concept
of a don’t care region: depth values were conveniently manipulated within a
suitable range to obtain a sparse depth representation and improve the coding performance
of a JPEG encoder without affecting the quality of synthesised views. Scalable
solutions for depth images had recently been proposed in order to improve the performance
of a JPEG2000 encoder around sharp discontinuities while maintaining its
scalable properties [70,71]; it was modified in [170] by higher order spatial inductions
to minimise quantisation errors during reconstruction.

Lossless compression of depth data has also been addressed in the literature. Lossless
coding ensures that no data loss occurs in the encoding/decoding process, allowing
for very precise processing operations at the decoder side. This can be of interest
not only in DIBR scenarios, but also when depth images are used for purposes such
as scene matting, object and face tracking. Recent results show that in contrast to
the case with natural images in which the average lossless compression factor is between
2 and 4, depth map compression can provide lossless compression factors of
50 and above for both still images and videos when specialised algorithms are used.
Kim et al. [72] proposed a bit-plane based coding method exploiting inter-bit-plane
redundancies and motion estimation. A scalable lossy-to-lossless solution based on
a cellular automata model was proposed by Cappellari et al. [73]. In the scheme, the
gray level depth image was converted into gray code and represented by bit-planes.
Inter-plane (i.e. between bit planes) correlations were utilised for prediction, and bitstreams
were produced by an arithmetic encoder.

Solutions based on amodified structure of theH.264/AVC CABAC [110]were also
proposed. Finally, a near-lossless and low-complexity scheme for Kinect-like depth
maps was presented in [74]. In a coding scenario for stereoscopic depth/disparity
maps, the main challenge is to exploit inter-view correlation in an effective manner.
In [79] a novel solution for lossless coding of stereo disparity maps was proposed.
The algorithmwas based on bit-plane decomposition, Gray coding, and context-based
arithmetic coding with an adaptive template selection. Gray coding was used to increase
the local correlation of bits in each bit-plane which favoured the context based
coding.

Motion-compensated prediction (MCP) as well as the disparity-compensated prediction
(DCP) (inter-view correlation exploitation) have been modified for depth map
coding (from3D-HEVC texture coding) without using any interpolation. A new intercoding
mode for depth maps is added in which the partitioning of a block into subblocks
and the associated motion parameters are inferred from the co-located block
in the associated video picture. For each block, it can be adaptively decided whether
the partitioning and motion information is inherited from the co-located region of the
video picture (motion parameter inheritance (MPI)), or new motion data is transmitted.
For signalling these cases, the skip (merge) mode of the HEVC has been used.
The usage of the merge mode syntax has the advantage that it allows very efficient
signalling when the MPI is used without transmitting a residual signal, since the skip
mode in the HEVC also uses the merge candidate list [152].

As the full available depth range (for 8-bit representation [0,255]) of values is high,
and not all values are evident in each depth map, using the depth lookup table allows
the absence of high depth values to be utilised. Only a small amount of different depth
levels occur due to strong quantisation. In the encoder, a dynamic depth lookup-table
is constructed by analysing a certain number of pictures (e.g. one intra period) of the
input sequence. This depth lookup-table is used during the coding process to reduce
the effective signal bit-depth of the residual signal.

With the growing demand for 3D and multi-view video content, efficient depth data
coding becomes a vital issue in the area of image and video coding. This chapter highlights
some unique depth map characteristics that are different from texture; specifically,
depth edges, which preserve an object’s shape. By exploiting those characteristics
a simple but efficient depth map coding technique has been proposed, which
inherently preserves depth edges, for applications requiring high-quality depthmaps.
The main aim of this contributory chapter is to investigate how a pixel based scalar
quantisation technique inherently preserves edges for different types of prediction
paths, and how this can be exploited to generate quality depth maps for view synthesis.

Current depth map coding techniques (reviewed in Section 2.5) reuse regular video
coding strategies for depth maps by assuming that depth exhibits characteristics like
those of texture. However, spatial as well as temporal characteristics of depth maps
are significantly different from texture images. Unlike texture, depth maps are characterised
by smooth regions with sharp edges that play an important role in the view
synthesis process.

Current depth map coding techniques, including the latest 3D-HEVC standard,
favour the intra-coding path to exploit this spatial smoothness. However, empirical
studies reported in this thesis reveal that the residual frames of depth map differences 
obtained in the inter-coding path usingmotion compensation are even smoother, leading
to lower zero-order entropy.

As depth maps are more sensitive to coding errors, the use of transformation
and/or an approximation of edges by explicit edge modelling has an adverse effect
on view synthesis quality. Moreover, lossy compression of depth maps introduces
additional geometrical distortion to synthetic view and hence, the respective coding
gain cannot be justified.

This chapter presents an extensive analysis of depth map characteristics from the
coding point of viewand proposes an efficient lossless to near-lossless depthmap coding
technique where predicted block residuals are quantised at pixel domain and the
quantised residual frames are encoded with lossless JPEG (JPEG-LS) [12] to inherently
preserve the depth edges.

On standard 3D video sequences, the proposed depth map coding has achieved
better edge preservation in decoded depth maps that helps producing superior image
quality in synthesised views compared to the new 3D-HEVC [85] standard, which
relies on explicit edge preservation and mostly intra edge modelling. This chapter
contains the first four contributions of this thesis outlined in Chapter 1. Preliminary
work from this chapter has been presented at the IEEE International Conference on
Multimedia and Expo Workshop (ICMEW) 2014 [82] and Digital Image Computing:
Techniques and Applications (DlCTA) 2014 [2].

The rest of the chapter is organised as follows. An analysis of unique depth map
characteristics is presented in Section 3.2. The shortcomings of the state-of-the-art
(3D-HEVC) technique are discussed in Section 3.3. The hypotheses of the proposed
method are highlighted in Section 3.4. Section 3.5 describes the proposed method in
detail. As the first contributory chapter, the experimental datasets and performance
metrics are highlighted in Section 3.6. A detailed performance study with in-depth
analysis of the results, including a comparative study between the proposed method
and 3D-HEVC, is presented in Section 3.7. Finally, Section 3.8 summarises and concludes
the chapter.

Details of these techniques are discussed in Chapter 2. In video coding,motion vectors
are encoded independently at component-level using predictive coding and signed
Exp-Golomb codes of order 0 [93]. Details on motion vector coding are presented
in Section 3.5.3.3. The empirical entropy of different components has been analysed
here without considering any specialized coding for residual or motion coding. The
analyses have been performed on eight test sequences and their details can be found
in Section 3.6.1.

The above objective analysis is supported by subjective analysis. From Figure 3.4,
for the Lovebird sequence, it can be visualized that for texture images (top two images),
spatial correlation is more prominent as neighbouring objects have a similar textural
pattern and lighting conditions. Thus, for texture images, the intra-predicted residual
hasmore zero values than the inter-predicted residual. Due to lighting conditions and
motion, in different time-frames, the same object can have different textural patterns
in consecutive texture frames (in Figure 3.4a, the images of the people have predominantly
non-zero residual values).

In 3D and multi-view video coding applications, depth maps are used for disparity
compensation and view synthesis using depth image based rendering (DIBR) [18, 20]
techniques. In DIBR, colour images and corresponding depth maps are used to render
virtual views at arbitrary viewpoints by projecting the colour values into the 3D
scene using the depth values of each pixel. The pixel-level depth value determines
how much the corresponding colour value needs to be shifted horizontally when the
virtual views are rendered. How DIBR is used to synthesise image at a virtual viewpoint
using depth has been described in detail in Section 2.4.2. If the original depth
value of a particular pixel changes due to the coding process, it results in a change of
shift in the corresponding colour pixel. This affects not only the particular block but
also the neighbouring blocks. This describes the effect of depth map coding errors on
the view generation process.

In a depth video, a major portion of a frame is homogeneous or smooth except for
the boundary regions that preserve the shape of an object. So, variation in depth values
mostly lies on the object edges. It is obvious that after lossy compression, prominent
errors occur mostly on the boundary of objects. As a consequence, an object’s
boundary is more sensitive to depth map coding errors compared to non-boundary
regions. This can be proved by subjective and objective evaluation of synthetic views
generated using decoded depth frames at the receiver side.

Standard DCT based approaches fail to efficiently represent sharp edges as they
have high frequency value and applying high quantisation can cause incorrect depth
values at edges. Consequently, erroneous depth values either introduce false depth
edges or eliminate some prominent edges. This clearly implicates the effect of transformation
and quantisation operation on a depth map. Moreover, in the previous
analysis in Figure 3.5, the effect of distorted depth map on the view synthesis process
has been highlighted. To preserve depth edges, it is necessary to encode depth maps
in lossless or near-lossless manner efficiently without any approximation (how thestate-
of-the-art approximates depth edges is discussed extensively in Section 2.6) and
lossy compression.

The latest 3D-HEVC standard encodes a depthmap using both intra-coding andmotioncompensated
inter-coding modes that are similar to texture coding. To deal with the
smoothness and sharp edge properties of a depth map, four additional intra-coding
modes are used with explicit edge detection, depth edge prediction from co-located
texture, and edge approximation at block-level. For inter-coding with existing explicit
motion estimation modes, 3D-HEVC also infers motion information from the corresponding
texture block.

In the quad-tree based block decomposition, a depth block containing edges is
partitioned into two non-rectangular regions that are represented by constant depth
values and signalled as difference from predicted values that are also available at the
decoder. Partitioning is carried out (i) on the depth block; (ii) fromthe partitions of the
already encoded neighbouring depth blocks; or (iii) on the co-located texture block,
which has already been encoded. In all three cases, partitioning is modelled with a
wedgelet, i.e., a straight-line dissection of the block and explicit wedgelet signalling
is needed only in the first case. For the third partitioning case, a contour partitioning
model is used where the decoder can estimate the same contour without any explicit
signalling. Details about these intra-coding modes along with inter-coding modes
have been discussed in Chapter 2. Based on these approaches, 3D-HEVC introduces
the following problems that are worth investigating.

Texture and depth motion are not always correlated as texture motion is affected
by lighting conditions and depth motion is completely induced by an object’s real
movement (see Section 3.4). Moreover, all edges in texture do not always appear in
the depth map. Where there are edges in a texture frame due to pattern or variation in
lighting conditions, the corresponding depth frame is homogeneous without having
internal edges. Moreover, where there is depth variation due to distance, the corresponding
texture cannot always have edges due to similar neighbourhood pixels.
Figure 3.8 presents edges in an arbitrary texture and depth frame Undodancer of
Undodancer and Lovebird sequence. From Figure 3.8, it can be observed that in the
Undodancer sequence, due to the textural pattern on the objects (floor and pillar in Figure
3.8a), edges are evident in the texture image but not in the corresponding depth
image (Figure 3.8b) which has no depth variance. Alternatively, in the Lovebird sequence,
due to variation in depth values in the bottom-left part of the depth map
(Figure 3.8d), edges can be visible where the corresponding texture (Figure 3.8c) has
no such edges (edges are around the object boundary only). For the Lovebird sequence,
8.7% and 5.71% of the total pixels contribute on texture and depth edges, respectively,
where only 0.68% of the total pixels are common in both texture and depth edges
(marked in blue). For the Undodancer sequence, 10.27% and 0.90% of the total pixels
contribute on texture and depth edges, respectively, where nearly 0.31% of total
pixels are common edge contributory pixels for both images.

When the depth residual is encoded using lossless compressionwithout any quantisation,
edges are preserved exactly as in the decoded depth map. A scalar quantiser
maps one sample of the input signal to one quantised output value and a vector quantiser
maps a group of input samples, such as a vector, to a group of quantised values
to exploit the correlation between these samples. In conventional block-based quantisation,
quantisation is applied on coefficients generated from transformation and
the quantisation matrix is designed to provide more resolution to more perceivable
frequency components over less perceivable components (usually lower frequencies
over high frequencies as the human visual system is less sensitive to high frequency
components of an image/block) in addition to transforming as many components
to 0, which will require fewer bits to encode. If scalar quantisation is introduced at
pixel-level, unlike the conventional block-based transformation and frequency based
quantisation (where a different quantisation level is applied on different frequency
components), quantisation errors are evenly distributed among the nearby pixels. By
restricting quantisation error to the near-lossless region, edge preservation can be
guaranteed within a reasonably small degree of uncertainty. Moreover, if the residual
is encoded at frame level without considering the underlying blocks, the impact
of blocking artefacts can also be minimised.

Two key things to note about the Pearson R test are the P and R values. The coefficient
value R ranges between −1.00 and 1.00. If R is in the negative range, the relationship
between the variables is negatively correlated, i.e.,when one value increases, the other
decreases. If the value is in the positive range, the relationship between the variables
is positively correlated, i.e., both values increase or decrease together. The closer R
is to −1.00 or 1.00, the more closely the two variables are related. The other value P
means statistical significance suggesting whether the null hypothesis, that there is no
linear relationship between these variables, can be rejected. If P is smaller than the significance
level (the default is 0.05), then the corresponding correlation in R is considered
significant.

The discrepancy in motion vectors of depth and texture is due to depth values
being simpler than texture inside objects. While searching for a best matching block,
texture motion search can result in a longer motion vector. On the other hand, a corresponding
depth block is more similar to co-located or neighbouring blocks in the
reference frame. In videos where high motion occurs due to zooming and panning,
matching blocks can rarely be found in texture and depth reference frames. In those
cases there can be a high peak value of frequency for maximum motion search length.
For low motion video sequences, zero motion is most prominent, at more than 80%.
Zero motion indicates that the current block is very similar to the co-located block in
the neighbouring frame. These observations clearly indicate that texturemotion reuse
is not always beneficial in depth map coding.

While the current 3D-HEVC technique depends on (i) approximating edges by intra
edge modelling (at the prediction step), (ii) applying transformation and quantisation
on block-level residuals, (iii) predicting depth edges and motion estimation information
(motion vectors) from corresponding texture, and (iv) texture-depth joint architecturewith
viewlevel dependency, the proposed technique relies on (a) inherent edge
preservation without any approximation, (b) avoiding transformation altogether and
applying pixel-level scalar quantisation on the prediction residual, (c) explicit depth
motion estimation, and (d) texture and view independent depth map coding.

The sequences produced by the Chair of Multimedia Telecommunication and Microelectronics
and maintained by the ISO/IEC MPEG group as 3D test material have
been selected for testing purposes. There are in total eight sequences (Newspaper, Bal-
loons, Kendo, Lovebird, Poznanstreet, Poznanhall, GTFly, and Undodancer) [83] in the test
set (Table 3.2), presenting various scenes, both natural and computer-generated. The
test set is provided with ground truth depth data. For natural test sequences, the
depth has been algorithmically estimated from the video. In some cases, additional
manual help was needed, and therefore it is said that those depth maps have been
estimated semi-automatically. For synthetic sequences (GTFly, Undodancer sequences)
the ground truth depth maps have been computer-generated along with the video.

For evaluating the efficacy of depth map coding algorithms a method has been developed
by the ISO/IEC MPEG group which employs view synthesis for quality evaluation
of decoded depth maps by any compression algorithm. During the evaluation
process, two views are explicitly considered: A and B as shown in Figure 3.21. Firstly,
depth maps for view A and view B are encoded. The decoded depths of view A
and view B, along with their corresponding texture images (fixed QP 25 for all sequences),
are then used to synthesise a virtual view in position of view V. In Table 3.2
coded views are A and B; synthetic views are three views. View V is generated using
the uncompressed original texture and depth of view A and B, which will have the
maximum quality of synthetic view generated from A and B. The image of view V is
used for reference and comparison, which provides indirect evaluation of the depth
map coding algorithm. Therefore, the quality of the depth is assessed indirectly by
evaluating the quality of the synthesised views by PSNR. For the synthesis of virtual
views, the ISO/IECMPEG group recommends the use of VSRS. Additionally, the EPA
and PSNR of the decoded depth compared to uncompressed depth have been used to
evaluate the quality of the decoded depth map.

The histogram of inter-coded depth residuals has been analysed and it has been observed
that the frequency of non-zero values diminishes quickly beyond a small range,
as shown in Figure 3.24 where the highly dominant (above 50%) zero residual value
is excluded to show the frequencies of other values more prominently. So, use of the
high quantisation step simply increases quantisation errors without any compression
benefit.

Figures 3.25 and 3.26 present the depth map coding performance of 3D-HEVC and
the proposed IEPDC technique (for all different predictive modes) with respect to
rate distortion optimisation (RDO). Figures 3.27 and 3.28 summarise the overall RDO
performance of the proposed and the 3D-HEVC coding techniques on synthetic view
generation for low and high motion test sequences, respectively.
The performance of the proposed technique has been compared with all modes
against one another and among them, mode Pdepth outperformed all of them in better
depth map quality and synthetic view for low to medium motion test sequences. For
highmotion test sequences, Pintra performs betterwith less entropy. Here due to depth
variance, the range of change in synthetic view PSNR is comparatively lower, because
texture is constant and deviation of texture quality plays an important role in synthetic
view eminence.

For the Balloons and Poznanstreet sequences although Pintra has better depth PSNR
quality than Pco-located and Pcombined modes, its edge preservation accuracy is lower
when the bit-rate is above 0.18 bpp for Balloons and 0.2 bpp for Poznanstreet sequence.
It directly reflects in the generated synthetic view where Pco-located and Pcombined outperform
Pintra with better synthetic views. Pintra performs better than other modes
with better synthetic view quality for bit-rates below 0.18 and 0.2 bpp for Balloons and
Poznanstreet sequences, respectively,with better depth edge quality. Pdepth always produces
better quality depth and synthetic view. Pco-located outperforms Pcombined and
Pdepth modes where motion vector coding requires significant amounts of bit compared
to residual coding.

Themain aimof this contributory chapter is to investigate howa pixel-based scalar
quantisation technique inherently preserves edge in depth maps for different types of
prediction and generates a quality depth map for view synthesis. To prove this initial
hypothesis the lossless JPEG has been used for entropy coding and a traditional
motion vector coding has been used to code depth motion values. As a depth map
is different from texture, its motion values and residuals also exhibit different properties
that should be exploited in depth motion and residual coding. Moreover, in some
scenarios, some prediction techniques perform better than others. In the following
chapters, these observations are investigated further and more efficient coding techniques
are proposed for encoding depth motion values and residuals by exploiting
their structural significance.

While state-of-the-art depthmap coding techniques have introduced specialised intracodingmodes
to exploit the high spatial correlation in depthmaps, the previous chapter
has shown that, inter-coding path for depth maps is also prominent. However, to
consider the inter-coding path,motion vector coding overheads need to be reduced by
exploiting significant structural properties of depth motion. In this chapter, a syntax
coding scheme is presented that efficiently exploits the spatial and inter-component
correlation of depth mode and motion information. Block- or coding unit-level prediction
mode and motion vector components are referred as mode and motion information.
The essential idea of the proposed syntax coding scheme is to represent the
mode and motion information of a frame using a set of bitmaps that greatly facilitates
efficient exploitation of the existing correlation using tree based partitioning.

To exploit distinct characteristics of depth in coding, the state-of-the-art 3D-HEVC
depth coder encodes modes of coding units with variable-length codes using hierarchical
referencing, which is very specific to the intricate coding tree unit structure of
3D-HEVC. In this chapter, it is demonstrated that a generic syntax coding scheme can
be developed by directly exploiting the frame-level clustering tendency using a simple
Binary Tree Based Decomposition (BTBD) technique to partition the frame into
disjoint rectangular coding regions with distinct data distributions. It has been observed
that depth motion vector components are also highly correlated in zero and
non-zero values. This extends the spatial clustering tendency in depth motion to the
component dimensionwhen frame-levelmotion vectors are considered as two successive
frames of x- and y-components, respectively. This provides further opportunity
for BTBD to achieve significant coding efficiency by partitioning the motion vector
component frames into disjoint cuboid regions of distinct data distribution. The proposed
joint coding of motion vector components is a novel concept, which challenges
the traditional wisdom of encoding these components independently as has been the
case in 3D-HEVC and its predecessor, the multi-view video coding (MVC) extension
of the H.264 video coding standard [88].

The superiority of the proposed syntax coding scheme has already been demonstrated
for the coding architecture of MVC-H.264 [113]. This chapter aims to demonstrate
the effectiveness of BTBD in encoding modes and motion vectors of 3D-HEVC
coding architecture, keeping all other coding elements such as Rate-Distortion Optimisation
(RDO) and residual coding unchanged. It is understandable that the overall
coding efficiency of the proposed technique is restricted due to such a constrained
application, which will be further addressed in Chapter 5.

From the operational point of view, mode and motion information of a depth
frame are extracted into several bitmaps and/or integer maps. These bitmaps are
successively encoded followed by the encoding of the non-zero motion components
only. Central to the proposed scheme is a bitmap encoding scheme that adaptively
partitions the bitmaps into cuboids of varying sizes. The partitioning induced by a binary
tree aims at isolating a large homogeneous cuboid that can be encoded efficiently
as a whole. This chapter contains two contributions (one and five) of this thesis outlined
in Chapter 1. Preliminary results from this chapter have been published in the
proceedings of the Data Compression Conference (DCC) 2015 [113] and the proceedings
of the IEEE Visual Communications and Image Processing Conference (VCIP)
2015 [166], respectively. This chapter extends the original work to 3D-HEVC (3D extension
of HEVC), which is the current video compression standard for 3D/multiview
video coding. A performance comparison is also done with 3D-HEVC mode
and motion vector coding.

The rest of the chapter is organised as follows. A brief review on how 3D-HEVC
handles depth mode and motion vector coding along with some potential shortcomings
of the technique is highlighted in Section 4.2. Unique observations on depth
mode and motion are outlined in Section 4.3 along with a way to utilise those properties
in coding. Steps involved in the proposed coding scheme are elaborated in
Section 4.5 which includes algorithms for bitmap formation, partitioning, and compression.
This section also describes the algorithm for independent coding of nonzero
components of depth motion vectors. The experimental setup with performance
metrics and results, demonstrating the efficacy of the proposed coding scheme, is presented
in Section 4.6. Finally, Section 4.7 provides a brief summary and concludes the
chapter. Table 4.5 summarises the definition of some important terms that are used
throughout this chapter.

merge, can be selected. By encoding a skip flag and merge index, skip and merge
modes are represented jointly. A motion vector scheme [16] is introduced to select a
motion candidate from a given candidate set that includes spatial and temporal motion
candidates. Multiple references to the motion estimation allows finding the best
reference in two possible reconstructed reference picture lists (namely List 0 and List
1). For the inter mode, inter prediction indicators (List 0, List 1, or bi-directional prediction),
reference indices (if multiple references are available, depth corresponding
texture also works as a reference), motion candidate indices, motion vector differences
(MVD) and prediction residuals are transmitted after transformation and quantisation.
For skip and merge modes, only merge indices are transmitted. Then, the
current PU inherits the inter prediction indicator, reference indices, and motion vectors
from a neighbouring PU . In the case of a skip coded CU, the residual is omitted
and that is signalled using a flag [17].

Any motion vector generated from a motion search for current inter PU has two
components representing x and y displacement from the reference CU (block). For
bi-directional prediction these components become four in number; x and y components
for forward motion search (predicted from a previous temporal frame) and x
and y components for backward motion search (predicted from an advance temporal
frame). The current state-of-the-art video coding schemes encode the components of
a motion vector (MV) separately and are thus unable to exploit the high correlation
between the components of the vectors. They encode the components of the motion
vectors using differential pulse code modulation (DPCM) [14] to exploit correlations
among the motion vectors of the neighbouring CU. Moreover, the components of the
MVs are encoded on a symbol-by-symbol basis, which requires allocating at least one
bit per component. Although for inter-coded CU using merge/skip mode, 3D-HEVC
tries to reuse motion information from neighbourhood, however, it needs to encode
the mode information per CU in a CTU individually, using Context Adaptive Binary
Arithmetic Coding (CABAC) [27]. Thus, it is unable to exploit the spatial correlation
in a frame where a neighbourhood CU may have a similar prediction mode.

Moreover, the non-zero motion component values do not arise in isolation, instead
they exhibit a strong clustering tendency. In a depth map, where an object has
movement, non-zero motion values occur around the object and the static surroundings
hardly contribute to the non-zeromotion values. There exists a strong correlation
between the components of the motion vectors, i.e., given that one of the components
of a motion vector is non-zero, it is highly likely, the other component will be nonzero.
This is because where non-zero motion values occur for horizontal movement
in an x component, it is highly likely that the y component will also contain non-zero
values for vertical movement, as a change in depth information along the temporal
dimension can only be induced by real object movements.

Figure 4.2 represents motion component (x and y) values for both depth and texture
frames for several test sequences, where the previous frame is used as a reference.
To generate motion vectors, diamond motion search [137] with Lagrange multiplier
based rate-distortion optimisation [23] technique has been used. It is clearly observable
that depthmotion component contains a noticeable amount of zeromotion values
due to high temporal correlation. While searching for a best matching block, a texture
motion search may result in long motion vectors and large residual values due to
the change in lighting conditions, motion compensation is dissimilar for texture and
depth. On the other hand, a corresponding depth block is more similar to co-located
or neighbouring blocks in the reference frame.

For low to medium motion test sequences (Newspaper and Balloons) texture motion
also exhibits those properties slightly but, for high motion test sequences (GTFly
and Undodancer), they induce a significant number of non-zero motion values and the
correlation between x- and y-component do not hold strongly. Moreover in that case,
motion component values become random and they do not arise in clusters. Due to a
change in lighting conditions, texture information may vary even if there is no object
motion in the scene.

Clustering is a widely used concept in machine learning research where objects are
partitioned into different groups based on some data similarity metrics. Some data
mining clustering algorithms also use data density as a means to perform bottom-up
partitioning where a cluster continues to grow as long as the distribution of data in
the neighbourhood of a boundary exceeds some threshold. While these clustering
algorithms have applications in multimedia signal processing such as content-based
image retrieval, they cannot be readily applied in image and video coding due to the
disproportionate overheads of encoding the partitioning information. In the extreme
case, indices of individual pixels belonging to each cluster may need to be encoded in
addition to encoding the values of the pixels.

Distribution of data plays a pivotal role in data compression efficiency. Uniformly
distributed data can hardly be compressed. This can be demonstrated from the definition
of zero-order entropy on an 8-bit depth image. If each of 28 depth values
appear in a frame with equal probability 1/28, the entropy of the image would be
−log2(1/28) = 8 bpp. Data compression efficiency is positively correlated with the
divergence of data distribution from uniformity. For example, in an extreme case of
divergence, when a depth frame has just one dominant depth value, it can be compressed
with negligible bits. Even for a frame with uniformity, significant compression
may be achieved if the pixels are classified based on their neighbourhood and
data distribution diverges from uniformity in each class of pixels. Context adaptive
arithmetic coding (CAAC), the foremost compression technique used in HEVC,
is specifically designed to exploit this in video coding. By considering only the neighbourhood
pixels that have already been encoded in defining the context (class) of a
pixel, CAAC avoids the overheads of encoding the context information.

The main coding tool of the proposed syntax coding scheme is to represent the frame
level mode and motion information using a set of bitmaps. A tree based partitioning
scheme is then used to greatly facilitate efficient exploitation of the correlations
in bitmap(s) by creating isolated large homogeneous cuboids, that can be encoded as
a whole using fewer bits. At each stage of the coding, information from the previous
stages is taken into consideration. After partitioning, the partition information is
coded for all bitmaps along with values of the non-zero motion vector components,
individually in a predefined scanning order. Residual values for inter and intra prediction
are coded identical to 3D-HEVC as the main contribution of this chapter is
on mode and motion vector coding. In the following subsections the bitmap formation
and partitioning, along with encoding and decoding techniques, are discussed in
detail.

In the bitmap partitioning technique, with an input binary bitmap (either mode or
CTU division), the proposed technique aims at exploiting spatial correlation through
identifying clusters of homogeneous regions in the bitmaps. The spatial and intercomponent
correlations in these bitmaps are jointly exploited by partitioning the bitmap
using a binary tree such that the portion of the bitmap covered by a leaf cuboid is either
highly correlated or not correlated at all. The tree is constructed by a simple
divide-and-conquer algorithm with greedy optimisation heuristic. The partitioning
technique will hence be referred to as binary tree based decomposition (BTBD).

This heuristic is expected to divide a bitmap into sub-cuboids that are either highly
polarised (i.e., almost all 0s or almost all 1swith nearminimumentropy) so that in two
different sub-cuboids either 0’s or 1’s concentration increases in one and decreases in
the other. The partitioning algorithm aims to find a split which creates maximum
polarisation (difference in either 0’s concentration or 1’s concentration) between two
cuboids. This heuristic tries to exploit spatial correlation in the bitmap by creating
homogeneous clusters of 0s and 1s.

The encoder compresses in two stages: bitmap compression and independent coding
of non-zero motion vector components. In the bitmap compression stage, different
types of bitmaps are passed through the bitmap partition technique (described in Section
4.5.2) in the order they are listed in Table 4.2. It generates a binary partitioning
tree as output for each of these bitmaps. The bitmap compression stage involves encoding
of tree and tree nodes.

There is an additional mode where any bitmap can be coded without partitioning,
considering the entire bitmap as a node using regular (BAC) or context-adaptive binary
arithmetic coding (CABAC). Bits are estimated (using Equations 2.13 and 4.11,
respectively), and among four modes which require fewer bits, one is selected and
indicated by two bits in bit-stream for a single bitmap at frame level. Bitmap coding
modes are summarised in Table 4.4.

While estimating the approximate bits for partitioning decisions using CABAC,
the model costs also need to be approximated to be able to compute the same probabilities
at the decoder (details in Section 2.1.3.3). For actual coding, the probabilities
of context model updates while processing the data and no extra model cost need to
be calculated.

Structure-context adaptive coding: How partitioning can influence context modelling
can be understood fromthe example presented in Figure 4.9. This figure presents
a visual example of the importance of structure-context adaptive coding where, after
partitioning, leaf nodes are coded individually and the frequency distribution of different
symbols varies in each partition that collectively reduce the overall bit requirement
of the message. From Figure 4.9a it can be observed the message is partitioned
into 4 leaf nodes by 3 divisions (2 orthogonal to y-axis and one orthogonal to x-axis)
and resultant partition segments can be identified as top, middle, bottom-left and
bottom-right. Figure 4.9b presents probability of each symbol in different context set
for different partition segments and bit requirement for encoding the message with
structure-context adaptive coding.

For overall mode and motion vector coding, the proposed scheme performs almost
equally with 3D-HEVC with no significant gain other than Balloons (a 2% to
5% bit reduction) and Newspaper ( a 2% bit reduction) sequences. The almost equal
performances in the techniques is that while the bit-rate has changed, the corresponding
rate-distortion optimisation (RDO) mechanism in the 3D-HEVC has not been adjusted.
Moreover, the current 3D-HEVC mostly depends on skip ( inherit coding unit
mode or motion information from the neighbourhood) or intra prediction rather than
the explicit depth motion search and estimation. The proposed motion vector coding
scheme relies on bitmap based zero/non-zero component coding, where non-zero
motion values are coded using a combination ofMVP, signed Exp-Golomb, and arithmetic
coding. However, this technique cannot prove its strength over 3D-HEVC motion
coding due to an insufficient number of explicit depth motion values. In the
following chapter, a complete depth map coding technique based on explicit motion
estimation and a rate-distortion mechanism has been proposed, where this extension
of bitmap based syntax coding reflect its superior performance against 3D-HEVC.
Figure 4.15 exhibits the RDOcomparison between entropy and polarisation heuristics
for the proposed bitmap based partitioning technique. Entropy based heuristic
outperforms polarisation based heuristic for higher QP values. Figure 4.16 summarises
the overall RDO performance of the proposed (entropy based) and 3D-HEVC
syntax coding technique. For low motion test sequences, the proposed syntax coding
outperforms 3D-HEVC for Balloons andNewspaper sequences. For Poznanstreet
and Lovebird sequences, they perform equally. For high motion test sequences with
lower QP values, 3D-HEVC performs better than the proposed technique; however,
for higher QP values they achieve similar compression efficiency.

Figure 4.18 outlines the significance of realmotion value coding compared toMVP
by presenting some statistics. In Figure 4.18a, the percentage of zero motion vector
difference values using motion vector prediction (MVP) can be observed. For low to
mediummotion test sequences, it is 4%to 20%of the total non-zeromotion values and
decreases gradually while increasing QP values. For medium motion test sequences,
this percentage is an average 6% to 10% for all QP values. For the GTFly test sequence,
this percentage is quite high throughout for all QP values. This analysis has an impact
on motion vector coding modes. For +ve/-ve motion values without zero values, one
side is favoured with 1 less bit which occurs prominently. But, based on how long
the prominent (+ve/-ve) values are, bit-rate savings can be more than half a bit per
motion component. As using motion vector prediction, the probability of zero valued
motion vector difference is quite prominent (observable from Figure 4.18a), therefore,
in Figure 4.18b, the percentage of motion vector modes MVMode 1 and MVMode2
(real non-zero motion value coding using modified signed Exp-Golomb codes and
arithmetic coding) is higher than that for MVMode 3 and MVMode 4 (motion coding
usingMVP). The use of arithmetic coding on signed Exp-Golomb codes of realmotion
value or MVD (MVMode 1 and MVMode3) brings coding efficiency and this is why
they have been chosen over regular MVP (MVMode4).

Apart from efficient syntax coding, this chapter also presents a mode analyser for
3D-HEVC. This mode analyser is a useful tool for extracting a variety of important information.
For example, it can extract the quad-tree decomposition statistics, coding
unit (CU) level prediction mode information (including which intra/depth modelling
mode), and motion vector components. Additionally, it can extract bit statistics for
each coding component (residual or different kinds of syntax). This extracted information
can be used to obtain a better visualisation of the 3D-HEVC technique. Using
this mode analyser, the following statistics are presented in this section for different
QP values.

Figure 4.19 presents the fractions of four coding unit modes used in the eight test
sequences for differentQP values. For lowerQPs, the intramode is prominent for CUs
and, with the increase in QP values, the skip (temporal) mode becomes prominent.
When bit allocation is high, the intra-coding mode is preferred as 3D-HEVC reduces
time complexity by avoiding motion search. Figure 4.20 shows that, for lower QP
values, residual coding consumes the majority of total encoding bits. However, if
the QP value increases, the percentage of bits needed to represent mode and coding
unit division gradually increases. The percentage of bits for motion vector coding
increases gradually with an increase in QPs. Due to the introduction of distortion,
finding an exactmatching CU in the temporal neighbourhood becomes difficultwhich
consequently, results in higher motion values.

Due to the low dynamic range in depth signals, when block based predictive depth
map coding is used, a clustering tendencymay be observed at frame level residual and
syntax data, including coding unit divisions, modes, and motion vectors. In the previous
chapter, to exploit this property for efficient depth syntax coding, a binary tree
based decomposition technique (BTBD) has been proposed using bitmap based depth
syntax representation. This chapter extends the idea of BTBD to design a complete
depth map coding scheme with superior compression efficiency that arranges frame
level coding unit (CU) divisions and their motion information into binary bitmaps,
and mode and residual into integer maps. High spatial correlations in these bitmaps
and integer maps are exploited in coding by the BTBD scheme followed by non-zero
motion encoding using the scheme developed in Chapter 4. Depth edges are inherently
preserved using pixel-based scalar quantisationwith a generic residualmapping
scheme.

Depth has different structural properties (spatio-temporal characteristics) than texture,
that have been exhibited through examples and statistics in Sections 3.2, 3.4,
and 4.3. These characteristics are not fully exploited and utilised properly by the existing
depth compression techniques (techniques are reviewed in Section 2.5). Existing
lossy compression techniques are effective in coding large texture-less regions
within depth maps; on the other hand, they tend to obscure the depth edges after
transformation and high quantisation operations as demonstrated in Chapter 3. In
some scenarios, highly efficient lossless or near-lossless compression of depth data
can provide an attractive alternative to lossy coding, since no or little distortion is introduced,
respectively, on synthesised views due to depth map coding. For natural
images, lossless coding typically provides a compression ratio of at most 2 to 3 times,
which makes the use of lossless compression infeasible for many practical applications.
For depth images, however, preliminary results suggest that a much higher
compression ratio can be achieved when specialised algorithms are applied for lossless
coding: 15 to 30 times in the case of intra coding and 20 to 50 times in the case
of stereo depth images [30]. This is also justified by the performance of the proposed
inherently edge preserving depth coding (IEPDC) (Chapter 3) and the complete BTBD
depth map coding schemes in this chapter, achieving compression ratios of 40.24 and
57.54 times, respectively, for lossless depth map coding.

In Chapter 3, a simple pixel-based scalar quantisation technique on predictive
residuals has been proposed to preserve depth edge inherently. Following that, in
Chapter 4, lossless coding scheme has been introduced to encode the frame level
mode and motion information along with CU division using a set of bitmaps that
greatly facilitates efficient exploitation of the correlation among them using BTBD.
The IEPDC depth map coding technique, based on explicit motion estimation, needs
an efficient motion-vector coding scheme to reducemotion related bit-overheads. The
bitmap based mode and motion information coding scheme proposed in Chapter 4
can be used jointly with a pixel-based scalar quantisation technique on residuals to
bring significant coding efficiency.

Inter-predicted-depth-residuals are mostly zero, due to the high temporal correlation
in successive depth frames, except in themotion area where the variance of depth
is observed. Moreover, intra-predicted-depth-residuals exhibit a dominant zero value
tendency due to the high level of spatial correlation in depth maps. Due to the low
dynamic range in depth signals, a significant proportion of a depth residual frame
has very low magnitudes of pixel-level depth differences. These ultimately contribute
to depth residual frames exhibiting a high clustering tendency. Therefore, similar to
depth syntax elements, a tree based partitioning technique can also be applied for efficient
coding of depth residuals by representing frame level residual values using an integer map.

The proposed depth map coding technique decides quad-tree based coding unit (CU)
divisions, CU level prediction mode, and motion information for a particular frame,
based on a rate-distortion optimisation scheme (Section 5.3.2). Computed CU level
depth residuals, having positive and negative values, are converted to non-negative
integers using a generic ranking based residual mapping technique (Section 5.3.3).
Frame level depth syntax (coding unit division, prediction mode and motion information)
as well as mapped residual data are then arranged into multiple maps
(binary and integer). The correlation in those maps is exploited using the binary tree
based decomposition technique, where large homogeneous blockswith similar values
are isolated and then encoded using context-adaptive arithmetic coding. The current
3D-HEVC standard encodes mode, motion, and residual information of a coding unit
(64×64, 32×32, 16×16, and 8×8) independently and hence, it cannot exploit the correlation
in a frame as a whole. The proposed technique arranges all the information at
frame level, after a coding unit based prediction is performed. Coding block size (leaf
nodes generated from the map partitioning) is adaptive and independent of coding
unit size, where the maximum spatial or component-level correlation dictates the size
of a coding block.

After map formation, the proposed technique tries to find the clustering tendencies in
all the binary and integer maps representing coding unit division, mode, motion, and
residual information for a single frame. In the previous chapter, evidence of clustering
property has been exhibited for syntax bitmaps. A clustering tendency in depth
residuals has also been demonstrated in Section 5.2. If there is no significant clustering
tendency in any of these maps, or the partitioning cost to exploit clustering is
higher, it is coded using context-adaptive integer or binary arithmetic coding (CAIAC
or CABAC) as a whole, based on their map type.

To avoid fixed length coding, arithmetic coding has been added to the traditional motion vector
difference coding using signed Exp-Golomb codes. Moreover, the advantage of
the absence of 0 values has been taken by coding the original non-zero motion value
with modified signed Exp-Golomb codes, where on average half a bit less is needed
per motion component . The advantages of these motion vector coding modes can
be exploited more efficiently in the case of explicit depth motion estimation. In the
previous chapter its superiority over motion vector prediction cannot be established
strongly due to an insufficient number of explicit depth motion values. One target
of this chapter is to establish the efficacy of real motion value coding against motion
vector prediction. To exploit zero motion component tendency in depth motion vectors,
the zero/non-zero (motion value) bitmap coding has been done prior to non-zero
motion value coding.

The performance of the proposed binary tree based decomposition coding technique
on the standard 3D test video sequences is evaluated as follows. For each sequence,
two views are selected and the corresponding depth maps are encoded using the proposed
scheme (reported in Chapter 3, Table 3.2 from cfp of 3D-HEVC). For the encoding
of the depth maps, a GOP of size 8 has been chosen with 24 fps rate. Motion
compensation is carried out on the decoded reference frames using the diamond motion
search [137] with search-width 32.

Coding unit division and prediction mode on CU level are decided using the proposed
scheme described in Section 5.3.2. A maximum pixel-level distortion is fixed
and mode selection and CU division is done based on rate optimisation. CU division,
CU level prediction mode and motion vectors are represented in frame level along
with pixel-level residuals using the proposed mode analyser. From the quad-tree division
of CUs, frame level division bitmaps (Mdiv64 , Mdiv32 , Mdiv16 ) have been formed.
Mode integer map (Mmode), zero/non-zero bitmap (M MVZ), and residual integer map (Mres) are 
formed using CU division, CU level mode, motion vector, and residuals.

Bitmaps and integer maps are encoded using the proposed binary tree based decomposition
technique.

This section presents the efficacy of the proposed BTBD technique for residual coding
using an integer map on different 3D test sequences (two low to medium motion sequences
and two high motion sequence as a representative) with varying distortion
(D) values.

An aimof the proposed depth codec is to reduce the dependency level which is significantly
high in the 3D-HEVC codec due tomultiple level referencing like bi-directional
motion search and motion and other information inferred fromcorresponding texture
frame and neighbouring depth views. For bidirectional prediction, the prior decoding
of other frame(s) needs to be ensured. However, that reduces interactivity and interrupts
real time operations. Moreover, in this thesis it has been exhibitedwith examples
and analyses that texture and depth statistics are not always correlated. Hence, the
use of texture information for depth map coding requires more bits to compensate, as
large residuals are generated due to texture-depth misalignment. Additionally, syntax
coding overheads to exploit texture-depth relevance (an additional bit is needed
to represent this dependency) cannot be made sufficiently low due to their low-level
correspondence.

Texture and depth can have similar motion characteristics in static background
where texture and depth motion are zero. By using skip mode, it can be addressed
within the proposed technique’s mode bit overheads. When motion in texture and
depth is not equivalent, this dependencymode causes an increase of syntax overheads
without any compression gain. Depth map coding is not only relevant to view synthesis,
encoded depth can be used for many other applications such as detection and
analysis. As texture-assisted encoded depth cannot be decoded alone, which makes
it infeasible to use for these kinds of application. To address this problem, one directional
explicit motion search (only from the previous temporally co-located frame)
has been proposed for depth map coding without any dependency or information infered
from texture or neighbouring depth views. Therefore depth maps (frames) of
any view can be encoded and decoded alone.

However, depth maps are not always changed by object motion as depth motions
are mostly lateral, parallel to the camera plane. Hence, the observation which is true
for texture video is not always true for depth videos. Moreover, prediction quality
is also influenced by the number of objects in a scene, especially for depth images.
If the number of objects is high in a depth map, that positively increase the number
of depth edges which need to be preserved for a quality depth map. Due to high
spatial similarities in depth maps, if any frame is intra-predicted, then, depending on
the number of objects in a scene, intra prediction performance may vary. For high
quantisation, intra mode is dominant for all types of sequence (Figure 5.21). Among
the test sequences used in this thesis, coincidentally all the lowmotion sequences have
a high number of objects in the scene compared to high motion sequences. Therefore,
due to quantisation, error introduced at pixel-level is higher for lowmotion sequences
which can be observed in Table 5.10.

As mentioned in the previous chapter, the cost of mapping data as represented in memory to
either files or databases (and vice-versa) is known as impedance mismatch, and adds significant
complexity to both software development and execution [31]. Orthogonally persistent object systems
propose to solve this problem by supporting a uniform treatment of objects irrespective of their types,
allowing values of all types to have whatever longevity is required.
In this chapter, we recap the evolution of orthogonal persistence research and resulting concepts
and challenges.

rsistent object store. PEVM is 10 faster than PJama Classic, largely due to its pointer swizzling
strategy, which made it simple to modify its optimizing JIT while preserving almost all of the speed
of the code it generates, and also due to its use of direct object pointers minimizing CPU overhead.
Programming interfaces are very similar to PJama.

Another implementation of Persistent Java was made in the Grasshopper operating system [30].
Grasshopper supported persistence at operating system level, so no modifications were made to the
Java language or virtual machine. All state was persisted in this implementation, but it relies on a
special operating system designed for persistent memory.

ANU-OPJ [58] implemented persistence transparently through the dynamic manipulation of
bytecodes, during class loading. All objects reachable from the roots during garbage collection
were considered persistent and stored in the Shore storage manager. This approach results in full
portability of Java programs, since it does not require a special JVM or Java compiler.

The Java Card programming interface [61], used in JVM-based Smartcards, follows some elements
from the Orthogonally Persistent approach. All objects are persistent by default, and stored in a
persistent heap on top of EEPROM memory. In order to make an object transient (and thus mapped to
RAM), it is necessary to declare it explicitly by using a static method makeTransient...Array,
as shown in Code Listing 2.5. However, only arrays of primitive types can be made transient, which
violates the principle of data type orthogonality. The lack of transient objects and the restriction
on transient arrays requires the programmer to load and store values from persistent objects into
transient arrays and vice versa.

An article from 1998 [61] proposes a transient environment composed by all objects referenced
directly or indirectly from a transient root. It is analogous to the concept of persistence by reachability,
defining transience by reachability in an environment where data is persistent by default. The condition
is that references to transient objects are forbidden to be held in the persistent set, which must be
enforced by the runtime environment.This scheme would be compliant with the principle of data
type orthogonality, but was never adopted by the Java Card API specification.

SoftPM’s implementation consists of two main components: the Location Independent Memory
Allocator (LIMA), and the Storage Optimized I/O Driver (SID). LIMA manages the container’s
persistent data as a collection of memory pages marked for persistence. When creating a persistence
point, LIMA is responsible for identifying the graph of data structures referenced by the container
and mark the ones which were modified, in order to be persisted. SID atomically commits container
data to persistent storage, which can be disks, flash drives, network, or memcachedb. SoftPM’s
architecture is depicted on Figure 2.2.

Several Persistent Operating Systems (POSs), i.e., OSs supporting abstractions to make persistence
interfaces simpler, were explored. This body of research is summarized in [29] and [42].
Many of the challenges faced by such POSs were due to the need of managing movements of data
between main memory and secondary storage. For instance, Multics, MONADS, and Clouds cannot
guarantee consistency in the presence of unexpected failures, since changes to main memory may
still not have been committed to secondary storage. Eumel/L3 solves this problem by taking periodic
snapshots of the entire machine state, but incurs in performance and latency penalties in order to do
so. Grasshopper faces overheads handling page faults due to secondary storage latency and several
switches between the user-level/kernel boundary [42].

Cooper and Wise [25] argue that unrestricted orthogonal persistence, where a great number of
objects live in a large memory addressing space and are shared by many different programs, can lead
to bugs where programmers unintentionally modify data used by other programs, or keep references
to objects no longer necessary. They propose a protection mechanism based on coarse-grained objects
that contain many fine-grained objects, and apply protection and authorization mechanisms for the
contained objects. The authors call this mechanism Type-Orthogonal Persistence, since any kind of
object can still be persisted.

The previous chapter described how orthogonal persistence (OP) was created in the past to
remove from the programmer the burden of explicitly managing data in both memory and storage,
but how OP implementations still had to cope with the complexity of moving data between these
two layers, and the resulting performance consequences. This chapter presents new non-volatile
memory technologies that make it possible to collapse main memory and storage into a single entity:
persistent memory (PM)

There are several new non-volatile memory (NVM) technologies under research today to provide
memory devices that are non-volatile, energy-efficient, with latencies close to DRAM and storage-class
capacity. We have surveyed NVM technologies extensively in [64], and reproduce here a summary of
the most mature technologies [49]: Phase-Change RAM (PCRAM), Resistive RAM (ReRAM), and
Spin-Torque Transfer Magnetoresistive RAM (STT-MRAM).

Phase-Change Random Access Memory (also called PCRAM, PRAM or PCM) is currently the
most mature of the new memory technologies under research. It relies on some materials, called phasechange
materials, that exist in two different phases with distinct properties: an amorphous phase,
characterized by high electrical resistivity, and a crystalline phase, characterized by low electrical
resistivity [75]. These two phases can be repeatedly and rapidly cycled by applying heat to the
material [18, 75].

Magnetoresistive RAM (MRAM), sometimes called Magnetic RAM, is a memory technology that
explores a component called Magnetic Tunnel Junction (MTJ), consisting MTJ of two ferromagnetic
layers separated by an oxide tunnel barrier layer. One of the ferromagnetic layers, called the reference
layer, keeps its magnetic direction fixed, while the other, called the free layer, can have its direction
changed by means of either a magnetic field or a polarized current. When both the reference layer
and the free layer have the same direction, the resistance of the MTJ is low. If they have different
directions, the resistance is high. This phenomenon is known as Tunneling Magneto-Resistance
(TMR) [18, 32, 49].

Conventional MRAM (also called “toggle-mode” MRAM) uses a current induced magnetic field
to switch the MTJ magnetization. The amplitude of the magnetic field must increase as the size
of MTJ scales, which compromises MRAM scalability. Spin-Torque Transfer MRAM (STT-MRAM)
technology tries to achieve better scalability by employing a different write mechanism based on spin
polarization [53].

There are several advantages in using persistent memory instead of separating the memory
hierarchy in volatile main memory and persistent storage. HDDs and SSDs are widely employed
today not for any goal-directed design rationale, but for the fact that today’s technology standards
make them extremely cost-effective, and it has been so since the beginning of commercial computing.
As Alan Cooper [24] pointed out, “Disks do not make computers better, more powerful, faster, or
easier to use. Instead, they make computers weaker, slower, and more complex. (...) Wherever
disk technology has left its mark on the design of our software, it has done so for implementation
purposes only, and not in the service of users or any goal-directed design rationale."
But decades of separation between volatile main memory and persistent storage have left deep
marks in computer design, including hardware, basic software and application software. If the
emerging memory technologies fulfill their promise of enabling persistent memory, several layers of
the hardware and software stack will be affected.

Changing hardware, operating systems and programming languages internal implementations can
be challenging for computer scientists and engineers, but they potentially have a mild impact on
application software when compared to conceptual changes that affect the interfaces exposed by
programming languages. Nonetheless, we believe that in order to reap the major rewards potentially
offered by persistent main memory, it will be necessary to change the way application developers
approach programming. Current programming interfaces have separate abstractions for handling
memory (data structures) and secondary storage (files, databases). A good deal of development
effort is directed towards moving data between these two layers; a seminal study estimates around
30% [11].

In our M.Sc. dissertation [64], we evaluated the impacts on latency and energy of a computer
system with persistent memory. We simulated scenarios of systems using different technologies for
main memory (DRAM, PCRAM and Memristor) using a full-system architectural simulator (Virtutech
Simics [56]), and compared the execution of workloads using both a typical scenario using the file
system as storage backend and an API specific for persistent memory in C/C++ (Mnemosyne [82]).
Additional results were also published in [66] and [67]. During that study, our own development
effort using an API designed for persistent memory produced less than half lines of code and was
more than 4x faster to develop than using traditional file-oriented APIs. Our conclusion was that in
order to leverage persistent memory to simplify the task of designing, developing and maintaining
software, we need to change programming interfaces.

The implementation of Mnemosyne consists of a pair of libraries and a small set of modifications
to the Linux kernel for allocating and virtualizing non-volatile memory pages. It runs on conventional
processors. Optionally, Makalu [14] provides an alternative memory allocator with improved
performance.

The purpose of Mnemosyne is to reduce the cost of making data persistent, since current programs
devote large bodies of code to handling file system and/or database access. On the other hand,
Mnemosyne is proposed not as a replacement for files, but as a fast mechanism to store moderate
amounts of data; thus, applications should still use files for interchanging data and for compatibility
between program versions when internal data-structure layouts change.

When a programmer creates a managed data structure, like this ManagedArray, MDS allocates
this object directly in the MDS Managed Space. This Managed Space is one large virtual pool of
memory; a shared, persistent heap in which MDS allocates and manages MDS application objects,
with the help of the Multi-Process Garbage Collector. MDS objects are allocated in the MDS
Managed Space using create method calls on an object type; in contrast with a programmer calling
the new method to create a standard Java object in the Java single-process volatile heap.

All MDS objects are strongly typed. Managed types support a createArray() method to create a
managed array of that type; for example, ManagedInt.TYPE (the managed type for integers) has
a createArray() method call to create instances of ManagedIntArray, a type for managed arrays of
integers. MDS is designed to support primitive types, strings, data structures and records, and it
enables users to specify their own record types composed of these MDS supported types. To avoid
the user having to write a lot of boiler-plate code for a ManagedRecord type, MDS provides support
for high-level specification of a ManagedRecord type using Java annotations.

To make an MDS object accessible beyond the process that creates it, object must be bound to
a name in the MDS namespace, as exemplified in Code Listing 3.13 line 2. Another application can
then look up the same MDS object by name (line 5), either concurrently or long after the original
creator process has finished, and get back a reference to it in the MDS Managed Space.

MDS also allows isolating updates to managed objects using a transactional IsolationContext
(Code Listing 3.13, lines 7—9), preventing potential conflicts with other updates being made concurrently
by another thread or process.

In Chapter 2, we have described how orthogonal persistence can be used to remove from the
programmer the burden of explicitly handling persistence, but how its implementation on top of
traditional storage adds complexity and decreases performance. In chapter 3, we described the
current efforts to design programming language interfaces supporting persistent memory, but how
the existing proposals still require the programmer to explicitly handle persistence (although in much
more simplified way, since they do not require crossing the semantic boundary between memory data
structures and files or databases).

In this chapter, we combine both approaches by leveraging persistent memory in an attempt
to solve the performance and complexity issues historically associated with orthogonal persistence.
We do this by introducing a design for the runtime environment for languages with automatic
memory management, based on an original combination of orthogonal persistence, persistent memory
programming, persistence by reachability, and lock-based failure-atomic transactions. Such design
can significantly increase programming and execution efficiency, as in-memory data structures are
transparently persistent, without the need for programmatic persistence handling, and removing the
need for crossing semantic boundaries.

The existence of different abstractions for representing data in memory and in persistent storage
is not a fundamental necessity of programming languages, but an accidental result of the current
technology that mandates the separation of fast, small, volatile main memory from slow, abundant,
non-volatile secondary storage. If these two different layers are collapsed into persistent memory, then
orthogonal persistence can overcome its past implementation challenges, and provide an interface
that requires less code complexity and programmer effort.

persistence was carried out. Many researchers invested effort creating programming interfaces and
abstractions that could simplify software development and maintenance by removing the impedance
mismatch between memory and storage data representations. However, implementing orthogonallypersistent
systems on top of traditional storage posed performance challenges that prevented many
of these abstractions from reaching mainstream software development.

But now, the imminent advent of persistent memory opens new possibilities for implementing
orthogonally-persistent systems using technologies that are more naturally suited to the task. The
main motivation for our design is to combine the previous studies on orthogonal persistence with
modern research on persistent memory in order to enable the creation of software applications that
are easy to both write and maintain, and have significantly superior performance than today.

Orthogonal persistence (OP) (described in Chapter 2) provides abstractions to address the
impedance mismatch between memory and storage by means of supporting a uniform treatment of
objects irrespective of their types, allowing values of all types to have whatever longevity is required.
We propose that object-oriented languages with automatic memory management can implement
OP concepts by means of a persistent heap that can be reused across executions. In this work, we
consider the term persistent heap as referring to all state that can be made persistent, which may
include not only the heap itself, but other elements, such as threads, stacks, type definitions, method
code, and other data as required by the particular implementation.

In order to determine the longevity of objects, we advocate the use of persistence by reachability,
i.e. all objects still reachable from roots of persistence are considered persistent, while objects that
are unreachable can be disposed. We also advocate using as roots of persistence the static variables
of all classes that are persistent and the stack of all active threads. This approach leverages the
same reachability criteria used for Java garbage collection [39], and has been successfully used in
previous OP Java implementations [9, 51, 57].

In the next sections we list what needs to be stored in the persistent heap in order to support
persistence by reachability of both class attributes and stack references.
We use Java as the programming language for our examples, but the design is generalizable to
other object-oriented languages with automatic memory management.

We assume a PM-aware file system providing namespace management and user access control
to chunks of persistent memory [77]. The file system contains persistent heap files, which can be
mapped to the addressing space of any process via persistent memory programming, either by directly
calling the mmap() system call or using higher-level APIs for PM such as [20,22,27,40,44,82]. Once
mapped to the addressing space, the process can manipulate persistent memory directly, without
page caches. This scenario is depicted on Figure 4.4

When the runtime environment of a program is invoked (e.g. JVM), the file containing the
persistent heap can be provided (for example, as an argument).

In this work, we explore only the scenario where one process is bound to a single persistent heap
at a given point in time. It is an interesting research problem for future work to consider multiple
persistent heaps to a single process, as well as multiple processes to a single persistent heap (i.e., a
shared persistent heap).

The persistent heap can contain only persistent data, or it can contain also a persistent execution
state. Both are described in the next sections.

Data persistence stores program data in a persistent heap, but does not store the program
execution state. After the process execution is terminated, or interrupted, all persistent data (defined
by class attribute reachability, as described in Section 4.5) is available to be bound to a new process,
which can be either from the same program or a different one.

Code Listing 4.4 provides an example of a program using data persistence. The program is
always executed from the beginning in every invocation. Line 32 verifies if the persistent object
addrListByName is uninitialized, which will be true in the first execution. It will be initialized in
subsequent executions using the same persistent heap.

The persistent heap must be kept in a consistent state even in the presence of failures, such
as abnormal program termination due to power outages, runtime faults, resource exhaustion, etc.
In order to accomplish this, it is necessary to make modifications to the persistent heap in the
context of failure-atomic transactions. Persistent memory programming APIs such as [27,82] provide
failure-atomic transactions by means of transactional memory and journaling, respectively.

It is also desirable to provide ways for the programmer to express application-specific transactional
semantics. We advocate the use of locks defined by the application programmer to identify the
scope of failure atomic transactions (lock-based failure-atomic transactions), since they can be
easily expressed by synchronized methods and/or blocks. In the context of Java, synchronized
methods and blocks define locks around specific objects by means of calls to monitorenter and
monitorexit opcodes. Atlas [20] introduces a similar approach for the C language, and [83]
demonstrates that Java synchronization locks can be used to express transactional semantics.

There are many challenges and open points that must be addressed in order to have a robust
persistent heap solution. In the next subsections, we list some of these challenges. Many of them are
not specific to OP, being applicable to other memory persistence approaches as well. In this work
we have addressed some of these points, as described in Section 5.1, while others must be addressed
by future research works.

Each object in a heap belongs to a specific type. In our proposed design, the type definition is
contained by the persistent heap (unless it is defined by the programming language, such as primitive
types in Java [39]). In this scenario, when a reference type is changed, objects that already exist in
the heap must be bound to a new type, and its data and code must be adjusted appropriately to
convey the semantics intended by the programmer. This process is known as Type or Class Evolution.
Farkas and Dearle [35] described system evolution as encompassing evolving data, evolving code,
and evolving types. They propose a mechanism for type evolution based on a persistent store browser,
i.e., a tool that traverses object graphs applying a function to the objects it encounters.

The persistent heap must be kept in a consistent state even in the presence of failures, such as
abnormal program termination due to power outages, runtime faults, resource exhaustion, etc. It
also must provide ways for the programmer to execute concurrent code without undesirable effects.
Blackburn and Zigman [15] discuss the challenges that OP faces handling concurrency and
consistency, and group the existing solutions in two approaches: transactions and checkpoints.

In databases, the main constructs to handle these concerns are transactions. Transactions are a
group of operations that exhibit Atomicity, Consistency, Isolation and Durability (ACID) properties.
According to the authors, OP would require that computation should take place only from within
ACID transactional contexts, in order to satisfy the principle of persistence independence. However,
since a transaction invoked from within a transaction becomes a nested transaction that can only
be successfully committed along with the parent one, the whole program execution would become
a single transaction. Such a coarse-grained transaction scope limits its usefulness for controlling
concurrency within a program. In other words, the transaction approach only makes sense when
transactions can be invoked from non-transactional context. For OP implementations persisting
execution state (as described in section 4.7.2) this would be a challenge, but transactions would be
more feasible for implementations that persist only data (see section 4.7.1).

PEVM [51] addresses consistency by means of checkpoints. There is a persistent store in disk and
an object cache (heap) in memory. When an object is written to the persistent store and removed
from the object cache, its pointer is replaced by the location of the object in the persistent store (a
process known as pointer swizzling). Objects are written to the persistent store during checkpoints.
The first step for a checkpoint is calling checkpoint listeners that can be defined by the programmer,
followed by stopping all threads but the one making the checkpoint. It then does a first pass to
identify objects that needs to be persisted, and a second pass to actually save the objects in the
persistent store. PEVM persists only data, and considers the execution state is volatile.

When using a persistent heap, potentially sensitive data becomes vulnerable to unauthorized
access. To circumvent that, persistent heap data should be encrypted. Software-based encryption
is possible but would have a huge performance impact. Hardware-based encryption provided by the
underlying platform would probably be the most efficient mechanism.
Security of persistent memory systems is an important and broad research topic, and is beyond
the proposed scope for this work.

Many object-oriented languages can be adapted to employ the OP abstractions. We opted
for Java since its use is widespread in a variety of environments, from embedded devices to highperformance
computing applications, and its garbage collection mechanism already manages the
object longevity by reachability, which can be leveraged for persistence by reachability.
Some previous OP implementations in Java, such as ANU OPJ [57], followed the approach of
bytecode transformation, allowing portability across different Java Virtual Machines (JVMs). Other
implementations, such as PJama [9] and PEVM [51], chose to modify the JVM itself, permitting
deeper modifications of the Java execution environment. We propose to use the latter approach,
making modifications to the JVM, in order to have complete control over the data structures used
to manage heap, stacks, threads, and type definitions. Since the mentioned OP-oriented JVMs are
not available anymore, only current JVMs can be used as implementation baseline. Our choice was
JamVM [55], due to its simple and straightforward implementation

We attempted a first implementation of the persistent heap simply modifying the mmap flags
to use a shared memory mapping backed by a file on top of a PM-aware file system. Functional
tests performed on this initial implementation answered affirmatively research question 1, i.e., that
"we can use a persistent heap on top of persistent memory to transparently store objects across
multiple program executions". No specific memory management features for persistent objects were
implemented, using the standard garbage collection mechanisms, and the results of functional testing
addressed research question 2, demonstrating that "it is possible to leverage automatic memory
management mechanisms to determine the persistence lifecycle of ordinary objects".
However, consistency was not guaranteed in the presence of failures, as confirmed by tests where
the prototype process was randomly killed.

Our current persistent heap implementation uses the NVML pmemobj library [27] for manipulating
persistent heap data in order to ensure fault-tolerance, as NVML provides journaled transactions via
an undo log in PM. As consequence, the persistent heap is stored in an NVML memory pool, which
is mapped to a file in PM. NVML has been chosen for currently being one of the most active and
widely adopted persistent memory programming API, and providing a suitable transactional model.
We have employed failure-atomic transactions provided by NVML, as described in more detail in
the next sections. The final prototype implementation, using this mechanism, successfully addressed
research question 3, demonstrating a way "to automatically ensure the consistency of a persistent
heap in the presence of failures".

The next sections summarize the main changes made to the baseline JamVM code. Most of
them were made to keep heap data and metadata in the NVML memory pool and manipulated in
a way that ensures fault-tolerance. Some changes were also made to the GNU Classpath library to
address the handling of external state (such as standard console input/output), as explained later.
The full source code can be found at [65], and Appendix A contains a more detailed description of
all modifications made to create JaphaVM.

initializer of the Java library class gnu.java.nio.FileChannelImpl. The Java Language
Specification [39] determines that a static initializer is executed when the class is initialized, which
takes place just once, when the class is originally loaded. However, we need to open the console file
descriptors every time the VM is executed. None of the existing Java language abstractions provides
a way to express this.

We have taken the approach used previously for PEVM [51] and PJama [9], exposing to developers
an API for making classes and objects resumable, i.e., providing an interface for specifying methods
to be executed every time the VM execution is resumed.
To be resumable, an object must implement the interface OPResumeListener, which defines a
void resume() method, and be registered by the OPRuntime.addListener() class method.
In order to execute resume code at the class scope, the class must implement a static resume()
method and be registered by the OPRuntime.addStaticListener() class method.
We have implemented the OPRuntime class as part of GNU Classpath (its interface is depicted
on Figure 5.2). In order to get the resume() methods to be executed whenever the JVM resumes
execution, JaphaVM invokes OPRuntime.resumeAllListeners()just before the execution
of the application’s main() method.

Finally, we applied it to reopen the console file descriptors, modifying gnu.java.nio.FileChannelImpl
to re-initialize them inside a static resume() method that is invoked in two occasions: 1) by its
static initializer; and 2) by OPRuntime.resumeAllListeners(), since it is registered using
OPRuntime.addStaticListener() by the static initializer.
This mechanism violates OP’s principle of persistence independence, as the programmer is required
to manually implement resume behavior when holding references to objects that represent volatile
external state, such as files and sockets. However, there seems to be no obvious way to solve this
transparently [31]. On the other hand, it requires small programmer involvement when compared to
traditional file- or database-oriented approaches, so we deemed it a reasonable compromise for our
prototype. A similar approach has been taken by the Apache Mnemonic library, described previously
in Section 3.4.9.

The ORM layer used on the relational database scenarios creates many additional short-lived
objects, and as consequence, the JVM spends more time doing Garbage Collection (GC). For the
medium database size, all objects created by JaphaVM fit into the 8GB heap, not requiring GC; while
the relational database scenarios required between 56—68 GCs (15—16 of them also performing
heap compactions), which accounted for 15.62%—17.31% of the overall database creation time.
We also observe that JaphaVM execution has less context switches. This happens because it
performs significantly less storage access, and thus is less blocked by I/O waits, which are a common
cause for voluntary context switches.

JaphaVM with automatic transactions (i.e., a distinct transaction for each individual Java opcode
modifying persistent heap data) executes the same traversals up to three times faster than PostgreSQL
and H2, as it requires significantly less computational effort than the ORM scenarios. These results
are achieved not only because JaphaVM runs completely in memory, but also because its software
stack is shorter and optimized for PM. These effects can be observed in the small amount of
data being read/written from the I/O subsystem. Its advantage is even more apparent with the
larger database configurations, as the ratio between overhead and data volume for the Disk-based
and Memory-based DB scenarios is higher. Additional gains can also be attributed to the lack of
interprocess communication between the application and a database server in the JaphaVM scenario.
JaphaVM with a single user transaction executes two orders of magnitude faster than the ORM
scenarios, and almost one order of magnitude faster than JaphaVM with automatic transactions. In
the latter, the great number of fine-grained NVML transactions impose a considerable overhead,
as each single transaction needs to issue memory fences and cache drains to ensure that changes
to the memory pool have reached the persistence domain upon transaction commit. By using a
single coarse-grained transaction instead of multiple fine-grained ones, performance is significantly
improved.

Results for JaphaVM with no transactions are listed as reference, because its practical use is very
limited due to its lack of fault-tolerance. We observe that this scenario executes about 30% faster
than JaphaVM with a single user transaction, due to the absence of transactional overhead.
On the next section, we describe another experiment, this time using a real-world application
(Apache Lucene) instead of a synthetic benchmark.

We have created a modified version of the Apache Lucene search engine library [4] using orthogonal
persistence on top of JaphaVM. Lucene is a library written in Java that provides the ability of creating
inverted indices of text documents, then allowing queries on what documents contains which terms.
Lucene version 3.6.2 was used as baseline.

The original version of Lucene stores the inverted index in a set of segment files. Our Lucene-OP
version uses instead a java.util.HashMap in the persistent heap, where each key is a term
and each value is a java.util.HashSet referencing the documents that contain that term (see
Figure 5.3). Other than that, both versions perform the same tasks, such as tokenization, stop word
filtering, stemming, and querying.

We have decided to experiment with a search engine since it is a critical, well-known real-world
task, performed both in personal computers (e.g. e-mail search) and huge parallel machines (e.g.
web search), which depends critically on persistent data that can be easily held either in memory
or in storage. We have chosen Lucene because it is the de facto standard library for text indexing
and searching in Java. The fact that its original version uses files as backing store makes a good
complement to our previous experiments with relational databases using OO7.

Examining the overall results of the Lucene experiments, we confirm the observations from OO7
that graph traversal and pointer-chasing tasks are completed more than one order of magnitude
faster using JaphaVM. However, we also observe that for tasks that are dominated by writes (such as
the creation of an inverted index), writing to traditional files is potentially faster than their JaphaVM
counterparts, due to the extra transactional overhead to keep the persistent heap failure-tolerant on
the latter. This overhead is introduced by the journaled transactions mechanism used by NVML [27].
Before modifying the contents of a memory address for the first time (and only on the first time),
it necessary to copy them to the undo log; in order to keep track of the address ranges that were
already added to the undo log, NVML uses a radix tree. As more and more contiguous ranges are
added to the undo log, the radix tree becomes deeper and traversal times increase. Since every data
modification requires going through this process, writing transactionally to PM using an undo log is
more expensive than reading.

This chapter has presented JaphaVM, a Java Virtual Machine designed for persistent memory and
based on orthogonal persistence. JaphaVM’s programming interface is inspired by previous research
on orthogonally-persistent Java Virtual Machines, but its design leverages persistent memory, resulting
in a simpler implementation with better performance. It uses an original combination of orthogonal
persistence, persistent memory programming, persistence by reachability, and lock-based failureatomic
transactions, a concept that can be applied for other managed runtime languages, such as
Python, Ruby, and JavaScript.

We have also described our prototype implementation, and evaluated its advantages over traditional
persistence approaches, such as relational databases and files, considering both execution
performance and programming complexity. In our experiments, JaphaVM executed the same operations
between one and two orders of magnitude faster than the traditional implementations,
with the exception of write-intensive tasks to files, and achieved the same results requiring significantly
less lines of code, addressing research question 4, i.e., "what is the performance of an
orthogonally-persistent system using a persistent heap relative to traditional storage approaches?".
The current version of JaphaVM presents some shortcomings, such as lack of type evolution and
execution persistence, which we suggest as interesting problems for future research work. However,
it provides a good starting point for evaluating the advantages that persistent memory is expected
to bring to Java and other managed runtime applications: programs that are easier to write and
maintain, and have significantly superior performance.

As discussed in section 1.2, the connectedness theory is built on the concept of
fuzzy set/subset and fuzzy connectedness is the most fundamental and popular way of defining
the degree of connectedness in digital space. However, the classical “Reviewers’
Problem” [11] shows that some tasks cannot be solved by fuzzy set-based approaches: two
reviewers (A and B) rank a paper with memberships μA and μB. Assume that μA = μB = 0.8,
and they have different background qualifications. Although they give the same membership,
the two membership values will have different effects on the paper decision. This
problem reveals that, besides the membership, some factors may also influence the final
decision, e.g., a reviewer’s academic background and paper reviewing history.

The message we can take from the “Reviewers’ Problem” is that some hidden
factors behind the membership functions could play an important role in decision making;
and in defining the degree of connectedness, we should consider the “hidden factors”, e.g.,
signal noise ratio (SNR), local homogeneity, and outliers. Therefore, we generalize the
theory of FC by proposing the Neutro-Connectedness theory which has one additional domain,
confidence of connectedness, utilized to model the “hidden factors”.

There are three main steps in Algorithm 2.1. In the initialization step (step 1), we
set NC of the target point x to (1, 1) and its parent node (and root node) is itself; and set the
NC values of all other points to (0, 0); and we create a priority queue Q to save data points
which will be calculated in the next steps; Q is initialized by using the target point x and
will be updated in steps 2 and 3. Note that x could be single point or a set of data points
In step 2, every time the point p with the strongest connectedness to point x is extracted
from Q based on the ≽ operator defined in Eq. (25). Because Q is designed as a
priority queue, so time complexity of the extraction is only O(1). In step 3, the NC values,
parent nodes and root nodes of point p’s neighbors are updated; and the newly updated
point is added into the Q. Algorithm 2.1 repeats steps 2 and 3 until Q is empty.

The time and space complexities: the time complexity of Algorithm 2.1 depends on
how to implement queue Q. In the first case, if we maintain Q as an array, both the exaction
operation in step s and the insertion operation in step 3 take O(N) time, and the total time
of Algorithm 2.1 is O(N2). Q can be implemented by using max-priority queue, then the
insertion operation takes O(logN), then the total time is O(NlogN). Since AT and AC are
both sparse matrices, the space complexity of Algorithm 2.1 is O(N). Algorithm 2.1 is a
modified version of Dijkstra’s method and the best time complexity can be linear [40]. The
image foresting transform (IFT) [28] generates spanning forest for image, which is similar
to NCF generation process in Algorithm 2.1, and a superpixel implementation of the IFT
can be found in [40]; nevertheless, there are two main differences between NCF and IFT:
first, the fundamental concept for constructing NCF is taking both the degree and confidence 
of connectednesses into consideration, while IFT constructs spanning forests by selecting
the path with the minimum path cost. Second, in the case of ties (two paths with
same T value), IFT follows the last-in-first-out (LIFO) queue policy to break ties; Algorithm
2.1 introduces the lexicographical order relation of (T, C) to determine the best path.

IIS is an interesting and challenging task in image processing. In this task, user
extracts objects from image by incorporating interactions such as marking seeds, setting
region of interest, etc. The advantage of IIS is that satisfactory segmentation results can
always be achieved because it can incorporate priori information (appearance, shape, topology,
context, etc.) and correct errors by interactions. However, intense and precise user
interactions are needed to segment complicated objects using seed-based approaches,
which lead to low usability; although ROI-based methods need much less user interaction,
their performances are sensitive to the initial ROI [32].

Graph cuts [12] is one of the most popular region seed-based approaches, and both
appearances models and boundary constraints are formulized in a combinational optimization
framework. The object and background seeds will be utilized to estimate the object
and background appearances, respectively. Lots of seeds must be marked in Graph cuts to
obtain accurate result; especially, when image has complicated structures, user need to set
as many seeds as possible to cover different structures. In [21], the IIS was formulated as
a spline regression problem. The parameters in the spline function were estimated by using
the user specified object and background pixels, and the labels of all other pixels were
determined by the signs of spline function values. However, same with Graph cuts, user
must specify quite many seeds to obtain controllable and desired accuracy. Spina et al. [22]
proposed a hybrid IIS approach, Live markers, by combining the boundary seed-based
methods (Live-wire-on-the-fly [23], Riverbed [24]) and region seed-based methods (Image
Foresting Transform (IFT) [25], Graph cuts [12]). Although the method needs less user
interaction than traditional boundary seed-based methods, it still requires user to specify
seeds on object boundary accurately.

MILCut [30] formulated IIS as a multiple instance learning (MIL) problem. It generates
negative and positive bags from the pixels outside a bounding box and the pixels of
sweeping lines within the bounding box, respectively. The segmentation result of MILCut
is very sensitive to the size of the initial bounding box. Tang el al. [28] proposed One-cut
to incorporate the measurement of L1 distance between object and background appearance
models into the Graph cuts energy function. A graph construction method for high order
potentials was also given in their work [28]. One-Cut needs less user interaction than Graph
cuts. In [31], Tang et al. proposed a parametric Pseudo-Bound Cuts (pPBC) method for
optimizing interactive segmentation with high-order and non-submodular energies. The
experiments in section IV demonstrate that pPBC achieves better results than other ROIbased
methods (Grabcut, MILCut and One-Cut); however, its performance is still sensitive
to the initial ROI.

NC-Cut is a novel hybrid interactive image segmentation approach, which formulates
segmentation based on both pixel-wise appearance models and NC properties. NC
models global topologic property among image regions and can reduce the dependence of
segmentation performance on the appearance models generated by user interactions. The
user interaction is to specify a polygon containing the object, and the image regions outside
the polygon are viewed as the background seeds. The NC computation algorithm is utilized
to calculate the NC values between each region in the polygon and background seeds, and
generates a NC forest rooted from the background seed set. The constructed graph based
on NC forest imposes NC as a global constraint to the segmentation.

To evaluate the five methods’ level of sensitivity to different ROIs quantitatively,
we generate 10 groups of bounding boxes with different looseness automatically. We use
the bounding box of the ground truth as the baseline, and set its looseness to 1; then move
the four sides of the bounding box toward image borders to increase the looseness. The
amount of move is proportional to the margin between the side and the image border. The
looseness of a bounding box is defined as the ratio of area of the new bounding box to the
area of the baseline bounding box. Twenty-eight images having bounding box with looseness
at least 2 are selected from the two datasets. As shown in Fig.3.6, the proposed NCCut
is much less sensitive to the looseness of ROI than the other four methods.

Effective interactive segmentation (EISeg) provides user with visual clues to guide
the interacting process based on Neutro-Connectedness (NC). The boundary connectedness
map and NC forest calculated in EISeg can lower the dependence of segmentation performance
on the appearance models and can also reduce user interaction greatly by guiding
user where to interact can achieve the best segmentation results. The main results of this
work have been published in [39].

The flowchart and an example of the proposed EISeg method are shown in Fig. 4.1.
The EISeg includes three main parts. The first part is the boundary connectedness mapping
which computes Neutro-Connectedness (NC) between each image region and boundary
regions. The boundary connectedness has two components: boundary connectedness map
(BCM) and boundary connectedness forest (BCF). The two components are quite informative
to distinct object and background; and we will apply them to provide visual clues for
user to determine seeds effectively. The second part is the interaction protocols. The protocols
explain the meanings of the BCM and BCF, and define three simple guidelines for
user to determine the most suitable the seeds. The EISeg formulation and optimization are
in the third part. The EISeg is formulated as energy minimization problem based on GMM
appearance models (Grabcut [26]) and boundary connectedness, which will reduce the sensitivity
of the proposed method to user interactions. Like NC-Cut and Grabcut, EISeg is
optimized iteratively with GMM and boundary connectedness updated in each iteration.

The performance of the newly proposed EISeg method is validated utilizing the
NC-Cut dataset [16] including 215 images from MSRA [36]. The manually marked regions
of objects are the ground truths (GTs). Traditional interactive segmentation methods
mainly focused on the improvement of segmentation models, but ignored the effectiveness
of user interaction. Due to the absence of how to interact to achieve better result, those
methods have great arbitrariness of user interaction, and good segmentation results can
only be achieved by involving intense user interactions, e.g., setting more seeds, or specifying
a new ROI to exclude more background regions.

In this section, we will compare EISeg with Grabcut [26] and pPBC [31]. Grabcut
and pPBC are two ROI-based methods. In Fig. 4.3, we specify two different ROIs for each
of the four original images (Fig. 4.3(a)); the segmentation results of Grabcut and pPBC
(Figs. 4.3(b) - (e)) illustrate that different ROIs may result in quite different segmentation
performance and user usually has no clue about where to set ROI which can generate the
best result. Fig. 4.3(f) shows the seeds specified according to the proposed interaction protocols
in EISeg; and only two or three seeds are specified for each image. Fig. 4.3(g)
demonstrates that the proposed EISeg can generate quite accurate results based on weak
user interaction.

Breast cancer occurs in the highest frequency in women among all cancers, and is
also one of the leading causes of cancer death worldwide [37]. Scientists do not definitely
know what causes breast cancer yet, and only know some risk factors that can increase the
likelihood of developing breast cancer: getting older, genetics, radiation exposure, dense
breast tissue, alcohol consumption, etc. The key of reducing the mortality is to find signs
and symptoms of breast cancer at its early stage by clinic examination. Breast ultrasound
(BUS) imaging has become one of the most important and effective modality for the early
detection of breast cancer because of its noninvasive, nonradioactive and cost-effective
nature; and it is most suitable for large-scale breast cancer screening and diagnosis in lowresource
countries and regions.

Computer-Aided Diagnosis (CAD) systems based on B-mode breast ultrasound
have been developed to overcome the considerable inter- and intra-variabilities of the
breast cancer diagnosis, and have been clinically tested their ability to improve the performance
of the breast cancer diagnosis. BUS segmentation, extracting tumor region of a BUS
image, is a crucial step for a BUS CAD system. Base on the segmentation results, quantitative
features will be calculated to describe tumor shape, size, echo pattern, etc., and be
input into a classifier to determine the category of the tumors. Therefore, the precision of
BUS segmentation directly affects the performance of the quantitative analysis and diagnosis
of tumors.

Automatic BUS image segmentation study attracted great attention in the last two
decades due to clinical demands and its challenging nature, and results in a lot of automatic
algorithms. We can classify existing approaches into semi-automatic and fully automatic
methods according to with or without user interactions in the segmentation process. In most
semi-automatic methods, user needs to specify a region of interest (ROI) containing the
lesion, a seed in the lesion, or an initial boundary. Fully automatic segmentation is usually
modeled as a top-down framework which models knowledge of breast ultrasound and oncology
as prior constraints, and needs no user intervention at all. However, it is quite challenging
to develop automatic tumor segmentation approaches for BUS images, due to the
low image quality caused by speckle noise, low contrast, weak boundary, and artifacts.
Furthermore, tumor size, shape and echo strength vary considerably across patients, which
prevent the application of strong priors to object features that are important for conventional
segmentation methods.

In PDMs-based BUS image segmentation approaches, the main work was focused
on generating good initial tumor boundary. Madabhushi et al. [40] proposed a fully automatic
approach for BUS tumor segmentation by initializing PDMs using the boundary
points produced in tumor localization step; and the balloon forces were employed in the
extern forces. Chang et al. [41] utilized the sticks filter [42] to enhance edge and reduce
speckle noise before using the PDMs. Huang et al. [43] proposed an automatic BUS image
segmentation approach by using the gradient vector flow (GVF) model [44], and the initial
boundary was obtained by using the watershed approach.

Three main approaches exist in BUS segmentation for selecting the threshold: (1) choose
the empirical value as threshold for the entire dataset [71]; (2) select threshold for each
image based on domain related rules [40, 78]; and (3) generate threshold automatically
based on statistical-decision theory [38, 11]. Region growing extracts image regions by
starting from a set of pixels (called seeds) and growing seeds to large regions based on
predefined growth criteria. In [78], Shan et al. proposed an automatic seed generation approach.
Thresholding was used to generate a group of candidate regions first; and then
region ranking criteria based on region location, size and local feature were utilized to
determine a true tumor region; and a pixel in the region was selected as the seed. In [72],
Kwak et al. defined the cost of growing a region by modelling common contour smoothness
and region similarity (mean intensity and size). Watershed could produce more stable
results than thresholding and region growing approaches, and selecting the marker(s) is the
key issue in watershed segmentation. The first solution is to choose the local minimum
gradient as the marker, and further step such as region merging should be involved to avoid
over-segmentation. Another solution is to select makers based on predefined criteria that
can utilize the task-related priors. Huang et al. [60] selected the markers based on grey
level and connectivity. [74] applied watershed to determine the boundaries on binary image.
The markers were set as the connected dark regions on the binary image. [75] applied
watershed and post-refinement based on grey level and location to generate candidate tumor
regions.

However, because these approaches heavily depend
on non-robust information such as the appearance models of tumor or background
tissues, their performance degrades greatly with BUS images having large variations in
image quality and degree and location of artifacts; and (2) all previous approaches were
evaluated by using private datasets and different quantitative metrics, which make the objective
and effective comparisons among the methods impossible. Therefore, it remains
challenging to determine the best performance of the breast tumor segmentation algorithms
available today, what segmentation strategies are valuable in clinic practice and study, and
what image features are helpful and useful in improving segmentation accuracy and robustness.

In the ROI generation step, the proposed adaptive reference point (RP) generation
algorithm can produce the RPs automatically based on the breast anatomy; and the multipath
search algorithm generates the seeds accurately and fast. In the tumor segmentation
step, we propose a segmentation framework in which the cost function is defined by using
NC map and features from the frequency domain. First, the frequency constraint is built
base on the newly proposed edge detector which is invariant to contrast and brightness;
and then the tumor pose, position and intensity distribution are modeled to constrain the
segmentation in the spatial domain. The well-designed cost function is graph-representable
and its global optimum can be found. The flow chart of the proposed approach is shown in
Fig. .1.

In this chapter, I formally present and discuss the core syntax and semantics of the programs
accepted by our systemand eventually define the problem of resource verification. As a first
step, I introduce a core language that captures the relevant syntactic aspects of the input
programs. Specifically, the core language supports recursive functions, recursive datatypes,
contracts and resource templates. For the sake of succinctness and reducing notational overhead,
for certain constructs of the core language I adopt the syntax of lambda calculus instead
of following the syntax of Scala. For instance, anonymous functions are denoted as ¸ terms
and variable declarations are replaced by “let" binders. Nonetheless, the constructs of the
language have a straightforward translation to Scala. The syntax description of the language
also includes higher-order constructs, lazy evaluation and memoization, and specification
constructs meant for use with these features. However since these features are quite involved
and are orthogonal to the definition of the problem, in this chapter I will focus mostly on
first-order constructs and defer the discussion of the semantics of other constructs to later
chapters.

The semantics I present here is a big-step, operational semantics that has two unorthodox
features. Firstly, the semantics not only characterizes the state changes induced by the language
constructs but also characterizes their resource usage. To succinctly formalize usage
of multiple resources supported by our system, the semantics is parameterized by “cost"
functions. These cost functions capture resource-specific parameters and are independently
(re)defined for each resource that is of interest. I also present the definition of these cost
functions for the important resources supported by our system: steps, alloc, stack and depth.

Consider now the semantics rules shown in Figure 2.2. Most rules are straightforward now
that the cost functions and structural equivalence relations have been introduced. Below I
describe some of the complex rules. The rule LAMBDA creates a closure for a lambda term t ,
which is a pair consisting of t , and a (singleton) assignment for the variable captured from the
enclosing context, which is given by FV(t ).

The rule CONCRETECALL defines the semantics of a call whose arguments have been evaluated
to concrete values (in Val). It models the call-by-value parameter passing mechanism: it binds
the parameters to argument values, and evaluates the body (an expression with contracts)
under the new binding. A call evaluates to a value only if the contracts of the callee are satisfied
as given by the rule CONTRACT (discussed shortly). This rule is used by the rule DIRECTCALL
which evaluates direct call expressions of the language. The handling of direct calls is separated
from that of concrete calls whose arguments are values to make the semantics amenable to
extensions for incorporating memoization (discussed in chapter 4). Concrete calls serve as
the keys of the memoization table, which can be evaluated independently.

While the semantics rules shown in Figure 2.2 may be applicable to arbitrary environments, in
reality, the environments under which an expression is evaluated satisfies several invariants
which are ensured either by the runtime (e.g. that every free variable in an expression is bound
in the environment), or by the program under execution (e.g. that a value of a variable is
always positive). As in prior works on data structure verification [Kapur et al., 2006], it is
only reasonable to define the problem of contract/resource verification with respect to such
valid environments under which an expression can be evaluated, instead of under arbitrary
environments. For this purpose, I first define a notion of valid environments.

This section establishes an important property about the semantics presented in Figure 2.2.
Under the assumption that all primitive operations are total, when an expression belonging to
a type correct programis evaluated under a valid environment, there are only two reasons why
its evaluation may be undefined as per the operational semantics: (a) the evaluation diverges,
or (b) there is a contract violation during the evaluation. This property is very important since
the definition of contract verification presented shortly relies on this property. Before formally
establishing this property I state and prove a few important notions and lemmas.

The reflexivity property trivially holds for integers and booleans. To prove the property for
addresses, we can induct over the well-founded relation Ç on Adr. The base case consists
of addresses in the heap that are mapped to values that do not use other addresses. The
reflexivity property clearly holds in this case. The inductive case consists of addresses that
are mapped to values, namely constructor or closure values. However, by hypothesis, every
address they use satisfy the reflexivity property. Given this, the claim holds, since for two
closure/constructor values to be structurally equal they have to invoke the same function or
use the same constructor.

Our approach is primarily aimed at programs where the targets of all indirect calls that may
be executed are available at the time of the analysis. This includes whole programs that take
only primitive valued inputs/parameters, and also data structures that use closures internally
but whose public interfaces do not permit arbitrary closures to be passed in by their clients.
Such data structures are quite ubiquitous and include numerous sophisticated functional
data structures. Some examples include lazy data structures proposed by Okasaki [1998] and
the Conqueue data structure of Scala’s data-parallel library [Prokopec and Odersky, 2015]
(discussed in section 5). I would like to remark that proving resource bounds of programs
where the full implementation is not available at the time of the analysis is quite challenging
and is orthogonal to the work presented in this dissertation. The Related Works chapter
(Chapter 6) discusses a plausible future direction for proving such programs. Below, I provide a
formal description of the kind of encapsulated programs which are supported by our approach.

The input to our system is a functional Scala programwhere the resource bounds of functions
are specified as expressions with numerical holes in their postconditions. Such expressions
are referred to as resource templates. The syntax of the resource templates were presented in
the previous chapter in Figure 2.1. The holes in the templates may appear as coefficients of
variables in scope, which themselves could be bound to arbitrary expressions of the program
through let-binders. The goal of our systemis to infer values for the holes in the template that
will yield an upper bound on the resource usage of the function under all possible executions,
as stated by the resource verification problem (see section 2.5). The programs inputted to
our system can have arithmetic/boolean operations, recursive functions, datatypes, closures
and memoization, as described by the core language syntax shown in Figure 2.1. Our system
therefore incorporates techniques that can solve resource templates in the presence of these
language features. In this chapter I described an analysis that can infer resource template
for programs having three of the five features listed above: arithmetic/boolean operations,
recursive functions and datatypes. The subsequent chapters discussmajor extensions to the
analysis for supporting higher-order functions and memoization.

Our approach for the verifying such programs operates in two phases. In the first phase, it
generates an instrumented program that accurately tracks the resource usage of expressions
(described in section 3.1). The resource bounds of the input program become invariants of
the instrumented program. In the second phase, using an assume-guarantee reasoning, the
algorithm generates a 98 formula called verification condition (VC) such that any satisfying
assignment to the formula yields a solution for the holes (described in section 3.2). The
coefficients in the templates that are holes become existentially quantified variables of the VC.
A novel decision procedure described in section 3.3 is used to infer values for the existentially
quantified variables of the VC.

Our approach decouples the encoding of the semantics of resources from their analysis. This
is accomplished by an exact instrumentation of programs with their resource usage that
does not approximate conditionals or recursive invocations. The instrumentation severs two
main purposes: (a) it makes the rest of the approach agnostic to the resource being verified
thus aiding portability across multiple resources, and (b) it eliminates any precision loss in
the encoding of the resource and restricts the incompleteness to a single source, namely the
verification algorithm. The latter aspect is very important since it allows the users to help the
verification algorithms with more specifications until the desired bounds are established.

Depth [Blelloch andMaggs, 1996] is a measure of degree of parallelism in an expression and,
intuitively, corresponds to the longest chain of data dependencies between the operations of
an expression. It can be viewed as an estimate of parallel execution time when infinite parallel
computation power is available and can be exploited without any overheads. Furthermore,
it has been shown by Blelloch and Maggs [1996] that depth can be combined with steps to
derive a good approximation of the parallel running time on a specific parallel architecture.
From our perspective, this resource is interesting as it ismeasure of parallelism inherent to
an implementation, and is independent of the runtime. Moreover, in principle, a similar
instrumentation strategy can be used to measure evaluation steps in the presence structured
parallelism constructs like fork-join parallelism.

This section discusses the reasoning that our systemuses to reduce the problem of checking
contracts of a (first-order) function to that of proving validity of predicates (i.e, boolean-valued
expressions). The constraints generated by this reasoning are solved by translation to formulas
in a suitable logic and by applying customized decision procedures, which is the subject of
the next section. The aim of this section is to separate the principle underlying reduction
of contract checking to constraint solving from the encoding of the constraints into logic
and its decision procedure. This separation simplifies the understanding of the soundness
and completeness of the system, and also allows enhancing or adapting one aspect while
reusing the other. In particular, in Chapter 4, I describe an extension to the assume-guarantee
reasoning that enables a more effective and less-overhead contract verification for higherorder
functions with memoization.

Under this reasoning, to establish that the contract of a function f is valid, one can assume (or
hypothesize) that the pre-and post-condition of the functions called by f (including itself )
hold at the call sites within the function f , and guarantee that the postcondition of f holds
for every valid environment that satisfy the precondition of f . Also, the precondition of each
function has to be guaranteed at its call sites. This assume-guarantee reasoning relies on
induction over the number of calls made by the function f , which is finite and well-founded if
the function f is terminating. In essence, this reasoning provides a sound way of checking
partial correctness of contracts. As mentioned in the Introduction, our system verifies the
termination of functions independently using termination checker of the underlying LEON
verifier [Nicolas Voirol and Kuncak, 2017]. Thus it ensures that all functions terminate on all
valid environments and hence their contracts hold for all valid environments.

I would like to remark that while this reasoning is a widely employed technique in correctness
verification, it has hitherto not been used in resource verification primarily because of the
need for an independent termination checking algorithm different from the resource analysis.
In this dissertation, I demonstrate that decoupling resource verification from termination
makes the former tangible on complex programs where the resource bounds and invariants
themselves rely on recursive functions (which is almost always the case with our benchmarks).
In such cases, establishing a meaningful time (or resource) bound on functions meant for
use in specifications is unnecessary and also sometimes infeasible without changing the 
implementation. For instance, in the programshown in Figure 3.5, it is meaningless to prove a
time bound for the function height. On the other hand, proving termination requires much
simpler reasoning and effort, but when established it permits a powerful inductive reasoning
such as the one described here for proving other properties including resource bounds. As
we proceed to a language with support for lazy evaluation and memoization (Chapter 4) this
decoupling becomes all the more important.

In the sequel, I formalize the assume-guarantee reasoning and subsequently show the soundness
of this reasoning for contract and resource verification (see section 2.5 for their definition).

Observe that this modular reasoning requires that the assume/guarantee constraints hold for
all environments in which the parameters are bound to a (type-correct) value (by the definition
of !). However, as per the definition of contract verification presented in section 2.5 it suffices
to consider only valid environments that reach the function bodies. This means that pre-and
post-conditions of functions should capture all necessary invariants needed for the verification
of contracts. That is, every other global program invariant that is known to hold for a function
(e.g. discovered through an independent static analysis) have to be encoded using the pre-and
post-conditions if they have to be used by this reasoning.

This section discusses the translation of the assume-guarantee obligations to a logical formula
called verification condition (VC) that uses only a restricted set of theories typically supported
by SMT solvers, and can be handled by the solveUNSAT procedure detailed in section 3.3.4.
The key ideas of the encoding presented here is based on the algorithm used by the LEON
verifier [Blanc et al., 2013, Suter, 2012]. However, an important difference is that the VC in our
case has holes, which become existentially quantified variables of the VC. Below I present an
overview of the VC generation algorithm using the list-reversal example shown in Figure 3.2.
At a high-level, the logical encoding of a predicate belonging to a first-order fragment of our
language is straightforward for most constructs except function calls, since most (first-order)
operations in our language have a direct mapping to a suitable theory operation supported by
SMT solvers. For instance, primitives types such as Int and Bool map to the integer and boolean
sorts. User-defined datatypes map to algebraic datatypes. If-else and Match expressions
correspond to disjunctions, and let expressions to equalities. However, there are two nontrivial
aspects to VC generation algorithm used by our system: (a) Clausification of predicates
using control literals, and (b) encoding of recursive functions as uninterpreted functions
through unfolding. The former aspect is used to efficiently obtain a satisfiable disjunct from
the VC given a satisfying assignment for the variables of the VC, which is used by the algorithm
for inferring holes (section 3.3). The latter aspect enables using abstractions of recursive
functions which can be refined (i.e, mademore precise) on demand.

In our approach, VCs are constructed incrementally wherein each increment makes the
VC more precise by unfolding the function calls that have not been unfolded in the earlier
increments. This process is referred to as VC refinement. The functions in the VCs at any given
step are treated as uninterpreted functions. Hence, every VC created is a sufficient but not
necessary condition for the corresponding assume-guarantee obligation to hold.

Another important aspect of using this Farkas’-lemma-based approach is that it is complete
only for linear real formulas but not for linear integer formulas. However, the incompleteness
did not manifest in any of our evaluations of the system on practical benchmarks. More
precisely, there wasn’t a benchmark used in the experiments for which the inference algorithm
failed to discover a valid assignment for the holes due to the incompleteness in applying
Farkas’ Lemma. Similar observation has also been documented in the previous works such as
by Gulwani et al. [2008]. However, it is not known if the incompleteness in applying Farkas’
Lemma prevented the algorithmfrom discovering minimumsolutions, since evaluating the
minimality of the inferred constants is quite difficult. Nevertheless, as described in section 5,
the minimality of the inferred constants are evaluated empirically.

I would like remark that in the evaluations that were carried out using the systemevery steps
bound that was established with this reasoning was also provable directly by the inference
algorithm. Nevertheless, I believe that on large real-world programs the reduction in the
instantiation of axioms of multiplication achieved by this divide-and-conquer reasoning
may provide significant speed ups. Furthermore, by breaking down the proof argument for
nonlinear steps bounds, this reasoning also offers a fine-grained control to the users of the
tool for establishing nonlinear bounds.

this chapter, I extend the technique presented in the earlier chapters to programs that
use higher-order features and memoization. Recall that the core language syntax shown in
Figure 2.1 supported an annotation @memoize on functions. This annotation serves to mark
functions that have to be memoized. Such functions are evaluated exactly once for each
distinct input passed to themat run time. The main challenge that arises in the presence of
these features is that while the source programs use rich abstractions such as memoization
and higher-order functions, the SMT solvers support more basic logical theories like theory
of uninterpreted functions, algebraic datatypes, and arithmetics. Much like a compiler that
translates a high-level program down to machine instructions, the goal of the system is to
translate these programs to formulas efficiently decidable by SMT solvers. However, this has
to be accomplished without sacrificing completeness.

In this chapter, I extend the semantics presented in section
2.3 with an built-in cache that memoizes the values of functions calls invoking memoized
functions. With this extension it becomes possible to define the semantics of the specification
constructs that are meant for expressing properties that depend on the memoization state.
The extended semantics is presented in section 4.1. However, the downside of these extension
is that the language is no longer referentially transparent with respect to the changes in the
cache, though it is with respect to the changes in the heap. This is somewhat expected since
the language allows constructs such as cached that query the cache. However, these constructs
are only restricted to the specification expressions and are not a part of the source expression.

Figure 4.1 shows the semantic rules for the constructs of the language that use memoization,
and new specification constructs that deal with higher-order and memoization features. There
are three types of direct calls rules: a call to function that is not memoized NONMEMOIZEDCALL,
which is same as a direct call, a call to memoized function that is a hit in the cache:
MEMOCALLHIT and that is a miss in the cache: MEMOCALLMISS. The MEMOCALLMISS is
the only rule that updates the cache. Every semantics rule presented in Figure 2.2 other than
rule CONTRACT remain unchanged, except that the input and the output environments now
also have a cache, so they omitted from the Figure 4.1. The reachability relation defined in
Figure 2.3 extends to the new semantic rules shown here in a straightforward way.

I would like to remark that the cost of the lookup operation depends on several factors such as
the implementation of the cache, whether or not datatypes are hash-consed etc. and hence
may requiremultiple interpretations. This definition for cost parameters was chosen in the
implementation since, in the benchmarks we target, functions that memoize data structures
are often methods of the data structure that relied on lazy fields for memoization. The cost of
memoization in this case is a small constant. However, in principle, the definition of the cost
function could be changed to run the system on a different memorymodel.

Note that since our language does not support existential quantifiers the matching construct,
in fact, makes the specification languagemore expressive. This construct is useful for specifying
the requirements on the captured variables of the closures that are passed into a function.
For instance, the following code snippet, shown in the syntax of the core language, shows
a function foo that accepts a closure whose target is the function divide and whose captured
argument (which is the divisor) is a positive value. The specification function posArgs defined
using the fmatch construct returns true if and only if the parameter closure invokes the divide
function and its captured argument is positive.

While the referential transparency or purely functional behavior of the first-order language
considered this far was quite evident. The introduction of cache and the specification constructs
has made this property more trickier. The language allows expressions to query the
state of the cache e.g. using the construct cached. While this is indispensable for specifying
properties about the state of the cache, this also makes the expressions of the language not referentially
transparent. However, as captured by the syntax shown in Fig. 2.1, these constructs
are restricted to the specifications.

In order to guarantee full referential transparency of the source
expressions, we impose the restriction that the contracts of memoized functions in the all
acceptable input programs should be cache monotonic. This property is soundly enforced by
translation to a model program, which is discussed in section 4.4. The property guarantees
that if a source expression evaluates to a value u at a point in the evaluation, then it will
evaluate to a value v at a later point in the evaluation (for the same or equivalent argument
values) such that u and v are structurally similar. That is, memoization has absolutely no
effect on the result of source expressions. In the sequel I formally establish this property.

As in the case of first-order programs, our approach through a series of transformations,
reduces the problem of resource bound inference to invariant inference for a functional
first-order programs. However, due to the higher-order andmemoization features this phase
is more involved than the instrumentation phases. This phase is referred to as the model
generation phase, as is the subject of discussion of this section. In the following section, I
described the generation of verification obligations for verifying the first-order program using
an extended assume-guarantee reasoning, which then solved using the inference algorithm
described in the earlier chapter in section 3.3.

In this section, we discuss the approach for verifying contracts of themodel programs generated
as described in the previous chapter. In principle, since the model program uses only
first-order features the techniques described in Chapter 3 can be used to verify the model
program. However, as I will describe in this section, applying the function-level, modular
assume-guarantee reasoning will result in obligations which to be established require dramatically
more specifications (which are provided by the user). In other words, the simple
function-level, modular reasoning increases the contact annotation overhead in the programs
dramatically. To address this difficulty I introduce an extension to the assume-guarantee
reasoning: creation-dispatch reasoning, which propagates cache-monotonic properties that
hold at creation point of closure to the invocation points.

I now explain this difficulty in applying the traditional, function-level modular reasoning for
verifying model programs using the example shown in Figure 4.4, which was generated for the
lazy take function shown in Figure 4.3.

If the functions in the input program have holes then the assume-guarantee obligations
generated as above will also have holes. The problemthen is infer values for the holes that will
make the creation-dispatch obligation hold. These creation-dispatch obligations are solved
using the inference algorithmpresented in section 3.3. For the lazy take function shown in
Figure 4.3, our algorithm inferred that it completed in at most 10 steps

In this section, I summarize the results of using ORB to verify first-order functional programs.
The resource verification of these programs uses techniques described in section 3. Figure 5.1
lists the benchmarks belonging to this class of programs that were verified using ORB. The
figure lists the benchmarks and the lines of code in each benchmark in column Loc. The
column T shows the number of functions in each benchmark with a resource template and
the column S the number of specification functions in each benchmark. Specification functions
are not verified for resource usage. They are only proven to be terminating using the
termination checker of the LEON verification system [Nicolas Voirol and Kuncak, 2017]. The
figure also shows a sample template for the steps resource for one or more functions in the
benchmark. The benchmarks comprise approximately 1.8K lines of Scala code, 150 functions
and 82 resource templates (for each resource considered). Below I explain the benchmarks in
more detail.

The benchmark sort contains the implementations of quick sort, insertion sort,
merge sort. The tool was able to establish the precise running time bound of these algorithms.
However, for quick sort andmerge sort the bounds relied on non-trivial axioms ofmultiplication,
which weremanually provided as proofs hints to the system. In the case of quick sort,
the axioms were also verified using the system. In the case of merge sort, the axioms weren’t
provable within the system due to the incompleteness in the nonlinear integer reasoning of
the underlying SMT solvers. However, those axioms were verified independently using another
verification engine.

In order to test the minimality
of the constants inferred, two kinds of experiments were performed. One experiment that
scaled to only simpler benchmarks is explained here. The other experiment based on runtime
evaluations is detailed in the context of higher-order, memoized benchmarks discussed in the
following section. In the first experiment, for each bound inferred by the tool, one coefficient
of the bound was decremented while keeping the others fixed. The system was used to check
if the bounds thus constructed were valid. Since these bounds do not have holes, the system,
in principle, is capable of discovering a counterexample, which is an input that violates the
bound, if the bounds are not valid. If a counterexample is found, it implies that there cannot
exist a valid bound where all constants are smaller than the inferred bound. In other words
the bound inferred is pareto optimal

Figure 5.4 shows the results of verifying the bounds
on the usage of the alloc resource on selected benchmarks. The alloc resource measures the
number of heap-allocated objects. Since our benchmarks are first order, the heap-allocated
objects comprises only datatype instance. As shown in the figure, the constants in this case
are rather small compared to steps bounds, and in many cases one. However, the alloc bound
asymptotically matches the steps bound in most cases. This is because the benchmarks
are mostly functional data structures, whose operations like insert or delete would have to
replicate the entire portion of the data structure that is traversed by the operation. Hence, the
heap-allocation is proportional to the work done i.e, steps.

nother closely related technique for inferring invariants for recursive functions
using user-provided templates is Counterexample Guided Abstraction Refinement (CEGAR).
CEGAR is also an iterative algorithm. In every iteration, CEGAR computes an abstraction of the
input program, and searches for a counterexample path in the abstract program that violates
the given property. If a counterexample is found, the abstraction is refined so that it (and also
other related counterexample paths) are not feasible in the refined abstraction. Typically, the
refinement is constructed by computing an interpolant that provides a succinct condition for
the infeasibilty of the concrete path that corresponds to the abstract counterexample path
in the original program. The concrete path represents a finite execution and typically goes
through recursive calls a finite number of times (unlike a static path). Tools such as HSF
[Beyene et al., 2013] can compute interpolants belonging to a given template. However, there
are not many off-the-shelf tools that can performinterpolation in the presence of ADTs, recursive
(or uninterpreted) functions and nonlinear operations. Nonetheless, interpolation also
suffers from similar issues as CEGIS, since for any finite execution, the resources consumed
by the execution is a constant. This suggest that, in theory, interpolation-based CEGAR can
always come up with increasing values for the constant term of a template, which would
provide a valid bound for the concrete paths explored until a particular

One of the most challenging class of benchmarks considered in our evaluation are the
scheduling-based lazy data structures proposed by Okasaki [1998]. The benchmarks rtq,
deq, num, and conq belong to this class. These data structures use lazy evaluation to implement
worst-case, constant time as well as persistent queues and deques using a strategy called
scheduling. These are one of the most efficient persistent data structures. For instance, the rtq
[Okasaki, 1995] benchmark takes a few nanoseconds to persistently enqueue an element into
a queue of size 230. The conq data structure [Prokopec and Odersky, 2015] is used to implement
data-parallel operations provided by the standard Scala library. To my knowledge there
exists no prior approach that proves the resource bounds of these benchmarks. Moreover,
the verification of these benchmarks also led to the discovery and fixing of a missing corner
case of the rotateDrop function shown in Fig 8.4 of [Okasaki, 1998], which was unraveled by the
system.

Though the data structures differ significantly in their internal representation, invariants,
resource usage and the operations they support, fundamentally they consists of streams called
spines that track content, and a list of references to closures nested deep within the spines:
schedules. The schedules help materialize the data structure lazily as they are used by a client.
I now provide a brief overview of the kind of specifications that were required to verify the
resource bounds of these benchmarks using the example of real-time queue.

Similar to rtq, other scheduling-based data structures
also consist of a spine that store the content, which corresponds to the front and rear
streams in the case of rtq, and a schedule, which is a list of references to closures possibly
nested deep within the spine. (In the case of rtq the schedule is a single reference.) The
content of the spine can be other data structures. In the case of conq, the content is a AVL-like
balanced tree called ConcTree [Prokopec and Odersky, 2015]. The schedules correspond to
unfinished operations like enqueue initiated previously. Every operation on the data structure
is performed lazily in increments that complete in a constant number of steps. Whenever a new
operation is initiated, the schedules are forced so that an increment of a previous operation
is performed. A complex invariant ensures that the pending operations do not cascade to
result in non-constant time worst-case behavior. Figure 5.12 pictorially depicts the invariants
of the conqueue data structure. In the figure, 1 or 0 represents a conc-tree – 1 at a position i
represents a conc-tree of size 2i and 0 represents a conc-tree of size 1. The symbol inc denotes
a suspension of a pushLeft function that takes two arguments: a conc-tree and (a reference to)
another conqueue. If the suspension is forced it will prepend the conc-tree to the beginning of
the queue and adjusts the data structure if necessary. Notice that unlike rtq here the schedules
are a list of references to closures.

This chapter introduces the preprocessing of gel image enhancement to get an image without noisy 
and degradation. The preprocessing can be done by convert it into gray, background subtraction and 
applying hybrid filters for preparing it to get the biological information. Lanes of gel image can 
be detected by get the distance between them. Finally bands gel image can be done by divide the 
enhanced gel image into sub images according to lanes. All preprocessing have been done by use our 
"Eg Gel Analyzer" software

There are many factors that could affect the gel image quality [43] such as voltage, field strength, 
time, reorientation angle, agarose type, and concentration, the buffer chamber temperature, etc. 
Image quality could affect the accuracy of extracting right information from these gel images. Thus, 
the need for enhancing and analyzing software the images is essential for biologists. Image preprocessing 
is done by using Image processing Toolbox R2013a software.

Molecular weight markers are pre-determined fragment sizes and concentrations are commercially available. 
They are used as a standard to determine unknown fragment sizes on the gel image. This chapter discusses 
what molecular weight markers are, types of the unknown bands, how to estimate the size of every unknown 
band with two methods and phylogenetic tree is generated.

DNA Markers, Protein Markers or RNA Markers is a set of standards that are used to identify the 
approximate size of a molecule which run on a gel during electrophoresis, using the principle that 
molecular weight is inversely proportional to migration rate through a gel matrix.

The determination of DNA molecular weight markers of unknown bands was estimated by two different methods entitled 
"Improved traditional method" and "Ruler method". The two methods could get the unknown molecular weight, the type 
of every band, generate phylogenetic clustering tree. The unknown molecular weight will be determined according 
to the molecular weight of the marker. The type of every band will be identified as unique, Polymorphism or 
monomorphism.

The term of unique morph means that there is only single individual have a distinct phenotype from his population. 
Polymorphism (poly) in biology happens when two or more obviously dissimilar phenotypes exist in the same population 
of a species [21]. In same manner, the same concepts can be applied in gel analysis. Polymorphic bands mean that the
bands existence is different between different lanes (samples). This means that the bands (markers) could present 
in an individual or some individuals but is absent in another individual(s). Monomorphism means having only one 
band with the same size in all the samples. Unique band is band with unique size that only can be found in this 
lane (sample). All of previous types are shown in Figure (3.2).

Ruler estimation is a new method that is applied to evaluate unknown bands between any two marker’s bands 
respectively. Because the distance between marker’s sizes not equal as it is observed in our test marker the 
bands closer to the top are wider than those closer to the bottom of the image. A new ruler scale is applied 
between every two respective marker’s bands based on create a scale between bands of the marker. The ruler 
between every two respective marker’s bands is created, so when the position of unknown band is determined on 
the ruler according to its raw in the image immediately its size is calculated.

This chapter presents an overview about classic and advanced clustering algorithms as a field of unsupervised 
learning. Two classic algorithms types of clustering as k-means and hierarchical dendogram clustering algorithms 
have been applied. K-means clustering algorithm has been divided bands into groups according to its sizes in 
addition to dividing gel electrophoresis images dataset into similar groups to be easy and fast to be used. 
Hierarchical dendogram clustering has been divided the convergence similar lanes by using different distances 
metric. Finally, fuzzy c-means clustering algorithm has been applied as a complex type of clustering to divide 
bands and a dataset of gel electrophoresis image into convergence groups.

The complex type of clustering algorithms includes those that do not define a hard or crisp membership for the 
objects belonging to a cluster. An example of a complex clustering algorithm is fuzzy c-means which relates each 
object to a given cluster centroid with different degrees of membership.

The other form of complex clustering is called probabilistic or model-based clustering in which the data are 
assumed to be a mixture of underlying probability distributions, such as Gaussian mixture models. The limitation 
however of probabilistic clustering is that is relies on the assumption that the dataset follows a specific 
distribution, which may not always be true [16].

The next step is to take each point belonging to a given data set and associate it to the nearest centroid. 
When no point is pending, the first step is completed and another group is done.

At this point we need to re-calculate k new centroids as bar centers of the clusters resulting from the previous 
step. After we have these k new centroids, a new binding has to be done between the same data set points and the 
nearest new centroid. A loop has been generated.

As a result of this loop we may notice that the k centroids change their location step by step until no more 
changes are done. In other words centroids do not move any more.

In the context of gene expression data, clusters can be formed using the genes or the samples that exhibit similar 
behaviors or patterns. In gene-based clustering the genes are treated as objects and the samples as features in 
contrast to sample-based clustering. Gene-clustering offers further insight and understanding of gene function, 
gene regulation and cellular processes of an organism. Sample-based clustering can be used to distinguish between 
samples that are possibly indistinguishable using classical morphological approaches [50].

Each band represents a piece of DNA. The extent to which they move through the gel has to do with the fragment's 
electrophoretic mobility. The lighter the molecule in general the faster it can move through the gel. Usually when 
performing a gel electrophoresis one would use markers. Identifying which bands have convergence in the sizes is 
very important to know which bands are replicated. Dividing bands into groups have converged sizes, made the 
process of taking biological decisions easy and simple.

In the first approach, the hierarchy is built by starting from the top and goes to the bottom which is called 
divisive approach (top-down approach). This approach accepts the whole data set as a cluster at the beginning 
of the algorithm and divides the data set into two most separated divisions. At each step, the most distant set 
of objects are separated from the others. This procedure continues until every individual object is left alone.

On the other hand, the second and more common approach starts to cluster the objects from the bottom and continues 
to the top which is known as agglomerative approach (bottom-up approach). Unlike the divisive approach, 
agglomerative approach takes each individual object as a cluster at the beginning of the clustering and continues 
to group the most similar objects at each step until all objects are clustered into one.

The main principle behind hierarchical clustering is to group the data into a tree structure through either a 
divisive or agglomerative process [50]. In agglomerative clustering, called a bottom-up approach, each data point 
is initially its own cluster and subsequently each datum is merged based on the pairwise distance metrics until 
there is a single remaining cluster. In divisive clustering all the data points start in the same cluster and 
divided until each datum forms its own cluster.

The fuzzy c-means algorithm is the most popular soft type clustering algorithm for gene expression analysis. 
The fuzzy c-means algorithm uses a similar approach to the k-means algorithm in optimizing the cluster centroids.

However, each gene is considered a member of all the clusters with varying amounts of membership. The membership 
is closely related to the distance or similarity measure between a gene and a given cluster centroid.

The fuzzy c-means algorithm is also based on the minimization of an objective function. The difference however 
of the fuzzy c-means objective function to equation 4.16 is a weighting or membership term for each data point 
and its corresponding cluster.

Data-parallel operations can be implemented for a wide range of different data structures
and operations – since the Scala standard library collections framework [Odersky(2009)]
consists of dozens of bulk operations, it would be very hard to write and maintain a
framework that reimplements those operations for every data structure. In this chapter
we study a generic approach to implementing data-parallel collections in a managed
runtime. Here, generic means that data-parallel operations can be implemented only
once and then used for a wide range of data structures granted that they meet certain
requirements. As we will see, this greatly simplifies the implementation and maintenance
of a data-parallel framework.

Data elements might be stored inside arrays, binary search trees or hash-tables – they
should all support the same data-parallel operations. Data-parallel operations should
also be generic in the type of elements contained in the data structure. If a client requests
to find the largest element of a collection, the same operation should work with string,
number or file handle elements. Finally, most data-parallel operations are parametrized
by the client at a particular invocation site. In the example with the largest element, the
operation is parametrized by the function that compares string lengths, two numbers or the space 
consumption of a file. We illustrate these relationships as three orthogonal axes in Figure 2.1.

We rely on several assumptions about data structures in this chapter. While these
assumptions might seem natural, they are essential for the parallelization approach we
propose. In the later chapters we will drop some of these constraints on specific data
structures and show several different approaches to data-parallelism.

First of all, in this chapter we assume that data-parallel operations execute in a bulksynchronous
manner. This means that the data-parallel operation caller blocks execution,
the data-parallel operation then starts and completes, and then the caller resumes
execution. Although this constraint can easily be dropped by applying Futures from
Section 4.1.1 to data-parallel operations in this chapter, it has a nice consequence that the
caller cannot modify the data structure while the data-parallel operation is in progress.
Second, we assume element presence – elements we want to execute the operation on are
already present in the data structure before the operation starts. Some data structures
are designed for streaming and are supply-driven, so their elements can be become
available as they arrive. We do not consider them in this chapter.

Finally, we assume quiescence during the execution of a data-parallel operation lifetime.
This means that the data structure is not modified during the entire execution of the
data-parallel operation. These modifications include adding new elements to the data
structure, and removing and modifying existing elements. Note that quiescence implies
element presence, but the presence of elements when the operation starts does not imply
quiescence – the distinction will be crucial in Section 4.2 when we apply data-parallelism
to concurrent data structures.

The parReduce method starts by calling a split method on the iterator. Given a freshly
created iterator this method returns a set of iterators traversing the subsets of the
original iterator. We defer the details of how split is implemented until Section 2.3.
Each of those subset iterators it is then mapped into a new Task object that traverses
the corresponding iterator it and computes the sum. The caller thread forks those
task objects and then calls join on each of them – the task objects are in this way
mapped into the list of results. The results computed by different task objects are
then reduced sequentially by the caller thread.

The simple implementation presented above has certain limitations. In particular, it is
unclear how to implement split efficiently on iterators for arbitrary data structures.
Here, efficiently means that the split takes O(P log n) time where n is the number of
elements that the iterator traverses and P is the number of workers. Then, the split
may return a certain number of subset iterators that is lower than the number of available
processors. A data-parallel scheduler must ensure that all the available processors are
assigned useful work whenever possible. Orthogonally to efficient scheduling, processors
must execute the workload assigned to them as efficient as possible. Iterators, function
objects and genericity in the collection element type represent an abstraction cost that
can slow down an operation dramatically, but can be overcome with proper compiletime
optimisations. Finally, certain data structures do not have an efficient split
implementation and must be parallelized in some other fashion.

For the benefit of easy extension to new parallel collection classes and easier maintenance
we want to define most operations in terms of a few abstract methods. The approach
taken by Scala standard library sequential collections is to use an abstract foreach
method and iterators. Due to their sequential nature, both are not applicable to dataparallel
operations – they only produce one element at a time and do not allow access to
subsequent elements before prior elements are traversed. In addition to element traversal,
we need a splitting operation that returns a non trivial partition of the elements of the
collection. The overhead of splitting the collection should be as small as possible – this
influences the choice of the underlying data structure for parallelization.

We therefore define a new abstraction called a splitter. Splitters are an extension of
iterators with standard methods such as next and hasNext that are used to traverse a
dataset. Splitters have a method split that returns two child splitters, which iterate
over disjunct subsets of the collection. The original iterator becomes invalidated after
calling split and its methods must no longer be called. The Splitter definition is
shown in Figure 2.2.

Method split returns a sequence of splitters such that the union of the elements they
iterate over contains all the elements remaining in the original splitter. These subsets
are disjoint. Additionally, these subsets may be empty if there is 1 or less elements
remaining in the original splitter 1. For the purposes of achieving decent load-balancing,
data-parallel schedulers may assume that the datasets returned by the split method
are in most cases roughly equal in size.
 
Flat data structures are data structures in which elements are contained in a contiguous
block of memory. This block of memory need not be completely filled with elements.
Such data structures are particularly amenable to parallelization, since a contiguous
block of memory can easily be divided into smaller blocks.

An array is a ubiquitous data structure found in almost all general purpose languages.
In the standard Scala collections framework it is one of the basic mutable sequence
collections. Arrays have efficient, constant time element retrieval by index. This makes
them particularly suitable for parallelization. Similar data structures like integer ranges
and various array-based sequences like Scala ArrayBuffers implement splitters in the
same manner.
An ArraySplitter implementation is shown in Figure 2.3. This splitter contains a
reference to the array, and two indices for the iteration bounds. The implementation
is trivial – method split divides the iteration range in two parts, the second splitter
starting where the first ends. This makes split an O(1) method.

Standard Scala collection library has four different collections implemented in terms
of hash tables – mutable HashMaps and HashSets, as well as their linked variants that
guarantee traversal in the order that the elements were inserted. Their underlying
hash table is an array with both empty and non-empty entries, but the elements are
distributed uniformly in the array. Every element in this array is mapped to a specific
index with a hashing function. In some cases multiple elements map to the same
index – we call this a collision. Hash tables can be implemented with closed addressing,
meaning that collisions are resolved by storing a data structure with potentially
several elements at the same index, or open addressing, meaning that collisions are
resolved by putting one of the collided elements at some other index in the hash table

Scala mutable sets are implemented with open addressing, whereas mutable maps and
linked variants of maps and sets use closed addressing. Figure 2.3 shows a splitter
implementation FlatHashSplitter for a hash set implemented with open addressing.
The closed addressing implementation is slightly more verbose, but similar. We note
that data-parallel operations on linked maps and sets cannot maintain insertion-order
traversal. The reason is that the simple, yet efficient linked list approach to maintaining
insertion order does not allow parallel traversal.

Previous section showed splitters for data structures composed of a single contiguous part
of memory. Trees are composed of multiple objects such that there is a special root node,
and that every other object in the tree can be reached in exactly one way by following
some path of pointers from the root to that node. Trees that we will consider need not
be binary – nodes can have any fixed upper bound on the number of children. In this
section we investigate splitters for trees that are balanced – any two paths between the
root and a node differ in length by a constant factor for any tree, irrespective of the
number of elements stored in that tree. One useful consequence of this constraint is that
the depth of balanced trees is bound by O(log n), where n is the number of elements
stored in the tree.

With this in mind, we will divide the trees in several categories. Combining trees are
trees in which every node contains some metric about the corresponding subtree. Sizecombining
trees are trees in which every node contains the size of the subtree – splitters
are easily implementable for such trees. Updates for persistent trees usually involve
updating the path from the root to the updated node, so augmenting such trees with size
information is often easy. Two-way trees are trees in which every node contains a special
pointer to its parent2, such that this path eventually leads to the root of the tree, which
has no parent pointer. The third class are one-way trees, in which nodes do not hold
a pointer to the parent tree. The splitters in the last two groups are more complex to
implement.

Scala standard library implements persistent hash tables in terms of hash tries with a high
branching factor (up to 32-way nodes) [Bagwell(2001)]. For a typical 32-bit hashcode
space used on the JVM, these tries have a maximum depth of 6 nodes, resulting in more
efficient update and lookup operations.
Hash tries are size-combining trees – every node contains the total number of elements
contained in the corresponding subtree. Nodes can be either internal nodes or leaves.
Every internal node contains an array of pointers to its children nodes, but it does not
contain elements. Elements are contained in leaves. Each leaf contains a single element
(or a single key-value pair in case of maps). Additionally, internal nodes compress empty
pointer entries using a special bitmap flag. Memory footprint is thus reduced compared
to binary tries. The hash trie data structure internals are discussed in more detail in
Section 4.2.2.

Hash trie splitters conceptually divide the hash trie into several smaller hash tries, as
illustrated in Figure 2.4. Each splitter resulting from calling the split method on a
splitter s holds a reference to one of the smaller hash tries. Although such splitting can
be done in O(log n) time, the implementation can avoid physically creating smaller hash
tries by maintaining iteration progress information in each of the splitters, as we show in
Figure 2.5.

The implementation in Figure 2.5 maintains the iteration state using an integer field
progress and two stacks of values. The stacks maintain the path from the root of the
hash trie to the currently traversed leaf node. Upon creation, the stacks are set to point
to the element at position progress in the left-to-right trie traversal. This is done by
the method init in line 40, which skips subtrees as long as the total number of skipped
elements is less than the specified progress, and then recursively descends into one of
the children. In line 46 size information is used to efficiently initialize a splitter.
Traversal is no different than traversal in standard iterators. Every time next is called,
the current leaf is read in line 80 and then the position is advanced to the next leaf node.
Method advance has to check if there are more leaf nodes in the current inner node in
line 68. If there are no more leaf nodes, the current inner node has to be popped of the
stack in line 75. If there are more leaf nodes, the advance method checks if the next leaf
node is a leaf. Non-leaf nodes need to be recursively pushed to the stack in line 71.
Method split divides the range from progress to until into two subranges of roughly
equal size, and then creates two new hash trie splitters whose init method is responsible
for setting their state correctly.

The previous two tree data structures were size-combining trees. We now turn our
attention to splitters for one-way and two-way trees. Note that any technique of splitting
a one-way tree is applicable to a two-way tree. This is because any splitter for a two-way
tree can simply ignore the parent pointer and treat the tree as if it were a one-way
tree. Therefore, in this section we examine the techniques for implementing one-way
tree splitters. In particular, we will assume that the trees are one-way binary balanced
trees for the purposes of this section, but the techniques presented here can be applied
to one-way n-ary balanced trees as well.

In the Scala standard library SortedSets and SortedMaps are implemented in terms of
one-way red-black trees which are binary balanced trees.

Similar to hash tries shown in Section 2.3.2, we note that the state of the iteration for
binary trees can be encoded using a stack. Unlike the size-combining trees, one-way
tree splitters cannot split the tree by maintaining a progress field and then initializing
new splitters using this information. The absence of this information prevents us from
skipping subtrees as in line 46 of HashTrieSplitters. Without skipping the subtrees
the entire tree needs to be traversed and this changes the complexity of split from
O(log n) to O(n). The only traversal information that BinaryTreeSplitters maintain
is the stack of nodes on the path from the root to the current leaf, so the split method
must use that.

While splitters allow assigning collection subsets to different processors, certain operations
return collections as their result (e.g. map). Parts of the collection produced by different
workers must be combined together into the final collection. To allow implementing such
data-parallel operations generically for arbitrary operations we introduce an abstraction
called a combiner.

In Figure 2.7, type parameter T is the element type, and That is the resulting collection
type. Each parallel collection provides a specific combiner, just as regular Scala collections
provide builders. The method combine takes another combiner and produces a combiner
that contains the union of their elements. Combining may occur more than once during a
parallel operation, so this method should ideally have complexity O(1) and no more than
O(P log n), where n is the number of elements in the combiners and P is the number of
processors.

Parallel operations are implemented within task objects, as discussed in Section 2.7. These
tasks correspond to those described in the previous section. Each task defines split
and merge methods. To illustrate the correspondence between task objects, splitters and
combiners, we give an implementation of the task for the parallel map operation in Figure
2.8.

The challenging part of implementing a combiner is its combine method. There are no
predefined recipes on how to implement a combiner for any given data structure. This
depends on the data-structure at hand, and usually requires a bit of ingenuity.
Some data structures have efficient implementations (usually logarithmic) of these
operations. If the collection at hand is backed by such a data-structure, its combiner
can be the collection itself. Finger trees, ropes and binomial heaps implement efficient
concatentation or merging operations, so they are particularly suitable. We say that
these data structures have mergeable combiners.

Another approach, suitable for parallel arrays and parallel hash tables, assumes that the
elements can be efficiently stored into an intermediate representation from which the
final data structure can be created. This intermediate representation must implement an
efficient merge operation, and must support efficient parallel traversal. In this approach
several intermediate data structures are produced and merged in the first step, and the
final data structure is constructed in the second step. We refer to such combiners as
two-step combiners.

While the last two approaches actually do not require any synchronization primitives in
the data-structure itself, they assume that it can be constructed concurrently in a way
such that two different processors never modify the same memory location. There exists
a large number of concurrent data-structures that can be modified safely by multiple
processors -– concurrent skip lists, concurrent hash tables, split-ordered lists, concurrent
avl trees, to name a few. They can be used to create concurrent combiners. An important
consideration is that the concurrent data-structure has a horizontally scalable insertion
method. For concurrent parallel collections the combiner can be the collection itself, and
a single combiner instance is shared between all the processors performing a parallel
operation.

operation. In particular, mutable sequences implemented with arrays or queues, mutable
maps and sets implemented with hash tables or binary search trees, or immutable maps
and sets implemented with hash tries do not have mergeable combiners. In most cases
these data structures have two-step combiners. These combiners use an intermediate
data structure to store results produced by different processors. The intermediate data
structure is used to produce the final data structure.

There are several constraints on these intermediate data structures. First, element
addition must be O(log n), and preferably O(1) with good constant factors. Second, they
must support a O(P log n) time merge operation. Finally, they must support parallel
traversal in a way that subsets of elements with a high spatial locality in the final data
structure can be traversed by a particular processor efficiently. The final constraint
ensures that the final data structure can be constructed with minimal synchronization
costs. We visit several examples of these intermediate data structures in the following
subsections and later in Sections 3.4.1 and 3.4.2. While the intermediate data structures
support efficient merging, they typically do not support efficient operations of the final
data structure.

As their name implies, two-step combiners are used in two steps. In the first step different
processors independently produce combiners with some results and then concatenate
them together. In the second step this concatenated intermediate data structure is
independently traversed to create the final data structure. We study several concrete
examples next.

require traversing both hash tables and merging corresponding buckets together, resulting
in an O(n) combine. Again, hash tables are inadequate as an intermediate data structure
[Prokopec et al.(2011c)Prokopec, Bagwell, Rompf, and Odersky].

Recall, from the beginning of this section, that the intermediate data structure needs to
assign different subsets of elements to different processors. In addition, we want each of
the subsets to occupy a contiguous chunk of memory, to avoid unnecessary synchronization
and false sharing [Herlihy and Shavit(2008)]. To achieve this, we partition the space of
elements according to their hashcode prefixes so that the elements end up in different
contiguous blocks in the final hash table. This partition is independent of the size of
the resulting hash table. The intermediate data structure serves as a set of buckets for
this partition. The blocks in the resulting hash table can then be filled in parallel by
multiple processors that traverse different buckets in the intermediate data structure.
We describe how to do this partition next.

Hash trie combiners can be implemented to contain hash tries as the intermediate data
structure. Merging the hash tries is illustrated in Figure 2.12. For simplicity, the hash
trie root nodes are shown to contain only five entries. The elements in the root hash
table are copied from either one hash table or the other, unless there is a collision as in
the case of subtries B and E. Subtries that collide are recursively merged and the result
is put in the root hash table of the resulting hash trie. This technique turns out to be
more efficient than sequentially building a hash trie.

We compare the performance of recursively merging two existing tries against merging
two hash tables against sequentially constructing a new merged trie in Figure 2.13 –
recursive merging results in better performance than either sequential hash trie or hash
table merging. The merging could also be done in parallel. Whenever two subtries
collide, we could fork a new task to merge the colliding tries. Since the elements in the
two colliding tries all share the common hashcode prefix, they will all end up in the
same subtrie – the subtrie merge can proceed independently of merging the rest of the
trie. This approach is applicable only if the subtries merged in a different task are large
enough.

Parallel recursive merging has O(n/P) complexity in the worst case. In a typical parallel
operation instance, the combine method is invoked more than once (see Figure 2.17).
Hash tries are thus not efficient intermediate data structures.

Combiners implement the combine method by simply going through all the buckets,
and concatenating the unrolled linked lists that represent the buckets, which is an O(P)
operation. Once the root combiner is produced, the resulting hash trie is constructed in
parallel in result – each processor takes a bucket and constructs the subtrie sequentially,
then stores it in the root array. Again, if computing the hash code is expensive, we can
cache the hash code of each key in the bucket. Another advantage of this approach is
that each of the subtries is on average one level less deep, so a processor working on a
subtrie does less work when adding each element.

In most languages and runtimes, ordered sets are implemented with AVL or red-black
trees. To the best of our knowledge, there is no algorithm for efficient (e.g. asymptotic
O(log n) time) AVL or red-black tree union operation. This means that we cannot
implement directly mergeable binary search tree combiners, but have to use the multiplestep
evaluation approach. Binary search tree combiners rely on bucketing and merging
non-overlapping subsets, as we describe in the following.

Combiners produced by separate processors are not guaranteed to contain non-overlapping
ranges (in fact, they rarely do), so we need to use a bucketing technique, similar to
the one used in hash tables and hash tries, to separate elements into non-overlapping
subsets. Recall that the buckets were previously conveniently induced by bits in the
hashing function. A good hashing function ensured that separate buckets have the same
expected number of elements, regardless of the element set contained in the hash table.
Since binary search trees can contain elements from arbitrary ordered sets, choosing
good pivot elements for the buckets depends on the elements produced by a particular
binary search tree bulk operation. This means that a combiner cannot put elements into
buckets before sampling the output collection, to choose pivot elements for the buckets.

The foreach operation takes a higher-order function f and invokes that function on
each element. The return value of f is ignored. The foreach method has two properties.
First, there are no dependencies between workers working on different collection subsets.
Second, it returns no value. Because of these properties, foreach is trivially parallelizable
– workers do not need to communicate while processing the elements or at the end of the
computation to merge their results.

When foreach is invoked, a new task object is created and submitted to the Fork/Join
pool. To split the elements of the collection into subsets, the framework invokes the
split method of its splitter. Two new child tasks are created and each is assigned one of
the child splitters. These tasks are then asynchronously executed by potentially different
workers – we say that the tasks are forked. The splitting and forking of new child tasks
continues until splitter sizes reach a threshold size. At that point splitters are used to
traverse the elements – function f is invoked on elements of each splitter. Once f is
invoked on all the elements, the corresponding task ends. Another example of a method
that does not return a value is copyToArray, which copies the elements of the of the
collection into a target array

The reduce operations takes a binary operator op. If, for example, the elements of the
collection are numbers, reduce can take a function that adds its arguments. Another
example is concatenation for collections that hold strings or ropes. Operator op must
be associative, because the grouping of subsets of elements is undeterministic. Relative
order of the elements can be preserved by the data-parallel scheduler, so this operator
does not have to be commutative.

The reduce operation is implemented in the same manner as foreach, but once a task
ends, it must return its result to the parent task. Once all the children of the parent
task complete, the op operator is used to merge the results of the children tasks. Other
methods implemented in a similar manner are aggregate, fold, count, max, min, sum
and product.

The setIndexFlagIfLesser method is a simple example of a concurrent protocol with
several interesting properties. First of all, it is linearizable. The linearization point is
the only write performed by the method in line 164. Then, the setIndexFlagIfLesser
method is lock-free. If the CAS operation in line 164 fails, then we know that the value
of flag has changed since the last read. It follows that in the finite number of steps
between the last read in line 162 and the CAS in line 164 some other thread completed
the operation. Finally, the updates by this method to flag are monotonic. Because of
the check in line 163, the value of the flag can only be decreased and there is no risk of
the ABA problem [Herlihy and Shavit(2008)].

Operation zip returns a sequence composed of corresponding pairs of elements of
this sequence and another sequence that. The regular split method would make
implementation of this method quite difficult, since it only guarantees to split elements
into subsets of arbitrary size – that may be a parallel sequence whose splitter produces
subsets of different sizes than subsets in this. In this case it would not be not clear which
elements are the corresponding elements for the pairs. The refined psplit method allows
both sequences to be split into subsequences of the same sizes. Other methods that rely
on precise splitting are startsWith, endsWith, patch, sameElements and corresponds.

In this section we discuss how to organize the collection hierarchy for data-parallel collection
frameworks and how to integrate them with existing sequential collection modules
in an object-oriented language. We will study two different approaches to doing this – in
the first approach new parallel collection classes are introduced for each corresponding sequential
collection class, and in the second parallel operations are piggy-backed to existing
sequential collection through the use of extension methods. We will refer to the former as
tight integration [Prokopec et al.(2011c)Prokopec, Bagwell, Rompf, and Odersky], and
to the latter as loose integration [Prokopec and Petrashko(2013)]. Both approaches add
data-parallel operations without changing the existing sequential collection classes, but
retain the same signatures for all the bulk operation methods.

We assume that sequential collections have bulk operations that guarantee sequential
access. This means that calling a bulk operation such as foreach on a collection
guarantees that the body of the foreach will be executing on the same thread on
which foreach was called and that it will be executed completely for one element before
processing the next element. Parallel collections have variants of the same bulk operations,
but they do not guarantee sequential access – the foreach may be invoked simultaneously
on different elements by different threads.

Note that sequential access is not related to ordering semantics of the operations. Dataparallel
operations can still ensure ordering given that the splitters produce two substrings3
of the elements in the original collection and that combiners merge intermediate results
produced by different workers in the same order as they were split.

Referential transparency is the necessary condition for allowing a parallel collection to be a
subtype of a sequential collection and preserving correctness for all programs. Since Scala
and most other general purpose programming languages are not referentially transparent
and allow side-effects, it follows that the program using a sequential collection may
produce different results than the same program using a parallel collection. If parallel
collection types are subtypes of sequential collections, then this violates the Liskov
substitution principle, as clients that have a reference statically typed as a sequential
collection can get different results when that reference points to a parallel collection at
runtime.

This design has several disadvantages. First, it clutters the collection hierarchy with
additional classes, making the API harder to understand. Second, it requires every
sequential collection to be convertible into a parallel collection by calling par. As we
saw earlier, data structures that satisfy certain parallelism constraints can be converted
with thin wrappers, but any non-parallelizable data structure must be converted into a
parallelizable data structure by sequentially traversing it when par is called, and this
cost is not apparent to the user. Parallel collections are required to implement methods
such as reduceLeft whose semantics do not allow a parallel implementation, leading to
confusion. Finally, this design ties the signatures of parallel collections and sequential
collections tightly together as they are shared in the Gen* traits. If we want to add
additional implicit parameters to parallel operations, such as data-parallel schedulers, we
have to change the signatures of existing sequential collection classes.

Another approach to augmenting existing parallel collections classes is through extension
methods. Here, collection classes exist only for the sequential version of the collection.
All the parallel operations are added to this sequential collection class through an implicit
conversion.

One of the goals is that the data-parallel operations have the same names and similar
signatures as the corresponding sequential operations. This prevents us from adding
extension methods directly to sequential collections classes, as it would result in name
clashes. Instead, users need to call the par method to obtain a parallel version of the
collection, as in tight integration. In loose integration, this call returns a thin wrapper
of type Par[Repr] around the collection of type Repr. The par method itself is added
through an implicit conversion and can be called on any type.

Workload scheduling is essential when executing data-parallel operations on multiple
processors. Scheduling is the process of assigning parts of the computational workload
to different processors. In the context of this thesis, the parts of the computational
workload are separate collection elements. Data-parallel scheduling can be done offline,
before the data-parallel operation starts, or online, during the execution of a data-parallel
operation. This thesis focuses on runtime data-parallel scheduling.

being the Java Fork/Join Framework [Lea(2000)] that is now a part of JDK. Fork/Join
Framework introduces an abstraction called a Fork/Join task which describes a unit of
work to be done. This framework also manages a pool of worker threads, each being
assigned a queue of Fork/Join tasks. Each task may spawn new tasks (fork) and later
wait for them to finish (join).

The simplest way to schedule work between processors is to divide it into fixed-size
chunks and schedule an equal part of these on each processor. There are several problems
with this approach. First of all, if one chooses a small number of chunks, this can result
in poor workload-balancing. Assuming that some of the elements have a lot more work
associated with them than the others, a processor may remain with a relatively large
chunk at the end of the computation, and all other processors may have to wait for it
to finish. Alternatively, a large number of chunks guarantees better granularity and
load-balancing, but imposes a higher overhead, since each chunk requires some scheduling
resources. One can derive expressions for theoretically optimal sizes of these chunks
[Kruskal and Weiss(1985)], but the driving assumptions for those expressions assume a
large number of processors, do not take scheduling costs into account and ignore effects
like false-sharing present in modern multiprocessor systems. Other approaches include
techniques such as guided self scheduling [Polychronopoulos and Kuck(1987)] or factoring
[Hummel et al.(1992)Hummel, Schonberg, and Flynn].

An optimal execution schedule may depend not only on the number of processors and
data size, but also on irregularities in the data and processor availability. Since these
circumstances cannot be anticipated in advance, runtime information must be used to
guide load-balancing. Task-based work-stealing [Blumofe and Leiserson(1999)] has been
a method of choice for many applications that require runtime load-balancing.

In task-based work-stealing, work is divided to tasks and distributed among workers
(typically processors or threads). Each worker maintains a task queue. Once a processor
completes a task, it dequeues the next one. If its queue is empty, the worker tries to steal
a task from another worker’s queue. This topic has been researched in depth, and in the
context of this thesis we rely on the Java Fork/Join Framework to schedule asynchronous
computation tasks [Lea(2000)].

expensive than popping them from the worker’s own queue. Second, the Fork/Join
framework allows only the oldest tasks on the queue to be stolen. The former means
the less times stealing occurs, the better – we will want to steal bigger tasks. The latter
means that which task gets stolen depends on the order tasks were pushed to the queue
(forked). We will thus push the largest tasks first. Importantly, after a task is stolen, it
gets split until reaching some threshold size. This allows other workers to potentially
steal tasks. This process is illustrated in Figure 2.17.

We show the pseudocode for exponential task splitting in Figure 2.18, where an abstract
Fork/Join task called Batch[R] is shown. Its abstract methods are implemented in
specific data-parallel operations.

Before we discuss the pseudocode, we note that two additional optimizations have been
applied in Figure 2.18. When splitting a task into two tasks, only one of them is
pushed to the queue, and the other is used to recursively split as part of computation.
Pushing and popping to the queue involves synchronization, so directly working on a
task improves performance. Furthermore, Java Fork/Join Framework support unforking
tasks previously put to the task queue by calling tryUnfork. After a worker finishes
with one of the tasks, it tries to unfork the previously forked task in line 20 and work on
it directly instead of calling join.

An important thing to notice here is that the threshold controls the maximum number of
tasks that get created. Even if the biggest tasks from each task queue always get stolen,
the execution degenerates to the balanced computation tree shown in figure 2.17. The
likelihood of this to happen has shown to be small in practice and exponential splitting
generates less tasks than dividing the collection into equal parts.

The task-based work-stealing data-parallel scheduling shown in this section does effective
load-balancing for relatively uniform data-parallel workloads. In Chapter 5 we show
a more complex work-stealing scheduling technique specifically designed for irregular
data-parallel computations.

This fold implementation assumes we have a uniform workload and only 2 processors,
so it divides the splitter of xs into two child splitters. These left and right child
splitters are processed in parallel – from a high-level perspective, this is done by the
forkAndJoin call. Once all the workers complete, their results can be aggregated
sequentially. We focus on the work done by separate workers, namely, lines 40 through 42.
Note that the while loop in those lines resembles the imperative variant of the method
mean, with several differences. The neutral element of the aggregation z is generic and
specified as an argument. Then, instead of comparing a local variable i against the array
length, method hasNext is called, which translates to a dynamic dispatch. The second
dynamic dispatch updates the state of the splitter and returns the next element and
another dynamic dispatch is required to apply the summation operator to the integer
values.

In the context of the JVM compilation techniques were proposed to eliminate boxing
selectively, like the generic type specialization transformation used in Scala [Dragos(2010)]
[Dragos and Odersky(2009)]. Most of the abstractions introduced so far were parametrized
by the type of the elements contained in the collections. For example, Splitters
have a type parameter T denoting the type of the elements contained in corresponding
collections. Abstractions and internals of a data-parallel framework that interact
with those abstractions must be specialized with the @specialized annotation
[Prokopec et al.(2014b)Prokopec, Petrashko, and Odersky].

To eliminate unneeded method call indirections and the use of splitters in the batch
processing loop, every invocation of a data-parallel operations needs to be specialized
for a particular callsite. This specialization involves inlining the splitter logic and the
operation parameters into the generic operation body, as was done in the ScalaBlitz
framework [Prokopec et al.(2014b)Prokopec, Petrashko, and Odersky].

At every callsite we create a kernel object that optimizes the data-parallel operation.
The Kernel object describes how a batch of elements is processed and what the resulting
value is, how to combine values computed by different workers and what the neutral
element for the result is. The kernel interface is shown in Figure 2.20. The method
apply takes the splitter as the argument. It uses the splitter to traverse its elements and
compute the result of type R. The method combine describes how to merge two different
results and zero returns the neutral element.

In the previous sections we focused on parallelizable data structures, i.e. data structures
that can be divided into subsets in O(log n) time or better, where n is the number of
elements in the data structure. Not all common data structures fit into this category.
Indeed, data structures such as linked lists are unsuitable for parallelization methods
seen so far, as splitting an arbitrary linked list requires traversing half of its elements.
For many operation instances such an O(n) splitting is unacceptable – in this case it
might be more efficient to first convert the linked list into an array and then use splitters
on the array.

In this section we study alternative approaches to parallelizing collection operations on
linear data structures like linked lists and lazy streams. We note that the parallelization
for these data structures does not retain information about the relative order of elements
– the operators applied to their data-parallel operations need to be commutative.

A linked list data structure consists of a set of nodes such that each node points to
another node in the set. A special node in this set is considered to be the head of the
linked list. We consider connected linked lists without loops – every node can be reached
from the head, no two nodes point to the same node and no node points at the root.

Lazy streams are similar to linked lists with the difference that some suffix of the lazy
stream might not be computed yet – it is created the first time it is accessed.

We show a way to parallelize linked list operations in Figure 2.23, where the linked list
itself is used as a stack of elements. In the concrete example above we use the immutable
Lists from the Scala standard library. The stack is manipulated using atomic READ and
CAS operations 4 to ensure that every element is assigned to exactly one worker. After
reading the stack field in line 84, every worker checks whether it is empty, indicating
there is no more work. If the list is non-empty, every worker attempts to replace the
current list by its tail with the CAS line 89. Success means that only that worker
replaced, so he gains ownership of that element – he must call workOn to process the
element. If the CAS is not successful, the worker retries by calling the tail-recursive
compute again.

The scheduling for linked lists shown in the previous section is lock-free, but it has two
main disadvantages. First, it is inefficient for low per-element workloads. Second, it is
not scalable since workers need to serialize their accesses to the stack field. It is thus
suitable only for operations where the per-element workload is much higher than the
scheduling cost – one should not attempt to compute a scalar product of two vectors
this way.

Unrolled linked lists consist of a set of nodes, each of which contains a contiguous
array of elements, called a chunk. In essence, the type Unrolled[T] is equivalent to
List[Array[T]]. Unrolled linked list amortize the scheduling costs of algorithm in Figure
2.23 by processing all the elements in the chunk in the workOn call. For low per-element
workloads this makes the scheduler efficient after a certain chunk size, but not scalable –
as the number of workers rise hardware cache-coherency protocols become more expensive,
raising the minimum chunk size for efficient scheduling. A similar contention effect is
shown later on a concrete experiment in Figure 5.7 of Chapter 5, where the number of
elements in the chunk is shown on the x-axis and the time required to process the loop
on the y-axis.

.NET langugages have support for common parallel programming patterns, such as
parallel looping constructs, aggregations and the map/reduce pattern [Toub(2010)].
These constructs relieve the programmer of having to reimplement low-level details such
as correct load-balancing between processors each time a parallel application is written.
The .NET Parallel LINQ framework provides parallelized implementations of .NET
query operators. On the JVM, the Java ParallelArray [Lea(2014)] is an early example
of a data-parallel collections. JDK 8 Parallel Streams are a recent addition, which
parallelizes bulk operations over arbitrary data structures. Data Parallel Haskell has a
parallel array implementation with parallel bulk operations [Peyton Jones(2008)]. Some
frameworks have so far recognized the need to employ divide and concquer principles
in data structure design. Fortress introduces conc-lists, a tree-like list representation
which allows expressing parallel algorithms on lists [Steele(2010)]. In this chapter, we
generalized their traversal concept to maps and sets, and both mutable and immutable
data structures.

Having learned about the basics of implementing a data-parallel collection framework, we
turn to more specific topics in the following chapters. Relying on what we learned so far,
we will explore data structures that are more suitable for data-parallelism, investigate
parallelization in the absence of quiescence and bulk-synchronous operation mode, and
see how to schedule data-parallel workloads that are particularly irregular. Throughout
the rest of the thesis, we apply the fundamentals presented in this chapter, and use
them as a guiding principle when designing efficient data structures and algorithms for
data-parallelism.

In this chapter we investigate several data structures that are better suited for dataparallelism.
Note that balanced trees are of particular interest here. They can be
efficiently split between CPUs, so that their subsets are independently processed, and
they allow reaching every element in logarithmic time. Still, providing an efficient tree
concatenation operation and retaining these properties is often challenging. Despite the
challenge, concatenation is essential for implementing declarative data-parallel operations,
as we have learned when we introduced combiners in Chapter 2.

The first sum implementation decomposes the data structure xs into the first element
head and the remaining elements tail. The sum is computed by adding head to the
sum of the tail, computed recursively. While efficient, this implementation cannot be
efficiently parallelized. The second sum implementation decomposes xs into two parts
ls and rs, using the (for now hypothetical) <> extractor. It then recursively computes
partial sums of both parts before adding them together. Assuming that xs is a balanced
tree, the second sum implementation can be efficiently parallelized.

As noted in Chapter 2, balanced trees are good for parallelism. They can be efficiently
split between processors and allow reaching every element in logarithmic time. Providing
an efficient merge operation and retaining these properties is often challenging. In this
section we focus on the basic Conc-tree data structure that stores sequences of elements
and provides a simple, fast O(log n) concatenation.

Trees with relaxed invariants are typically more efficient to maintain in terms of asymptotic
running time. Although they provide less guarantees on their balance, the impact of
being slightly imbalanced is small in practice – most trees break the perfect balance by
at most a constant factor. As we will see, the Conc-tree list will have a classic relaxed
invariant seen in red-black and AVL trees – the longest path from the root to a leaf is
never more than twice as long as the shortest path from the root to a leaf.

The Conc-tree data structure may be composed of several types of nodes. We will denote
that node type of the Conc-tree as Conc. This abstract data type will have several
concrete data types, similar to how the functional List data type is either an empty list
Nil or a :: (pronounced cons) of an element and another list. The Conc may either be
an Empty, denoting an empty tree, a Single, denoting a tree with a single element, or a
<> (pronounced conc), denoting two separate subtrees.

We show these basic data types in Figure 3.1. Any Conc has an associated level, which
denotes the longest path from the root to some leaf in that tree. The level is defined
to be 0 for the Empty and Single tree, and 1 plus the level of the deeper subtree for
the <> tree. The size of a Conc denotes the total number of elements contained in the
Conc-tree. The size and level are cached as fields in the <> type to prevent traversing
the tree to compute them each time they are requested. Conc trees are immutable like
cons-lists – they are never modified after construction. We defer the explanation of the
normalized method until Section 3.2 – for now normalized just returns the tree.

What are the sufficient conditions to make this tree balanced? Similar to AVL trees
[Adelson-Velsky and Landis(1962)], we will require that the difference in levels of the
left subtree and the right subtree is never greater than 1. This relaxed invariant imposes
the following bounds on the number of elements in the tree with the height level. Assume
first that the tree is completely balanced, i.e. every <> node has two chilren of equal
level. 

From the way that the indexing operation apply is defined it follows that a left-to-right
in-order traversal visits the elements of the Conc-tree – a foreach operation looks very
similar. The update operation produces a new Conc-tree such that the element at index i
is replaced with a new element y. This operation only allows replacing existing Conc-tree
elements and we would like to able to insert elements into the Conc-tree as well. Before
showing an O(log n) insert operation implementation we will study a way to efficiently
concatenate two Conc-trees together.

The expression xs <> ys is different than the expression new <>(xs, ys), which would
simply link the two trees together with one <> node – invoking the new constructor
directly can violate the balance invariant. We thus refer to composing two trees together
with a <> node as linking. Creating a Conc-tree that respects the invariants and represents
the concatenated sequence of the two input trees we term concatenation.

The bulk of the concatenation logic is in the concat method, which is shown in Figure
3.2. This method assumes that the two trees are normalized, i.e. composed from the
basic data types in Figure 3.1 and respecting the invariants.

In explaining the code in Figure 3.2 we will make an assumption that concatenating two
Conc-trees can yield a tree whose level is either equal to the larger input Conc-tree
or greater by exactly 1. In other words, concatenation never increases the Conc-tree
level by more than 1. We call this the height-increase assumption. We will inductively
show that the height-increase assumption is correct while explaining the recursive concat
method in Figure 3.2. We skip the trivial base case of merging Single trees.

The trees xs and ys may be in several different relationships with respect to their levels.
We compute their difference in levels into a local value diff, and use it to disambiguate
between three cases. First of all, the absolute difference between the levels of xs and
ys could differ by one or less. This is an ideal case – the two trees can be linked directly
by creating a <> node that connects them. In this case, concatenating two trees is a
constant time operation.

Otherwise, one tree has a greater level than the other one. Without the loss of generality
we assume that the left Conc-tree xs is higher than the right Conc-tree ys. To concatenate
xs and ys we need to break xs into parts, and concatenate these parts in a different
order before linking them into a larger tree. Depending on whether xs is left-leaning or
right-leaning, we proceed by cases.

Insert unzips the tree along a certain path by dividing it into two subtrees and inserting
the element into one of the subtrees. That subtree will increase its height by at most one
by Theorem 3.1, making the height difference with its sibling at most two. Merging the
two new siblings is thus O(1) by Theorem 3.2. Since the length of the path from the root
to any leaf is O(log n), the total amount of work done becomes O(log n). Note that the
complexity of the insert method remains O(log n) specifically because we concatenate
the new trees in that order. Had we been linking the trees going top-down instead of
bottom-up, the complexity would increase to O(log2 n), as we would be concatenating
consecutively smaller trees to a large tree that is slowly increasing its depth.

This implementation is concise, but trips on one unfortunate aspect of the JVM – it
is not possible to return both trees at once without allocating a tuple, as the JVM
does not yet provide non-boxed value types. In our real implementation, we avoid the
need to create tuples by storing one of the resulting trees into a reference cell of type
ObjectRef[Conc[T]], which is passed around as a third argument to the split method.

In this section we show a modification of the Conc-tree data structure that supports an
amortized O(1) time append operation when used ephemerally. The reason that append
shown in the last section took O(log n) time is that it had to traverse the Conc-tree
from the root to some leaf. We note that the append position is always the same – the
rightmost leaf. However, even if we could expose that rightmost position by defining the
Conc-tree as a tuple of the root and the rightmost leaf, updating the path from that leaf
to the root would still take O(log n) time – this is because the Conc-tree is immutable.
Instead, we will relax the invariants on the Conc-tree data structure.

First, we introduce a new type of a Conc-tree node called Append. The Append node
shown below has a structure isomorphic to the <> node. The difference is that the Append
node is not required to respect the balance invariant, that is, the heights of its left
and right Conc-trees are not constrained in any way. However, we impose the append
invariant on Append nodes, which says that the right subtree of an Append node is
never another Append node. Furthermore, the Append tree cannot contain Empty nodes.
Finally, only an Append node may point to another Append node. The Append node is
thus isomorphic to a linked list with the difference that the last node is not Nil, but
another Conc-tree.

The normalized method behaves differently for Append nodes. We define normalized
to return the Conc-tree that contains the same sequence of elements as the original
Conc-tree, but is composed only of the basic Conc-tree data types in Figure 3.1. We call
this process normalization. The definition in Section 3.1 already does that by returning
this. The normalized method in Append calls the recursive wrap method. The wrap
method simply folds the Conc-trees in the linked list induced by Append.

We postpone making any claims about the normalization running time for now. Note,
however, that the previously defined concat method invokes normalized twice and is
expected to run in O(log n) time. Thus, the normalized method should not be worse
than O(log n) either.

Conc-tree on which the normalized method, and hence concat, takes O(n log n) time.
This attempt sheds light on the fact that the amount of work done by append impacts the
complexity of the subsequent concatenation. The more time append spends organizing
the relaxed Conc-tree, the less time will a concat have to spend later when normalizing.
Before attempting a different approach to append, we note that there is a correspondence
between a linked list of trees of different levels and the digits of different weights in
a standard binary natural number representation. This correspondence is induced by
directly linking two Conc-tree nodes of the same level with a new <> node, and adding
two binary digits of the same weight.

An important property of binary numbers is that counting up to n takes O(n) computational
steps, where one computational step is rewriting a single digit in the binary
representation. Adding 1 is usually an O(1) operation, but the carries chain-react and
occasionally require up to O(log n) rewrites. It follows that adding n Single trees in the
same way results in O(n) computational steps, where one computation step is linking
two trees with the same level together – a constant-time operation.

We therefore expand the append invariant – if an Append node a has another Append
node b as the left child, then a.right.level < b.right.level. If we now interpret
the Conc-trees in under Append nodes as binary digits with the weight 2level we end up
with the sparse binary number representation [Okasaki(1998)]. In this representation
zero digits (missing Conc-tree levels) are not a part of the physical structure in memory.
This correspondence is illustrated in Figure 3.3, where the binary digits are shown above
the corresponding Conc-trees and the dashed line represents the linked list formed by
the Append nodes.

Note that the public append method takes a Leaf node instead of a Single node. The
conc-lists from Section 3.1 and their variant from this section have a high memory
footprint. Using a separate leaf to represent each element is extremely inefficient. Not
only does it create a pressure on the memory management system, but traversing all
the elements in such a data structure is suboptimal. Conc-tree travesal (i.e. a foreach)
should ideally have the same running time as array traversal, and its memory consumption
should correspond to the memory footprint of an array. We therefore introduce a new
type of a Leaf node in this section called a Chunk that packs the elements more tightly
together. As we will see in Section 3.4, this will also ensure very efficient mutable +=
operation.

Let us recount some of the difficulties with conc-ropes that lead to worst-case O(log n)
operations when conc-ropes were used persistently. The O(log n) linking steps ensue
when appending to a Conc-tree that corresponds to a sequence of 1-digits. In Figure
3.3 this sequence of 1-digits had the length two, but the sequence can have an arbitrary
length.

The reason why this happens is that the standard binary number system is too lazy in
pushing the carries forward. This number system allows the carry work to accumulate
more quickly than it is performed, so, occasionally, incrementing needs a logarithmic
amount of time. We need a number system that can push carries more often and avoid
arbitrary length 1-sequences – it can serve as a blueprint for designing deque operations.
One such number system is the Fibonacci number system shown in Figure 3.5. In this
number system the weight of a digit at position n is the n-th Fibonacci number F(n).

Sequence 110 represents the number 5, but so does the sequence 1000 – this number
system is redundant. Its important property is that for every number there exists a
representation that has no two subsequent 1 digits. We can take advantage of this
property by removing all the occurrences of the pattern 11 after every increment. In fact,
it is possible to count by removing at most one occurrence of 11 after very increment.
This number system is ideal for Conc-trees, since the transformation between two
redundant representations is a simple O(1) linking operation. The balance invariant
ensures that we can link two trees adjacent in level. If we remove one occurence of 11
after every append operation, an append operation will never take more than O(1) time.
The data structure can keep track of the lowest occurrence of the 11 pattern with an
auxiliary stack – at any point there might be several such occurrences.

However, the push operation defined like this merges the Conc-trees in the append-list
too often. As shown in that last tree in Figure 3.5, at some point we end up with a long
sequence of 0-digits at the right side of the append-list – in this state a pop operation
needs O(log n) time, because a tree with level 1 needed for the pop operation is buried
O(log n) pointer hops away in the first non-empty Conc-tree (i.e. the rightmost 1-tree
labeled 7 in the last append-list in Figure 3.5). It turns out this counting approach is
too eager in pushing the carries forward.

Interestingly, the deque based on the Fibonacci number system described above can be
modified to eliminate only occurrences of patterns 111 and 000. Although this approach
allows O(1) pop operations, it has the following disadvantage. Removing occurrences of
000 patterns requires tearing apart the tree preceding the pattern to fill in the gap. In
a number system with two digits this means that the tree preceding the 000 gap must
break in two trees of differing heights to replace the gap with the new pattern 110. This
means that every tree has to be left-leaning and this is problematic for deque operations,
which would have to break trees in either direction.

In this section we show the conqueue push-head and pop-head operations. Note that the
push-last and pop-last operations are their mirrored versions. Similar to the conc-rope
shown in Section 3.2 we will introduce several new data types for conqueues that rely on
the basic Conc-trees from Figure 3.1. We show the conqueue data types in Figure 3.7.
Conqueue data types are divided into two groups. The Num, or number, data types on
the left correspond to digits in our number system. We only show implementations for
Zero and Three – One and Two are similar. The Num data type requires the leftmost
and the rightmost method that simply returns the leftmost or the rightmost Conc-tree
contained in it. Its rank operation returns the integer value of the corresponding digit,
e.g. One returns 1 and Two returns 2.

We will impose the following invariant on the conqueue data structure. A Spine that
is k steps away from the root of the conqueue has a left and a right wing containing
Conc-trees whose level is exactly k. The digit in the Tip that is k steps away from the
root also contains Conc-trees with level k. With this invariant, adding an element into
the conqueue corresponds to incrementing a number in the 4-digit base-2 number system.
We call this the rank invariant

We show the implementation of the push-head operation in Figure 3.8. The pushHead
operation relies on a helper method inc. This method appends a Conc-tree to a Num
data type from the left, under the precondition that the Num node is less than Three.
The pushHead operation checks the shape of the conqueue conq. If conq is a Tip with
less than three Conc-trees, it simply calls the inc method. In the case of a Tip with
three trees, it distributes the four Conc-trees into a spine with a Zero tip. This is shown
in Figure 3.9 at the top of the leftmost column. If conq is a spine, we link the new tree
c to the left wing lw, and leave the right wing rw intact. If necessary, a carry is created
from two rightmost Conc-trees of the left wing and a new lazy rear is created for the
new spine. We do not call pushHead recursively, as that could trigger a chain of carry
operations and invalidate the O(1) running time bound

Figure 3.9 uses the 4-digit number system with base 2 to illustrate push-head operations
in a conqueue. It uses the dollar sign $ followed by the digit 4 to denote the locations in
this data structure where there is an unevaluated rear. Different entries in each column
represent different conqueue states. Entries preceded by the ? sign denote that a lazy
rear was evaluated to reach this state. All other entries represent states immediately
after a pushHead operation. In this evaluation schedule we choose to evaluate exactly one
lazy rear after each pushHead operation (assuming there are any lazy rears to evaluate).

Note that at any point there might be more than one lazy rear, as is the case in the
rightmost column. The important property of this evaluation schedule is that a dollar
sign never directly precedes another dollar sign, so the evaluations can never chain-react.
To allow this evaluation schedule, we introduce the lazy conqueues with the data type
Lazy, shown in Figure 3.10. This data type wraps a main Conqueue q and maintains
two lists of references to Spines in the main Conqueue that need to have their rears
evaluated. These lists are called lstack and rstack and they point to specific left and
right wings in the conqueue. The new lPushHead method always evaluates at most one
rear from the top of each stack.

However, the concat operation returns a Conc-tree and not a conqueue. To support
concatenation for conqueues, we must have some means of denormalizing any Conc-tree
back into a conqueue. This denormalized operation does the opposite of the wrap
operation – it unwinds smaller and smaller Conc-trees until it ends up with two long lists.
It consists of two tail recursive methods unwrap and zip. The unwrap method maintains
two stacks of Num nodes for the left and the right wings and the remaining middle of
the Conc-tree. It continuously prunes the middle of the Conc-tree and adds Conc-trees
to the smaller stack of Num nodes to ensure there are as many left wings as there are
right wings. Once this is done, the zip method simply zips the wings together. We show
the denormalized implementation in Figure 3.13 – its added complexity confirms the
folklore that it is easier to break things than to build them.

Although the ArrayCombiner shown in Section 2.4 has O(1) appends, its resizing strategy
requires it to on average write every element twice to memory. Additionally, in a managed
runtime like the JVM reallocating an array requires a sweep phase that zeroes out its
values, only to have the combiner rewrite those default values afterwards. All these
computations add to the constant factors of the += operation.

Conc-ropes with Chunk leaves ensure that every element is written only once. The larger
the maximum chunk size k is, the less often is a Conc operation invoked in the method
pack – this amortizes conc-rope append cost, while giving the benefits of fast traversal.
The ConcBuffer shown above is much faster than the ArrayBuffer when streaming
in elements, while in the same time supporting efficient concatenation. Due to the
underlying immutable conc-rope this buffer also allows efficient copy-on-write snapshot
operations.

Ropes are heavily relied upon in the Xerox Cedar environment. Their description by
Boehm, Atkinson and Plass [Boehm et al.(1995)Boehm, Atkinson, and Plass] performs
bulk rebalancing, after the rope becomes particularly skewed. These ropes guarantee
amortized, but not worst-case, O(log n) complexity. VList [Bagwell(2002)] is a functional
data structure for storing sequences, with logarithmic time lookup operations. Scala
Vectors [Bagwell and Rompf(2011)] are based on the immutable sequence implementation.

Its deque operations have a low constant factor, but require O(log n) time. Compared
to the standard implementation, conc-ropes have more efficient appends and are thus
more suited as combiners. The standard Vector implementation does not support
concatentation, since adding concatenation slows down the append and other operations.
Fortress expresses parallelism using recursion and pattern matching on three node types
[Allen et al.(2007)Allen, Chase, Hallett, Luchangco, Maessen, Ryu, Jr., and Tobin]. All
Conc-tree variants in this chapter provide the same programming model as Fortress Conc
lists [Steele(2009)], and this chapter investigates how to efficiently implement Conc list
operations.

Relaxing the balancing requirements to allow efficient updates was first proposed for a data
structure called the AVL tree [Adelson-Velsky and Landis(1962)]. The recursive slowdown
techniques were first introduced by Kaplan and Tarjan [Kaplan and Tarjan(1995)].
Okasaki was one of the first researchers to bridge the gap between amortization and
persistence through the use of lazy evaluation [Okasaki(1996)].

Okasaki [Okasaki(1998)] gives a good overview and an introduction to the field of
persistent data structures. The catenable real-time queues due to Okasaki allow efficient
concatenation but do not have the balanced tree structure and are thus not
suitable for parallelization, nor they support logarithmic random access [Okasaki(1997)].
Hinze and Paterson describe an implementation of a lazy finger tree data structure
[Hinze and Paterson(2006)] with amortized constant time deque and concatenation operations.

Chapter 2 we made several assumptions about the way data structures are used.
First of all, we assumed that operations execute in bulk-synchronous mode. Second, we
assumed that all the elements in the collection are present before a data-parallel operation
is invoked. Finally, we assumed quiescence during the execution of the operation. In this
chapter we will drop some of these constraints. In Section 4.1, we will not require that
the elements are present in the data structure before invoking the operation, and study
two examples of reactive parallelization.

In Section 4.2 we will study how data structure operations can be parallelized efficiently
without the quiescence assumption. In doing so, we will rely on a lock-free, linearizable,
lazy snapshot operation. Analogous to how lazy evaluation allows applying amortization
techniques to persistent data structures [Okasaki(1998)], in Section 4.2, we will find that
adding lazyness to concurrent data structures allows applying scalable parallel snapshot
techniques. This surprising duality comes from the fact that, while lazyness allows
separate persistent data structure operations to share work, concurrent data structure
operations rely on lazyness to execute parts of the work in isolation.

There are many applications in which an operation needs to be performed on data
elements that arrive later. Streaming is one such example. We want to be able to
express what happens to the data although the data arrives from some source much
later. Another example is an asynchronous computation. Result from a potentially
long-running asynchronous operation is added to a data structure long after specifying
what to do with that result.

Dropping the assumption about the presence of elements has several consequences. The
fact that the elements are not present when the operation begins means that they have
to be added by some other thread. The fact that the data structure must be concurrently
modified during the execution of the operation further implies that we must drop the
quiescence assumption as well – we already noted that quiescence implies the presence
of elements, so this follows naturally. Furthermore, we will focus on non-blocking data
structure operations. Since we do not know when more elements will become available,
we do not want to block the thread calling the operation, so we drop the bulk synchronous
execution assumption as well.

We call this type of parallelization reactive, because the operation reacts to the addition of
new elements into the data structure. It is not known when the elements will arrive, but
when they do, a certain operation needs to be executed on them. In the context of this
section, we focus on reactive data structures that result in deterministic programming
models.

To illustrate these concepts we start by introducing the simplest possible such data
structure, which can hold at most one element. This data structure is additionally
restricted by not allowing the contained element to change after it is added. We call this
data structure a future or a single-assignment variable.

The single-assignment variable is a data structure that can be assigned to only once.
Before it is assigned it contains no data elements – we say that it is empty or unassigned.
After it is assigned an element, it is never mutated again – trying to reassign it is a
program error. This error may manifest itself at compile-time or at runtime, and for
simplicity we will assume the latter1. A single assignment variable may be read at any
point. Reading it before it has been assigned is either a runtime error, results in an
invalid value, or postpones the read until the data element is available. Of the three
alternatives we assume the last one – this ensures that multithreaded programs written
using single-assignment variables are deterministic. Postponing could either block the
reading thread or install a callback that is called later. In the interest of achieving better
throughput we choose the latter.

In this section we divide the single-assignment variables into two separate abstractions –
the future end that allows reading its value and the promise end that allows assigning a
value [Haller et al.(2012)Haller, Prokopec, Miller, Klang, Kuhn, and Jovanovic]. Every
future corresponds to exactly one promise and vice versa. In the implementation these
will actually comprise the same data structure in memory, exposed through two different
interfaces.

The future object returned by this statement is eventually completed with the result
of the body computation. A consumer can subscribe to the result using the onSuccess
method, executing a side-effect such as completing another future. This side-effect will be
executed eventually, but only after the future is completed with a value. In the example
below we show how the result of the future can be used to fetch a url using the http
protocol and another using the https protocol. The second example cannot display the
obtained value in the browser directly, but must decrypt them first.

Replacing newPromise, onSuccess and success with newBuilder, foreach and +=
shows that, despite the important semantic differences in these operations, a map implementation
is identical to the one seen earlier. Thus, a Promise can be regarded as
a Builder for the Future, and its onSuccess method can be regarded as its foreach.
We can define a number of other functional combinators such as flatMap, filter and
zip in a similar way.

It remains to show a typical future implementation. We note that the future can be in
several states. First, it is in the uninitialized state after it is created (U). Then, callbacks
may be added to the future – each addition of a callback represents a different state (Ci).
Finally, after it is assigned, it is in the assigned state (A). The flow between these states
is well defined and no state can be entered more than once. We show the flow in Figure
4.2. This useful property ensures that the implementation in this section will not suffer
from the ABA problem.

Due to the simplicity of its state diagram along with the fact that there is at most a
single element that can logically reside in a Future, the future implementation serves as
a textbook concurrent data structure example. The future data structure will occupy a
single location in memory globally visible to all the threads and manipulated by CAS
instructions. Figure 4.2 illustrates the states of this single memory location p. When
unassigned, this memory location will point to an empty list Nil. To add a callback is to
atomically replace the current list with an updated list of callbacks fi. Once the element
is finally assigned, we replace the callbacks with the value of the element. After that the
callbacks need not be added for later execution anymore, but can be executed directly
with the value of the element.

We show the complete implementation in Figure 4.3. Note that the implementation,
as most lock-free algorithms we will show, uses tail-recursion. An unsuccessful CAS
instruction simply restarts the subroutine. The implementation is lock-free – a failed
CAS can only mean that another thread succeeded in installing a callback or assigning
the value. The monotonicity of the future’s state diagram ensures that the CASes succeed
if and only if there was no change between the last read and the CAS. The changes are
thus atomic, and it is trivial to see that the implementation is correct

In this section, we describe a data type called a FlowPool, together with its implementation
[Prokopec et al.(2012b)Prokopec, Miller, Schlatter, Haller, and Odersky]. As
we will see, FlowPools are prone to reactive parallelization and, as an added benefit,
programs built using FlowPools have deterministic semantics, as was the case with
Futures described in the previous section.

FlowPools are a generalization of Futures in the sense that they allow multiple values
to be assigned. While a callback registered to a Future is called only once for the value
of the future, a callback registered to a FlowPool is called for every value assigned to the
FlowPool. This allows some new combinator methods that were not previously defined
on Futures. Besides a richer programming model, FlowPools process values in bulk,
which allows higher scalability.

We start by describing the FlowPool abstract data type and its operations in more detail.
We then show how basic FlowPool operations are used to implement more expressive
combinators. Finally, we present a concrete lock-free FlowPool implementation that
aims to achieve a low memory footprint. The specific implementation that we will study
is not limited to the FlowPool abstract data type – it can easily be modified to serve as
a concurrent queue implementation.

FlowPools define four basic operations 3. The first operation is called create – it creates
a FlowPool instance. The state-changing append operation (+=) adds values to the
FlowPool. FlowPool traversal registers a callback that is called every time a value is
added to the FlowPool. These three operations are analogous to the ones already seen
on Futures and Promises. The fourth operation seal disallows further writes to the
FlowPool after it reaches a certain size. Note that for a Promise this value is always 1
because it can only be assigned a single value – subsequent writes are implicitly disallowed
after calling the success method, so there is no need for a seal operation on Future.
Although we can make a distinction between the producer and consumer interfaces
[Prokopec et al.(2012b)Prokopec, Miller, Schlatter, Haller, and Odersky] as we did with
Futures and Promises, we will avoid it for simplicity.

The program creates a fresh FlowPool named pool and asynchronously starts another
thread. It then adds a single value 1 to the pool. The asynchronously running thread
uses contains to check if the value 1 is in the pool or not and prints a message "Found."
based on that. Depending on whether the main program or the newly created thread
runs faster, the output of the program is different.

Instead of providing query operations on a specific values which may or may not be
present at a specific time index, a FlowPool defines operations that visit all its values –
namely, traversal operations.

Typically, traversal is provided through iterators whose state may be manipulated by
several threads, which could also violate determinism. Another way to traverse the
elements is to provide a higher-order foreach operator which takes a callback argument
and applies it to each element. Again, determinism is ensured by calling foreach on
every value that is eventually added to the FlowPool, instead of only the values present
in the FlowPool at the time it was created. Values can be added to the FlowPool as long
as the limit set by seal is not reached, so a synchronous foreach seen on traditional
collection types would have to block the caller thread. For this reason the foreach is
asynchronous as it was on Futures – invoking it installs a callback, which is called later.

The aggregate operator can divide elements into subsets, apply the aggregation operator
op to aggregate elements in each subset starting from the zero aggregation, and then combine
different subset aggregations by applying the cb operator to them. Alternative implementations
[Schlatter et al.(2012)Schlatter, Prokopec, Miller, Haller, and Odersky] can
guarantee horizontal scalability by evaluating these subsets in parallel. In essence, the
first part of aggregate defines the commutative monoid and the functions involved must
be non-side-effecting. In contrast, the operator op is guaranteed to be called only once
per element and it can have side-effects.

While in an imperative programming model foreach and aggregate are equivalent in
the sense that one can be implemented in terms of the other, in a single-assignment
programming model aggregate is more expressive than foreach. The foreach operation
can be implemented using aggregate, but not vice versa, due to the absence of mutable
variables.

The operations used in Scala for-comprehensions are shown in Figure 4.4. The higherorder
map operation maps each value of the FlowPool to produce a new one. This
corresponds to chaining the nodes in a dataflow graph. We implement map by traversing
the values of the this FlowPool and appending each mapped value. Once all the values
have been mapped and there are no more values to traverse, we can safely seal the
resulting FlowPool.

The filter operation produces a new FlowPool containing only the elements for which
a specified predicate p holds. Appending the elements to a new pool works as with the
map operation, but the seal needs to know the exact number of elements added. The
aggregate accumulator is thus used to track the number of added elements.

The flatMap operation retrieves a separate FlowPool from each value of this pool and
appends its elements to the resulting pool. The implementation is similar to that of
filter, but the resulting FlowPool size is folded over the future values of intermediate
pools. This is because intermediate pools possibly have an unbound size. The flatMap
operation corresponds to joining nodes in the dataflow graph.

One straightforward way to implement a growing pool is to use a linked list of nodes that
wrap elements. As we are concerned about the memory footprint and cache-locality, we
store the elements into arrays instead, which we call blocks. Whenever a block becomes
full, a new block is allocated and the previous block is made to point to the next block.
This way, most writes amount to a simple array-write, while allocation occurs only
occasionally. Each block contains a hint index to the first free entry in the array, i.e.
one that does not contain an element. An index is a hint, since it may actually reference
an earlier index. The FlowPool maintains a reference to the first block called start.

It also maintains a hint to the last block in the chain of blocks, called current. This
reference may not always be up-to-date, but it always points to some block in the chain.
Each FlowPool is associated with a list of callbacks which have to be called in the future
as new elements are added. Each FlowPool can be in a sealed state, meaning there is a
bound on the number of elements it stores. This information is stored as a Terminal
value in the first free entry of the array. At all times we maintain the invariant that the
array in each block starts with a sequence of elements, followed by a Terminal delimiter.
From a higher-level perspective, appending an element starts by copying the Terminal
value to the next entry and then overwriting the current entry with the element being
appended

It then reads the nexto after the first free entry, followed by a read of the
curo at the free entry. The check procedure checks the bounds conditions, whether
the FlowPool was already sealed or if the current array entry contains an element. In
either of these events, the current and index values need to be set – this is done in
the advance procedure. We call this the slow path of the append method. Notice that
there are several causes that trigger the slow path. If some other thread completes the
append method but is preempted before updating the value of the hint index, then the
curo will have the type Elem. The same happens if a preempted thread updates the
value of the hint index after additional elements have been added, via unconditional
write in line 21. Finally, reaching an end of block triggers the slow path.

Otherwise, the operation executes the fast path and appends an element. It first copies
the Terminal value to the next entry with a CAS instruction in line 19, with nexto
being the expected value. If it fails (e.g. due to a concurrent CAS), the append operation
is restarted. Otherwise, it proceeds by writing the element to the current entry with a
CAS in line 20, the expected value being curo. On success it updates the b.index value
and invokes all the callbacks (present when the element was added) with the future
construct. In the implementation we do not schedule an asynchronous computation
for each element. Instead, the callback invocations are batched to avoid the scheduling
overhead – the array is scanned for new elements until there are no more left.

The seal operation continuously increases the index in the block until it finds the first
free entry. It then tries to replace the Terminal value there with a new Terminal value
which has the seal size set. An error occurs if a different seal size is set already. The
foreach operation works in a similar way, but is executed asynchronously. Unlike seal,
it starts from the first element in the pool and calls the callback for each element until it
finds the first free entry. It then replaces the Terminal value with a new Terminal value
with the additional callback. From that point on the append method is responsible for
scheduling that callback for subsequently added elements. Note that all three operations
call expand to add an additional block once the current block is empty, to ensure
lock-freedom.

Unlike the previous section, in this section we once more assume that the elements are
present in the data structure when the operation starts. While the quiescence assumption
had to be implicitly dropped in the last section, in this section we drop this assumption
intentionally. We focus on data structures that can be concurrently modified without
restrictions – for such data structures it is typically hard for the programmer to guarantee
quiescence.

Futures can be modified at a single location and only once. Concurrent queues underlying
flow pools shown in Section 4.1.2 can only be modified at the tail, but can be adapted to
be modified at the head as well. Data structure like concurrent maps and sets, concurrent
linked lists and concurrect priority queues are different in this regard. They allow
operations such as the atomic lookup, insert and remove, which can occur anywhere in
the data structure. Parallelizing bulk operations on such data structures is considerably
more difficult, since the threads participating in the operation have to agree on whether
their modifications occur before or after the point when the bulk operation occurs.

The goal of this section is to show that certain concurrent data structures support an
efficient, lock-free, linearizable, lazy snapshot operation. Data-parallel and other bulk
operations on these data structures are built from the atomic snapshot, and guarantee
linearizability as a consequence. The snapshot is applied to a scalable concurrent map
implementation called a Ctrie, and we show that this extension is accompanied with a
minimal performance penalty, both in terms of running time and memory usage.

Interestingly, applying lazyness to concurrent data structures allows parallel copying,
and improves the scalability of the snapshot operation. This is particularly evident for
tree-like data structures like the Ctrie, which is shown in this section. While adding
lazyness to persistent data structures clears the road to amortization [Okasaki(1998)],
adding lazyness to concurrent data structures is the path to scalable parallel snapshots.
The task of providing linearizable snapshot semantics seems to coincide with that of (still
unavailable) hardware transactional memories [Knight(1986)] [Herlihy and Moss(1993)],
and software transactional memories [Shavit and Touitou(1995)], whose task is to automatically
provide transactional semantics to arbitrary operations, as part of the program
runtime. Despite their obvious convenience, and the cleverness in many STM implementations,
STMs have still not caught on. A part of the reason for this is that most STMs
incur relatively large performance penalties for their generality. Compared to the Ctrie
snapshot operation, an atomic snapshot on a transactional map collection is much slower.

The reason for this is that the atomic snapshots are specifically designed for concurrent
data structures such as the Ctrie, and do not have to pay the price of STMs’ generality.
Before showing how concurrent, atomic, lock-free snapshots are implemented for the
Ctrie concurrent map data structure, we will illustrate the idea of the atomic snapshot
operation on a simpler concurrent data structure, namely, the concurrent linked list.

We show an implementation of a concurrent linked list that contains a sorted set of
elements and supports the insert operation in Figure 4.9. This linked list internally
maintains a list of nodes we will call I-nodes. The insert operation takes O(n) time, where
n is the number of elements in the list at the linearization point. This implementation
maintains two references to the nodes at the root and the end of the linked list. These
sentinel nodes contain the values negInf and posInf denoting the element smaller and
bigger than all the other elements, respectively. The insert method traverses the nodes
of the linked list until it finds a node such that its next node contains an element greater
than the element that needs to be inserted. We will call the pointer to the next node
main.

The correctness of the insert crucially depends on three properties. First, the elements
of the list are always sorted. Second, once added to the list, the node is never removed.
Third, the element in the root node is smaller than any other element. These properties
inductively ensure that once the private insert method is called with some node n
as an argument, all the nodes preceding n will forever contain elements smaller than
n.elem. In the same time, the insert method ensures that the argument elem is always
greater than n.elem. It follows that the location to insert elem must always be after
n.elem. The private insert method simply checks the local condition that n.main.elem
is additionally greater than elem, meaning that elem must be between n.elem and
n.main.elem. It then attempts to atomically insert elem with a CAS instruction.

The GCAS procedure starts by setting the prev field in the new main node n to point at
main node old, which will be the expected value for the first CAS. Since the preconditions
state that no other thread sees n at this point, this write is safe. The thread proceeds
by proposing the new value n with a CAS instruction in line 131. If the CAS fails then
GCAS returns ? and the CAS is the linearization point. If the CAS succeeds (shown in
Figure 4.12B), the new main node is not yet committed – the generation of the root
has to be compared against the generation of the I-node before committing the value,
so the tail-recursive procedure GCAS_Commit is called with the parameter m set to the
proposed value n. This procedure reads the previous value of the proposed node and the
data structure root (we explain the ABORTABLE_READ procedure shortly – for now we
can assume it is a normal atomic READ). It then inspects the previous value.

If the proposed value is rejected, the linearization point is the CAS in line 149, which
sets the main node of an I-node back to the previous value (this need not necessarily be
done by the current thread). If the proposed value is accepted, the linearization point is
the successful CAS in the line 142 – independent of that CAS was done by the current
thread or some other thread. If the linearization point is external [Bronson(2011a)], we
know it happened after GCAS was invoked. We know that the gen field does not change
during the lifetime of an I-node, so it remains the same until a successful CAS in the
line 142. If some other thread replaces the root with a new I-node with a different gen
field after the read in the line 138, then no other thread that observed the root change
will succeed in writing a failed node, since we assumed that the CAS in the line 142
succeeded.

How to use the GCAS procedure to allow linearizable snapshots on a concurrent linked
list? First, we augment the internal data structure according to the GCAS preconditions
– we identify I-nodes and main nodes, and add the gen and prev fields to these nodes,
respectively. Then, we replace all occurrences of CAS instructions with GCAS invocations
and we replace all the atomic READ operations with GCAS_READ invocations, except the
read of root.

GCAS is only one of the requirements for linearizable snapshots on lock-free data
structures. If we only did the GCAS-modifications, the operations would detect a
snapshot, but would then restart repetitively. The second requirement is augmenting the
operations to rebuild the data structure when they detect it is necessary – in essence, to
do a copy-on-snapshot.

We show the complete lock-free concurrent linked list with linearizable snapshots implementation
in Figure 4.14. The snapshot operations must atomically change the root
reference of the linked list to point to an I-node with a fresh generation tag. The CAS
instruction alone is insufficient for this task. The snapshot operation can copy and
replace the root I-node only if its main node does not change between the copy and the
replacement. If it changed, an insert near the root could potentially be lost. This is
why we need a stronger primitive that writes only if there are changes to the main node
pointed to the root and the root reference itself.

It is, in effect, a CAS instruction with two conditions. RDCSS works in a similar way as
GCAS, but proposes the new value by creating an intermediate descriptor object, which
points to the previous and the proposed value. The allocation cost that we initially
wanted to avoid is not critical here, since we expect a snapshot to occur much less often
than the other update operations. We specialize RDCSS – the first compare is on the
root and the second compare is always on the main node of the old value of the root.
GCAS_READ is used to read the main node of the old value of the root. The semantics
correspond to the atomic block shown in Figure 4.13.

Recall now the ABORTABLE_READ in line 138 in Figure 4.11. The ABORTABLE_READ is a
of RDCSS_READ that writes back the old value to the root field if it finds the proposal
descriptor there, causing the snapshot to be restarted. Without the ABORTABLE_READ,
two threads that simultaneously start a GCAS on the root I-node and an RDCSS on the
root field of the linked list would wait on each other forever.

With GCAS the actual snapshot operation becomes a simple O(1) operation. Importantly,
note that the modified insert operation does not need to rebuild the parts of the
concurrent linked list data structure that it does not touch. While this is a constant-factor
optimization that does not change the inherent O(n) operation complexity of inserting
to a linked list, it opens an interesting optimization opportunity that we will explore in
the next section.

In this section we show how to apply a snapshot-based parallelization to a more complex
data-structure called a Ctrie [Prokopec et al.(2011b)Prokopec, Bagwell, and Odersky]
[Prokopec et al.(2012a)Prokopec, Bronson, Bagwell, and Odersky]. A Ctrie can be used
to implement efficient, concurrent, lock-free maps and sets. We start by describing the
Ctrie data structure and its basic operations, and then augment the Ctrie to support a
constant time, linearizable, lock-free snapshot operation. Finally, we show how to use
the snapshot operation to implement various atomic parallel operations.

Many algorithms exhibit an interesting interplay of parallel traversal and concurrent
updates. One such example is the PageRank algorithm, implemented using Scala
parallel collections [Prokopec et al.(2011c)Prokopec, Bagwell, Rompf, and Odersky] in
Figure 4.15. In a nutshell, this iterative algorithm updates the rank of the pages until
the rank converges. The rank is updated based on the last known rank of the pages
linking to the current page (line 4). Once the rank becomes smaller than some predefined
constant, the page is removed from the set of pages being processed (line 5). The for
loop that does the updates is executed in parallel. After the loop completes, the arrays
containing the previous and the next rank are swapped in line 7, and the next iteration
commences if there are pages left.

The main point about this algorithm is that the set of pages being iterated is updated
by the remove operation during the parallel traversal. This is where most concurrent
data structures prove inadequate for implementing this kind of algorithms – an iterator
may or may not reflect the concurrent updates. Scala parallel collections can remedy this
by removing the test in line 5 and adding another parallel operation filter to create a
new set of pages without those that converged – this new set is traversed in the next
iteration. The downside of this is that if only a few pages converge during an iteration
then almost the entire set needs to be copied. The filter approach is not applicable
to inputs with short convergence tails. Alternatively, we could implement PageRank
by always traversing the same list of pages, and call setMembership to disable boolean
flags of pages that have already converged. This way we do not need to rebuild the
dataset in each iteration. The downside of this approach is that we need to traverse all
the pages at the end of the algorithm, when there are only a few pages left. We say that
the setMembership approach is not applicable to long-convergence tails. If the splitters
used for parallel traversal reflected only the elements present when the operation began,
both of these issues would be addressed.

Hash array mapped tries [Bagwell(2001)] [Baskins(2000)] (or simply, hash tries) are trees
composed of internal nodes and leaves. Leaves store key-value bindings. Internal nodes
have a 2W-way branching factor. In a straightforward implementation, each internal
node is a 2W-element array. Finding a key proceeds as follows. If the internal node is at
the level l, then the W bits of the hashcode starting from the position W  l are used as
an index to the appropriate branch in the array. This is repeated until a leaf or an empty
entry is found. Insertion uses the key to find an empty entry or a leaf. It creates a new
leaf with the key if an empty entry is found. Otherwise, the key in the leaf is compared
against the key being inserted. If they are equal, the existing leaf is replaced with a new
one. If they are not equal (meaning their hashcode prefixes are the same) then the hash
trie is extended with a new level.

A more space-efficient version of HAMT was worked on independently by Bagwell
[Bagwell(2001)] and Baskins [Baskins(2000)]. Each internal node contains a bitmap of
length 2W. If a bit is set, then the corresponding array entry contains either a branch or
a leaf. The array length is equal to the number of bits in the bitmap. The W bits of the hashcode
relevant at some level l are used to compute the bit position i as before. At all times an
invariant is preserved that the bitmap bitcount is equal to the array length. Typically,
W is 5 since that ensures that 32-bit integers can be used as bitmaps. Figure 4.16A
shows a hash trie example.

Intuitively, a concurrent insertion operation could start by locating the internal node it
needs to modify and then create a copy of that node with both the bitmap and the array
updated with a reference to the key being inserted. A reference to the newly created
node could then be written into the array of the parent node using the CAS instruction.
Unfortunately, this approach does not work. The fundamental problem here is due to
races between an insertion of a key into some node C1 and an insertion of another key
into its parent node C2. One scenario where such a race happens is shown in Figure 4.16.
Assume we have a hash trie from the Figure 4.16A and that a thread T1 decides to insert
a key k5 into the node C2 and creates an updated version of C2 called C20. It must then
do a CAS on the first entry in the internal node C3 with the expected value C2 and the
new value C20. Assume that another thread T2 decides to insert a key k4 into the node
C1 before this CAS. It will create an updated version of C1 called C10 and then do a
CAS on the first entry of the array of C2 – the updated node C10 will not be reachable
from the updated node C20. After both threads complete their CAS operations, the trie
will correspond to the one shown in Figure 4.16B, where the dashed arrows represent the
state of the branches before the CASes. The key k4 inserted by the thread T2 is lost.
We solve this problem by introducing indirection nodes, or I-nodes, which remain present
in the Ctrie even as nodes above and below change. The CAS instruction is performed
on the I-node instead of on the internal node array. We show that this eliminates the
race between insertions on different levels.

The second fundamental problem has to do with the remove operations. Insert operations
extend the Ctrie with additional levels. A sequence of remove operations may eliminate
the need for the additional levels – ideally, we would like to keep the trie as compact as
possible so that the subsequent lookup operations are faster. In Section 4.2.2 we show
that removing an I-node that appears to be no longer needed may result in lost updates.
We describe how to remove the keys while ensuring compression and no lost updates.

When a new Ctrie is created, it contains a root I-node with the main node set to an
empty C-node, which contains an empty bitmap and a zero-length array (Figure 4.19A).
We maintain the invariant that only the root I-node can contain an empty C-node – all
other C-nodes in the Ctrie contain at least one entry in their array. Inserting a key k1
first reads the root and calling the procedure iinsert.

Note that insertions to I-nodes at different levels may proceed concurrently, as shown in
Figures 4.19E,F where a new key k4 is added at the level 0, below the I-node I1. No
race can occur, since the I-nodes at the lower levels remain referenced by the I-nodes at
the upper levels even when new keys are added to the higher levels. This will not be the
case after introducing the remove operation.

The described approach has certain pitfalls. A remove operation may at one point create
a C-node that has a single S-node below it. This is shown in Figure 4.21A, where the
key k2 is removed from the Ctrie. The resulting Ctrie in Figure 4.21B is still valid in
the sense that the subsequent insert and lookup operations will work. However, these
operations could be faster if (k3, v3) were moved into the C-node below I1. After having
removed the S-node (k2, v2), the remove operation could create an updated version of C1
with a reference to the S-node (k3, v3) instead of I2 and write that into I1 to compress
the Ctrie. But, if a concurrent insert operation were to write to I2 just before I1 was
updated with the compressed version of C1, the insertion would be lost.

The remove operation starts by reading the root I-node and calling the recursive procedure
iremove. If the main node is a C-node, the flagpos function is used to compute the
relevant bit and the branch position. If the bit is not present in the bitmap (line 69),
then a NOTFOUND value is returned. In this case, the linearization point is the read in
the line 66. Otherwise, the branch node is read from the array. If the branch is another
I-node, the procedure is called recursively. If the branch is an S-node, its key is compared
against the key being removed. If the keys are not equal (line 75), the NOTFOUND value is
returned and the linearization point is the read in the line 66. If the keys are equal, a
copy of the current node without the S-node is created. The contraction of the copy is
then created using the toContracted procedure. A successful CAS in the line 79 will
substitute the old C-node with the copied C-node, thus removing the S-node with the
given key from the trie – this is the linearization point.

If a given C-node has only a single S-node below and is not at the root level (line 101)
then the toContracted procedure returns a T-node that wraps the S-node. Otherwise,
it just returns the given C-node. This ensures that every I-node except the root points
to a C-node with at least one branch. Furthermore, if it points to exactly one branch,
then that branch is not an S-node (this scenario is possible if two keys with the same
hashcode prefixes are inserted). Calling this procedure ensures that the CAS in the
line 79 replaces the C-node C2 from the Figure 4.21A with the T-node in Figure 4.21C
instead of the C-node C2 in Figure 4.21B. This CAS is the linearization point since
the S-node (k2, v2) is no longer in the trie. However, it does not solve the problem of
compressing the Ctrie (we ultimately want to obtain a Ctrie in Figure 4.21D). In fact,
given a Ctrie containing two keys with long matching hashcode prefixes, removing one
of these keys will create a arbitrarily long chain of C-nodes with a single T-node at the
end. We introduced the invariant that no tombed I-node changes its main node. To
remove the tombed I-node, the reference to it in the C-node above must be changed with
a reference to its resurrection. A resurrection of a tombed I-node is the S-node wrapped
in its T-node. For all other branch nodes, the resurrection is the node itself.

To ensure compression, the remove operation checks if the current main node is a Tnode
after removing the key from the Ctrie (line 83). If it is, it calls the cleanParent
procedure, which reads the main node of the parent I-node p and the current I-node
i in the line 113. It then checks if the T-node below i is reachable from p. If i is
no longer reachable, then it returns – some other thread must have already completed
the contraction. If it is reachable then it replaces the C-node below p, which contains
the tombed I-node i with a copy updated with the resurrection of i (CAS in the line
122). This copy is possibly once more contracted into a T-node at a higher level by the
toContracted procedure.

Both insert and lookup are tail-recursive and may be rewritten to loop-based variants,
but this is not so trivial with the remove operation. Since remove operations must be able
to compress arbitrary long chains of C-nodes, the call stack is used to store information
about the path in the Ctrie being traversed.

In this implementation, hash tries use a 32-bit hashcode space. Although hash collisions
are rare, it is still possible that two unequal keys with the same hashcodes are inserted.
To preserve correctness, we introduce a new type of nodes called list nodes (L-nodes),
which are basically persistent linked lists. If two keys with the same hashcodes collide,
we place them inside an L-node.

We add another case to the basic operations from Section 4.2.2. Persistent linked list
operations lookup, inserted, removed and length are trivial and not included in the
pseudocode. We additionally check if the updated L-node in the iremove procedure has
length 1 and replace the old node with a T-node in this case.

Another important change is in the CNode constructor in line 42. This constructor was a
recursive procedure that creates additional C-nodes as long as the hashcode chunks of
the two keys are equal at the given level. We modify it to create an L-node if the level is
greater than the length of the hashcode – in our case 32.

For example, the ConcurrentMap interface in Java defines four additional methods:
putIfAbsent, replace any value a key is mapped to with a new value, replace a
specific value a key is mapped to with a new value and remove a key mapped to a
specific value. All of these operations can be implemented with trivial modifications to
the operations introduced in Section 4.2.2. For example, removing a key mapped to a
specific value can be implemented by adding an additional check sn.v = v to the line 74.

As argued earlier, methods such as size, iterator or clear commonly seen in collection
frameworks cannot be implemented in a lock-free, linearizable manner so easily. The
reason for this is that they require global information about the data structure at one
specific instance in time – at first glance, this requires locking or weakening the contract
so that these methods can only be called during a quiescent state. These methods can
be computed efficiently and correctly by relying on a constant time lock-free, atomic
snapshot.

A persistent hash trie data structure seen in standard libraries of languages like Scala or
Clojure is updated by rewriting the path from the root of the hash trie to the leaf the
key belongs to, leaving the rest of the trie intact. This idea can be applied to implement
the snapshot. A generation count can be assigned to each I-node. A snapshot is created
by copying the root I-node and setting it to the new generation. When some update
operation detects that an I-node being read has a generation older than the generation
of the root, it can create a copy of that I-node initialized with the latest generation and
update the parent accordingly – the effect of this is that after the snapshot is taken, a
path from the root to some leaf is updated only the first time it is accessed, analogous
to persistent data structures. The snapshot is thus an O(1) operation, while all other
operations preserve an O(log n) complexity, albeit with a slightly larger constant factor.

Still, the snapshot operation will not work as described above, due to the races between
the thread creating the snapshot and threads that have already read the root I-node with
the old generation and are traversing the Ctrie in order to update it.The problem is that
a CAS that is a linearization point for an insert (e.g. in the line 48) can be preceeded
by the snapshot creation – ideally, we want such a CAS instruction to fail, since the
generation of the Ctrie root has changed. If we used a DCAS instruction instead, we
could ensure that the write occurs only if the Ctrie root generation remained the same.
However, most platforms do not support an efficient implementation of this instruction
yet. On closer inspection, we find that an RDCSS instruction described by Harris et al.
[Harris et al.(2002)Harris, Fraser, and Pratt] that does a double compare and a single
swap is enough to implement safe updates. The downside of RDCSS is that its software
implementation creates an intermediate descriptor object. While such a construction is
general, due to the overhead of allocating and later garbage collecting the descriptor, it
is not optimal in our case.

Next, we replace every occurence of a CAS instruction with a call to the GCAS procedure.
We replace every atomic read with a call to the GCAS_READ procedure. Whenever we read
an I-node while traversing the trie (lines 12, 37 and 71) we check if the I-node generation
corresponds to startgen. If it does, we proceed as before. Otherwise, we create a copy
of the current C-node such that all of its I-nodes are copied to the newest generation
and use GCAS to update the main node before revisiting the current I-node again. This is
shown in Figure 4.24, where the cn refers to the C-node currently in scope (see Figures
4.18, 4.20 and 4.22). In line 43 we copy the C-node so that all I-nodes directly below it
are at the latest generation before updating it. The readonly field is used to check if the
Ctrie is read-only - we explain this shortly. Finally, we add a check to the cleanParent
procedure, which aborts if startgen is different than the gen field of the I-node.

All GCAS invocations fail if the generation of the Ctrie root changes and these failures
cause the basic operations to be restarted. Since the root is read once again after
restarting, we are guaranteed to restart the basic operation with the updated value of
the startgen parameter.

As with linked lists, one might be tempted to implement the snapshot operation by
simply using a CAS instruction to change the root reference of a Ctrie to point to an
I-node with a new generation. However, the snapshot operation can copy and replace the
root I-node only if its main node does not change between the copy and the replacement.

We again use the RDCSS procedure described by Harris to propose the new value by
creating an intermediate descriptor object, which points to the previous and the proposed
value. Once more, we specialize RDCSS to suit our needs – the first compare is on the
root and the second compare is always on the main node of the old value of the root.

Since both the original Ctrie and the snapshot have a root with a new generation, both
Ctries will have to rebuild paths from the root to the leaf being updated. When computing
the size of the Ctrie or iterating the elements, we know that the snapshot will not be
modified, so updating paths from the root to the leaf induces an unnecessary overhead.

To accomodate this we implement the readOnlySnapshot procedure that returns a read
only snapshot. The only difference with respect to the snapshot procedure in Figure
4.25 is that the returned Ctrie has the old root r (line 138) and the readonly field is set
to >. The readonly field mentioned earlier in Figures 4.17, 4.11 and 4.24 guarantees
that no writes to I-nodes occur if it is set to >. This means that paths from the root to
the leaf being read are not rewritten in read-only Ctries. The rule also applies to T-nodes
– instead of trying to clean the Ctrie by resurrecting the I-node above the T-node, the
lookup in a read-only Ctrie treats the T-node as if it were an S-node. Furthermore, if the
GCAS_READ procedure tries to read from an I-node in which a value is proposed, it will
abort the write by creating a failed node and then writing the old value back (line 141).

Having studied the snapshot implementation, we turn to examining several important
performance aspects. Unlike the concurrent linked list from Section 4.2.1, which has O(n)
asymptotic running time and was introduced as a proof of concept data structure, Ctries
have expected O(log32 n) modification and lookup operations, with good scalability and
absolute performance (see Section 6.4 for comparison with similar data structures). If
extending Ctries with snapshots were to compromise performance, their practicality
would be greatly diminished. Fortunately, this is not the case. In this section, we quantify
the performance penalties of adding snapshots to Ctries, analyze the worst-case space
consumption of a Ctrie augmented with snapshots, compare it the memory footprint of
the Ctrie against that of similar data structures, and compare Ctrie snapshot performance
to that of using an STM.

These benchmarks show us that augmenting Ctries with support for snapshots increases
the running time of lookups by up to 23%. The running time of the insert operation
is increased by up to 20%. This indicates that augmenting Ctries with GCAS incurs
acceptable overheads.

In both benchmarks, accessing all the elements in the Ctrie after taking a snapshot
adds a copying overhead which is 30% for small Ctries, and approaches only 2% as we
increase the number of elements. This indicates that the cost of rebuilding the Ctrie
is almost completely amortized by the cost of accessing all the elements, particularly
for larger Ctries. Note that this result shows the average performance when we access
many elements – some initial proportion of accesses, ocurring immediately after taking a
snapshot, can require more time, as larger parts of the Ctrie need to be rebuilt.

First, we consider the worst-case space requirements of a Ctrie augmented with snapshots.
Note that the prev field in each main node points to the previous value at the same I-node.
It would be quite unfortunate if a chain of prev pointers, captured each time when a
snapshot occurs, induced a linked list of main nodes belonging to different generations of
the Ctrie, eventually leading to the first occurence of the main node. If this situation were
possible, the worst-case space consumption of a Ctrie would be O(n + t · S), where S is
the number of snapshots taken during the lifetime of the Ctrie, a potentially unbounded
value.

The theoretical treatment gives us a solid proof of the worst case spatial complexity,
but it tells us little about the constant factors involved. In Figure 4.28, we compare
the memory footprint of Ctries with snapshots against that of ConcurrentHashMap
and ConcurrentSkipListMap from JDK. We find that our implementation consumes
approximately 13% more space than ConcurrentHashMap and 17% more space than
ConcurrentSkipListMap. We consider both these overheads acceptable for typical
applications.

Lastly, we contrast the benefits of snapshots against an alternative technology, namely,
software transactional memories. As argued earlier, an STM can automatically provide
transactional semantics for arbitrary data structures, but pays high runtime overheads
for this generality. For this reason, specific STM implementations like ScalaSTM
[Bronson et al.(2010b)Bronson, Chafi, and Olukotun] [Bronson(2011b)] provide collections
such as transactional arrays and transactional maps as basic primitives. These
transactional collections are not implemented directly in terms of transactional references,
and are integrated with the STM in a more optimal way. ScalaSTM even goes a
step further by providing the transactional snapshot operation on its transactional map
collection, and implements it using a data structure similar to the Ctrie.

Interestingly, the destructive operations on the snapshot of the Ctrie did not change
their complexity with respect to the same operations on the non-snapshot Ctrie instance.
The copy-on-snapshot does not have to be done eagerly for the entire data structure –
instead, it can lazily rebuild only those parts of the data structure traversed by specific
operations. Concurrent data structures that have efficient persistent variants seem to be
particularly amenable to this technique.

Concurrently accessible queues have been present for a while, an implementation is described
by [Mellor-Crummey(1987)]. Non-blocking concurrent linked queues are described
by Michael and Scott [Michael and Scott(1996)]. This CAS-based queue implementation
is cited and used widely today, a variant of which is present in the Java standard library.
More recently, Scherer, Lea and Scott [Scherer et al.(2009)Scherer, Lea, and Scott] describe
synchronous queues, which internally hold both data and requests. Both approaches
above entail blocking (or spinning) at least on the consumer’s part when the queue is
empty.

While these concurrent queues fit well in the concurrent imperative model, they have
the disadvantage that the programs written using them are inherently nondeterministic.
Roy and Haridi [Roy and Haridi(2004)] describe the Oz programming language, a subset
of which yields programs deterministic by construction. Oz dataflow streams are built
on top of single-assignment variables – using them results in deterministic programs.
Here, a deterministic program is guaranteed to, given the same inputs, always yield
the same results, or always throw some exception. They allow multiple consumers, but
only one producer at a time. Oz has its own runtime which implements blocking using
continuations.

The concept of single-assignment variables is embodied in futures proposed by Baker
and Hewitt [Henry C. Baker and Hewitt(1977)], and promises first mentioned by Friedman
and Wise [Friedman and Wise(1976)]. Futures were first implemented in MultiLISP
[Halstead(1985)], and have been employed in many languages and frameworks since. Scala
2.10 futures [Haller et al.(2012)Haller, Prokopec, Miller, Klang, Kuhn, and Jovanovic] define
monadic operators and a number of high-level combinators that create new futures.
These APIs avoid blocking.

A number of other models and frameworks recognized the need to embed the concept
of futures into other data-structures. Single-assignment variables have been generalized
to I-Structures [Arvind et al.(1989)Arvind, Nikhil, and Pingali] which are essentially
single-assignment arrays. CnC [Burke et al.(2011)Burke, Knobe, Newton, and Sarkar]
is a parallel programming model influenced by dynamic dataflow, stream-processing and
tuple spaces. In CnC the user provides high-level operations along with the ordering
constraints that form a computation dependency graph.

A lot of research was done on concurrent lists [Harris(2001)], queues and concurrent
priority queues. A good introduction to concurrent, lock-free programming is given by
Herlihy and Shavit [Herlihy and Shavit(2008)].

While the individual concurrent hash table operations such as insertion or removal can
be performed in a lock-free manner as shown by Maged [Michael(2002)], resizing is
typically implemented with a global lock. Although the cost of resizing is amortized
against operations by one thread, this approach does not guarantee horizontal scalability.
Lea developed an extensible hash algorithm that allows concurrent searches during the
resizing phase, but not concurrent insertions and removals [Lea(2014)]. Shalev and Shavit
give an innovative approach to resizing – split-ordered lists keep a table of hints into a
single linked list in a way that does not require rearranging the elements of the linked
list when resizing the table [Shalev and Shavit(2006)].

Skip lists store elements in a linked list. There are multiple levels of linked lists that allow
logarithmic time insertions, removals and lookups. Skip lists were originally invented by
Pugh [Pugh(1990a)]. Pugh proposed concurrent skip lists that achieve synchronization
using locks [Pugh(1990b)]. Concurrent non-blocking skip lists were later implemented by
Lev, Herlihy, Luchangco and Shavit [Y. Lev and Shavit(2006)] and Lea [Lea(2014)].

Kung and Lehman [Kung and Lehman(1980)] proposed a concurrent binary search tree
– their implementation uses a constant number of locks at a time that exclude other
insertion and removal operations, while lookups can proceed concurrently. Bronson
presents a scalable concurrent AVL tree that requires a fixed number of locks for deletions
[Bronson et al.(2010a)Bronson, Casper, Chafi, and Olukotun]. Recently, a non-blocking
binary tree was proposed [Ellen et al.(2010)Ellen, Fatourou, Ruppert, and van Breugel].
Tries were proposed by Briandais [De La Briandais(1959)] and Fredkin [Fredkin(1960)].
Trie hashing was applied to accessing files stored on the disk by Litwin [Litwin(1981)].
Litwin, Sagiv and Vidyasankar implemented trie hashing in a concurrent setting, however,
they use mutual exclusion locks [Litwin et al.(1989)Litwin, Sagiv, and Vidyasankar]. Hash
array mapped trees, or hash tries, are tries for shared-memory described by Bagwell
[Bagwell(2001)]. To our knowledge, there is no nonblocking concurrent implementation
of hash tries prior to Ctrie.

Almost every data-parallel workload is to some extent irregular on a multicore or a
multiprocessor machine. Even if the number of instructions that need to be executed is
equal for every element of the collection, there are many other factors that influence the
workload. Some of the workers may wake up slower than other workers or be occupied
executing other data-parallel operations, so they can make a workload seem irregular in
practice. Then, accessing specific regions of memory might not take the same time for
all the processors, and contended writes can slow some processors more than the others,
depending on the access pattern. Similarly, a memory allocator might take different
amounts of time to return a chunk of memory depending on its internal state. A processor
may be preempted by the operating system and unavailable for a certain amount of
time, effectively executing its work slower than the rest of the processors. In a managed
runtime, a garbage collection cycle or a JIT compiler run can at any time freeze some or
all of the worker threads.

Work-stealing [Frigo et al.(1998)Frigo, Leiserson, and Randall] is one solution to loadbalancing
computations with an irregular workload. In this technique different processors
occasionally steal batches of elements from each other to load balance the work – the
goal is that no processor stays idle for too long. We have seen an example of task-based
data-parallel work-stealing scheduler in Section 2.7. In this chapter we propose and
describe a runtime scheduler for data-parallel operations on shared-memory architectures
that uses a variant of work-stealing to ensure proper load-balancing. The scheduler
relies on a novel data structure with lock-free synchronization operations called the
work-stealing tree.

As argued in Chapter 2 and illustrated in Figure 5.1, data-parallel operations are highly
generic – for example, reduce takes a user-provided operator, such as number addition,
string concatenation or matrix multiplication. This genericity drives the workload
distribution, which cannot always be determined statically. To assign work to processors
optimally, scheduling must occur at runtime. Scheduling in this case entails dividing the
elements into batches on which the processors work in isolation.

Before assessing the goals and the quality of scheduling, we need to identify different
classes of computational workloads. We will refer to workloads that have an equal amount
of work assigned to every element as uniform workload. A uniform workload that has the
least possible possible amount of possible useful work assigned to each element is called
the baseline workload. When comparing two uniform workloads we say that the workload
with less work per element is more fine-grained, as opposed to the other that is more
coarse-grained. All workloads that are not uniform are considered irregular workloads.

The design of the algorithm is driven by the following assumptions. There are no fast,
accurate means to measure elapsed time with sub-microsecond precision, i.e. there is
no way to measure the running time of an operation. There is no static or runtime
information about the cost of an operation – when invoking a data-parallel operation
we do not know how much computation each element requires. There are no hardwarelevel
interrupt handling mechanisms at our disposal – the only way to interrupt a
computation is to have the processor check a condition. We assume OS threads as
parallelism primitives, with no control over the scheduler. We assume that the available
synchronization primitives are monitors and the CAS instruction. CAS can be used
for stronger primitives [Harris et al.(2002)Harris, Fraser, and Pratt], but we do not use
those directly1. We assume the presence of automatic memory management. These
constraints are typical for a managed runtime like the JVM.

In this section we describe the work-stealing tree data structure and the scheduling
algorithm that the workers run. We first briefly discuss the aforementioned fixed-size
batching. We have mentioned that the contention on the centralized queue is one of it
drawbacks. We could replace the centralized queue with a queue for each worker and
use work-stealing. However, this seems overly eager – we do not want to create as many
work queues as there are workers for each parallel operation, as doing so may outweigh
the actually useful work. We should start with a single queue and create additional ones
on-demand. Furthermore, fixed-size batching seems appropriate for scheduling parallel
loops, but what about the reduce operation? If each worker stores its own intermediate
results separately, then the reduce may not be applicable to non-commutative operators
(e.g. string concatenation). It seems reasonable to have the work-stealing data-structure
store the intermediate results, since it has the division order information.

With this in mind, we note that a tree seems particularly applicable. When created it
consists merely of a single node – a root representing the operation and all the elements
of the range. The worker invoking the parallel operation can work on the elements and
update its progress by writing to the node it owns. If it completes before any other
worker requests work, then the overhead of the operation is merely creating the root.
Conversely, if another worker arrives, it can steal some of the work by creating two
child nodes, splitting the elements and continuing work on one of them. This proceeds
recursively. Scheduling is thus workload-driven – nodes are created only when some
worker runs out of work meaning that another worker had too much work. Such a tree
can also store intermediate results in the nodes, serving as a reduction tree.

How can such a tree be used for synchronization and load-balancing? We assumed that
the parallelism primitives are OS threads. We can keep a pool of threads [Lea(2000)]
that are notified when a parallel operations is invoked – we call these workers. We
first describe the worker algorithm from a high-level perspective. Each worker starts by
calling the tail-recursive run method in Figure 5.2. It looks for a node in the tree that is
either not already owned or steals a node which some other worker works on by calling
findWork in line 3. This node is initially a leaf. The worker works on the leaf by calling
workOn in line 5, which works on it until the leaf until the leaf is either completed or
stolen. This is repeated until findWork returns ? (null), indicating that all the work is
completed.

In Figure 5.2 we also present the work-stealing tree and its basic data-types. We use
the keyword struct to refer to a compound data-type – this can be a Java class or a
C structure. We define two compound data-types. Ptr is a reference to the tree – it
has only a single member child of type Node. Write access to child has to be atomic
and globally visible (in Java, this is ensured with the volatile keyword). Node contains
immutable references to the left and right subtree, initialized upon instantiation. If
these are set to ? we consider the node a leaf. We initially focus on parallelizing loops
over ranges, so we encode the current state of iteration with three integers. Members
start and until are immutable and denote the initial range – for the root of the tree
this is the entire loop range. Member progress has atomic, globally visible write access.
It is initially set to start and is updated as elements are processed. Finally, the owner
field denotes the worker that is working on the node. It is initially ? and also has atomic
write access. Example trees are shown in Figure 5.3.

Before we describe the operations and the motivation behind these data-types we
will define the states work-stealing tree can be in (see Figure 5.3), namely its invariants.
This is of particular importance for concurrent data structures which have nonblocking
operations. Work-stealing tree operations are lock-free, a well-known advantage
[Herlihy and Shavit(2008)], which comes at the cost of little extra complexity in this
case.

A worker that claimed ownership of a node repetitively calls tryAdvance, which attempts
to reserve a batch of size STEP by atomically incrementing the progress field, eventually
bringing the node into the COMPLETED state. If tryAdvance returns a nonnegative
number, the owner is obliged to process that many elements, whereas a negative number
is an indication that the node was stolen.

A worker searching for work must call trySteal if it finds a node in the OWNED state.
This method returns true if the node was successfully brought into the EXPANDED
state by any worker, or false if the node ends up in the COMPLETED state. Method
trySteal consists of two steps. First, it attempts to push the node into the STOLEN
state with the CAS in line 29 after determining that the node read in line 23 is a leaf. This
CAS can fail either due to a different steal, a successful tryAdvance call or spuriously.
Successful CAS in line 29 brings the node into the STOLEN state. Irregardless of success
or failure, trySteal is then called recursively. In the second step, the expanded version
of the node from Figure 5.3 is created by the newExpanded method, the pseudocode of
which is not shown here since it consists of isolated singlethreaded code. The child
field in Ptr is replaced with the expanded version atomically with the CAS in line 33,
bringing the node into the EXPANDED state.

We explore alternative findWork implementations later. For now, we state the following
claim. If the method findWork does return ?, then all the work in the tree was obtained
by different workers that have called tryAdvance. This means that all the elements have
been processed, except some number of elements M · C distributed across M < P leaf
nodes where P is the number of workers and C is the largest value passed to tryAdvance.
In essence, C is the largest allowed batch size in the algorithm.

In Figure 5.6 we show a slightly more complicated implementation of the worker run
method. This implementation is purely a performance optimisation. After a worker
completes work on a node it can check if the node has been stolen. If the node is stolen,
there is a high probability that there is a free node in the left subtree, so the worker
attempts to own it before searching the entire tree to find more work.

Note that workOn is similar to fixed-size batching. The difference is that an arrival of a
worker invalidates the work-stealing tree node, whereas multiple workers simultaneously
call tryAdvance in fixed-size batching, synchronizing repetitively and causing contention.
We will now study the impact this contention has on performance and scalability – we
start by focusing on choosing the STEP value from Section 5.2.2.

As hinted at the end of Section 5.1, the STEP value in tryAdvance from Figure 5.4
should ideally be 1 for load-balancing purposes, but has to be more coarse-grained due
to communication costs that could overwhelm the baseline. In Figure 5.7A we plot
the running time against the STEP size, obtained by executing the baseline loop with a
single worker. By finding the minimum STEP value with no observable overhead, we seek
to satisfy criteria C1. The minimum STEP with no noticeable synchronization costs is
around 50 elements – decreasing STEP to 16 doubles the execution time and for value 1
the execution time is 36 times larger (not shown for readability).

Having shown that the work-stealing tree is as good as fixed-size chunking with a
properly chosen STEP value, we evaluate its effectiveness with multiple workers. Figure
5.7B shows that the minimum STEP for fixed-size chunking increases for 2 workers, as
we postulated earlier. Increasing STEP decreases the frequency of synchronization and
the communication costs associated with it. In this case the 3x slowdown is caused by
processors having to exchange ownership of the progress field cache-line. The workstealing
tree does not suffer from this problem, since it strives to keep processors isolated
– the speedup is linear with 2 workers.


Inspecting the number of tree nodes created at different parallelism levels in Figure 5.12B
reveals that as the number of workers grows, the number of nodes grows at a superlinear
rate. Each node incurs a synchronization cost, so could we decrease their total number?
Examining a particular work-stealing tree instance at the end of the operation reveals
that different workers are battling for work in the left subtree until all the elements are
depleted, whereas the right subtree remains unowned during this time. As a result, the
workers in any subtree steal from each other more often, hence creating more nodes. 
The cause is the left-to-right tree traversal in findWork as defined in Figure 5.5, a particularly
bad stealing strategy we will call Predefined. As shown in Figure 5.12B, the average
tree size for 8 workers nears 2500 nodes. The reason for this is that all the workers
consistently try to steal from each other in the left work-stealing subtree, while the right
subtree remains occupied by at most a single worker. This pressure on the left subtree
causes those nodes to be recursively divided until no more elements remain. Once work in
the left subtree is completely consumed, the same sequence of events resumes in the left
child of the right subtree. After a certain parallelism level, workers using this strategy
start spending more time expanding the work-stealing tree than doing actual useful work.

Building on the randomization idea, we introduce an additional strategy called RandomAll
where the traversal order in findWork is completely randomized. This strategy
also randomizes all the other choices that the stealer and the victim make. Both the tree
traversal order and the node chosen after the steal are thus changed in findWork. We
show it in Figure 5.9 on the right.

In the RandomWalk strategy we only change the tree traversal order that the stealer
does when searching for work and leave the rest of the choices fixed – victim picks the
left node after expansion and the stealer picks the right node. The code is shown in
Figure 5.10.

However, these completely random strategies result in a lower throughput and bigger
tree sizes. Additionally randomizing the choice in lines 77 and 60 (RandomAll) is even
less helpful, since the stealer and the victim clash immediately after steal more often.

The results of the five different strategies mentioned so far lead to the following observation.
If a randomized strategy like RandomWalk or AssignTop works better than a
suboptimal strategy like Predefined then some of its random choices help reduce the
overall execution time and some increase it. Sometimes random decisions are good, but
sometimes they are detrimental. So, there must exist an even better strategy which only
makes the choices that lead to a better execution time.

Rather than providing a theoretical background for such a strategy, we propose a
particular one which seems intuitive. Let workers traverse the entire tree and pick a
node with most work, only then attempting to own or steal it. We call this strategy
FindMax. Note that this cannot be easily implemented atomically. By the time that
the work-stealing tree is completely traversed, the remaining work in each of the nodes
will probably change since the last read. Still, an implementation that is only accurate
given quiescence still serves as a decent heuristic for finding the node with most work.
The decisions about which node the victim and the stealer take after expansion remain the
same as in the basic algorithm from Figure 5.5. We show the pseudocode for FindMax
in Figure 5.11. This strategy yields an average tree size of 42 at P = 8, as well as a
slightly better throughput.

The diagrams in Figure 5.12 reveal the postulated inverse correlation between the tree
size and total execution time, both for the Intel i7-2600 and the Sun UltraSPARC T2
processor, which is particularly noticeable for Assign when the total number of workers
is not a power of two. For some P RandomAll works slightly better than FindMax on
UltraSPARC, but both are much more efficient than static batching, which deteriorates
heavily once P exceeds the number of cores.

The results from the previous sections show that C1 can be fulfilled with a proper choice
of node search. We focus on the C2 and C3 next by changing the workloads, namely
the kernel method. Figures 5.13, 5.14 show a comparison of the work-stealing tree
and some traditional data-parallel schedulers on a range of different workloads. Each
workload pattern is illustrated prior to its respective diagrams, along with corresponding
real-world examples. To avoid memory access effects and additional layers of abstraction
each workload is minimal and synthetic, but corresponds to a practical use-case. To
test C3, in Figure 5.13-5,6 we decrease the number of elements to 16 and increase the
workload heavily. Fixed-size batching fails utterly for these workloads – the total number
of elements is on the order of or well below the required STEP value. These workloads
obviously require smaller STEP sizes to allow stealing, but that would break the baseline
performance constraint, and the data-parallel scheduler is unable to distinguish the two
types of workloads.

The solution to this conundrum is that the STEP does not have to be a constant value.
The worker owning a work-stealing node can change the size of the batch between the
subsequent tryAdvance calls. Since there are many such calls between the time the
work-stealing node becomes owned and the time it is completed or stolen, the sufficient
requirement for the baseline workload costs to amortize the scheduling costs is that the
average STEP size is above some threshold value. We will call the sequence of STEP values
that are passed to the tryAdvance calls on a specific work-stealing node the batching
scheduler. To solve the issues with coarse-grained workloads, the default work-stealing
node batching schedule is exponential – the batch size is doubled from 1 up to some
maximum value.

Triangular workloads such as those shown in Figures 5.13-8,9,10 show that static batching
can yield suboptimal speedup due to the uniform workload assumption. Figure 5.13-20
shows the inverse triangular workload and its negative effect on guided self-scheduling –
the first-arriving processor takes the largest batch of work, which incidentally contains
most work. We do not invert the other increasing workloads, but stress that it is neither
helpful nor necessary to have batches above a certain size.

Figure 5.14-28 shows an exponentially increasing workload, where the work associated
with the last element equals the rest of the work – the best possible speedup is 2. Figures
5.14-30,32 show two examples where a probability distribution dictates the workload,
which occurs often in practice. Guided self-scheduling works well when the distribution is
relatively uniform, but fails to achieve optimal speedup when only a few elements require
more computation, for reasons mentioned earlier.

There are two reasons why the work-stealing tree is amenable to implementing reductions.
First, it preserves the order in which the work is split between processors, which
allows using non-commutative operators for the reduce (e.g. computing the resulting
transformation from a series of affine transformations can be parallelized by multiplying
a sequence of matrices – the order is in this case important). Second, the reduce can
largely be performed in parallel, due to the structure of the tree.

In this section, we describe the lock-free approach to combining intermediate results in the
work-stealing tree. An advantage of this approach is increased throughput, as either of the
child nodes can push the combined value to its parent. As we will see, one disadvantage
is that the reduction operation may be repeated more than once in a lock-free reduction
tree. Unless the reduction operation is non-side-effecting or idempotent, this can affect
user programs. In such cases, we need to use locks – for example, the combine method
on combiners in ScalaBlitz internally does locking to protect access to combiner state.

The goal of this section is to augment the iterator abstraction with the facilities that
support work-stealing, in the similar way the iterators were enriched with the split
method in Chapter 2. This will allow using the work-stealing tree data-parallel scheduler
with any kind of data structure and not just parallel loops. The previously shown
progress value served as a placeholder for all the work-stealing information in the case
of parallel loops.

There are several parts of the work-stealing scheduler that we can generalize. We read
the value of progress in line 40 to see if it is negative (indicating a steal) or greater
than or equal to until (indicating that the loop is completed) in line 41.
Here the value of progress indicates the state the iterator is in – either available (A),
stolen (S) or completed (C). In line 14 we atomically update progress, consequently
deciding on the number of elements that can be processed. This can be abstracted
away with a method nextBatch that takes a desired number of elements to traverse and
returns an estimated number of elements to be traversed, or −1 if there are none left.
Figure 5.17 shows an updated version of the loop scheduling algorithm that relies on
these methods. Iterators should also abstract the method markStolen shown earlier.

We show the complete work-stealing iterator interface in Figure 5.18. The additional
method owner returns the index of the worker owning the iterator. The method next
can be called as long as the method hasNext returns true, just as with the ordinary
iterators. Method hasNext returns true if next can be called before the next nextBatch
call. Finally, the method split can only be called on S iterators and it returns a pair of
iterators such that the disjoint union of their elements are the remaining elements of the
original iterator. This implies that markStolen must internally encode the iterator state
immediately when it gets stolen.

is applicable to parallel ranges, arrays, vectors and data-structures where indexing is
fast. The implementation for ranges in Figure 5.19 uses the private keyword for the
fields nextProgress and nextUntil used in next and hasNext. Since their contracts
ensure that only the owner calls them, their writes need not be globally visible and are
typically faster. The field progress is marked with the keyword atomic, and is modified
by the CAS in the line 254, ensuring that its modifications are globally visible through a
memory barrier.

A considerable number of applications use the tree representation for their data. Text
editing applications often represent text via ropes, and HTML document object model
is based on an n-ary tree. Ordered sets are usually implemented as balanced search
trees, and most priority queues are balanced trees under the hood. Persistent hash tables
present in many functional languages are based on hash tries. Parallelizing operations
on trees is thus a desirable goal.

Stealers for flat data structures were relatively simple, but efficient lock-free tree stealiterators
are somewhat more involved. In this section we do not consider unbalanced
binary trees since they cannot be efficiently parallelized – a completely unbalanced tree
degrades to a linked list. We do not consider n-ary trees either, but note that n-ary
tree steal-iterators are a straightforward extension to the steal-iterators presented in this
section. We note that steal-iterators for trees in which every node contains the size of its
subtree can be implemented similarly to IndexIterators presented earlier, since their
iteration state can be encoded as a single integer. Finally, iteration state for trees with
pointers to parent nodes (two-way trees) can be encoded as a memory address of the
current node, so their steal-iterators are trivial.

The trees we examine have two important properties. First, given a node T at the depth
d and the total number of keys N in the entire tree, we can always compute a bound on
the depth of the subtree rooted at T from d and N. Similarly, we can compute a bound
on the number of keys in the subtree of T. These properties follow from the fact that
the depth of a balanced tree is bounded by its number of elements. Red-black trees, for
example, have the property that their depth d is less than or equal to 2 logN where N is
the total number of keys.

In this section we show experimental performance evaluations for algorithms and data
structures introduced in the previous chapters. Sections 6.1 and 6.2 evaluate the performance
of the basic data-parallel collections shown in Chapter 2. Section 6.3 compares the
efficiency of various Conc-tree operations against similar data structures. and Section 6.4
does the same for the Ctrie data structure and alternative state-of-the-art concurrent map
implementations. Finally, in Section 6.5 we compare the work-stealing tree scheduling
against different data-parallel schedulers.

In all the benchmarks we take the nature of a managed runtime like the JVM into account.
Each benchmark is executed on a separate JVM invocation. Each method is invoked
many times, and the total time is measured – this is repeated until this value stabilizes
[Georges et al.(2007)Georges, Buytaert, and Eeckhout]. Effects of the JIT compiler and
automatic garbage collection on performance are thus minimized. In some benchmarks we
rely on the ScalaMeter benchmarking framework for Scala [Prokopec(2014b)] to ensure
reproducable and stable results.

In the first benchmark, a total of n elements are inserted into a concurrent collection.
Insertion of these n elements is divided between p processors. This process is
repeated over a sequence of 2000 runs on a single JVM invocation, and the average
time required is recorded. We apply these benchmarks to ConcurrentHashMap and
ConcurrentSkipListMap, two concurrent map reference implementations for the JVM,
both part of the Java standard library, and compare multithreaded running time against
that of inserting n elements into a java.util.HashMap in a single thread. The results
are shown in Figure 6.1. Horizontal axis shows the number of processors and the vertical
axis shows the time in milliseconds.

The microbenchmarks shown in Figure 6.4 were executed for parallel arrays, hash tries
and hash tables on a machine with 4 Dual-Core AMD Opteron 2.8 MHz processor. The
number of processors used is displayed at the horizontal axis, the time in milliseconds
needed is on the vertical axis. All tests were performed for large collections, and the size
of the collection is shown for each benchmark.

The entry Sequential denotes benchmarks for sequential loops implementing the operation
in question. The entry HashMap denotes regular flat hash tables based on linear hashing.
If the per-element amount of work is increased, data structure handling cost becomes
negligible and parallel hash tries outperform hash tables even for two processors.

We should comment on the results of the filter benchmark. Java’s parallel array first
counts the number of elements satisfying the predicate, then allocates the array and
copies the elements. Our parallel array assembles the results as it applies the predicate
and copies the elements into the array afterwards using fast array copy operations. When
using only one processor the entire array is processed at once, so the combiner contains
only one chunk – no copying is required in this case, hence the reason for its high
performance in that particular benchmark.

The map benchmark for parallel arrays uses the optimized version of the method which
allocates the array and avoids copying, since the number of elements is known in advance.
Benchmark for flatMap includes only comparison with the sequential variant, as there is
currently no corresponding method in other implementations.

