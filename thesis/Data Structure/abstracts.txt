Current computer systems separate main memory from storage. Programming languages typically
reflect this distinction using different representations for data in memory (e.g. data structures,
objects) and storage (e.g. files, databases). Moving data back and forth between these different
layers and representations compromise both programming and execution efficiency. Recent nonvolatile
memory technologies, such as Phase-Change Memory, Resistive RAM, and Magnetoresistive
RAM make it possible to collapse main memory and storage into a single layer of persistent memory,
opening the way for simpler and more efficient programming abstractions for handling persistence.
This Ph.D. thesis introduces a design for the runtime environment for languages with automatic
memory management, based on an original combination of orthogonal persistence, persistent memory
programming, persistence by reachability, and lock-based failure-atomic transactions. Such design
can significantly increase programming and execution efficiency, as in-memory data structures are
transparently persistent, without the need for programmatic persistence handling, and removing the
need for crossing semantic boundaries.

In order to validate and demonstrate the proposed concepts, this work also presents JaphaVM,
the first Java Virtual Machine specifically designed for persistent memory. In experimental results
using benchmarks and real-world applications, JaphaVM in most cases executed the same operations
between one and two orders of magnitude faster than database- and file-based implementations,
while requiring significantly less lines of code.

Connectedness is an important topological property and has been widely studied
in digital topology. However, three main challenges exist in applying connectedness to
solve real world problems: (1) the definitions of connectedness based on the classic and
fuzzy logic cannot model the “hidden factors” that could influence our decision-making;
(2) these definitions are too general to be applied to solve complex problem; and (3)
many measurements of connectedness are heavily dependent on the shape (spatial
distribution of vertices) of the graph and violate the intuitive idea of connectedness.

This research focused on solving these challenges by redesigning the
connectedness theory, developing fast algorithms for connectedness computation, and
applying the newly proposed theory and algorithms to solve challenges in real problems.

The newly proposed Neutro-Connectedness (NC) generalizes the conventional
definitions of connectedness and can model uncertainty and describe the part and the
whole relationship. By applying the dynamic programming strategy, a fast algorithm was
proposed to calculate NC for general dataset. It is not just calculating NC map, and the
output NC forest can discover a dataset’s topological structure regarding connectedness.

In the first application, interactive image segmentation, two approaches were
proposed to solve the two most difficult challenges: user interaction-dependence and
intense interaction. The first approach, named NC-Cut, models global topologic property
among image regions and reduces the dependence of segmentation performance on the
appearance models generated by user interactions. It is less sensitive to the initial region
of interest (ROI) than four state-of-the-art ROI-based methods. The second approach,
named EISeg, provides user with visual clues to guide the interacting process based on
NC. It reduces user interaction greatly by guiding user to where interacting can produce
the best segmentation results.

In the second application, NC was utilized to solve the challenge of weak
boundary problem in breast ultrasound image segmentation. The approach can model the
indeterminacy resulted from weak boundaries better than fuzzy connectedness, and
achieved more accurate and robust result on our dataset with 131 breast tumor cases.
Connectedness plays an important role in human cognitive and learning activities.
Human vision system is very sensitive to the connectedness property of objects and
effective in “calculating” the property. The study of connectedness will help us explore
the way of human brain extracting the global properties of objects, and enable different
avenues to design new artificial intelligent (AI) systems with better performances.

However, connectedness is rarely considered in current AI systems because of the
lack of complete theoretic system and efficient computation algorithm. In this work, I
focus on building the connectedness theory and algorithms in digital space, and apply
them to solve many challenging problems in natural image and low-quality biomedical
image segmentation.

The newly proposed Neutro-Connectedness (NC) theory makes it possible to
describe the part and the whole relationship and to model the “hidden factors”
influencing the decision-making. By applying the dynamic programming strategy, a fast
algorithm is proposed to calculate NC for general dataset. It calculates the NC map, and
also outputs the NC forest to discover the topological structure of a dataset. The power of
NC is demonstrated by applying it to solve two challenging applications: interactive
image segmentation (IIS) and breast ultrasound image segmentation (BUSIS).

Static estimation of resource utilization of programs is a challenging and important problem
with numerous applications. In this thesis, I present new algorithms that enable users to
specify and verify their desired bounds on resource usage of functional programs. The resources
considered are algorithmic resources such as the number of steps needed to evaluate
a program (steps) and the number of objects allocated in the memory (alloc). These resources
are agnostic to the runtimes and platforms on which the programs are executed yet provide
a concrete estimate of the resource usage of an implementation. Our system is designed to
handle sophisticated functional programs that use recursive functions, datatypes, closures,
memoization and lazy evaluation.

In our approach, users can specify in the contracts of functions an upper bound they expect to
hold on the resource usages of the functions. The upper bounds can be expressed as templates
with numerical holes. For example, a bound steps · ?*size(inp) + ? denotes that the number
of evaluation steps is linear in the size of the input inp. The templates can be seamlessly
combined with correctness invariants or preconditions necessary for establishing the bounds.
Furthermore, the resource templates and invariants are allowed to use recursive and first-class
functions as well as other features supported by the language. Our approach for verifying
such resource templates operates in two phases. It first reduces the problem of resource
inference to invariant inference by synthesizing an instrumented first-order program that
accurately models the resource usage of the program components, the higher-order control
flow and the effects of memoization, using algebraic datatypes, sets and mutual recursion.
The approach solves the synthesized first-order program by producing verification conditions
of the form 98 using a modular assume/guarantee reasoning. The 98 formulas are solved
using a novel counterexample-driven algorithm capable of discovering strongest resource
bounds belonging to the given template.

I present the results of using our system to verify upper bounds on the usage of algorithmic
resources that correspond to sequential and parallel execution times, as well as heap and
stack memory usage. The system was evaluated on several benchmarks that include advanced
functional data structures and algorithms such as balanced trees, meldable heaps, Okasaki’s
lazy data structures, streams, sorting algorithms, dynamic programming algorithms, and
also compiler phases like optimizers and parsers. The evaluations show that the system is
able to infer hard, nonlinear resource bounds that are beyond the capability of the existing
approaches. Furthermore, the evaluations presented in this dissertation show that, when
averaged over many benchmarks, the resource consumption measured at runtime is 80% of
the value inferred by the system statically when estimating the number of evaluation steps
and is 88% when estimating the number of heap allocations.

The data-parallel programming model fits nicely with the existing declarative-style bulk
operations that augment collection libraries in many languages today. Data collection
operations like reduction, filtering or mapping can be executed by a single processor
or many processors at once. However, there are multiple challenges to overcome when
parallelizing collection operations.

First, it is challenging to construct a collection in parallel by multiple processors. Traditionally,
collections are backed by data structures with thread-safe variants of their
update operations. Such data structures are called concurrent data structures. Their
update operations require interprocessor synchronization and are generally slower than
the corresponding single-threaded update operations. Synchronization costs can easily
invalidate performance gains from parallelizing bulk operations such as mapping or filtering.
This thesis presents a parallel collection framework with a range of data structures
that reduce the need for interprocessor synchronization, effectively boosting data-parallel
operation performance. The parallel collection framework is implemented in Scala, but
the techniques in this thesis can be applied to other managed runtimes.

Second, most concurrent data structures can only be traversed in the absence of concurrent
modifications. We say that such concurrent data structures are quiescently consistent. The
task of ensuring quiescence falls on the programmer. This thesis presents a novel, lock-free,
scalable concurrent data structure called a Ctrie, which supports a linearizable, lock-free,
constant-time snapshot operation. The Ctrie snapshot operation is used to parallelize
Ctrie operations without the need for quiescence. We show how the linearizable, lock-free,
constant-time snapshot operation can be applied to different concurrent, lock-free tree-like
data structures.

Finally, efficiently assigning parts of the computation to different processors, or scheduling,
is not trivial. Although most computer systems have several identical CPUs, memory
hiearchies, cache-coherence protocols and interference with concurrent processes influence
the effective speed of a CPU. Moreover, some data-parallel operations inherently require
more work for some elements of the collection than others – we say that no data-parallel
operation has a uniform workload in practice. This thesis presents a novel technique
for parallelizing highly irregular computation workloads, called the work-stealing tree
scheduling. We show that the work-stealing tree scheduler outperforms other schedulers
when parallelizing highly irregular workloads, and retains optimal performance when
parallelizing more uniform workloads.

Concurrent algorithms and data structure operations in this thesis are linearizable
and lock-free. We present pseudocode with detailed correctness proofs for concurrent
data structures and algorithms in this thesis, validating their correctness, identifying
linearization points and showing their lock-freedom.


