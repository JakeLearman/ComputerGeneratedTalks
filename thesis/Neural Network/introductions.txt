The annual cost to the global economy due to cybercrime could be as high as $575 billion, which includes both the gain to criminals and the costs to companies for defense and recovery[6]. It is even projected that the said cost will reach $2 trillion by 2019[12, 19].
Among the contributory felonies to cybercrime is intrusions, which is defined as illegal or unauthorized use, misuse, or exploitation by either authorized users or external attackers[24]. To identify intrusions in a computer system, an intrusion detection system (IDS) is used [24]. The most common method used for uncovering intrusions is the analysis of user activities[8, 15, 20, 24].
It could be argued that the aforementioned method is laborious when done manually, since the data of user activities is massive in nature[16], e.g. 3每35 MB of data in an eight-hour period in the 1990s [7]. To simplify the problem, automation through machine learning must be done.
A study by Mukkamala, Janoski, & Sung (2002)[20] shows how support vector machine (SVM) and artificial neural network (ANN) can be used to accomplish the said task.
In machine learning, SVM separates two classes of data points using a hyperplane[5]. On the other hand, an ANN is a computational model that represents the human brain, and shows information is passed from a neuron to another[21].
An approach combining ANN and SVM was proposed by Alalshekmubarak & Smith[2], for time-series classification. Specifically, they combined echo state network (ESN, a variant of recurrent neural network or RNN) and SVM. This research presents a modified version of the aforementioned proposal, and use it for intrusion detection.
The proposed model will use recurrent neural network (RNNs) with gated recurrent units (GRUs) in place of ESN. RNNs are used for analyzing and/or predicting sequential data, making it a viable candidate for intrusion detection[21], since network traffic data is sequential in nature.
This chapter is the foundation of the research conducted during the period and discussed in this thesis. It also overview the problem statement, methodology, objectives, achievements and motivation of the proposed study. It also provides an overview of the organization of this thesis.
In this chapter, the main focus is to review the existing digital image watermarking techniques based on artificial neural network, support vector regression, support vector machines, genetic algorithm and combination of these machine learning algorithms called hybrid techniques based image watermarking. The reviews of these watermarking schemes give the effective solution for copyright protection applications. It is observed that machine learning algorithms are used to reduce the trade off between robustness and imperceptibility in spatial as well as transform domain. In addition to robustness and imperceptibility, some of the researchers have focused on security of the watermark and how much information is stored in multimedia contents.
In the ages of growing internet technologies, digital contents is widely used on internet because of less or almost no storage cost, high speed transmission and everywhere availability. The ownership, illegal copying, duplicity and copyrights protection of these digital contents such as image, audio and video are main concerns over high speed growth of internet users. Digital watermarking provides a solution to the problem of copyright protection, copy protection, and proof of ownership etc.[Cox et al., 1997, 2002, Kutter and Petitcolas, 1999, Petitcolas, 2000, Xianghong et al., 2003]. Watermarking schemes are divided in two categories based on the domain analysis; first is spatial domain and second is frequency or transform domain. In literature survey of digital watermarking, it is observed that frequency domain based watermarking is more robust as compared to spatial domain based watermarking [Chang et al., 2000, Chu, 2003, Khemchandani et al., 2013, Nikolaidis and Pitas, 1998]. In spatial domain schemes, watermark is directly embedded by modifying the pixels value [Nikolaidis and Pitas, 1998] but in frequency domain schemes, first the host image is converted from spatial domain to frequency/transform domain by using different transformation functions such as singular value decomposition (SVD) [Aslantas, 2008], discrete wavelet transform (DWT) [Agarwal et al., 2013, Mehta et al., 2015b,c, Peng et al., 2010], discrete Cosine transform (DCT) [Chu, 2003, Eyadat and Vasikarla, 2005, Liyun et al., 2006] and discrete Fourier transform (DFT) [Chen and Bui, 1999, Chen et al., 2005, 2006] then watermark is inserted into coefficients of these frequency/transform domains. Spatial domain schemes are easy to implement, having high payload but less robust; Transform domain schemes are hard to implement, having less payload but highly robust against image processing attacks [Chu, 2003, Liyun et al., 2006, Mehta et al., 2015b,c, Peng et al., 2010]. Different techniques are used by various researchers to set the tradeoff between imperceptibility and robustness of watermarking schemes. Tradeoff between imperceptibility and robustness is a big issues in watermarking and also important to control. Moslty single scaling factor (SSF) is used in for this purpose. but every block have different sensitivity and tolerance level to noisy attacks so SSF is not suitable every time for all blocks of image [Mehta et al., 2015a,b]. Keeping tradeoff in mind, various researchers developed digital image watermarking techniques using artificial neural networks [Mehta and Rajpal, 2013a], genetic algorithms [Mehta et al., 2015c], support vector regression [Mehta et al., 2015b], particle swarm optimization (PSO) [Tao et al., 2010], differential evolution (DE) [Ali et al., 2014], and their combinations [Mehta et al., 2015c] based on machine learning algorithms to enhance the imperceptibility and robustness. These high generalization machine learning algorithms are having good learning ability of image properties. Significant amount of robustness and imperceptibility is obtained by various researchers under different image processing attacks using these machine learning algorithms [Mehta et al., 2015b,c]. Lagrangian Twin Support Vector Regression (LTSVR) which is proposed by Balasundaram et al. [Balasundaram and Tanveer, 2013] is a newly designed machine learning algorithm and its performance is already examined [Balasundaram and Tanveer, 2013, Khemchandani et al., 2013] on synthetics and real world dataset[Murphy and Aha, 1992]. The rest of the Chapter 3 is organized in the following manner. In Section 3.2, mathematical formulation which is used in the proposed algorithms such as Arnold transformation, formulation of LTSVR and fuzzy entropy is described. A new approach of image watermarking scheme using fuzzy entropy for block selection and LTSVR in DCT domain is presented in Section 3.3. In Section 3.4, we discussed a new hybrid approach of digital image watermarking in DCT domain using LTSVR and GA. In Section 3.5, a brief summary of the Chapter 3 is concluded. 
The internet is a very popular distribution medium all over world for digital contents as it is inexpensive, no storage requirements, and fast access since few decades. With the success of digital communication on the Internet, various problems related to copyright protection of digitized properties, illegal copying, ownership of multimedia data, data security etc. have arisen. Digital watermarking provides a solution to soft contents [Cox et al., 1997, Moulin and Mihcak, 2002] for copyright protection and its applications. Watermarking is the method of embedding the secret information known as watermark in an imperceptible manner into the original digital media without losing its visual quality [Moulin and Mihcak, 2002]. Spatial and frequency domain watermarking are two different domains of digital image watermarking. In the literature of digital watermarking [Mehta et al., 2015b,c, Peng et al., 2010, Wen et al., 2009], it has been found that frequency domain watermarking is more robustness against attacks as compared to spatial domain watermarking [Shen et al., 2005]. The use of machine learning algorithms like BPN, PNN [Tang and Liao, 2004, Wen et al., 2009], support vector regression [Peng et al., 2010, Shen et al., 2005], genetic algorithms [Mehta et al., 2015c] and their combination based hybrid image watermarking system are designed by various researchers [Mehta et al., 2015c] to increase the imperceptibility and robustness. Significant amount of imperceptibility and robustness against image processing attacks is achieved [Mehta et al., 2015b,c] due to the adaptive learning capability of image data sets and good generalization ability against noise of these machine learning algorithms. In this paper, a newly designed LTSVR machine learning approach by Balasundaram et al. [Balasundaram and Tanveer, 2013] is employed in image watermarking. The generalization performance of LTSVR on synthetic datasets which is obtained from UCI repository and against noisy datasets is already examined [Balasundaram and Tanveer, 2013]. With the help of the work presented in this paper, high generalization against noisy datasets and the adaptive learning capability of LTSVR onto image watermarking for improving the robustness is examined. In this chapter two novel work presented. First a grayscale image watermarking approach using LTSVR and based upon the feature extracted with the help of hybrid LWT-QR factorization is developed. The selected blocks of the approximate subband (LL) of the image are used to insert the watermark. QR transformation [Song et al., 2011] method is applied to non-overlapping regions selected using fuzzy entropy [Kumar et al., 2011] to get Q and R matrix. The elements (feature vector) of rare used to make dataset for LTSVR training. The predicted value obtained using function generated by the LTSVR training is used to insert the watermark bits. A number of gray scale images are used to verify the performance of the approach presented against attacks. Due to the limitations on the number of pages, the results on Lena and Elaine image are explained in this paper. The generalization performance of LTSVR against noisy datasets is measured by the visual quality of extracted watermark which indicates the robustness of the approach and the security of the watermark is obtained through Arnold transformation. Fuzzy entropy [Kumar et al., 2011] is sensitive to image variations; it is used for selecting smooth non overlapping blocks and discards blocks with redundant data. Second scheme using Lagrangian twin support vector regression (LTSVR) in integer wavelet domain with QR factorization onto grayscale images is explored to handle the main issues for copyright application. After transforming the image from spatial domain to frequency domain using integer wavelet transform (IWT), the low frequency subband is divided into non-overlapping blocks. Based on fuzzy entropy of each block, selected number of blocks is decomposed using QR factorization to extract the prominent features to embed the scrambled image of binary watermark. LTSVR is used to find the non linear regression function between the input and target vector, obtained by the elements of R matrix of selected blocks. The non linear regression function obtained after training the LTSVR is used to embed the scrambled watermark. The security to the watermark is provided by transforming original watermark image into distorted by applying Arnold transformation, which is to be implanted into the original signal. The effectiveness of the proposed scheme is demonstrated by the good visual quality of the watermarked image as measured by high peak-signal-to-noise ratio (PSNR) and successful extraction of watermark against a series of image processing attacks as quantified by bit correct ratio (BCR). The outlines of the remaining part of the chapter is as follows. The mathematical description of QR transformation, Lifting scheme of wavelet transform is explained in Section 4.2. In Section 4.3, a novel grayscale image watermarking approach using LTSVR and LWT-QR decomposition in wavelet domain is discussed. Image watermarking in wavelet domain using LTSVR and IWT with QR decomposition is described in Section 4.4. At the end, the summary of the chapter is concluded in Section 4.5.
Time series prediction has important applications in various domains such as finance, economics, medicine, ecology or meteorology. The classical methods used for time series prediction, like Box-Jenkins or ARIMA, assume that there is a linear relationship between inputs and outputs. In recent years, artificial neural networks (ANN) have been used to model non-linear time series. ANN are able to approximate nonlinear functions, they can be applied to time series modeling without assuming a prior function form and they do not require strict assumptions like in regression analysis. The different architectures of ANN that have been proposed for time series analysis [1] can be split into two main categories depending on if internal network feedback is allowed or not.
﹛﹛In Feed-forward neural networks (FFNNs) the examples flow in one direction without feedback. FFNNs can accommodate dynamics by including past input and target values. In contrast, Recurrent neural networks (RNNs) allow for internal network feedbacks. They are basically sets of FFNNs with recursive connections. This means that they have a feedback loop where data can be fed back into the input at some point before they are fed forward again for further processing towards the final output. This procedure allows the network to have a feedback (internal memory), making it capable of modelling temporal dependencies between the inputs and the desired outputs. This is the most important advantage of RNNs compared to FFNNs.
﹛﹛RNNs possess interesting universal approximation capabilities [2], that make them good candidates for time series modelling. They make efficient use of the temporal information in the input sequence, both for classification as well as for prediction [3, 4, 5, 6]. It has been also shown that RNNs are a type of non-linear-autoregressive-moving-average (NARMA) models [3]. Moreover, RNNs are able to approximate nonlinear functions without assuming a prior function form and they do not require strict assumptions like in linear regression analysis. These make them promising candidates in the setting of non-linear time series prediction.
﹛﹛When using recurrent networks the training is more difficult than feed forward ones because of the vanishing and exploding gradient problems [7, 8]. This means that RNNs fail to learn in the presence of time lags, with more than about 10 time steps difference between relevant input events and targets. Long Short Term Memory networks (LSTMs) [9, 10], the descendants of RNNs, address this problem through their forgetting mechanism. However forgetting makes their analysis more demanding. Owing to this, and although the LSTMs definitely deserve a thorough study in the setting of time-series, the thesis focuses only on RNNs which are simpler and lead themselves better to a basic understanding of how the models work. With plain vanila RNNs the vanishing and exploding gradient problems can be handled in different ways, such as by using different initialization procedure of the weight matrices (Xavier initialization [11]), adding regularization terms [12] and truncated backpropagation with a few steps instead of the full one, which are applied in this thesis.
﹛﹛RNNs have the advantage that can approximate nonlinear functions, they can be applied to time series modeling without assuming a prior function form and they do not require strict assumptions like in regression analysis. When RNNs are used for forecasting, a time window is used as input. The correct selection of its length is not an obvious exercise. One solution is to keep it small (usually equal to 1) and enable the model to develop its own memory of the past [13]. This was one of the motivations of this thesis, because we could not find any existing study comparing these two approaches in combination. Moreover the exploring Granger causality is closely related to the analysis of vector autoregressive (VAR) models and it is a real challenge especially for non-linear data. So another motivation of this project was to impose sparsity on multivariate time series using RNNs and discover the Granger causal links among them. The main objectives of this thesis are summarized as:
1. Investigate empirically if the order of the time series should be taken into account in the structure of the network for time series forecasting
2. Explore empirically the use of RNNs in the setting of multiple time serie ans also examine if and how we can use RNNs to discover the Grange causal [14] relationships in the setting of multiple time series
﹛﹛We believe that by using RNNs for forecasting, no prior knowledge of the number of the lags in the time series is required, especially for big data sets. It should be enough to give as input one time step at a time. The internal memory that the model develops should be sufficient and we no longer need a time window to take into account the past values of the time series. To investigate if a previous knowledge of the order of the process for a time ahead prediction is important/ helpful univariate time series is trained with three different approaches.
﹛﹛To explore the use of RNNs in the setting of multiple time series we train the network with the classic way of training RNNs and we propose also a new method to discover the
Granger causal links among the time series
﹛﹛Also, pursuing the improvement of forecasting, different architectures of RNNs are explored (such as the size of the hidden layer) and regularization techniques are applied (norm based methods: L1norm, L2norm and dropout [15, 16]).
﹛﹛We start with a small introduction on the time series in chapter 2. We present an introduction of the RNNs in chapter 3 along with background material on the learning and optimization of the network and the different regularization techniques that are used. In chapter 4, the methodology and the technical details of our experiments, including the learning algorithm, are presented in detail. We describe the three different approaches that are used to train the network to figure out if a prior knowledge of the time lag dependencies of the time series is needed. We also explain how RNNs are used in the setting of multiple time series. Moreover, we introduce a new method for discovering Granger causal relationships among the time series. The experimental results are presented in chapter 5 while chapter 6 lists some concluding remarks.
﹛﹛A time series is a sequence of observations over time. One of the most important objectives in time series analysis is to forecast its future values, at a given time t. We want to estimate the value of the time series at time t + L, L-steps ahead prediction, using just the information given by the past values of the time series. In other words we predict the future values based on previously observed values. The classical method used for time series prediction is Box-Jenkins or ARIMA models, where we assume that the there is linear relation between inputs and outputs. The accuracy of the forecasting is typically assessed using the Mean Squared Error
﹛﹛Understanding how the human brain works has been a long-standing quest in human history. In that journey, neuroscientists have made interesting discoveries, including that certain neurons in human brains selectively fire in response to specific, abstract concepts such as Halle Berry or Bill Clinton, shedding light on the question of whether learned neural codes are local or distributed [33]. These neurons were identified by finding the preferred stimuli (here, images) that highly excite a specific neuron, which was accomplished by showing subjects many different images while recording a target neuron＊s activation. This experiment revealed another interesting finding that neurons can be multifaceted. For example, the ※Halle Berry neuron§ was found to respond to very different stimuli related to the actress〞from pictures of her face, to pictures of her in costume, to the word ※Halle Berry§ printed as text [33]. Inspired by such neuroscience research, we are interested in shedding light into the inner workings of DNNs by finding the preferred inputs for each of their neurons. As the neuroscientists did, one could simply show the network a large set of images and record the subset of images that highly activate a given neuron [124,141]. However, that method has disadvantages vs. synthesizing preferred stimuli: (1) it requires a distribution of images that are similar to those used to train the network, which may not be known (e.g. when probing a pretrained network, one may not know which data were used to train it); (2) even in such a dataset, many informative images that would activate the neuron may not exist because the image space is vast [171]; (3) with real images, it is unclear which of their features a neuron 1 has learned: for example, if a neuron is activated by a picture of a lawn mower on grass, it is unclear if it ※cares about§ the grass, but if an image synthesized to highly activate the lawn mower neuron contains grass, we can be more confident the neuron has learned to pay attention to that context.
﹛﹛Synthesizing preferred stimuli is called activation maximization [27,28,165每167,170,171]. The process starts from a random image and iteratively calculates how the color of each pixel in the image should be changed to increase the activation of a neuron. That gradient can be computed analytically via back-propagation and chain rule when we have access to the network parameters [167]. It can also be approximated via a stochastic process such as evolutionary algorithms [31]. Activation maximization can be performed on any neurons of a network (e.g. output or hidden neurons). This technique is the common trait of the five research projects described in this dissertation. However, in each project, activation maximization is performed in a different way to answer a different research question. In what follows, I will summarize each project and list its main contribution.  
﹛﹛Deep neural networks (DNNs) learn hierarchical layers of representation from sensory input in order to perform pattern recognition [2,3]. Recently, these deep architectures have demonstrated impressive, state-of-the-art, and sometimes human-competitive results on many pattern recognition tasks, especially vision classification problems [4每7]. Given the near-human ability of DNNs to classify visual objects, questions arise as to what differences remain between computer and human vision. A recent study revealed a major difference between DNN and human vision [1]. Changing an image, originally correctly classified (e.g. as a lion), in a way imperceptible to human eyes, can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). In this paper, we show another way that DNN and human vision differ: It is easy to produce images that are completely unrecognizable to humans (Fig. 2.1), but that state-of-the-art DNNs believe to be recognizable objects with over 99% confidence (e.g. labeling with certainty that TV static is a motorcycle). Specifically, we use evolutionary algorithms or gradient ascent to generate images that are given high prediction scores by convolutional neural networks (convnets) [4,8]. These DNN models have been shown to perform well on both the ImageNet [9] and MNIST [10] datasets. We also find that, for MNIST DNNs, it is not easy to prevent the DNNs from being fooled by retraining them with fooling images labeled as such. While retrained DNNs learn to classify the negative examples as fooling images, a new batch of fooling images can be produced that fool these new networks, even after many retraining iterations. Our findings shed light on current differences between human vision and DNN-based computer vision. They also raise questions about how DNNs perform in general across different types of images than the ones they have been trained and traditionally tested on.
﹛﹛Stochastic optimization and search algorithms, such as simulated annealing and evolutionary algorithms (EAs), often outperform human engineers in several domains [216]. However, there are other domains in which these algorithms cannot produce effective solutions yet. Their Achilles Heel is the trap of local optima [215], where the objective given to an algorithm (e.g. a fitness function) prevents the search from leaving sub-optimal solutions and reaching better ones. Novelty Search [182, 214] addresses this problem by collecting the stepping stones needed to ultimately lead to an objective instead of directly optimizing towards it. The algorithm encourages searching in all directions by replacing a performance objective with a reward for novel behaviors, the novelty of which is measured with a distance function in the behavior space [218]. This recent conceptual breakthrough has been shown to outperform traditional stochastic optimization on deceptive problems where specifying distances between desired behaviors is easy [182,214]. Reducing a high-dimensional search space to a lowdimensional one is essential to the success of Novelty Search, because in high-dimensional search spaces there are too many ways to be novel without being interesting [217]. For example, if novelty is measured directly in the high-dimensional space of pixels in a 60,000 pixel image, being different can mean different static patterns, which are not interestingly different types of images. 
﹛﹛Here we propose a novel algorithm called an Innovation Engine that enables searching in high-dimensional spaces for which it is difficult for humans to define what constitutes interestingly different behaviors. The key insight is to use a deep neural network (DNN) [2] as the evaluation function to reduce a high-dimensional search space to a low-dimensional search space where novelty means interesting novelty. State-of-the-art DNNs have demonstrated impressive and sometimes human-competitive results on many pattern recognition tasks [2,4]. They see past the myriad pixel differences, such as lighting changes, rotations, zooms, and occlusions, to recognize abstract concepts in images, such as tigers, tables, and turnips. Here we suggest harnessing the power of DNNs to recognize different types of things in the abstract, high-level spaces they can make distinctions in. A second reason for choosing DNNs is that they work by hierarchically recognizing features. In images, for example, they recognize faces by combining edges into corners, then corners into eyes or noses, and then they combine these features into even higher-level features such as faces [124,138,167,170]. Such a hierarchy of features is beneficial because those features can be produced in different combinations to produce new types of ideas/solutions. 
﹛﹛Despite their impressive performance, DNNs can also make mistakes. [1] found that it is possible to add imperceptible changes to an image originally correctly classified (e.g. as a bell pepper) such that a DNN will label it as something else entirely (e.g. an ostrich). [171] showed a different, but related, problem: images can be synthesized from scratch that are completely unrecognizable to human eyes as familiar objects, but that DNNs label with near-certainty as common objects (e.g. DNNs will declare with certainty that a picture filled with white noise static is an armadillo). While such shortcomings of DNNs impair Innovation Engines a fraction of the time, in this paper we emphasize that remaining fraction of the time wherein using DNNs as evaluators works well. Innovation Engines will only improve as DNNs are redesigned to not be so easily fooled. 
﹛﹛We first describe our long-term, ultimate vision for Innovation Engines that require no labeled data to endlessly innovate in any domain. Because there are many technical hurdles to overcome to reach that vision, we also describe a simpler, version 1.0 Innovation Engine that harnesses labeled data to simulate how the ultimate Innovation Engine might function. While Innovation Engines should work in any domain, we test one in the image generating domain that originally inspired the Novelty Search algorithm [139] and show that it can automatically produce a diversity of interesting images (Fig. 3.1). We also confirm some expectations regarding why Innovation Engines are expected to work. 
﹛﹛Recently, deep neural networks (DNNs) have demonstrated state-of-the-art〞and sometimes human-competitive〞results on many pattern recognition tasks, especially vision classification problems [4,97,277]. That success has motivated efforts to better understand the inner workings of such networks, which enables us to further improve their architectures, learning algorithms, and hyperparameters. An active area of research in this vein, called Deep Visualization, involves taking a trained DNN and creating synthetic images that produce specific neural activations of interest within it [27,28,166,167,171,287]. There are two general camps within Deep Visualization: activation maximization [28] and code inversion [166]. Activation maximization is the task of finding an image that maximally activates a certain neuron (aka ※unit§, ※feature§, or ※feature detector§), which can reveal what each neuron in a DNN has learned to fire in response to (i.e. which features it detects). This technique can be performed for the output neurons, such as neurons that classify types of images [27], and can also be performed for each of the hidden neurons in a DNN [28,167]. Code inversion is the problem of synthesizing an image that, for a specific DNN layer, produces a similar activation vector at that layer as a target activation vector produced by a specific real image [166,287]. It reveals what information about one specific image is encoded by the DNN code at a particular layer. Both activation maximization and code inversion start from a random image and calculate via backpropagation how the color of each pixel should be changed to either increase the activation of a neuron (for activation maximization) or produce a layer code closer to the target (for code inversion). However, previous studies have shown that doing so using only the gradient information produces unrealistic-looking images that are not recognizable [27,171], because the set of all possible images is so vast that it is possible to produce images that satisfy the objective, but are still unrecognizable. Instead, we must both satisfy the objective and try to limit the set of images to those that resemble natural images. Biasing optimization to produce more natural images can be accomplished by incorporating natural image priors into optimization, which has been shown to substantially improve the recognizability of the images generated. Many regularization techniques have been introduced that improve image quality such as: Gaussian blur [167], 汐-norm [27], total variation [166], jitter [164], and data-driven patch priors [165].
﹛﹛While these techniques have improved Deep Visualization methods over the last two years, the resultant images provide room for improvement: (1) the color distribution is unnatural (Fig. B.3b-e); (2) recognizable fragments of images are repeated, but these fragments do not fit together into a coherent whole: e.g. multiple ostrich heads without bodies, or eyes without faces (Fig. B.3b) [27,166]; (3) previous techniques have no systematic method to visualize different facets (types of stimuli) that a neuron respond to, but high-level neurons are known to be multifaceted. For example, a face-detecting neuron in a DNN was shown to respond to both human and lion faces ( [167]). Neurons in human brains are similarly multifaceted: a ※Halle Berry§ neuron was found that responds to very different stimuli related to the actress, from pictures of her in costume to her name printed as text [33].
﹛﹛We name the class of algorithms that visualize different facets of a neuron multifaceted feature visualization (MFV). [28] found that optimizing an image to maximally activate a neuron from multiple random starting images usually yielded the same final visualization. In contrast, [165] found that if the backpropagation neural pathway is masked out in a certain way, the resultant images can sometimes reveal different feature facets. This is the first MFV method to our knowledge; however, it was shown to visualize only two facets per output neuron, and is not able to systematically visualize all facets per neuron. In this paper, we propose two Deep Visualization techniques. Most importantly, we introduce a novel MFV algorithm that: (1) Sheds much more light on the inner workings of DNNs than other state-of-the-art Deep Visualization methods by revealing the different facets of each neuron. It also reveals that neurons at all levels are multifaceted, and shows that higher-level neurons are more multifaceted than lower-level ones (Fig. B.1). (2) Improves the quality of synthesized images, producing state-of-the-art activation maximization results: the colors are more natural and the images are more globally consistent (Fig. 4.5. See also Figs. 4.1, 4.4, 4.2) because each facet is separately synthesized. For example, MFV makes one image of a green bell pepper and another of a red bell pepper instead of trying to simultaneously make both (Fig. 4.2). The increased global structure and contextual details in the optimized images also support recent observations [167] that DNNs encode not only knowledge of a sparse set of discriminative features for performing classification, but also more holistic information about typical input examples in a manner more reminiscent of generative models. (3) Is simple to implement. The only main difference is how activation maximization algorithms are initialized. To obtain an initialization per facet, we project the training set images that maximally activate a neuron into a low-dimensional space (here, a 2D space via t-SNE), cluster the images via k-means, and average the n (here, 15) closest images to each cluster centroid to produce the initial image (Section 4.4).
﹛﹛We also introduce a center-biased regularization technique that attempts to produce ne central object. It combats a flaw with all activation maximization techniques including FV: they tend to produce many repeated object fragments in an image, instead of objects with more coherent global structure. It does so by allowing, on average, more optimization iterations for center pixels than edge pixels.
﹛﹛Understanding how the human brain works has been a long-standing quest in human history. Neuroscientists have discovered neurons in human brains that selectively fire in response to specific, abstract concepts such as Halle Berry or Bill Clinton, shedding light on the question of whether learned neural codes are local vs. distributed [33]. These neurons were identified by finding the preferred stimuli (here, images) that highly excite a specific neuron, which was accomplished by showing subjects many different images while recording a target neuron＊s activation. Such neurons are multifaceted: for example, the ※Halle Berry neuron§ responds to very different stimuli related to the actress〞from pictures of her face, to pictures of her in costume, to the word ※Halle Berry§ printed as text [33]. 
﹛﹛Inspired by such neuroscience research, we are interested in shedding light into the inner workings of DNNs by finding the preferred inputs for each of their neurons. As the neuroscientists did, one could simply show the network a large set of images and record a set of images that highly activate a neuron [124]. However, that method has disadvantages vs. synthesizing preferred stimuli: 1) it requires a distribution of images that are similar to those used to train the network, which may not be known (e.g. when probing a trained network when one does not know which data were used to train it); 2) even in such a dataset, many informative images that would activate the neuron may not exist because the image space is vast [171]; 3) with real images, it is unclear which of their features a neuron has learned: for example, if a neuron is activated by a picture of a lawn mower on grass, it is unclear if it ＆cares about＊ the grass, but if an image synthesized to highly activate the lawn mower neuron contains grass (as in Fig. 5.1), we can be more confident the neuron has learned to pay attention to that context.
﹛﹛Synthesizing preferred stimuli is called activation maximization [27, 28, 165每167, 170, 171]. It starts from a random image and iteratively calculates via backpropagation how the color of each pixel in the image should be changed to increase the activation of a neuron. Previous studies have shown that doing so without biasing the images produced creates unrealistic, uninterpretable images [27,171], because the set of all possible images is so vast that it is possible to produce ＆fooling＊ images that excite a neuron, but do not resemble the natural images that neuron has learned to detect. Instead, we must constrain optimization to generate only synthetic images that resemble natural images [166]. Attempting that is accomplished by incorporating natural image priors into the objective function, which has been shown to substantially improve the recognizability of the images generated [166, 167, 170]. Many hand-designed natural image priors have been experimentally shown to improve image quality such as: Gaussian blur [167], 汐-norm [27,165,167], total variation [166, 170], jitter [164,166,170], data-driven patch priors [165], center-bias regularization [170], and initializing from mean images [170]. Instead of hand-designing such priors, in this paper, we propose to use a superior, learned natural image prior [85] akin to a generative model of images. This prior allows us to synthesize highly human-interpretable preferred stimuli, giving additional insight into the inner functioning of networks. While there is no way to rigorously measure human-interpretability, a problem that also makes quantitatively assessing generative models near-impossible [163], we should not cease scientific work on improving qualitative results simply because humans must subjectively evaluate them.
﹛﹛Learning generative models of natural images has been a long-standing goal in machine learning [71]. Many types of neural network models exist, including probabilistic [71], autoencoder [71], stochastic [70] and recurrent networks [71]. However, they are typically limited to relatively low-dimensional images and narrowly focused datasets. Recently, advances in network architectures and training methods enabled the generation of high-dimensional realistic images [68,82,85]. Most of these works are based on Generative Adversarial Networks (GAN) [61], which trains two models simultaneously: a generative model G to capture the data distribution, and a discriminative model D to estimates the probability that a sample came from the training data rather than G. The training objective for G is to maximize the probability of D making a mistake. Recently [85] trained networks capable of generating images from highly compressed feature representations, by combining an auto-encoder-style approach with GAN＊s adversarial training. We harness these image generator networks as priors to produce synthetic preferred images. These generator networks are close to, but not true, generative models because they are trained without imposing any prior on the hidden distribution as in variationally auto-encoders [70] or GANs [61], and without the addition of noise as in denoising auto-encoders [60]. Thus, there is no natural sampling procedure nor an implicit density function over the data space.
﹛﹛The image generator DNN that we use as a prior is trained to take in a code (e.g. vector of scalars) and output a synthetic image that looks as close to real images from the ImageNet dataset [11] as possible. To produce a preferred input for a neuron in a given DNN that we want to visualize, we optimize in the input code space of the image generator DNN so that it outputs an image that activates the neuron of interest (Fig. 5.2). Our method restricts the search to only the set of images that can be drawn by the prior, which provides a strong bias toward realistic visualizations. Because our algorithm uses a deep generator network to perform activation maximization, we call it DGN-AM.
﹛﹛Recent years have seen generative models that are increasingly capable of synthesizing diverse, realistic images that capture both the fine-grained details and global coherence of natural images [70,82,85,142,147,148]. However, many important open challenges remain, including (1) producing photo-realistic images at high resolutions [145], (2) training generators that can produce a wide variety of images (e.g. all 1000 ImageNet classes) instead of only one or a few types (e.g. faces or bedrooms [82]), and (3) producing a diversity of samples that match the diversity in the dataset instead of modeling only a subset of the data distribution [61,163]. Current image generative models often work well at low resolutions (e.g. 32℅32), but struggle to generate high-resolution (e.g. 128℅128 or higher), globally coherent images (especially for datasets such as ImageNet [9] that have a large variability [46,61,84]) due to many challenges including difficulty in training [46,84] and computationally expensive sampling procedures [52,142].
﹛﹛Nguyen et al. [138] recently introduced a technique that produces high quality images at a high resolution. Their Deep Generator Network-based Activation Maximization (DGN-AM) involves training a generator G to create realistic images from compressed features extracted from a pre-trained classifier network E (Fig. 6.3f). To generate images conditioned on a class, an optimization process is launched to find a hidden code h that G maps to an image that highly activates a neuron in another classifier C (not necessarily the same as E).
﹛﹛Not only does DGN-AM produce realistic images at a high resolution (Figs. 6.2b & D.3b), but, without having to re-train G, it can also produce interesting new types of images that G never saw during training. For example, a G trained on ImageNet can produce ballrooms, jail cells, and picnic areas if C is trained on the MIT Places dataset (Fig. D.10, top).
﹛﹛A major limitation with DGN-AM, however, is the lack of diversity in the generated samples. While samples may vary slightly (e.g. ※cardoons§ with two or three flowers viewed from slightly different angles; see Fig. 6.2b), the whole image tends to have the same composition (e.g. a closeup of a single cardoon plant with a green background). It is noteworthy that the images produced by DGN-AM closely match the images from that class that most highly activate the class neuron (Fig. 6.2a). Optimization often converges to the same mode even with different random initializations, a phenomenon common with activation maximization [28,165,170]. In contrast, real images within a class tend to show more diversity (Fig. 6.2c). In this paper, we improve the diversity and quality of samples produced via DGN-AM by adding a prior on the latent code that keeps optimization along the manifold of realistic-looking images (Fig. 6.2d).
﹛﹛We do this by providing a probabilistic framework in which to unify and interpret activation maximization approaches [27,138,167,170] as a type of energy-based model [71,146] where the energy function is a sum of multiple constraint terms: (a) priors (e.g. biasing images to look realistic) and (b) conditions, typically given as a category of a separately trained classification model (e.g. encouraging images to look like ※pianos§ or both ※pianos§ and ※candles§). We then show how to sample iteratively from such models using an approximate Metropolis-adjusted Langevin sampling algorithm. We call this general class of models Plug and Play Generative Networks (PPGN). The name reflects an important, attractive property of the method: one is free to design an energy function, and ※plug and play§ with different priors and conditions to form a new generative model. This property has recently been shown to be useful in multiple image generation projects that use the DGN-AM generator network prior and swap in different condition networks [150,161,162]. In addition to generating images conditioned on a class, PPGNs can generate images conditioned on text, forming a text-to-image generative model that allows one to describe an image with words and have it synthesized. We accomplish this by attaching a recurrent, image-captioning network (instead of an image classification network) to the output of the generator, and performing similar iterative sampling. Note that, while this paper discusses only the image generation domain, the approach should generalize to many other data types. We publish our code and the trained networks at http://EvolvingAI.org/ppgn.
Texture classification is an important task in image processing and computer vision, with a wide range of applications, such as computer-aided medical diagnosis [1], [2], [3], classification of forest species [4], [5], [6], classification in aerial/satellite images [7], writer identification and verification [8] and music genre classification [9].
﹛﹛Texture classification commonly follows the standard procedure for pattern recognition, as described by Bishop et al. in [10]: extract relevant features, train a model using a training dataset, and evaluate the model on a held-out test set.
﹛﹛Many methods have been proposed for the feature extraction phase on texture classification problems, as reviewed by Zhang et al. [11] and tested on multiple datasets by Guo et al. [12]. Noteworthy techniques are: Gray-Level Co-occurrence Matrices (GLCM), the Local Binary Pattern operator (LBP), Local Phase Quantization (LPQ), and Gabor filters.
﹛﹛The methods above rely on domain experts to build the feature extractors to be used for classification. An alternative approach is to use models that learn directly from raw data, for instance, directly from pixels in the case of images. The intuition is using such methods to learn multiple intermediate representations of the input, in layers, in order to better represent a given problem. Consider an example for object recognition in an image: the inputs for the model can be the raw pixels in the image. Each layer of the model constitutes an equivalent of feature detectors, transforming the data into more abstract (and hopefully useful) representations. The initial layers can learn low-level features, such as detecting edges, and subsequent layers learn higher-level representations, such as detecting more complex local shapes, up to high-level representations, such as recognizing a particular object [13]. In summary, the term Deep Learning refers to machine learning models that have multiple layers, and techniques for effectively training these models, commonly building Deep Neural Networks or Deep Belief Networks [14], [15].
﹛﹛Methods using deep architectures have set the state-of-the-art in many domains in recent years, as reviewed by Bengio in [13] and [16]. Besides improving the accuracy on different pattern recognition problems, one of the fundamental goals of Deep Learning is to move machine learning towards the automatic discovery of multiple levels of representation, reducing the need for feature extractors developed by domain experts [16]. This is especially important, as noted by Bengio in [13], for domains where the features are hard to formalize, such as for object recognition and speech recognition tasks.
﹛﹛In the task of object recognition, deep architectures have been widely used to achieve state-of-the-art results, such as in the CIFAR dataset[17] where the top published results use Convolutional Neural Networks (CNN) [18]. The tasks of object and texture classification present similarities, such as the strong correlation of pixel intensities in the 2-D space, and present some differences, such as the ability to perform the classification using only a relatively small fragment of a texture. In spite of the similarities with object classification we observe that deep learning techniques are not yet widely used for texture classification tasks. Kivinen and Williams [19] used Restricted Boltzmann Machines (RBMs) for texture synthesis, and Luo et al. [20] used spike-and-slab RBMs for texture synthesis and inpainting. Both consider using image pixels as input, but they do not consider training deep models for classification among several classes. Titive et al. [21] used convolutional neural networks on the Brodatz texture dataset, but considered only low resolution images, and a small number of classes.
﹛﹛The similarities between texture classification and object recognition, and the good results demonstrated by using deep architectures for object recognition suggest that these techniques could be successfully applied for texture classification.
﹛﹛Neural Networks are widely used for a variety of tasks [WRL94] including tasks from clinical medicine [Bax95]. These networks are often adjusted through the process of evolution [SWE92]. Therefore it is necessary for a neural network to perform well in terms of evolvability.
﹛﹛In an attempt to find neural networks that perform better for certain tasks, or have improved characteristics (e.g. higher memorisation capabilities), new kinds of networks are constantly developed. Two examples are:
﹛﹛? GasNets [HSJO98, Hus98]. These networks are inspired by the discovery of freely floating nitric oxide in the brain. The neurons in this kind of networks have the ability to release different gases which modulate the behaviour of nearby neurons.
﹛﹛? Spiking neural networks [Maa97, BRC + 07]. These kind of networks try to imitate the natural spiking behaviour of neurons instead of using an artificial activation function.
﹛﹛In his bachelor thesis Stefan Bruhns [Bru15] has combined these two network types to create a new type of neural network called ※modulated spiking neural network§. He has also performed some testing on the performance of this network type in terms of evolvability which has shown some promising results. His analysis was done on a single task though, so it can not be generalised in terms of complexity and type of task.
﹛﹛In order to get a better understanding of how well these modulated spiking neural work in different conditions and to get a better understanding of the characteristics of this new network type, I did some more empirical study on different simulations with different attributes (e.g. some where pattern generators or memory are useful and some where they are not) in my bachelor thesis. This way I hope to find out more about the advantages and disadvantages of the modulated spiking neural network architecture. One focus will lie on the usage of gas in the networks because the usage of gas has already shown a great impact for GasNets [Hus98]. The assumption is that the release of gas in combination with the spiking neurons will lead to a powerful network architecture.
﹛﹛The company sells stone products, and accessories primarily for outdoor usage. A substantial portion of sales are of imported product sold from inventory. For the bulk of imported inventory items, the lead time from order to delivery is 10-12 weeks. And occasionally longer when disruptions occur in sourcing raw material. Inventory control is very important as sales can be lost when the company runs out of stock and the customer is unwilling to wait until new stock arrives or accept an alternative. Furthermore, sales have strong seasonality, where most sales occurring between spring and mid-autumn. Ordering is currently guided only by experience and historical sales; however it is clear that improvement is necessary. The purpose of this project is to implement a predictive model which uses internal and external data to estimate future sales in order to improve the ordering process system. The demand forecasts will be provided to suppliers, with appropriate estimates, to provide an indication of probable future orders.
﹛﹛The company currently uses Iris Exchequer as accounting and inventory control measure. Data is relatively easy to extract from Exchequer on a batch basis. The intention in the first instance is to continue to use Exchequer in its current form and use extracted sales and purchase data to drive the predictive modelling project. This is relatively straightforward as sales and purchase data are readily available.
﹛﹛However Exchequer has a number of shortcomings and the intention is to replace it at the beginning of the company＊s fiscal year (1 st October, 2014). Extensive research has revealed that there is no prepacked accounting software that provides the type of inventory control, supply chain management, and final customer delivery logistics that the company requires. Therefore the company outsourced the accounting to Xero, a cloud based accounting system and to build in-house inventory management as a front end with all other accounting function performed by Xero. As a consequence all data relating to inventory will be meticulously examined where this can help ensure that this predictive modelling project will be well supported.
﹛﹛The aim of this dissertation is to build a sales forecasting model for an English company and its implementation as a decision support tool. During this process, a review of most common uses, methods, challenges and application in the area of Business Analytics and Data Mining are presented. A more narrow view of the discipline of Data Mining as a tool of Business Analysts is taken. Special emphasis is made on the applications and the process of Mining Data with Artificial Neural Networks (ANN or ANNs for short). The main objective of the dissertation is to apply this modelling approach (along with other more common Data Mining methods) to develop a Sales Forecasting Tool (SFT) for the company.
﹛﹛The secondary objective is to show the range of applications for Artificial Neural Networks and in particular, their potential for interpretable or ※grey-box§ Regression Analysis.
﹛﹛The dissertation is divided in three main parts. The first one is the Literature Review (Chapter 2). Here, the main theory of Business Analytics and its applications are explained. Later in the chapter an explanation of Data Mining is given.
﹛﹛At the end of the Literature Review, a more technical review on the challenges, applications, and modelling process with ANNs is presented. On Chapters 3 and 4 the Case Study for the development of a Sales Forecasting tool (SFT) is detailed. These chapters contain the approach, methodology, evaluation and implementation of the model. From Chapter 4 and on alternative applications of ANNs to a set of problems in business contexts are assessed. At the end of the dissertation, a conclusion and the discussion are presented.
     The research conducted in this dissertation created an algorithm to automatically engineer features that might increase the accuracy of deep neural networks for certain data sets.  The research built upon, but did not duplicate, prior published research by the author of this dissertation.  In 2008, the Encog Machine Learning Framework was created and includes advanced neural network and genetic programming algorithms (Heaton, 2015).  The Encog genetic programming framework introduced an innovative algorithm that allows dynamically generated constant nodes for tree-based genetic programming.  Thus, constants in Encog genetic programs can assume any value, rather than choosing from a fixed constant pool. 
     Research was performed that demonstrated the types of manually engineered features most likely to increase the accuracy of several machine learning models (Heaton, 2016).  The research presented here builds upon this earlier research by leveraging the Encog genetic programming framework as a key component of the dissertation algorithm that automatically engineered features for a feedforward neural network that might contain many layers.  This type of neural network is commonly referred to as a deep neural network (DNN).  Although it would be possible to perform this research with any customizable genetic programming framework or deep neural network framework, Encog is well suited for the task because it provides both components.  
     This dissertation report begins by introducing both neural networks and feature engineering.  The dissertation problem statement is defined, and a clear goal is established.  Building upon this goal, the relevance of this study is demonstrated and includes a discussion of the barriers and issues previously encountered by the scientific community.  A brief review of literature shows how this research continued previous investigations of deep learning.  In addition to the necessary resources and the methods, the research approach to achieve the dissertation goal is outlined. 
     Most machine learning models, such as neural networks, support vector machines (Smola & Vapnik, 1997), and tree-based models, accept a vector of input data and then output a prediction based on this input.  For these models, the inputs are called features, and the complete set of inputs is called a feature vector. Most business applications of neural networks must map the input neurons to columns in a database; these inputs allow the neural network to make a prediction.  For example, an insurance company might use columns for age, income, height, weight, high-density lipoprotein (HDL) cholesterol, low-density lipoprotein (LDL) cholesterol, and triglyceride level (TGL) to make suggestions about an insurance applicant (B. F. Brown, 1998).   
     Inputs such as HDL, LDL, and TGL are all named quantities.  This can be contrasted to high-dimensional inputs such as pixels, audio samples, and some time-series data.  For consistency, this dissertation report will refer to lower-dimensional data set features that have specific names as named features.  This dissertation focused upon such named features. High-dimensional inputs that do not assign specific meaning to individual features fell outside the scope of this research. 
     Classification and regression are the two most common applications of neural networks.  Regression networks predict a number, whereas classification networks assign a non-numeric class. For example, the maximum policy face amount is the maximum amount that the regression neural network suggests for an individual.  This is a dollar amount, such as $100,000.  Similarly, a classification neural network can suggest the non-numeric underwriting class for an individual, such as preferred, standard, substandard, or decline. Figure 1 shows both types of neural network. 
     The left neural network performs a regression and uses the six original input features to set the maximum policy face amount to issue an applicant.  The right neural network executes a classification and utilizes the same six input features to place the insured into an underwriting class.  The weights (shown as arrows) establish the final output.  A backpropagation algorithm fixes the weights through many sets of inputs that all have a known output. In this way, the neural network learns from existing data to predict future data. Furthermore, for simplicity, the above networks have a single hidden layer. Deep neural networks typically have more than two hidden layers between the input and output layers (Bengio, 2009).  Every layer except the output layer can also receive a bias neuron that always outputs a consistent value (commonly 1.0).  Bias neurons enhance the neural network＊s learning ability (B. Cheng & Titterington, 1994).  
     Output neurons provide the neural network＊s numeric result.  Before the output neurons can be determined, the values of previous neurons must be calculated.  The following equation can determine the value of each neuron: 
     The function phi (?) represents the transfer function, and it is typically either a rectified linear unit (ReLU) or one of the sigmoidal functions.  The vectors 牟 and x represent the weights and input; the variable b represents the bias weight.  Calculating the weighted sum of the input vector (x) is the same as taking the dot product of the two vectors.  This calculation is why neural networks are often considered to be part of a larger class of machine learning algorithms that are dot-product based.  
     For input neurons, the vector x comes directly from the data set.  Now that the input neuron values are known, the first hidden layer can be calculated, using the input neurons as x.  Each subsequent layer is calculated with the previous layer＊s output as x.  
     Feature engineering adds calculated features to the input vector (Guyon, Gunn, Nikravesh, & Zadeh, 2008). It is possible to use feature engineering for both classification and regression neural networks.  Engineered features are essentially calculated fields that are dependent on the other fields.  Calculated fields are common in business applications and can help human users understand the interaction of several fields in the original data set.  For example, insurance underwriters benefit from combining height and weight to calculate body mass index (BMI).  Likewise, insurance underwriters often use a ratio of the HDL, TGL and LDL cholesterol levels.  These calculations allow a single number to represent an aspect of the health of the applicant.  These calculations might also be useful to the neural network.  If the BMI and HDL/LDL ratios were engineered as features, the classification network would look like Figure 2. 
     In Figure 2, the BMI and HDL/LDL ratio values are appended to the feature vector along with the original input features. This calculation produces an augmented feature vector that is provided to the neural network.  These additional features might help the neural network to calculate the maximum face amount of a life insurance policy.  
Similarly, these two features could also augment the feature vector of a classification neural network.  BMI and the HDL/LDL ratio are typical of the types of features that might be engineered for a neural network.  Such features are often ratios, summations, and powers of other features.  Adding BMI and the HDL/LDL ratio is not complicated because these are well-known calculations.  Similar calculations might also benefit other data sets.  Feature engineering often involves combining original features with ratios, summations, differences, and power functions.  BMI is a type of engineered feature that involves multiple original features and is derived manually using intuition about the data set. 
     For this dissertation research, an algorithm was built that was capable of the automated creation of engineered features that could increase the accuracy of a deep neural network.  This algorithm was designed to be a metaheuristic search over all combinations of the original feature set and had the goal to return features from this search space that had the potential of increasing the accuracy of a deep neural network.  Because the search space is mathematical expressions, genetic programming was a natural choice of a metaheuristic.   
     This chapter includes the description of five experiments that provided background information for the design of this AFE algorithm.  These experiments provided insights into search space constraints, objective function design, and other areas important to the creation of a genetic programming solution.  The sixth experiment described at the end of the chapter provided a means for benchmarking the AFE algorithm. 
     The five experiments were conducted, and data from their results were compiled. These results helped to guide the creation of the final algorithm that automatically engineers features for deep neural networks.  This algorithm was developed and improved by rerunning and optimizing the neural network and genetic programming
     hyperparameters for the neural and genetic programming algorithms. The final set of hyperparameters that were used across all experiments is provided in Appendix B, ※Hyperparameters.§ The complete source code for all experiments and the final algorithm is available at the author＊s GitHub account.  For more information on obtaining the source code refer to Appendix H, ※Dissertation Source Code Availability.§ 
     Finally, the sixth experiment was conducted to demonstrate the degree to which the AFE algorithm increased the accuracy of deep neural networks.  This chapter begins with a summary of the results obtained from each of the five experiments.  The complete detail is provided in Appendix G, ※Detailed Experiment Results.§  Next, the final AFE algorithm is defined and the rational for its design explained. Finally, this chapter concludes with the results of Experiment 6, where the effectiveness of the AFE algorithm is demonstrated. 
Over the last several decades, researchers have been trying to understand and model the human brain. Lately, this is becoming not only a goal to pursue, but an issue that needs prioritizing, as the community has to deal with the dramatic rise of neurological disorders and above all cognitive impairment and early onset of Alzheimer＊s Disease (AD) ( Vialatte, Dauwels, Maurice, Musha & Cichocki, 2011). As a consequence,serious health questions have emerged as well as a need for reorganization of social care services (Pritchard, Mayers & Baldwin, 2013). In response, scientists from all over the word have focused their resources and efforts toward the understanding of the central nervous system (CNS) and especially the mechanisms involved in learning formation and synaptic plasticity, as neurological decline affects these function as a start. The efforts carried out and the improvement of the techniques used have resulted in a considerable amount of data now available. However, this has raised a new problem: 
how to find a suitable technique in order to understand the information available and make adequate use of it.
In this context, the scientific contribution in the area of Information Science (IS) and especially Neuroinformatics plays a pivotal role. They have been developing and offering new computational techniques that model the data available, thus emulating cognitive and learning functions of natural intelligent systems such as the human brain. On the one hand, many of the techniques already in use are not adequate, the data is heavily preprocessed at time and information cost; traditional artificial intelligence (AI) models lack biological plausibility and therefore they cannot represent the phenomena of study. On the other hand, new techniques developed appear to be expensive and made for scientific elite use only. On the contrary, they need to be affordable, understandable and accessible to a wider number of researchers. 
This thesis proposes a few biologically plausible methodologies for modelling and understanding brain data by means of computational models of neurological systems. For the first time, a spiking neural networks (SNN) approach is used to analyse, classify and extract knowledge from Spatio-Temporal Brain Data (STBD) such as Electroen-cephalography (EEG) data. 
Over the last six decades, numerous advances have been achieved by researchers using SNN machine learning approaches. Especially, Neuroinformatics experts have contributed to the advancement of the understanding of neurological disorders and the development of predictive system. Even so, many of the techniques that are still in use do not make proper use of the available data and lack realistic representation of the events.
Considering all the above discussed, the study aims can be summarised as follows:
? to explore the potential of an evolving spatio-temporal data machine (eSTDM) for developing new biologically plausible methodologies for modelling and understanding EEG data and the neurological events that generate it.
? By developing a new computational model of a neurological system, we aimed at revealing the information concealed in the data and learning from it in a new way, so that it can be analysed and localised in the brain area where it was originally generated.
? We aim at combining the EEG information with the eSTDM-based methodology proposed providing a more accessible and reliable brain-computer interface (BCI) system that can help detecting and understanding the course of a neurological event, such as cognitive impairment, which is one of the first symptoms related with the appearance of AD, considered the most alarming neuropathology of the century ( Hebert, Scherr, Bienias, Bennett & Evans, 2004; Wimo, Winblad, Aguero-Torres & von Strauss, 2003).
Dealing with the enormous amount of data available has been identified in the previous chapter as one of the major problems that motivates this study. In this chapter this problem will be analysed as follows: section 2.2 explains the limitations of STBD processing; section 2.3 reviews some of the most important types of STBD; section 2.4 focuses on EEG data, which has been identified as the most appropriate source of STBD to model for the purposes of this study; and section 2.5 provides a brief review of the main STBD processing approaches.
As discussed in the previous chapter, the vast amount of data in hand can proportionate great knowledge, when dealing with STBD; however, appropriate computational models are needed that are able to interpret and correctly represent this information. SNN techniques, named also third generation of brain-inspired neural network techniques ( Maass, 1997; Ghosh-Dastidar & Adeli, 2007) have emerged as the method of choice, as they are able to learn time, space, and frequency from the STBD. As a matter of fact, they have been used in several applications such as neuromorphic design and implementation ( Gerstner, 2001; Izhikevich, 2006; Furber & Temple, 2007; Gerstner et al., 2012), spatio-temporal pattern recognition ( Humble, Denham & Wennekers, 2012; Kasabov, 2012b), encoding continuous input data into spike trains ( Lichtsteiner & Delbruck, 2005; Chan, Liu & Van Schaik, 2007; Indiveri et al., 2011; Dhoble, Nuntalid, Indiveri & Kasabov, 2012), neurogenetic computation ( Benuskova & Kasabov, 2007; Kasabov, 2010), and neurocomputational studies of brain pathology ( Reggia, Ruppin & Glanzman, 1999). 
All SNN techniques developed are based on the emulation of some of the main principles that regulate information processing and plasticity in the CNS. Thus, in this chapter, first the functionality of SNNs will be assessed by introducing the main unit of information processing in the brain, the neural cell (section 3.1); then some of the most important computational models that have been derived from it will be discussed.
Chapter 3 discusses how neuroscientific notions have been used by IS researchers to develop new SNN techniques. It also explains why SNNs are valuable methods to mine STBD properly and to advance brain data modelling and improve our knowledge about synaptic and neural processes. In this chapter, Section 4.2 presents an eSTDM implemented in the Knowledge Engineering and Discovery Research Institute of the Auckland University of Technology by Prof.N.Kasabov and his group. It is named NeuCube. The NeuCube architecture is used in this research to develop novel methodologies for EEG data modelling, classification and interpretation, especially by gaining new knowledge from the neural functional pathways that generated the collected data. 
A paper about eSTDMs based on the NeuCube architecture ( Kasabov et al., 2015) has been submitted, peer-reviewed and accepted by Neural Networks Journal Special Issue: Neural Network Learning in Big Data.
As seen in the previous chapter, NeuCube is an SNN architecture in which both spatio and temporal neurogenetic brain data can be encoded according to the location of the stimulus and reflecting the timing of their spiking activity ( Kasabov, 2014a). NeuCube is capable of learning noisy data and it can be used not only for modelling and classification of STBD, but also for extracting knowledge from it.
In this chapter, a new NeuCube-based methodology is proposed in section 5.2 for the classification and analysis of EEG STBD and for the understanding of the brain processes that generate it. In section 5.3, the proposed methodology is applied on a small data set for a case study in neurorehabilitation.
This study was peer-reviewed and published by IEEE in the Proceedings of the 2014 International Joint Conference on Neural Networks (IJCNN). 
Chapter 5 proposes a new SNN NeuCube-based methodology for the classification and analysis of EEG STBD. The methodology is now applied on a bigger data set to model, recognise and understand complex cognitive EEG data recorded during mental tasks. Cognitive brain processes are difficult to analyse and understand with the use of a computational model, unless a technique is available that is able to deal with the data in a proper way. Many applications related to STBD still rely on traditional AI techniques; however, more efficient techniques are required, as cognitive tests and memory screening can provide powerful information that, if understood, can be used to stage Cognitive EEG Data and diagnose cognitive impairments such as AD ( Reggia et al., 1999). In fact, one of the first symptoms that follow the onset of the pathology is cognitive decline and memory loss. Early detection is important, as it allows for initial treatments. In this field, Neuroinformatics research can play a pivotal role. A major contribution that can be brought about by a much more efficient methodology based on machine learning techniques that can be used to evaluate the cognitive ability of a person with a deeper understanding of the underlying brain processes.
In this chapter, we first applie the NeuCube-based methodology on a benchmark data recorded during cognitive tasks (section 6.2). The results obtained are also compared to already published classification results on the same data set. In section 6.3, conclusions and future work are drawn. Section 6.4 summarises the chapter. This study resulted in a peer-reviewed paper publication, which appeared in Information Sciences Journal.
In this chapter, we challenge ourselves with the problem of finding functional changes in brain activity that forewarn of the onset and/or the progression of a neurodegenerative process that may result in a number of disorders, including AD. This particular problem
is of high importance for society, especially since the increase in human lifespan has been followed by the dramatic rise of cognitive impairments and neurodegenerative diseases. While AD has been the pathology affecting most victims in both developing and developed countries ( Pritchard et al., 2013), EEG has been for long used to analyse and stage AD decline from MCI, as reported by ( Labate, Foresta, Morabito, Palamara & Morabito, 2013; Morabito, Labate, Bramanti et al., 2013; Morabito et al., 2012). We believe that by classifying and analysing EEG data by means of the SNN NeuCube-based methodology proposed in Chapter 5, we can achieve a better understanding of the neurodegenerative process and develop a predictive system.
This chapter is constructed in the following way: Section 7.2 presents the case study and the data used for the analysis following three methodologies; Section 7.3 discusses the conclusion and future work; and the chapter is summarised in Section 7.4. The study presented in Section 7.2.3 was peer-reviewed and published in the Springer Series of Smart Innovation, Systems and Technologies for the 22nd Italian Workshop on Neural Networks (WIRN) 2014. 
In this chapter, by extending the NeuCube-based methodology proposed in Chapter 5, we introduce a new methodology for the understanding of functional changes in the brain. The methodology is applied on EEG data to analyse the connectivity and spiking activity evolved in the SNN module under different conditions and across different groups of subjects. Using standard statistical or machine learning techniques to classify EEG data from groups of patients under different treatment can suggest whether there is a functional improvement, as a result of treatment. As demonstrated in already published studies ( Wang, Wouldes, Kydd & Russell, 2012; Wang, Wouldes & Russell, 2013; Wang, Wouldes, Kydd, Jensen & Russell, 2014), cognitive functions improved in patients dependent on illicit opiates who are undergoing methadone maintenance treatment (MMT) in contrast to those dependent on illicit opiates that do not undergo any treatment. By comparing these two groups of patients with healthy controls, they also demonstrated that the MMT patients＊ cognitive functions are comparable to healthy subjects. In ( Wang et al., 2012, 2013, 2014), groups＊ difference was investigated by analysing spectral power of EEG data using traditional statistical methods (i.e. analysis of covariance, independent sample t-test, etc.) and the results revealed the fluctuation of neural activity within an individual channel only.
Traditional statistical and AI methods applied to the study of functional brain changes lack the ability to explain which functional areas of the brain are affected during treatment, how much they are affected and what are the changes of the dynamics of the brain as a chain of spiking activity over time. Time information is present in STBD such as EEG data, but when the data is processed (e.g. classified) this information is obscured and there is no explanation of the final results related to the time component.
In this respect traditional methods can be considered ※black boxes§ in a sense ( Kasabov, 2014a). These are important questions that need to be addressed for a better understand- ing of functional changes in the brain under various conditions, including: neurological
disease progression, disease treatment over time and ageing.
Methadone has been used as a pharmacological substitute for the treatment of opiate dependence since the mid-1960s. As a substitute for illicitly used opiates, thepurpose of MMT is not to achieve a drug-free state but to reduce the harm associated with illicit drug use and to improve life quality and psychosocial functioning for the individual ( Lobmaier, Gossop, Waal & Bramness, 2010). The benefits of MMT have been demonstrated by many studies. For example, MMT has been shown to effectively reduce the use of other drugs, injection-related risky behaviour, criminal activity, mortality, and the transmission of HIV and other blood-borne pathogens, such as hepatitis-B ( Ball, Lange, Myers & Friedman, 1988; Bell, Hall & Byth, 1992; Marsch, 1998; Gibson, Flynn & McCarthy, 1999). Consequently, MMT is now the most common treatment for opiate dependence in many countries, including the United States of America, Australia, the United Kingdom and New Zealand ( Adamson et al., 2012; Joseph, Stancliff & Langrod, 1999).
Despite methadone＊s effective clinical use, it remains uncertain whether MMT has negative effects on cognitive functions, given that methadone has clinically similar actions and analgesic effects to morphine ( Dole, 1988). Methadone primarily binds to 米 receptors that are found throughout the brain and are densely concentrated in the periaqueductal gray, amygdalae, hippocampus, thalamus and striatum (Martin, Hurley & Taber, 2007; McBride, Chernet, McKinzie, Lumeng & Li, 1998). In humans, these areas are critical for pain perception, visual and sensory processing, memory and attention. Therefore, there is particular concern whether long-term use of a sedative opiate
antagonist, such as methadone, has effects on cognitive function. In this chapter, these problematics are addressed by means of a new NeuCube-based methodology, which makes use of EEG data to identify differences between healthy subjects, patients with opiate addiction and those undertaking substitution treatment for opiate addiction. The chapter is constructed in the following way: section 8.2 presents the new NeuCube-based methodology for connectivity and spiking activity analysis. Section 8.3 presents a case study on EEG data. Changes in functional brain Chapter 8. NeuCube-based Methodology for the Understanding of Functional Changes in the Brain activity are analysed across groups of opiate addicts and those who have taken MMT in comparison with control subjects. This section also presents and discusses the results.
The conclusion in section 8.4 clearly identifies the NeuCube methodology as being advantageous for the purpose of revealing functional brain changes and the prediction of response to treatment.
In this chapter, we explore the potentiality of the NeuCube-based methodology proposed in Chapter 8 for personalised modelling and events prediction. The methodology is applied here on a case study on epilepsy data. Epilepsy is the most diffuse brain disorder that can affect people＊s lives even in its early stage. Many studies reported in the literature suggest that seizures are part of a complex network phenomenon, a sort of ※epileptogenic process§ that, for unknown reasons, arises, evolves, and finally results in a seizure. So far, no tool has been developed in order to monitor such mechanisms which, if discovered and explored, would allow for a much deeper understanding of the pathology that could lead to novel therapeutic perspectives. With this information in hand and by utilising the NeuCube-based methodology proposed, we aim at studying the development of a seizure. This could potentially lead to automatically marking the critical events on the EEG data to provide deep and novel diagnostic information about brain networks (off-line utility) and a warning tool in case of high probability of an impending seizure (on-line utility). The chapter is constructed in the following way: section 9.2 presents the case study problem and the available data, and reports the results obtained by using a small data set; section 9.3 draws the conclusions and proposes future work; a summary of the chapter is presented.
As emerged in Chapter 9, the new SNN NeuCube-based methodology proposed for connectivity and spiking activity analysis could be used, if extended, to study the effect of neuroreceptors＊ modifications on the learning ability of an SNN model. In this chapter, we propose a more biologically plausible system, which incorporates the dynamic mechanisms responsible for modifying neuronal synaptic plasticity through neurotransmitters/neuroreceptors cross-talk. To achieve that, we implement a new unsupervised learning algorithm based on local synaptic competition between biochemical processes. The chapter is constructed in the following way: section 10.2 introduces the proposed new learning rule and reports the results obtained by applying it on a small recurrent network of four spiking neurons; section 10.3 presents the conclusions; and in section 10.4 the chapter is summarised.
In the previous chapter, we presented a new biologically plausible unsupervised learning rule, called NRDP rule, that if combined with the proposed NeuCube-based methodologies can dynamically learn the spatio-temporal relationship from the data. The combined NeuCube-NRDP system allows for computational neurogenetic modelling and this is demonstrated here on a case study on cognitive EEG data. This chapter is constructed in the following way: section 11.2 explains how the proposed learning rule is applied to a case study on cognitive EEG data and presents some preliminary results; in section 11.3 conclusions are drawn and future work are planned; finally, section 11.4 summarises the chapter. 
The principle target of the understudy execution forecast framework is to decide understudies who might be normal do well in the Faculty of Science in Jagannath University. The nature of understudies selected into any university impacts the exploration and preparing level inside the university, and besides, overall affects the advances of the nation itself, as these understudies may turn into key players in the issues of the nation in a wide range of the economy. In Jagannath University, understudies get admitted to the Department of Computer Science and engineering in the Faculty of Science after they effectively passed the logical branch of the higher secondary school with no less than 70%, understudies are required to concentrate their first year without getting a noteworthy in engineering yet. Once the understudy completed the first year, he or she can major in either Computer and Communication Engineering or ICT. An understudy can major in one of the Engineering major in the event that he/she fulfil particular prerequisites, for example, Higher secondary school score, number of credits completed, pass a few subjects in the first year, for example, Math, Electrical Circuits, and Electronics. It ought to be noticed that this module of anticipating understudy execution will help in distinguishing which understudy perhaps will prevail with regards to examining Engineering programs. Henceforth this review adopts an Engineering strategy to make the procedure an understudy choosing Engineering real more powerful and effective. Particularly the review looks to investigate the likelihood of utilizing an Artificial Neural Network model to foresee the execution of an understudy before he/she begins his/her sophomore year in building considers. Actually one expects the execution of an understudy to be some sort of capacity with various variables (contentions) including his experience and knowledge. It is then again obvious that it will be quite troublesome finding a scientific model that may enough models this execution/elements relationship. Be that as it may one practical approach for foreseeing the execution of an understudy may be by concentrate his authentic information of past understudies experience and their related exhibitions. A practical approach to this type of problem is to apply common regression analysis in which historical data are best fitted to some function. 
The issue here is the multifaceted nature of choosing an appropriate capacity equipped of catching all types of information relationship and in addition consequently conforms yield if there should be an occurrence of additional data, on the grounds that the execution of a competitor is controlled by various variables, and this control/affiliation is not going to be any clear understood relapse demonstrate. An artificial neural network, which imitates the human brain in taking care of an issue, is a more typical approach that can handle this sort of issue. In this manner, the endeavor to set up a versatile framework, for example, Artificial Neural Network to anticipate the execution of an understudy in light of the result of these variables. 
Deep neural networks (DNNs) have shown a great success in various kinds of applications, such as image recognition [30, 47, 50], speech recogni- tion [16, 22], and natural language processing [6, 48]. One of notable image recognition competitions is ImageNet Large Scale Visual Recognition Challenges (ILSVRC), which includes a task for classifying images from ImageNet dataset [45] into 1,000 classes. Based on ILSVRC history in Figure 1.1, DNNs have shown impressive results in images classification from ImageNet dataset compare to non-DNNs technique, and even the performance exceeds human performance in the present.
The aims of this study are: 
?	To recognize some reasonable elements that influences an understudy execution, 
?	To change over these components into structures suitable for a versatile framework coding. 
?	To demonstrate an Artificial Neural Network that can be utilized to anticipate an understudy execution in view of some foreordained information for a given understudy. 
Convolutional Neural Networks (CNNs) is one of DNN variants that often applied in image recognition. There is a couple of representative CNN architectures from ILSVRC, such as VGG, GoogleNet, and AlexNet. VGG and GoogleNet [50] achieve around 90% accuracy for top-5 classification in ImageNet dataset; AlexNet [30] achieves around 80% top-5 accuracy with the same dataset.
Simultaneously, the number of mobile users are increasing, even now exceeding desktop users, as seen in Figure 1.2. Image recognition is needed in mobile phone, such as for recognizing users＊ interest from photos taken by mobile phone or just for entertainment, like Snapchat. Besides mobile phones, image recognition is also needed in drones, such as recognizing suspicious behavior in parking lot [3] and following user based on visual input [2]. Therefore,combining with CNNs notable performance, there is high demand on executing CNNs on those small devices.
However, there is problem in executing CNNs on devices. CNNs require high amounts of memory and computational cost. For example, AlexNet
requires roughly 240MB memory and more than 700 million float points operations for one inference. Hence, the hope for executing CNNs on devices that have limited resources will naturally require us to reduce the necessary memory and computational cost, which is the main theme of this thesis. The aim of compression is to approximate the full network with smaller amount of parameters without or with little accuracy drop. To reach the aim, the followings are the contributions of this work. 
? Overcoming CP instability. Based on previous works [28, 31], the all layers compression by CP decomposition had not succeeded because of the CP instability. Instability makes the accuracy hard to restored. We propose to use iterative fine-tuning to overcome this instability, hence be able to compress all layers.
? Approximating tensor rank with sensitivity. We define sensitivity of each layer as the effect of each layer decomposition, which can be measured by accuracy loss for each decomposition. The layer that is more sensitive, i.e. has more accuracy loss, needs higher rank.
? Achieving low-rank approximation with greedy optimization. Allen (2012) [1] shows that greedy approach can explain more variance in
lower rank compare to non-greedy approach, as in the greedy approach, the first several components will tend to explain most of the variance. Therefore, the greedy approach is expected to be more robust in lower rank.
These are notations that we use in this work. Tensors will be notated in calligraphy font capital letters, e.g. X . Matrices will be notated in bold capital letters, e.g. X. Vectors will be notated in bold small letters, e.g. x. Scalars will be notated in italic font small letters, e.g. x. Italic font capital letters, e.g. X, will be used for dimension size.
Selecting a rank for tensor decomposition has a great influence on the overall compression rate of a network and its performance: if it is too low, the accuracy would be hardly recovered by fine-tuning; if it is too high, the compression rate would be not maximized. Unfortunately, however, no such algorithm exists that can find the optimal tensor rank [29]. In fact, determining the optimal rank is NP-hard [21].
Neural networks have been popular in the machine learning community since the 1980s with repeating rises and falls of popularity. Their main benefit is their ability to learn complex, non-linear hypotheses from data without the need of modeling complex features. This makes them of particular interest for computer vision, in which feature description is a long-standing and largely non-understood topic.
Neural networks are difficult to train and for the last ten years they have come to enormous fame under the topic §deep learning§. New advances in training methods and the movement of training from CPUs to GPUs allow to train more reliable models much faster. Deep neural networks are not a silver bullet, as training is still heavily based on model selection and experimentation. Overall, significant progress in machine learning and pattern recognition has been made in natural language processing, computer vision and audio processing. Leading IT companies have made significant investments into deep learning for these reasons, such as Baidu, Google, Facebook and Microsoft.
Concretely, previous work of the author on deep learning for facial expression recognition in [12] resulted in a deep neural network model that significantly outperformed the best contribution to the 2013 Kaggle facial expression competition [25]. Therefore, a further investigation on the recognition of action units and in particular smile using deep neural networks and convolutional neural networks seems desirable. Only very few works on this topic have been reported so far, such as in [16]. It would also be interesting to compare the input of the entire face versus the mouth to study differences in the performance of deep convolutional models.
Supervised learning (SL) is the task of automatically inferring a mathematical function,startingfromafinitesetofexamples[ 66 ]. Together with unsupervised learning and reinforcement learning, it is one of the three main subfields of machine learning ML).Its roots as ascientific discipline can be traced back to the introduction of the first fully SL algorithms,namely the perceptron in 1957 [ 141 ],and the k-nearest neighbors ( k -NN) in 1967 [ 37 ]. The perceptron, in particular, became the basis for a wider family of models, which are known today as artificial neural networks (ANNs). ANNs model the unknown desired relation using the interconnection of several building blocks, denoted as artificial neurons, which are loosely inspired to the biological neuron. Over the last decades, hundreds of variants of ANNs, and associated learning algorithms, have been proposed. Their development was sparked by a few fundamental innovations, including the Widrow-Hoff algorithm in 1960 [ 193 ], the popularization of the back-propagation (BP) rule in 1986 [ 142 ] (and its later extension for dynamical systems [ 190 ]), the support vector machine (SVM) in 1992 [ 16 ], and additional recent developments on ＆deep＊ architectures from 2006 onwards [157].
As a fundamentally data-driven technology, SL has been changed greatly by the impact of the so-called ＆big data＊ revolution [ 196 ]. Big data is a general terminology, which is used to refer to any application where data cannot be processed using ＆conventional＊ means. As such, big data is not defined axiomatically, but only through its possible characteristics. These include, among others, its volume and speed of arrival. Each of these aspects has influenced SL theory and algorithms [ 196 ], although in many cases solutions were developed prior to the emergence of the big data paradigm itself. As an example, handling large volumes of data is known in the SL community as the large-scale learning problem [ 17 ]. This has brought forth multiple developments in parallel solutions for training SL
models [59], particularly with the use of commodity computing frameworks such as MapReduce [ 35 ]. Similarly, learning with continuously arriving streaming data is at the center of the subfield of online SL [205].
In this thesis, we focus on another characteristic of several real-world big data applications, namely, their distributed nature [ 196 ]. In fact, an ever-increasing number of authors is starting to consider this last aspect as a defining property of big data in many real-world scenarios, which complements the more standard characteristics (e.g. volume, velocity, etc.). As an example, Wu et al. [ 196 ] state that ※autonomous data sources with distributed and decentralized controls are a main characteristic of Big Data applications§. In particular, we consider the case where training data is distributed among a network of interconnected agents, a setting denoted as distributed learning (DL). If we assume that the agents can communicate with one (or more) coordinating nodes, then it is possible to apply a number of parallel SL algorithm, such as those described above. In this thesis, however, we focus on a more general setting, in which nodes can communicate exclusively with a set of neighboring agents, but none of them is allowed to coordinate in any way the training process. This is a rather general formalization, which subsume multiple applicative domains, including learning on wireless sensor networks (WSNs) [ 9 , 128 ], peer-to-peer (P2P) networks [ 40 ], robotic swarms, smart grids, distributed databases [86], and several others. More specifically, we develop distributed SL algorithms for multiple classes of ANN models. We consider first the standard SL setting, and then multiple extensions of it, including online learning [ 205 ], semi-supervised learning (SSL) [ 29 ], and learning with time-varying signals [ 183 ]. Due to the generality of our setting, we assume that computational constraints may be present at each agent, such as a sensor in a WSN. Thus, we focus mostly on relatively simple classes of ANNs, where both training and prediction can be performed with possibly low computational capabilities. In particular, in the first and last part of this thesis, we will be concerned with two-layered ANNs, where the weights of the first layer are stochastically assigned from a predefined probability  istribution. These includes random vector functional-link (RVFLs) [ 76 , 120 ], and echo state networks (ESNs) [ 103 ]. Despite this simplification, these models are capable of high accuracies in most real-world settings. Additional motivations and an historical  erspective on this are provided in the respective chapters. The rest of the thesis deals with linear SVMs, kernel ridge regression, and single neurons with flexible activation functions.
Another important point to note is that the algorithms developed here do not require the exchange of examples between the nodes, but only of a finite subset of parameters of the ANN itself (and a few auxiliary variables). 2 Thus, they are able to scale easily to large, and possibly time-varying, networks, while keeping a fixed communication overhead. Constraining the exchange of data points is a reasonable assumption in big data scenarios, where datasets are generally large. However, it might be desirable even in other contexts, e.g.  henever privacy concerns are present [ 185 ]. A prototypical example in this case is that of distributed medical databases, where sensible information on each patient must go through strict controls on its diffusion.
As we said in Section 2.2.2, the use of these models is widespread in the centralized case, due to their good trade-off of algorithmic simplicity and nonlinear modeling capabilities. At the same time, as detailed in Section 3.4.4, their use in the DL setting has been relatively limited, which is the main motivation for this chapter. After introducing the RVFL network, we describe the two distributed strategies, based on the DAC protocol and the ADMM optimization algorithm. Next, we evaluate them on multiple real-world scenarios.
As we saw in the previous chapters, many centralized SL algorithms have been extended successfully to the distributed setting. However, many crucial sub-areas of machine learning remain to be extended to the fully distributed scenario. Among these, the DL setting could benefit strongly from the availability of distributed protocols for semi-supervised learning (SSL) [ 29 ]. In SSL, it is assumed that the labeled training data is supplemented by some additional unlabeled data, which has to be suitably exploited in order to improve the test accuracy. State-of-the-art research on SSL is concerned on the single-agent (centralized) case, e.g. with the use of manifold  regularization (MR) [ 11 , 109 ], transductive learning [ 28 ], and several others. To the best of our knowledge, the case of SSL over multiple agents has been addressed only in very specific settings, such as localization over WSNs [ 32 ], while no algorithm is available for the general case. However, we argue that such an algorithm would be well suited for a wide range of applications. As an example, consider the case of medical diagnosis, with labeled and unlabeled data distributed over multiple clinical databases. Distributed music classification (which we considered in Chapter 5), and so on. In all of them, labeled data at every agent is costly to obtain, while unlabeled data is plentiful. The overall setting is summarized in Fig. 7.1, where each agent in a network receives two training datasets, one composed of labeled patterns and one composed of unlabeled patterns.
In this chapter, we propose the first fully distributed algorithm for SSL over networks, satisfying the above requirements. In particular, we extend an algorithm belonging to the MR family, namely laplacian kernel ridge regression (LapKRR) [ 11 ]. MR algorithms, originated in the seminal works of [ 10 ] and [ 11 ], are based on the assumption that data often lie in a low-dimensional manifold M  mbedded in the higher-dimensional input space. When the structure of the manifold is unknown, it can be approximated well by a weighted graph where the vertexes are represented by the data points and the weights of the edges represent a measure of similarity between the points. In the MR framework, the classification function is obtained by solving an extension of the classical regularized optimization problem, with an additional regularization term, which incorporates information about the function＊s smoothness on the manifold.
The algorithm presented in this chapter starts from the observation that, in the MR optimization problem, information is mostly encoded in a matrix D of pairwise distances between patterns. In fact, both the additional regularization term, and the kernel matrix (for any translation-invariant kernel function) can be computed using the information about the distance between points. In the distributed  etting, each agent can compute this matrix relatively only to its own training data, while information about the distance between points belonging to different agents are unknown. Obtaining this information would allow a very simple protocol for solving the overall optimization problem. As a consequence, we subdivide the training algorithm in two steps: a distributed protocol for computing D , followed by a distributed strategy for solving the optimization problem. For the former step, in the initial phase of the algorithm, we allow a small exchange of data patterns between agents. In this phase, privacy can be preserved with the inclusion of any privacy-preserving protocol for the computation of distances [ 185 ]. For completeness, we describe the strategies that are used in our experiments in Section 7.2.3. As a second step, we recover the rest of the global distance matrix D by building on previous works on Euclidean distance matrix (EDM) completion [ 22 , 111 ]. To this end, we consider two strategies. The first one is a simple modification of the state-of-the-art algorithm presented in [ 91 , 93 ], which is based on a column-wise partitioning of D overtheagents. In this chapter, we modify it to take into account the specific nature of Euclidean distance matrices, by the incorporation of non-negativity and symmetry constraints. As a second strategy, we propose a novel algorithm for EDM completion, which is inspired to the framework of diffusion adaptation (DA) (see Section 3.4.2). The algorithm works by interleaving gradient descent steps with local interpolation of a suitable low-rank factorization of D . While the first algorithm has a lower computational cost, we found that this comes at the cost of a worse performance, particularly when the sampling set of the matrix to complete is small. On the opposite, our algorithm exploits the particular structure of EDMs, at the cost of a possibly greater computational demanding. We discuss in more detail the advantages and disadvantages of the two approaches in Section 7.3 and in the experimental section. As we stated before, once the matrix D is known, solving the rest of the optimization problem is trivial. In this chapter we focus on the LapKRR algorithm, and we show that its distributed version can be solved using a single operation of sum over the network. Our experimental results show that, in most cases, the
performance of the novel diffusion adaptation-based algorithm for distributed theoretical tools upon which our algorithm is based. In particular, we detail the problem of SSL in the framework of MR in Section 7.2.1, some notions of EDM completion in Section 7.2.2, and two strategies for privacy-preserving similarity computation in Section7.2.3. InSection 7.3 we proposeour algorithm to completean EDM in a decentralized fashion. Then, Section 7.4 details the proposed framework for distributed LapKRR. In Section 7.5 we present the results for both the distributed EDM completion and distributed LapKRR. EDM completion overcome those of the state-of-the-art column-wise  artitioning strategy. Secondly, experiments show that the distributed LapKRR is competitive with a centralized LapKRR model trained on the overall dataset. 
I
n the previous chapter, we have explored the problem of training a semi-supervised Laplacian KRR using a distributed computation of the underlying kernel matrix. However, despite its good performance, the resulting algorithm requires a large amount of computational and/or communication resources, which might not be available on specific devices or communication channels. To this end, in this chapter we propose two simpler algorithms for a different family of semi-supervised SVM, denoted as S 3 VM. The S 3 VM has attracted a large amount of attention over the last decades [ 28 ]. It is based on the idea of minimizing the training error and maximizing the margin over both labeled and unlabeled data, whose labels are included as additional variables in the optimization problem. Since its first practical implementation in [ 82 ], numerous researchers have proposed alternative solutions for solving the resulting mixed integer optimization problem,
including branch and bound algorithms [ 30 ], convex relaxations, convex-concave procedures [ 28 ], and others. It has been applied in a wide variety of practical problems, such as text inference [ 82 ], and it has given birth to numerous other algorithms, including semi-supervised least-square SVMs [ 1 ], and semi-supervised random vector functional-link networks [152]. In order to simplify our derivation, in this chapter we focus on the linear S 3 VM formulation, whose decision boundary corresponds to an hyperplane in the input space. Due to this, the algorithms presented in this chapter can be implemented even on agents with stringent requirements in term of power, such as sensors in a WSN. At the same time, it is known that limiting ourselves to a linear decision boundary can be reasonable, as the linear S 3 VM can perform well in a large range of settings, due to the scarcity of labeled data [28].
Specifically, starting from the smooth approximation to the original S 3 VM presented in [ 31 ], we show that the distributed training problem can be formulated as the joint minimization of a sum of non-convex cost functions. This is a complex problem, which has been investigated only very recently in the distributed optimization literature [ 15 , 44 ]. In our case, we build on two different solutions. The first one is based on the idea of diffusion gradient descent (DGD), similarly to the previous chapter. Nevertheless, since it is a gradient-based algorithm exploiting only first order information of the objective function, it generally suffers of slow practical convergence speed, especially in the case of non-convex and large-scale optimization problems. Recently, it was showed in [ 44 , 161 ] that exploiting the structure of nonconvex functions by replacing their linearization (i.e., their gradient) 
with a ※better§ approximant can enhance practical convergence speed. Thus, we propose a distributed algorithm based on the recently proposed In-Network Successive Convex Approximation (NEXT) framework [ 44 ]. The method hinges on successive convex approximation techniques while leveraging dynamic consensus as a mechanism to distribute the computation among the agents as well as diffuse the needed information over the network. Both algorithms are proved convergent to a stationary point of the optimization problem. Moreover, as shown in our experimental results, the NEXT exhibits a faster practical convergence speed with respect to DGD, which is paid by a larger  computation cost per iteration. The rest of the chapter is structured as follows. In Section 8.2 we introduce the S 3 VM model together with the approximation presented in [ 31 ]. In Section 8.3, we first formulate the distributed training problem for S 3 VMs, and subsequently we derive our two proposed solutions. Finally, Section 8.4 details an extensive set of experimental results.
In the previous part of this thesis, we considered static classification and regression tasks, where the order of presentation of the different examples does not matter. In many real world applications, however, the patterns exhibit a temporal dependence among them, as in time-series prediction. In this case, it is necessary to include some form of memory of the previously observed patterns in the ANN models. In this respect, there are two main possibilities. The first is to include an external memory, by feeding as input a buffer of the last K patterns, with K chosen a priori. Differently, it is possible to consider recurrent connections inside the ANN, which effectively create an internal memory of the previous state, making the ANN a dynamic model. This last class of ANNs are called recurrent neural networks (RNNs).
In the DL setting, the former option has been investigated extensively, particularly using linear and kernel adaptive filters (see Section 3.4.2 and Section 3.4.5). The latter option, however, has received considerably less attention. In fact, despite numerous recent advances (e.g. [ 68 ]), RNN training remains a daunting task even in the centralized case, mostly due to the well-known problems of the exploding and vanishing gradients [ 124 ]. A decentralized training algorithm for RNNs, however, would be an invaluable tool in multiple large-scale real world applications, including time-series prediction on WSNs [ 129 ], and multimedia classification over P2P networks.
In this chapter we aim to bridge (partially) this gap, by proposing a distributed training algorithm for a recurrent extension of the RVFL, the ESN. ESNs were introduced by H. Jaeger [ 77 ]. Together with liquid state machines and back propagation decorrelation, they form the family of RNNs known as reservoir computing [ 103 ].The main idea of ESNs, similar to RVFLs, is to separate the recurrent part of the network (the so-called ＆reservoir＊), from the non-recurrent part (the ＆readout＊). The reservoir is typically fixed in advance, by randomly assigning its connections, and the learning problem is reduced to a standard linear regression over the weights of the readout. Due to this, ESNs do not required complex back-propagation algorithms over the recurrent portion of the network, thus avoiding the problems of the exploding and vanishing gradients. Over the last years, ESNs have been applied successfully to a wide range of domains, including chaotic time-series prediction [ 79 , 89 ], grammatical inference [ 178 ], and acoustic modeling [ 181 ], between others.
While several researchers have investigated the possibility of spatially distributing the reservoir [ 117 , 166 , 184 ], to the best of our knowledge, no algorithm has been proposed to train an ESN in the DL setting. The remaining of the chapter is formulated as follows. In Section 9.2 we introduce the basic concepts on ESNs and a least-square criterion for training them. Section 9.3 details a distributed algorithm for ESNs, extending the ADMM-RVFL presented.
T
his chapter continues the investigation on distributed training algorithms for time-varying data. Particularly, we focus on models with external memory (i.e., with a buffer of the last input elements), adequate to devices with extremely low computation resources. Available approaches in this sense include the linear diffusion filters (see Section 3.4.2), and kernel-based distributed filters (see Section 3.4.5). However, the former applicability is limited to scenarios where the assumption of a linear model between the output and the observed variables is meaningful. Kernel methods, instead, are hindered by the fact that a kernel model depends by definition on the full observed dataset.
In this chapter, we propose a novel nonlinear distributed filtering algorithm based on the recently proposed spline adaptive filter (SAF) [ 154 ]. Specifically, we focus on the Wiener SAF filter [ 154 ], where a linear filter is followed by an adaptive nonlinear transformation, obtained with spline interpolation. They are attractive nonlinear filters for two main reasons. First, the nonlinear part is linear-in-the-parameters (LIP), allowing for the possibility of adapting both parts of the filter using standard linear filtering techniques. Secondly, while the spline can be defined by a potentially large number of parameters, only a small subset of them must be considered and adapted at each time step (4 in our experiments). Due to this, they allow to approximate non-trivial nonlinear functions with a small increase in computational complexity with respect to linear filters. Based on the general theory of DA, 1 in this chapter we propose a diffused version of the SAF filter, denoted as D-SAF. In particular, we show that a cooperative behavior can be implemented by considering two subsequent diffusion operations, on the linear and non-linear components of the SAF respectively. Due to this, the D-SAF inherits the aforementioned characteristics of the centralized SAF, namely, it enables the agents to collectively converge to a non-linear function, with a small overhead with respect to a purely linear diffusion filter. In fact, D-LMS can be shown to be a special case of D-SAF, where adaptation is restricted to the linear part only. To demonstrate the merits of the proposed D-SAF, we perform an extensive set of experiments, considering medium and large-sized networks, coupled with mild and strong non-linearities. Simulations show that the D-SAF is able to efficiently learn the underlying model, and strongly outperform D-LMS and a purely non-cooperative SAF.
The rest of the chapter is organized as follows. Section 10.2 introduces the basic framework of spline interpolation and SAFs. Section 10.3 formulates the D-SAF algorithm.
This diploma thesis deals with NEAT (NeuroEvolution Of Augmenting Topologies) - it is a state of the art evolutionary system for optimizing topology and parameters of recurrent neural networks. This area of research attracts considerable interest at present. In order to experiment with the algorithm, system is implemented in Java programming language. Then experiments to reproduce previously published results are performed. Work continues with the proposal of a modified system DC NEAT which is then compared to original NEAT. 2-D and 3-D visualizations of recurrent neural networks are proposed and implemented. The first part of this work contains the theoretical background needed to understand my further work. Chapter 2 deals with evolutionary algorithms, chapter 3 describes artificial neural networks. Then in chapter 4 the combination of both is discussed. Chapter 5 describes the NEAT algorithm. The latter includes some basic information, obtainable from related articles and books along with unpublished details acquired from source codes and general experience with NEAT.
Prediction of the natural gas price is imperative to producers, suppliers, traders, market makers, and bankers involved in the natural gas exploration, production, transportation and trading as well as consumers involved in the utilization of the natural gas (Mishra, 2012). The price of energy commodity, including natural gas price, is dramatically volatile that encounters the parties to a high risk and uncertain situations. More accurate forecasting helps them to select an appropriate strategy in order to reduce the uncertainty by means of hedging the risks. This research attempts to find the best way to model and forecast the natural gas prices among different approaches, and choose the optimum practice.
The natural gas price has also considerable effect on the evaluation of gas reserves (Caldwell & Heather, 1996), which is a major part of Gas Company＊s assets. Fluctuation in gas prices disturb the company＊s value and increase the investment risk which result in reduction of the company＊s stock price. Determination of the factors which control the natural gas market and price, as well as spread of  nformation between the all parties in the market, stabilize the firm value and reduce the volatility. Comparing to the other fossil fuels like coal or petroleum products, natural gas is less pollutant and more environment friendly. Since natural gas is the cleanest and the most abundant fossil fuel compared to other fossil resources in the world, by improving transportation technology and decreasing the handling costs, it is becoming the most popular source of energy globally (Conti, et al., 2015).
Agriculture is the cultivation of products to feed the population and it was a key development of human civilization. According to the UN Food and Agriculture Organization [3] the world will need to produce 70% more food in 2050 than it did in 2006 in order to feed the growing population. Moreover, today demand for efficient agricultural products is increasing. [4]. In order to improve agriculture processes, we can acquire field data with sensors, make data analytics, perform analysis and take appropriate decisions and actions. Collecting big data from the field gives us a clearer understanding of product variability and quality of products[5]. As indicated by [6], the effect of "Precision farming" [7] advances on rural creation is normal for the most part in two territories: productivity for the makers and biological and ecological advantages to general society. In agriculture, physical parameters such as temperature, relative humidity and soil moisture [8] are important. There are several applications and well established measuring instruments to collect these data[9]. In addition, sensors to measure soil properties [10], detect and monitor foliar disease or fertilizer management[11] are already exist.
Data acquisition systems in agriculture need to cover large areas, collect representative samples and exchange measured information and control commands.
To satisfy the need of population, farmers and agriculture companies are turning to the Internet of Things (IoT). The IoT is pushing the future of farming to the next level. Smart agriculture is becoming more commonplace among farmers, and high tech farming is quickly becoming the standard thanks to agricultural sensors [12].
Usage of IOT applications in agriculture domain proposed by other authors is analyzed in the following references. Authors of [13] have shown farm management system architecture which uses IoT features. Following architecture gives easy access to acquired data and devices. The authors of [14] presented the use cases of Cloud Computing in the agriculture area. Moreover, they described this in the context of service providers and supply chain for cost-effective services for farmers. In [3] authors have described the controlled architecture of smart agriculture based on IoT and Cloud Computing. Another issue comes with Wireless Sensor
Networks which has to be connected all sensor nodes in a smart way. Authors of [15] proposed wireless sensor network for precision agriculture where real-time data of the climatological and other environmental properties are sensed and relayed to a central repository. Moreover, they proposed Wireless Mesh Network architecture which is convenient for agriculture applications. The use and evolution of WSNs within the wider context of IoT, and a review of WSN applications, infrastructure technologies, applications and standards featured in WSN designs are discussed in [16].
Despite the fact that there are many types of research, proposed architectures and WSN applications in agriculture domain, still, main problems rely on allocation sensors in the long-range area and thereby establishing reliable data communications. A robust and reliable data acquisition system enables synchronizing, exchange and storing of measured data. This kind of system is required to efficiently evaluate sensor signals and allow real-time or a posterior analysis of the behavior of single parameters and their mutual impact [17].
The aim of this thesis work is to propose a practical approach of WSN and predictive model for smart agriculture sector which is different than above-mentioned technologies. The structure of the thesis is designed in the following way: In section 2 enabling tools and solutions for IoT system are described.
Section 3 describes new designed WSN for short range applications using Bluetooth Low Energy(BLE). BLESensor node＊s hardware, firmware flowchart and Android application are represented and described. Moreover, we proposed tools and methods to measure battery life of the sensor node.
In section 4 we described our IoT system that can monitor ambient temperature, humidity and soil moisture of plant. Detailed information about a prototype device (hardware and firmware algorithm) and Android application which acquires physical data to the cloud through WiFi protocol are described. Further, collected data could be used to train a predictive model.
Section 5 describes maximum and minimum temperature predictive model using ANN and results of three non-linear models such as NNARX, NNOE and NNARMAX. We applied selected model for 10-days step ahead forecasting maximum and minimum temperature and described results.
Artificial neural networks are becoming a significant method for prediction and forecasting in almost every field. Utilizing time series approach for forecasting problem, forecasters actually collect and analyze the historical observations to create a model to capture the underlying behavior of the data. In meteorology, the weather forecasting is a typical unbiased time series forecasting problem. The real-life processes are mostly non-linear which brings deficiency in the traditional mathematical model. The meteorological data is very irregular and it follows nonlinear Trend. Application of neural network in time series forecasting is based on the ability of the neural network to approximate non-linear functions. ANN is a mathematical model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN and it is generally based on three layers: an input layer, hidden and the output layer. Each layer contains nodes or neurons. Input layer consists of input nodes and output layer has as many nodes as the output. The middle layer or hidden layer contains an arbitrary number of nodes which is chosen after some trial and error initially. Afterwards, the network must be trained and when it starts to give
5.2 Multilayer Perceptron
reasonably accurate results, we can use the same network to predict unseen data. This short review discusses results of time series forecasting with non-linear models. In [80] NARX network architecture is applied to long term(multi-step-ahead) time series prediction. They successfully used its output feedback loop to improve its predictive performance in complex time series predicting tasks. Authors of [81] proposed a pruning-based algorithm to determine the embedded memory order of NARX recurrent neural networks and degenerate forms such as NSAR networks. This algorithm can also incorporate several useful heuristics, e.g., weight decay, which have been used extensively in static networks to optimize the non-linear function. In one-step ahead prediction, ANN models calculate the next sample value of a time series without feeding back it to the model＊s input regressor. Particularly, when the input regressor contains only actual sample points of the time series. If the user is interested in a longer prediction, the model＊s output should closed loop to the input regressor for a fixed but finite number of time step[82]. Next sections describe multilayer perceptron, models of dynamic systems, the procedure of system identifications and selection of best prediction model structure. Afterwards, we developed 10-step-ahead prediction model for maximum and minimum temperature and discussed results.
The stock market is where investors can officially gamble on the prices of stocks to get some type of profit or sometimes can fail to the plummeting influx of the volatile market [1]. It offers investors the opportunity to earn more income if they learn how to run smart in this game of stock market prediction. It is definitely a favorite field of analysis in financial data-mining. The aim of prediction study has been largely beyond the ability of traditional study which includes mainly centered on developing intelligent systems that are likely to emulate human intelligence. Currency markets are highly volatile, which is unarguably very hard to predict effectively predicated on certain variables. The stock market is highly volatile and is also very hard to forecast effectively predicated on certain parameters unarguably. 
The Iraqi stock market is the main engine of economic growth as the economic development is closely linked to the existence of a thriving securities market and sophisticated. On the other hand, led boom and the increase of such securities and diversity as well as attractive to the public to increase to deal with the market for such securities issuance and traded, supported by the encouragement and support of the custodians in terms of providing laws march and exercise control in order to ensure that the rights of dealers and required to do their duties It specializes these transactions medium and long term and offer came in which of several categories of owners fiscal surplus of savers who want to invest their money for a long time the demand comes from the owners of the fiscal deficit wishing to invest these funds in long-term projects. That the securities by trading in this market loans through bonds or equity shares through or other movable securities. These markets have seen in developed countries a significant development in technologies and regulations while still taking its first steps in the Arab world along the lines of those that made an important step but sought to develop its financial markets try to catch up with advanced countries. 
        The subject of time-series and entered the broad areas in our life and in particular economic areas, specifically financial ones under the financial time series title since the last century, a rapid development in the field of the stock exchange, and here it seemed interested in the study of a financial time series and which is characterized by a kind of instability or uncertainty that there are no time periods of volatility followed by periods of relative calm, which makes them experiencing severe fluctuations and turns fail and succeed in analysis and interpretation. And this, in turn, requires the use of analytical models can formulate these fluctuations, mathematical models that allow for future planning, where it is known that most of the financial markets. Markets and foreign exchange, local and even some economic variables (inflation and stock prices) characterized by volatility and feature of this property it means to get large fluctuations beyond uncommon, for example, in the stock price or the number of shares traded and of course that these fluctuations are Undesirable by investors or even decision-makers and politicians, because this creates a kind of uncertainty in the financial and economic transactions that may happen as a result of windfall profits or losses. Financial markets of the most important factors of the economy of any country in the world, but in order to address  financial crises that occur in the market has to be the use of statistical models take into account fluctuations that occur during the trading periods, and trying to explain these fluctuations and these models a Simple Moving Average Index (SMA), Moving Average Convergence Divergence (MACD) and The Relative Strength Index (RSI) that we are going to be studied in detail and analysis through the data application of the stock market which takes into consideration the daily fluctuations in the index over time, In order to build an appropriate model for the index of the market process has been smooth data collection time were analyzed .The test this time series was a better model for the representation of this data by relying on several Criteria of informational for comparison between a model. 
 
          Stock prediction uses widely one of the very most techniques that are called Artificial Neural Networks (ANN). We have attempted to consider some vital input variables, which were neglected by a lot of the systems. Utilizing the backpropagation algorithm to train the network by error correction and modifying the weights predicated on these corrections. ANN has the capability for arbitrary nonlinear function approximation and information handling other methods do not have. Artificial Neural Networks (ANN) are well put on problems were reproducing the relationships among data is absolutely difficult so long as. On the other hand, there are a huge enough training data sets. 
 
        Another study [31] has used fuzzy logic techniques to solve the same problem, in spite of their good results; they still have a wide margin of uncertainty in their predictor. In our study we propose to use an Artificial Neural Networks (ANN) based solution to combine three indexes of closing prices, SDMA, MACD, RSI, and the existing closing prices into the following predicted day close price. The artificial neural network (ANN) model utilizes these indexes as input parameters to find the guidance, which describes the change of indices by the dependence of the change of close price utilizing the training data set.  
※You shall know a word by the company it keeps§ [ 16 ]. In line with this idea, the distributional hypothesis states that words that occur in similar contexts tend to have similar meanings [ 17 ]. Assuming this, several models have been developed that exploit word co-occurrence to find word representations. Among them, word embeddings are distributed vector representations, which are dense, low-dimensional, real-valued and can capture latent features of the word [ 45 ]. All the dimensions of the vector participate in the representation of an specific word, but also each dimension participates in the representation of all words of the defined vocabulary. Interestingly, the latent features encode syntactical and semantical information so that some natural language processing (NLP) tasks can be solved by simple linear vector operations due to the distributed nature of the word representations. For example, we can answer analogy questions just with additions and subtractions: if we subtract from a vector representing the word ※Madrid§ the vector corresponding to ※Spain§ and we add the vector ※France§, the resulting vector should be very close to the vector ※Paris§ [ 33 ]. These vector word representations have also the advantage that they can be trained efficiently by simple (shallow) neural network architectures such as the continuous skip-gram model [31].
Word embeddings have been very successful in various NLP tasks such as machine translation [ 23 , 32 ], semantic and syntactic language modeling [ 33 ]; and named entity recognition [ 37 ] among many others. Some researchers even explicitly recommend to switch from more traditional models that rely on counting co-occurrence to word embedding models, which are set to predict word contexts [8].
Despite the high expressivity of vector representations, mapping a word to a point in a vector space has two main limitations: the lack of an uncertainty measure about the word embedding and the impossibility of asymmetric comparisons [ 46 ].
The latter is specially relevant to perform meaning entailment. For example, since Bach was a composer (and thus also a man), we would expect that the words ※Bach§, ※composer§ and ※man§ are close to each other in the vector space. However, we  cannot entail neither ※composer§ from ※Bach§ nor ※man§ from ※composer§ just by comparing their respective vectors with distance measures since any well defined distance (like the Euclidean distance or the cosine distance) has to be symmetric.
The model proposed by Vilnis and McCallum [ 46 ] overcomes these limitations by using Gaussian distributions. This way, we can imagine words not just as points in the space but as ellipsoids whose center is the mean of the respective Gaussian distribution. Recalling the previous example, the embedding for ※man§ should comprehend the embedding of ※composer§, since the latter is a specific case of the former.
Although Gaussian embeddings have some advantages such as the already mentioned, they involve a cost in model complexity which, at the end, may involve not finding good word representations, as we detail later in this text. In fact, if we consider only the tasks that word vectors already perform, they do not show any significant improvement. The main aim of this work is to reproduce the proposed model in [ 46 ] and evaluate it in two NLP tasks: word similarity and word specificity. Due to the omission of some important information in the published paper and since their source code was not released, our work implicates developing our own code based on word2vec and completing those missing details such as the derivation of the error gradients to adapt the neural network. Furthermore, this text serves as a detailed explanation of the presented models and techniques, whose interpretation may be tricky due to the somewhat obscure and sometimes incomplete available literature, specially for readers that are not familiar with distributed word representations based on neural network models.
Chapter 2 is aimed to introduce the concepts that are necessary for a full understanding of the text, focusing on the basics of artificial neural networks and on the continuous skip-gram model as example of state-of-the-art word embedding model. We continue with chapter 3 explaining the model proposed in [ 46 ] and completing the missing details that are necessary to implement it. The presented
approach is applied the experiments shown in chapter 4, whose results are displayed in chapter 5. Finally, we end with the conclusions of our work.
Using Artificial Intelligence to teach programs to play games has grabbed the attention of many researchers over the past decades. The chances of finding a game that has not been approached from a Machine Learning perspective yet are in fact very low. Over the years, particular attention has been given to boardgames. Othello, Backgammon, Checkers, Chess and most recently Go are all proofs of how a combination of proper Machine Learning techniques and sufficient computer power, make it possible for computer programs to outperform even the best human players.
In this thesis, the game of chess has been researched. Developed around the 6th century A.D. in China, Persia and India, chess has been part of human history for a very long time now (Murray, 1913). Despite its age, it continues to grab the attention of millions of players around the world. A recent survey has estimated that at the moment there are ＞ 600 million regular chess players over the world 1 and more than 170.000 rated players. These statistics show that chess is one of the most played and popular boardgames of all time.
Besides being so popular, chess has been very interesting from an Artificial Intelligence perspective as well. It can, in fact, be considered as a challenging testbed that keeps being used by the AI community to test the most recent Machine Learning developments. Driven by these two reasons, this work investigates the use of Deep Learning algorithms (LeCun, Bengio, and Hinton, 2015) that make it possible for a computer program to play as a highly ranked player.
Besides this, we explore if it is possible to teach a program to play chess without letting it use any lookahead algorithms. This means that the program should be able to maximize the chances of winning a chess game, without having to evaluate a lot of future board states. On the contrary, given any board position, the program should be able to find the next optimal move by only exploring the board states of the immediate possible moves.
More information about this main research question will be presented in section 1.2, but before that, the following main topics will be approached in this chapter: in section 1.1 we investigate the general link between Machine Learning and board games. We explore what it means to teach a computer program to master a board game and present the most popular and successful algorithms that have been used in this domain. Specific attention is given to the game of chess in section 1.1.1, where we present how strong the link between Artificial Intelligence and the game of chess is.
 The research is split into four chapters. Chapter one offers in details the study background on stock market price, research objectives, and research scope and research motivation. Chapter two provides a comprehensive literature review of this thesis. It starts by presenting the value of Artificial Neural Networks (ANN) concepts, challenges, characteristics, Chapter three show details of the methodology utilized in this thesis. It specializes in describing the different parts of a model; and it also presents the model details, parameters, and performance of prediction for the proposed method. Chapter four shows the result using different parameters. 
Additional, it shows and discusses the results.  
       Finally, Chapter four concludes the consequence of the thesis, the benefits of the proposed study and recommendations for future work.  
How are the languages of the world related and how have they evolved? This is the central question in one of the oldest linguistic disciplines: historical linguistics. Recently, computational methods have been applied to aid historical linguists. Independently, in the computer science and natural language processing community, machine learning methods have become popular: by learning from data, a computer can learn to perform advanced tasks. In this thesis, I will explore the following question: How can machine learning algorithms be used to predict words between languages and serve as a model of sound change, in order to reconstruct the ancestry of languages? In this introductory chapter, I will first describe existing methods in historical linguistics. Subsequently, I will cover the machine learning methods used in natural language processing. Finally, I will propose word prediction as task to apply machine learning to historical linguistics.

Phonemic changes are sound changes which change the inventory of phonemes. When a phonemic change occurs, a sound changes into another sound in all words in a language, this is thus a regular change. Two general patterns of phonemic change can be distinguished: mergers and splits. A merger is a phonemic change where two distinct sounds in the phoneme inventory are merged into one, existing or new, sound. A split is a phonemic change where one phoneme splits into two phonemes. Based on these two general patterns on the phoneme inventory level, a number of concrete changes in word forms can occur, of which I will highlight two: assimilation and vowel changes.
Computational methods are applied to automate parts of the workflow in historical linguistics. Reasons to apply computational methods include speeding up the process and having a process following more formal guidelines, instead of expert intuition (J?ger and Sofroniev, 2016). Approaches which received a lot of attention were Gray and Atkinson (2003), which charted the age of Indo-European languages, and Bouckaert et al. (2012), which proposed to map the Indo-European homeland to Anatolia. Some computational methods stay conceptually closer to the comparative method than others. List (2012) distinguishes between computational methods which act based on genotypic similarity and meth- ods based on phenotypic similarity. Genotypic methods compare languages based on the language-specific regular sound correspondences that can be established between the languages. Phenotypic methods compare languages based on the surface forms of words. I will now describe computational methods for different tasks in historical linguistics. I will refer to these tasks later, when I introduce a model that can be applied to a number of these tasks.
In protoform reconstruction, word forms for an ancestor of known current languages are reconstructed. Bouchard-C?t谷 et al. (2013) perform protoform reconstruction by directly comparing phonetic strings, without manual cognate judgments, in order to reconstruct protoforms. Probabilistic string transducers model the sound changes, taking into account context. A tree is postulated, and in an iterative process, candidate protoforms are generated. Parameters are estimated using Expectation Maximization. This approach works for protolanguage reconstruction, cognate detection. Furthermore, the results support the functional load hypothesis of language change, which states that sounds that are used to distinguish words, have a lower probability of changing.
UPGMA (Sokal and Michener, 1958) and neighbor joining (Saitou and Nei,
1987) are methods which hierarchically cluster entities based on their pairwise distances. At every step, the UPGMA method joins the two clusters which are closest to each other. UPGMA implicitly assumes a molecular clock (a term which originates from phylogenetic models in bioinformatics): the rate of change through time is constant. The neighbor joining method uses a Q matrix at every step, in which the distance of a language to a newly created node is based on the distances to all other languages. Neighbor joining does not assume a molecular clock, so different branches can evolve at different rates. An example of using a distance-based algorithm for tree reconstruction is J?ger (2015). String similarities between alignments of words are directly used as distances between the languages. This enables the use of data without cognate judgments, which is more widely available. Taking into account surface forms, instead of cognates and sound correspondences, resembles Greenberg＊s mass lexical comparison, described in 1.1.1. 
Now I will describe methods which operate on character data, where every language is represented by a string of features. Two types of these character-based methods are maximum parsimony methods and likelihood-based methods. Maximum parsimony methods try to create a tree by using a minimum number of evolutionary changes to explain the character data, in most cases cognate judgments. One of the problems of this approach is long branch attention: long branches, branches with a lot of change, tend to be falsely clustered together. Likelihood-based methods solve this problem, by looking at the likelihood: the probability of the data, given a certain tree. Evaluating the likelihood for all possible trees is computationally infeasible. Maximizing the likelihood can be efficiently performed using Bayesian Markov Chain Monte Carlo (Bayesian MCMC) methods. These methods randomly sample from the space of possible trees, in order to find the tree with the highest likelihood (Dunn, 2015). Applications of Bayesian MCMC methods for phylogenetic tree reconstruction include Gray and Atkinson (2003), Bouckaert et al. (2012) and Chang et al. (2015).
The field of natural language processing (NLP) is involved with ※getting computers to perform useful tasks involving human language, tasks like enabling human-machine communication, improving human-human communication, or simply doing useful processing of text or speech§ (Jurafsky, 2000, p. 35). Contrary to linguistics, the main objective of NLP is to perform practical tasks involving language, getting a better understanding of language as a system is only a secondary goal. However, the methods employed in natural language processing, can be useful to apply to problems in linguistics. Tasks in natural language processing include the syntactic parsing of sentences, sentiment analysis, machine translation and the creation of dialogue systems. Several approaches exist in natural language processing, including the logical and the statistical approach. In recent years, the statistical approach, using machine learning methods, has shown success in performing a range of tasks.
Machine learning ※is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories§ (Bishop, 2006). In a supervised setting, this is performed by learning from training examples (x,y) during the training phase. During the prediction phase, the algorithm is presented with test examples x, without a label y. The goal of the algorithm is to predict correct labels y ? for the test examples. The algorithm is able to do this by its ability to generalize over the training examples. Language is a sequential phenomenon: when a speaker performs an utterance, this does not happen at one moment in time, but stretches out over time. Furthermore, there are dependencies between the linguisticitemsatdifferenttimesteps. Forexample, inmanylanguages, aspeakerhasahighprobability of using a vowel, after two consonants have occurred. It is also likely that a determiner will be followed by a noun or an adjective.
When using machine learning methods for prediction tasks in language, this sequential nature can be exploited. Instead of predicting the linguistic items at different time steps independently, the dependencies between the items at different time steps can be taken into account. 
The human brain has been undergoing serious investigations by many researchers in the field of neuroscience. In time past, there have been considerable investigations of the structure of the brain (anatomy of the brain), but studies on the functional operation of its complex neural network, paraded all sorts of fantasies as knowledge for many centuries (Sundal et al., 2014). Around the middle of the 18 th century, a functional understanding of the human brain began to take shape. At that time, studies on the brain revealed that nerve signals formerly thought of as ※animal spirits§ are actually electric signals not very different from the currents that flow in an electrical circuit (Sundal et al., 2014). Not only that, advancements in microscope and neuroscience revealed the morphology of neurons, and presented a vision of the brain as a network of neurons; unraveling how neurons interact among themselves using chemical signals.
The adult human brain is made up of about 100 billion neurons, each with about 1,000 - 10,000 connections, making a total 10
14 -10 15  connections in the brain  ( Sundal et al., 2014). It is perhaps the most complex system, more complex than the entire mobile network of the world, with its neurons making and breaking connections at a time-scale that can be as short as a few tens of seconds  ( Sundal et al., 2014). How the brain enables human beings to think has remained a mystery until the present day. Significant ventures in the field of Artificial Intelligence have enabled scientists to come close to the nature of thought processes inside a brain (Zhang, 2011). In the area of artificial intelligence, Artificial Neural Network (ANN) is employed as computational tools to model a biological brain (Willamette, 2014). Artificial intelligence seeks to answer questions like ※how network of neurons in the visual processing areas of the brain transduce the optical image that falls on the retina and how they can be simulated to make intelligent device?§ Answers to
questions like this are best described in the language of mathematics, which is the primary preoccupation of science in computational neuroscience (Sundal et al., 2014). In the same vein, computation in the brain involves understanding human and animal brains using computational models (computational neurobiology); and the process of simulating and building a machine to emulate the real brain (neural computing). All these and other diverse neural network models are examined in the emerging field of computational neuroscience. 
The purpose of this experiment is to find out if Machine Learning can be used to reliably forecast financial markets. Machine Learning is a sub-discipline of Statistics and Computer Science, with the primary goal for gaining insight from data. Artificial Intelligence has traditionally been based in mostly in symbolic models to represent relations, correlations, and especially decisions. Algorithms such as
MiniMax and Hill Climbing are perhaps some of the most fundamental algorithms in Artificial Intelligence. Although these algorithms are extremely useful for solving a multitude of problems, they are based almost entirely in symbolism. In recent years, the rise of Machine Learning has broadened, if not redefined, Artificial Intelligence. Algorithms such are hill climbing remain useful for locating global optima, yet nowadays, some of the most effective learning algorithms are based in fuzzy logic and statistics. The reason why I wanted to do this project is because I want to improve my understanding of machine learning, computational finance, and time-series forecasting. Neural Networks, in particular, have sparked enormous curiosity in researchers in almost every discipline, especially Computer Science. A Neural Network is a learning algorithm inspired by the human brain. If we subsume that the computational operations of a single neuron within a nerve in the brain are simple, and either activated or not, neural networks show great promise for simulating the computing power of the mind. There are many types of Neural Networks. Some are better suited to solving certain types of problems better than others. Neural Networks were originally conceived in the 1980＊s as a mathematical replica of a brain, and there was much interest in the algorithm until the 1990＊s. Because of the expensive computing power required by Artificial Neural Networks (ANN), they were impractical and abandoned as a useless algorithm. Today, with more advanced technology and more computing power, ANN＊s have resurfaced as a state-of-the-art learning algorithm and have proven to be extremely effective problem solvers (Fletcher). The project was conducted with the following goals in mind:
?  Successfully forecast pricing components in financial markets using machine learning techniques
?  Investigate a novel model, specifically using machine learning techniques
?  Summarize results of the model, and evaluate its success at making money

Some tasks are generally thought of as easy for humans and tough for machines. One area where this would be true is recognition of visual style. As there are no clear mathematical rules for what defines a style it is hard for machines to recognize style. Hence it would be difficult for machines to recognize what style of art a painting is, what genre of music is being played, what literary style the book is written in etc. In this thesis the former is explored. Art Style Art style is defined as portraying in a distinctive manner which permits grouping of works into related groups [2]. It is the visual appearance and how it is related to other art pieces. Historically it is the primary means by which art is classified [3]. The style of art is influenced by the region, time and people who create the work. The same object is portrayed in different styles across different region and time. An illustrative example is shown in Figure 1.1 where a fly killer is portrayed in 15 different styles. Some of the things that define a visual style are choice of colors palette, composition, scene, lighting, contours and brush strokes etc. [4] [5]. Classifying the style of art has been traditionally the work of a trained art historian. It takes many years of training to do this. Some of the visual styles are easily detected by a layman while others are more subtle [6] [5]. When the number of paintings is huge it would be very useful if a machine could do it. It is also an interesting philosophical question to see if machines can recognize style.
Machine Learning From a machine learning point of view classifying paintings can be thought of as an image classification problem where different classes are different genres of painting. We will approach this as a supervised learning problem. Given a list of paintings from different genres, then when given a new painting it should be able to identify which genre it belongs to. Note that this is different from object recognition. The same landscape may be depicted in two different styles e.g. realism and impressionism painting of the same location. Thus this is an interesting machine learning problem. This task is also challenging because of the extremely high dimensional nature of the data (100 pixels x 100 pixels = 10k dimension sample). Art classification is a task that requires a lot of non-verbal reasoning. Humans have the special ability to feel for different visual characteristics. With the newer techniques perhaps
maybe machines can also be taught to perform this task.
Lately there has been much interest in machine learning techniques called Deep Learning [7]. Deep learning has pushed the performance benchmark in various machine learning areas such as image classification, speech recognition etc. This would seem an ideal task to try with the new deep learning techniques. We plan to compare deep neural networks with other Machine Learning techniques that would require some sort of hand crafted feature extraction. The advantage of deep learning techniques is they remove the need for laborious hand crafted feature extraction. We plan to see if deep learning is better at classification than other machine learning techniques for painting classification. Layered Neural Network After exploring the existing techniques and inspired by the best performing technique a new classifier called Layered Neural Network was designed. This is a transfer learning technique where in every task a linear classifier is fit that uses raw features plus the output of previously trained classifiers. The idea is that first the classifier is trained on simpler tasks which are related to the main task to create helpful features for later tasks. Benfit An artistic visual style classifier would certainly interest art museums, art collection organizers etc. No longer would a human expert need to label each painting. Achieving good results in this task would also benefit other more complex tasks like face recognition, scene understanding etc. Similar techniques can be applied in other domains such as song genre identification, food cuisine prediction, photo era estimation etc. Any task that would require categorization based on some complex criteria. This also explores the limits of machine capabilities, seeing if
they can do things that humans can.
A new type of classifier called the Layered Neural Network is designed that uses a form of transfer learning. Building a linear classifier on top of pre trained CNN gives very good results as seen in many of the previous results [5,10,16,32]. Inspired by this, we designed a classifier that extends this.
Suppose we need to build a human face detecting classifier. Suppose we ask a person to describe the image of a human face. Then they would say it contains eyes, ears, nose etc. Hence why not build a set of classifier that instead of trying to detect face, tries to detect it＊s components like eyes, ears, nose etc.? Later if one wants to detect monkey faces the same eyes, ears and nose detectors will be useful with some minor modifications. The idea is to have a set of classifiers that are helpful as features for more complex classifiers. This is also an attempt to build a neural network like classifier but where each neuron denotes presence or absence of a well-defined concept, created from a particular classification task.
In Neuroscience, the action potentials (spikes) are the main components of the real-time information processing in the brain. Indeed, thanks to the synaptic integration, the membrane voltage of a neuron depends on the action potentials emitted by some others, whereas if this membrane potential is sufficiently high, there is production of action potentials.
To access those phenomena, schematically, one can proceed in two ways: extracellularly record in vivo several neurons, at a same time, and have access to simultaneous spike trains (only the list of events corresponding to action potentials) or intracellularly record the whole membrane voltage of only one neuron at a time, being blind to the nearby neurons.
Many people focus on spike trains. Those data are fundamentally random and can be modelled easily by time point processes, i.e. random countable sets of points on R+. Several point processes models have been investigated in the literature, each of them reproducing different features of the neuronal reality. The easiest model is the homogeneous Poisson process, which can only reproduce a constant firing rate for the neuron, but which, in particular, fails to reproduce refractory periods . It is commonly admitted that this model is too poor to be realistic. Indeed, in such a model, two points or spikes can be arbitrary close as soon as their overall frequency is respected in average. Another more realistic model is the renewal process [126], where the occurrence of a point or spike depends on the previous occurrence. More precisely, the distribution of delays between spikes (also called inter-spike intervals, ISI) is given and a distribution, which provides small weights to small delays, is able to mimic refractory periods. A deeper statistical analysis has shown that Wold processes is showing good results, with respect to goodness-of-fit test on real data sets [127]. Wold processes are point processes for which the next occurrence of a spike depends on the previous occurrence but also on the previous ISI. From another point of
view, the fact that spike trains are usually non stationary can be easily modelled by inhomogeneous Poisson processes [159]. All those models do not reflect one of the main features of spike trains, which is the synaptic integration and there has been various attempts to catch such phenomenon. One of the main model is the Hawkes model, which has been introduced in [31] and which has been recently shown to fit several stationary data [136]. Several studies have been done in similar directions (see for instance [18]). More recently a vast interest has been shown to generalized linear models [123], with which one can infer functional connectivity and which are just an exponential variant of Hawkes models.
There has also been several models of the full membrane voltage such as HodgkinHuxley models. It is possible to fit some of those probabilistic stochastic differential equations (SDE) on real voltage data [78] and to use them to estimate meaningful physiological parameters [45]. However, the lack of simultaneous data (voltages of different neurons at the same time) prevent these models to be used as statistical models that can be fitted on network data, to estimate network parameters. A simple SDE model taking synaptic integration into account is the well-known Integrate-and-Fire (IF) model. Several variations have been proposed to describe several features of real neural networks such as oscillations [21, 22]. In particular, there exists hybrid IF models including inhomogeneous voltage driven Poisson process [78] that are able to mimic real membrane potential data. However up to our knowledge and unlike point processes models, no statistical test have been applied to show that any of the previous variations of the IF model fit real network data.
Both, SDE and point processes, approaches are microscopic descriptions, where random noise explains the intrinsic variability. Many authors have argued that there must be some more macroscopic approach describing huge neural networks as a whole, using PDE formalism [33, 150]. Some authors have already been able to perform link between PDE approaches as the macroscopic system and SDE approach (in particular IF models) as the microscopic model [99, 113, 134]. Another macroscopic point of view on spike trains is proposed by Pakdaman, Perthame and Salort in a series of articles [114, 115, 116]. It uses a nonlinear age-structured equation to describe the spikes density. Adopting a population view, they aim at studying relaxation to equilibrium or spontaneous periodic oscillations. Their model is justified by a qualitative, heuristic approach. As many other models, their model shows several qualitative features such as oscillations that make it quite plausible for real networks, but once again there is no statistical proof of it, up to our knowledge.
In this context, the main purpose of the present chapter is to build a bridge between several point processes models that have been proved to statistically fit real spike trains data and age structured PDE of the type of Pakdaman, Perthame and Salort. The point processes are the microscopic models, the PDE being their meso-macroscopic counterpart. In this sense, it extends PDE approaches for IF models to models that statistically fit true spike trains data. In the first section, we introduce Pakdaman, Perthame and Salort PDE (PPS) via its heuristic informal and microscopic description, which is based on IF models. Then, in Section 2.3, we develop the different point process models, quite informally, to draw the main heuristic correspondences between both approaches. In particular, we introduce the conditional intensity of a point process and a fundamental construction, called Ogata＊s thinning [110], which allows a microscopic understanding of the dynamics of a point process. Thanks to Ogata＊s thinning, in Section 2.4, we have been able to rigorously derive a microscopic random weak version of (PPS) and to propose its expectation deterministic counterpart. An independent and identically distributed (i.i.d) population version is also available. Several examples of applications are discussed in Section 2.5. To facilitate reading, technical results and proofs are included in two appendices. The present work is clearly just a first to link point processes and PDE: there are much more open questions than answered ones and this is discussed in the final conclusion. However, we think that this can be fundamental to acquire a deeper understanding of spike train models, their advantages as well as their limitations.
The communication between neurons relies on their capacity to generate characteristic electric pulses called action potentials. These action potentials are usually assumed to be identical stereotyped events. Their time of occurrence (called spike) is considered as the relevant information. That is why the study of spike frequencies (firing rates) of neurons plays a key role in the comprehension of the information transmission in the brain [1, 57, 147]. Such neuronal signals are recorded from awake behaving animals by insertion of electrodes into the cortex to record the extracellular signals. Potential spike events are extracted from these signals by threshold detection and, by spike sorting algorithms, sorted into the spike signals of the individual single neurons. After this preprocessing, we dispose of sequences of spikes (called spike trains).
The analysis of spike trains has been an area of very active research for many years [20]. Although the rules underlying the information processing in the brain are still under burning debate, the detection of correlated firing between neurons is the objective of many studies in the recent years [46, 123, 144]. This synchronization phenomenon may take an important role in the recognition of sensory stimulus. In this article, the issue of detecting dependence patterns between simultaneously recorded spike trains is addressed. Despite the fact that some studies used to consider neurons as independent entities [10], many theoretical works consider the possibility that neurons can coordinate their activities [72, 117, 145, 160]. The understanding of this synchronization phenomenon [149] required the development of specific descriptive analysis methods of spike-timing over the last decades: cross-correlogram [118], gravitational clustering [56] or joint peristimulus time histogram (JPSTH, [3]). Following the idea that the influence of a neuron over others (whether exciting or inhibiting) results in the presence (or absence) of coincidence patterns, Gr邦n and collaborators developed one of the most popular and efficient method used this last decade: the Unitary Events (UE) analysis method [62] and the corresponding independence test, which detects where dependence lies by assessing p-values (A Unitary Event is a spike synchrony that recurs more often than expected by chance). This method is based on a binned coincidence count that is unfortunately known to suffer a loss in synchrony detection, but this flaw has been corrected by the multiple shift coincidence count [66].
In order to deal with continuous time processes, a new method (Multiple Tests based on a Gaussian Approximation of the Unitary Events method), based on a generalization of this count, the delayed coincidence count, has recently been proposed for two parallel neurons (Section 3.1 of [158]). The results presented in this article are in the lineage of this newest method and are applied on continuous point processes (random set of points which are modelling spike trains). Testing independence between real valued random variables is a well known problem, and various techniques have been developed, from the classical chi-square test to re-sampling methods for example. The interested reader may look at [91]. Some of these methods and more general surrogate data methods have been applied on binned coincidence count, since the binned process transforms the spike train in vectors of finite dimension. However, the case of point processes that are not preprocessed needs other tools and remains to study. Although the binned method can deal with several neurons (six simultaneously recorded neurons are analysed in [64]), both of the improvements (Multiple Shift and MTGAUE) can only consider pairs of neurons. Thus, our goal is to generalize the method introduced in [158] for more than two neurons. Unlike MTGAUE, our test is not designed to be performed on multiple time windows. However it can be multiple with respect to the different possible patterns composed from n 2 neurons (see Section 5.5.c)).
In Section 5.2, we introduce the different notions of coincidence used through this article. In Section 5.3, a test is established and the asymptotic control of its false positive rate is proven. In Section 5.4 our test is confronted to the original UE method on simulated data and the accuracy of the Gaussian approximation is verified. In Section 5.5 the relevance of our method when our main theoretical assumptions are weakened is also empirically put on test. 
Research in modeling dynamics of complex systems has a long tradition and is still highly active due to its crucial role n many real-world applications (Lipton et al., 2015) like weather forecast, stock quotations, comprehend trajectories of objects and agents, or solving number puzzles (Ragni & Klein, 2011; Gl邦ge & Wendemuth, 2013). The analysis of respective time series allows among others
(1) data compression, i.e., compact representation of time series, e.g., by a function f ( t ), and (2) prediction of further values. Numerous research addresses these topics by recurrent neural networks (RNNs), in particular variants of networks with long short-term memory (LSTM) (cf. Hochreiter & Schmidhuber, 1997). In the following, we propose an alternative, namely a very simple type of RNN which we all predictive neural network (PrNN). It only uses linear activation and attempts to minimize the network size. Thus
in contrast to other approaches not only network weights but also the network architecture is learned. The rest of the paper is structured as follows: First we briefly review related works (Sect. 2). We then introduce PrNNs as a special and simple kind of RNNs together with their prop- erties, including the general network dynamics and their long-term behavior (Sect. 3). Afterwards, learning PrNNs is explained (Sect. 4). It is a relatively straightforward procedure which allows network reduction; no backpropagation
or gradient descent method is needed. We also discuss results and experiments (Sect. 5). The paper ends up with conclusions.
Mode-locking is a ubiquitous phenomenon in the auditory system. Recent research has uncovered evidence of mode-locking in single-unit extracellular chopper and onset cells of guinea pigs[2, 25], in the auditory midbrain of the fish Pollimyrus in response to acoustic signals[24, 21, 23] and in saccular hair bundle cells when exposed to periodic mechanical deflections[7]. In order to study the mode-locking behavior of a single neuron one must focus on the periodic external forcing (input) and the resulting neuronal spike pattern (output). In the aforementioned studies sinusoidal stimuli was used, therefore in order to address the phase relations seen in these experiments one can use sinusoidal current injections into the model neuron and then measure mode-locking behavior utilizing an Arnold tongue analysis[16, 27].
In order to analyze the synchronization of such an oscillator undergoing external forcing, it is constructive to obtain a global map of synchronization regions. Synchronization between a neuron＊s action potentials or spike trains and an external input depends on both the amplitude and frequency of the input. Hence, one can obtain regions on the amplitude-frequency plane that are indicative of mode-locking and synchronization of the two signals i.e. synchronization of the injected periodic signal and the neuronal output. Within these regions, which are commonly referred to as Arnold tongues[31], Eq. (2.1) holds. In the presence of noise, synchronization still occurs. However, in order to measure the stability of the synchronized states one must introduce a measure. This can be done using vector strength (VS) so that synchronization can be measured both with and without the presence of noise in this model. The VS takes a value of 1 if all spike occur at one precise point and 0 for a uniform distribution of phases across the stimulus cycle. The VS gives a good indication as to whether a phase preference exists in the data both with and without noise [25]. There have been studies to measure the stability of the mode-locked patterns using different neuronal models such as Morris-Lecar [30] and leaky integrate and fire (LIF) neurons [15]. Here we measure the stability of the mode-locked states using Izhikevich model.
Arnold tongue diagrams have been produced for Hodgkin-Huxley models[26, 18] and LIF neurons[25]. This is the first paper which utilizes and reports Arnold tongue diagrams for single Izhikevich neurons. Additionally, Arnold tongues are provided for neurons both with and without the presence of noise. The main advantage for computing Arnold tongues with Izhikevich model is the computational and relatively high speed of processing. In this paper, first we explain the neuronal model (Izhikevich 2003) that will be utilized. We then present a brief description of Class-1 and Class-2 excitable neurons and obtain Arnold tongues for both classes of excitability. Then we com-
puted the Arnold tongues for the deterministic case and show examples of modelocking. The formation of harmonics and sub-harmonics in the frequency response of the neuron were then analyzed for some example points in the mode-locking regions. Next we considered mode-locking in the presence of noise. This was done by computing the vector strength to measure the stability of the corresponding mode-locking regions of the Arnold tongues . This analysis was then continued for the noisy model which more accurately simulates biological conditions. The com putational and analytic tools developed here can also be applied to physiological spike trains for any type or class of neuron.
Many studies have shown that phase-locking occurs in different cell types of the auditory system. The neuronal responses to sound stimuli are not passive responses, we must consider the dynamics of the neuron and its interaction with the environment. For example Laudanski et el.(2009) showed responses that resemble mode-locked responses in different cells of the auditory system. These observations suggest that a dynamical process affects the response of neurons to a stimulus. Here we use different sets of data to study mode-locking behavior in the auditory system. One set was recorded from rabbit inferior colliculus neurons, and the other set is from cochlear nucleus neurons in a dial-anesthetized cat. Sensitivity to AM has been investigated extensively in humans because fluctuations in amplitude represent an important form of temporal information in speech, music, and environmental sounds[6]. The inferior colliculus (IC) is a key location for studying AM coding because it is the first location in the ascending pathway where rates are tuned to AM frequency[3, 16].
Mode-locking constitutes a general description of discharge patterns synchronized to periodic stimulation. Mode-locking results from the interaction between the dynamics of a nonlinear oscillator and a periodic stimulus[25]. For example, we can think of a neuron in a given layer of the auditory system as a nonlinear oscillator, which responses to a periodic stimulus such as a tone, click train or sinusoidally amplitude modulated (SAM) stimulus. This response may be §locked§ to the stimulus in terms of the mode of oscillation, i.e. the oscillation of the neuron and the corresponding spike train have the same mode, or an integer ratio of the mode, of oscillation as the stimulus.
We make use of the Arnold tongues diagrams to help understanding the organization of the responses to SAM tones across a range of amplitudes and frequencies. Arnold tongues are regions of the parameter space (amplitude-frequency plane) that specify the stable bounded regions where the response of the neuron is locked to the stimulus for any given ratio of modelocking. The physiological data was analyzed by producing the ISI histogram, ISI scattergram and phase histograms[25]. We shuffle the phase histograms to check the p-value of the Rayleigh test, a statistical test to test the hypothesis that the responses are due to chance. For data that passed the test, we modeled the phase histograms by fitting the parameters of the model to the recordings. 
Wilson-Cowan model is a model of neural oscillations based on excitatory and inhibitory neural populations. Neurons are called excitatory if they increase the probability of a postsynaptic action potential occurring, and inhibitory if they decrease this likelihood.
According to Hugh D. Wilson and Jack D. Cowan, all nervous processes of any complexity are dependent upon the interaction of excitatory and inhibitory cells [38]. The original model was introduced by Wilson and Cowan in their 1973 paper[38]. Consider an interconnected pair of excitatory and inhibitory neurons as in Fig. 3.1. Each unit in the oscillator represents a local population of neurons, in cerebellum, olfactory cortex, and neocortex. This pair is called Wilson-Cowan neural oscillator, one of the basic mechanisms of the generation of oscillatory activity in the brain[11].
Dynamical system theory is the study of time evolution given by systems of differential equations. The typical focus of the theory is not to solve the differential equations for general initial conditions, but to study the qualitative behavior. In
general they focus on bifurcations.
In order to make these terms more understandable, we present some physical analogies. Stability is a fundamental property of a dynamical system, which means that the qualitative behavior of its time evolution is not affected by small perturbations of the trajectory. So, equilibrium phases of thermodynamics corresponds to fixed-points in the dynamics and phase transitions corresponds to bifurcations[34]. Analysis of the transition between states in dynamical models is called bifurcation analysis[37]. Bifurcation analysis is the mathematical term analogous to abrupt and continuous or first and second order phase transitions in physics, particularly statistical mechanics. For example bifurcation parameter is defined as a parameter in a system of differential equation(s) that by changing its  alue, the system undergoes a qualitative change in its behavior. This is similar to the order parameter in statistical mechanics. A good example can be liquid-gas transitions in studying the phase of materials.
Complex systems can undergo transitions between different dynamical states, especially close to the transition point, show low-dimensional behavior, i.e. even though the system has many degrees of freedom its dynamics can be modeled by a set of a few nonlinear ordinary differential equations[8]. In fact, complex systems can be modeled on different levels and it is sometimes even possible to derive the dynamics on a higher (typically more abstract or microscopic) level of description.
In many cases, particularly when transitions between macroscopic patterns occur, the description can be reduced to a low dimensional set of nonlinear ordinary differential equations[8]. A canonical model is the simplest (in analytical terms) of a class of equivalent dynamical models, and can be derived using either normal form theory or centre manifold theory. The canonical model we introduce in Eq. (3.4) was derived, using normal form theory, from the Wilson-Cowan model of the interaction between excitatory and inhibitory neural populations[20, 38]. Our goal in this chapter is to produce this derivation by following step-by-step calculations and showing all the mathematical proves. However, the canonical model is generic, so it could also be derived from other models of nonlinear oscillation, including outer hair cell models[17]. The canonical model uncovers universal properties, making predictions that hold under a rather general set of assumptions[11]. This makes the canonical model especially attractive from the point of view of modelling
human perception and behavior[22].
MSCL is a parallelizable algorithm, as other neural networks, that admits on-line data training and follows the general Competitive Learning steps:
Selection the winner prototype . Given an input data vector, the competitive units compete each other to select the winner neuron comparing their prototypes with the input. This Best Matching Unit (BMU) is selected in MSCL as the one that minimizes the product of the magnitude (provided with data or calculated from a user-defined function and assigned to each unit) and the distance of the unit prototypes to the input data vector. This procedure differs from other usual competitive algorithms where the BMU is determined only by distance. Updating the winner and magnitude . Winner＊s weights are adjusted iteratively for each training sample, with a learning factor specific for each unit, and forced to decay with training time. Concurrently, magnitude at each unit is also updated.
Soft competitive learning comprises a set of methods were more than a single neuron adapt on presentation of a sample pattern. These algorithms possess some features that are advantageous over hard competitive learning methods: avoiding unused (＆dead＊) units, accelerating the learning phase, filling empty areas in the dataset space or avoiding local minima. Self Organizing Maps (SOM) [38] is one of these algorithms with the property of generating topographic organization of neurons in a grid of reduced dimension. This makes SOM useful for visualizing low-dimensional views of high-dimensional data, akin to multidimensional scaling. SOM has also been used for data classification (i.e. [51], [7]).
On the other hand, Magnitude Sensitive Competitive Learning (MSCL) [61] is a hard competing algorithm which has the capability of distributing the unit centroids following any user defined magnitude that may have no kind of relation with the data density (as it has been demonstrated in the previous chapter).
Comparing both algorithms, the main disadvantage of SOM neural networks against MSCL is that SOM only can distribute unit in direct function of the data density. Only magnification control methods ([81], [54]) present an alternative to SOM that allows the modification of the relation between data and weight vector density for a given model. However, it is important to highlight that in this kind of methods, final unit distribution is always somehow related with data density.
In this chapter we describe a new algorithm, Magnitude Sensitive Self Organizing Map (MS-SOM), an hybrid between MSCL and SOM, which synthesizes the advantages of both methods. 
we have explained the MSCL algorithm in detail. MSCL as many competitive learning algorithms use homogeneous dataset in the sense that each of the samples has to have the same dimension (the number of components), and the trained neural network also has the same dimension. Similar issue affects MS-SOM.
However sometimes, due to the nature of the dataset, some of the data samples may have unknown values for any of its components. This forces to some kind of preprocessing that usually introduces undesired artifacts during training. There may be different reasons for the inconsistency in the dataset＊s component size. Data collected in different periods of time or by different entities may be inconsistent. For instance meteorological data may lack of some variables as wind velocity when remounting may years ago, or just come from different meteorological stations. Then, it is no possible to process directly this dataset with common competitive learning methods. To do it, incomplete components in some of the samples must be dealt in some way. Another example is statistical information about citizens in different countries. Even though there are international entities that are doing a huge effort to unify measures, they are usually different in some way.
Undoubtedly, Fencing is one of the finest individual sports; its capacity is very competitive and recreational. According to a study that was published by FIE in 2012, Fencing had the highest participation in Europe at 6.7% of the entire population.
Fencing is one of the most complex sports, Teaching its methods has to be in an individual training centre with a professional expert to be as a mentor and observer for the fencer to give a real-time feedback for the body and speed performance, some of those experts are using a high-frame rate video recordings for making more accurate analysis. Although, this is a qualitative approach in training, it needs an amount of considerable effort and time. During the last 5 years, technology has started to be a huge part of analysing the fencer (Sword Moves, Fencer＊s Techniques and The Body Reaction) in more quantative approach, the quantative data became more available for different method in Fencing techniques and science researches (NYT, 2014).
Nowadays, with the surge in motion in hi-tech, it is available to capture and track the human movements in detail that offers slow motion for the fencer＊s body, three-dimensional recordings and analysis the fencer＊s sword movements (Le Monde, 2014). As this technology could be very advanced for researches and training instruments, theses motion systems have limited applications due to their specific setup methods and high price products.
In the middle ages, sport was considered as an essential part of people＊s life; because the lifestyle they were living -due to the lack of technology at that time- forced them to make the sport one of their life principals. Maybe they are not as known as the known sports nowadays, but as long as it relates to the physical ability improvement then it is categorized as a sport, which people has to keep practicing it, in order to adapt with their hard life-style and the surrounded environment.
Today, sports are being practiced by people for entertainment or healthcare issues. Athletes from different parts of the world need -before every single competition, game or match- to a rehabilitation period in order to reach a certain level: mentally, physically and
psychologically, as an example for professional athletes heck their physical ability level and to put as well a suitable training program. Training program -that is the core of this topic- is needed to be developed so it can suit the need of the athlete with taking many factors into consideration such as: age, gender, training facilities, weaknesses, strengths and objectives, there are two main training programs (Young or inexperienced athletes) and (Experienced Athletes), every single athlete is having a different plan depends on the needs of his/her body and mental nature. However, it is impossible to make a one single training program suits all the athletes.
Nowadays, the combat western art that uses a small sword in the hand is known now as (Olympic Fencing), the beginning of the fencing art was in Italy in the 19th century. Fencing is one of five main sports -that was featured in every one of the Olympic Games, the other four are Cycling, Gym, Swimming, and Athletics. There are three main weapons in the modern fencing which are: Foil: a light thrusting weapon, with a sword weight of 450 grams. The foil targets the neck, the groin, and the human trunk (including the back) but not the legs or the arms. When the fencer touch any other area, it will not be considered as a touch谷 and this called ＆off-target touches＊, both of the fencers will stop start the whole attack again. Also, the rules of the modern fencing -that are used in sanctioned world-wide events - are maintained by the FIE, including Olympic Games.
When the data from Fencing are gathered, the next move is extracting the knowledge from this information. Many statistical Methods or data mining techniques can be applied; as fencing with its different types (Epee - Foil - Sabre) is full of different moves and therefore rich in data. Data analysis and traditional methods of predictions have been the source for a numerous players and organizations for a long time. This approach leads to sport-oriented development number, such as machine learning methods. Neural Networks are widely used systems in machine learning used in Fencing. Using neural networks with the collected data-sets are analysed in order to find tendencies due to financial gain. Other techniques are categorized as genetic algorithms, Support Vector Machine (SVM) and C4.5 and C5.0 decision trees.
Fencing is a competition between two players who are aiming to win. A win is attained by scoring the most points when the time count down ends or one of the two players reaches the score limit before the time ends. In both cases, the winner will be decided by his/her score points. Points can be scored by the fencer in many different ways, the difficult way to achieve the point is by making an attack from a long distance and the sword could be able to touch the opponent from the first try, the less difficult way is to end the opponent＊s attack and reflect his attack from his sword into yours with two moves which is known by ※Counter Attack§. If one of the fencers could be able to score a touch谷 against his opponent under the condition that it was not the first fencer＊s attack, then the touch谷 will
not counted by the referee.
Solutions to real world control problems often make simplifying assumptions to enable automated design techniques. To ensure that the resulting controllers behave as expected on the actual system, the simplifications made for analysis generally involve constructing a set of simpler systems which cover all the behaviors of the real world plant. These types of relaxations generally result in controllers that are suboptimal with respect to the desired performance metric and the plant of interest. Adaptive control techniques allow the initial design to be modified with the goal of improving performance based on observations of the plant＊s actual behavior. The application of adaptive control systems to real world control problems is, however, often impossible or too risky because of a lack of guarantees about the adaptive system＊s performance and stability.
Most existing research on stable adaptive control takes a specific algorithm and proves stability on some class of plants. For example, a Lyapunov function might be constructed showing that a certain gain scheduling procedure is stable for a class of linear, time-invariant plants [64]. A second approach is to ensure stability by restricting the updates an adaptive system makes to the controller parameters [49, 62]. This type of approach is not restricted to a given adaptive control or learning algorithm, but it requires online monitoring of the control parameter updates. Compared to the first approach, online update monitoring increases the computational cost of the adaption, sometimes considerably. On the other hand, this type of approach is more general.
This document focuses on the second approach for ensuring the stability of adaptive control systems. Specifically, the use of recurrent neural networks in adaptive control systems is considered. Recurrent neural networks are capable of representing a wide class of both static and dynamic mappings. In fact it can be shown that recurrent neural networks are capable of approximating a many dynamical systems arbitrarily well [32]. This makes them suitable for many different uses in control systems. For example, a recurrent neural network can be used as a controller or as an estimator of system parameters or unobservable states. Since recurrent neural networks are dynamic systems, they have their own stability properties which must be understood. The first part of this dissertation analyzes existing techniques for assessing the stability of recurrent neural networks and extends them in several ways to improve their performance. Recurrent neural network stability is analyzed in terms of the input-output relationship of the nonlinear, time-varying mapping defined by the network parameters. The main stability analysis tool that is applied is the theory of integral quadratic constraints [59]. Stability properties are framed in terms of the feasibility of certain matrix inequalities. Such formulations are addressed mathematically and computationally by semidefinite programming [92]. The second part of this dissertation proposes an algorithm for ensuring the stability of a recurrent neural network that is adapted as part of a larger control system. The algorithm is applicable to the adaption of recurrent neural networks in isolation or in a loop with a controlled plant. Examples are presented of application in both of these situations.
Reinforcement learning is a term used to describe a large class of problems and algorithms that involve learning through observation of, and interaction with, an environment. While the theory and practice of reinforcement learning for control has improved dramatically over the last two decades, applications of this approach for online adaptation on real systems remain elusive. Two problems persist that impede the deployment of reinforcement learning systems: guarantees of performance and guarantees of safety. Much of the progress in reinforcement learning has come in the form of proofs of convergence and characterizations of solutions. Sufficient conditions for the convergence of reinforcement learning algorithms have emerged for increasingly large classes of algorithms and problems. This body of research gives results concerning the properties of the resulting controllers. More recently, literature addressing the dynamic behavior of these algorithms has appeared [49, 67]. Here, the research seeks to address the problem of providing guarantees of safety. The dynamic effects of reinforcement learning algorithms interacting with a controlled system must be understood.
Consider a class of problems in which the goal is to drive the state of a system to some operating point. This class of problems includes stabilization of unstable systems, reference tracking problems, and regulation problems. Controllers for these problems must meet some performance goal while also guaranteeing stability of the closed loop system. A large body of research exists that addresses the problem of designing and characterizing controllers for these types of problems. These design methods are generally restricted to time-invariant linear systems and controllers that are linear in their inputs. In most cases, however, the system that is to be controlled will exhibit nonlinear dynamics that change over time. To specify the discrepancy between the design assumptions and reality, linear, time-invariant (LTI) models can be derived with the unmodeled dynamics of the system 〞 its time varying components and its nonlinearities 〞 characterized as uncertainty in the basic LTI model. Robust control theory provides methods for designing controllers for these uncertain models and for characterizing the uncertainties.
Robust controllers, however, can exhibit suboptimal performance because of restrictions in their design. In this situation performance can be improved through the use of adaptation. Rather than adapting the given robust controller, an extra adaptive component can be added to the control loop. Retaining the fixed controller gives the system a guaranteed level of initial performance. The adaptive component is allowed to have a more general structure than the fixed controller. The actual control signal is generated by combining the output of the fixed controller and the adaptive controller. The stability of the closed loop system is given for the fixed controller case, but the addition of an adaptive component can drive the system to instability. This instability can result in damage to the physical plant or its environment and an associated decrease in performance. Thus, a method is needed to guarantee the stability of the system with the adaptive component. In Chapters 5 and 6 such a method is given using recurrent neural networks as the adaptive component.
To guarantee stability during learning, transitions from one controller to another must be considered. The stability analysis presented in Chapter 4 considers a range of possible controllers and assesses whether or not the system will remain stable as the controller changes within this range. In Figure 1.6, for example, several regions are shown in which the control parameters can vary while not making the control loop unstable. Notice that the regions are smallest near the stability boundary.
Because of this problem, it is useful to consider whether or not the stability analysis provides information that can be used to influence the updates made to the adaptive controller. The stability analysis that resulted in Figure 1.4 produces an upper bound on the gain around the feedback loop 〞 the amount of amplification of the input signal 〞 between the plant and controller. These upper bounds are shown in Figure 1.7. The gain increases rapidly near the stability boundary and is useful for biasing the learning algorithm away from this boundary. These ideas are explored more fully in Chapter 5, where a method of exploiting this information is developed.
The immediate goals of this research are two fold: to improve the state of the art in the analysis of recurrent neural network stability and to provide a method for efficiently applying such stability analysis to the online adaptation of recurrent neural networks in control systems. Improvements in stability analysis techniques can be measured along two axes. The first is the conservativeness of the analysis. A conservative analysis provides only sufficient conditions for stability as opposed to conditions that are both sufficient and necessary. Less conservative methods provide conditions which are closer, in some sense, to the necessary conditions of stability. The second axis is computational complexity. The methods presented in the following chapters rely on testing the feasibility of certain matrix constraints. Reducing the size or number of such constraints allows the stability analysis to be applied to larger or more complex systems. Often, reduction in conservativeness and reduction in computational complexity are at odds. Understanding the relationship between the two allows better choices to be made in practical systems. Improvements along both axes are presented in Chapters 3 and 4.
Improvements in the stability analysis of recurrent neural networks are only applicable to adaptive control systems in a framework that considers the whole control system. The second goal of this research is to develop a method that allows recurrent neural networks to be adapted in control systems with guarantees of stability for the entire system. This type of system has been proposed in earlier work [49, 50, 2]. This research pointed to a problem with naively applying the proposed stability analysis techniques to an adaptive control system. Often the adaptation drives the system to the boundary in the parameter space between parameter settings for which stability could be proved and those settings for which it could not. When this occurs, an increasingly large number of stability analysis computations must be made. This, in turn, makes the method very expensive computationally. The objective in proposing a new algorithm here is to reduce the cost of guaranteeing stability of adaptive control systems with recurrent neural network components and make such methods more practically applicable.
A secondary goal of this document is to illustrate some practical aspects of working with the proposed stability analysis computations and stable adaptation algorithms. To this end computational comparisons are made throughout the document between different formulations of problems and different optimization algorithms. Rather than attempting to provide a comprehensive picture of the computational aspects of these problems, results are presented simply to give a sense of the class of computations involved.
The basic ideas presented in the motivating example will be developed more fully in the remaining chapters of this document. In Chapter 2 background material on recurrent neural networks and stability analysis is presented. A formal definition of stability is established, and the basics of integral quadratic constraints analysis are explained. The presentation assumes some knowledge of dynamical systems theory and analysis, but briefly covers all of the necessary control theoretic material used in the dissertation. Pointers to more thorough presentations of the material are provided. Previous work in this area is described at the end of the next chapter. Some limitations of this previous work are described.
Chapter 3 is an in-depth study of the integral quadratic constraint approach to the stability analysis of recurrent neural networks. Theoretical as well as computational aspects of the approach are considered. Specifically, work in [19, 52] is applied to reduce the conservativeness of the stability analysis. Also, it is shown that for solving the resulting matrix constraint feasibility problems, the augmented Lagrangian method of [46, 47] is much more efficient than standard semidefinite programming algorithms. These results are important for practical application of the algorithm presented in Chapter 5.
When applied in an adaptive control context, the parameters of recurrent neural networks vary with time. Chapter 4 examines the stability analysis of recurrent neural networks in the time varying case. The application of integral quadratic constraints analysis to recurrent neural networks requires formulation of the networks as feedback systems between linear, time-invariant components, and the nonlinear, time-varying and uncertain parts of the system. A new formulation of timevarying recurrent neural networks as feedback systems is developed in Chapter 4. Experiments show that compared to the formulation developed in [79], this results in a less conservative stability analysis. Modifications of the basic stability problem that improve its numerical conditioning are also presented.
The example earlier in this chapter hinted at how certain problems might arise in the application of stability analysis techniques to time varying neural networks in a feedback loop with a plant. Naive application of these techniques requires a large number of stability analysis problems to be solved. Since these problems can be expensive, this limits the applicability of these results to extremely simple systems. In Chapter 5 a general algorithm for filtering parameter updates to ensure stability is described. A method of biasing the parameter trajectory away from the stability boundary is developed. This bias reduces the number of expensive stability analysis computations that must be performed. On the other hand, it requires the solution of many smaller, but non-trivial problems. A technique for reducing the cost of solving these problem is also presented.
In Chapter 6, the proposed algorithm is applied to a multiple spring-mass-damper system with nonlinear friction. The example demonstrates the capability of the algorithm to ensure stability of an adaptive controller for a non-trivial system. The example also exposes some remaining problems with the approach that are proposed as future research. Some general conclusions and a description of this future research are presented in the final chapter.
Humans and other intelligent animals are characterized by their ability to learn from past experience and adapt to their changing environments. This allows them to perform well in diverse settings. Mechanisms such as learning, memory and exploration play a crucial role in enabling such behaviors. Creating software agents that exhibit similar characteristics and adapt in a variety of settings is a key goal of artificial intelligence (AI). This is not only beneficial and valuable, as it has large potential implications in various areas such as recommendation systems, healthcare, e-commerce and robotics, but essential for achieving bold dreams such as the creation of AIscientists or autonomous robots for deep space exploration. This endeavor, however, presents a number of difficult problems and addressing them all is not an easy task.
In this thesis, we focus on two promising fields of AI research, namely reinforcement learning (RL) and artificial neural networks (NNs), that aim to investigate and present solutions to the problem of adaptation in dynamic environments. In such settings, there is a high need for fast adaptation, and standard methods are not very efficient as they assume that the environment does not change. The overall purpose of this dissertation is to identify situations in dynamic environments that could benefit from faster adaptation, and prescribe mechanisms or methods to use in each situation, by investigating their effectiveness.
It is often the case that such artificial agents are controlled by computational models of biological NNs known as artificial NNs (ANNs). ANNs aim at abstracting away the details of how biological NNs work in a form that is sufficient for emulating their ability to learn and generalize from previous experience. It is known that feedforward ANNs (which are directed acyclic graphs and compositions of functions) can approximate any function, therefore, they are universal function approximators (Hornik et al., 1989; Park and Sandberg, 1991). Recurrent NNs (which have delayed or feedback connections, thus, cycles in the directed graph) on the other hand are known to be universal approximators of dynamical systems (Funahashi and Nakamura, 1993). When talking about adaptive ANNs we usually mean either (i) recurrent ANNs, because their feedback connections can create some form of memory by integrating information through time, or (ii) plastic ANNs, which are ANNs (feeforward or recurrent) that change over time by utilizing appropriate learning/plasticity mechanisms. Both approaches create adaptive networks (Floreano and Urzelai, 2000; Stanley et al., 2003) and can complement each other.
In order to cope with general dynamic settings, an agent needs to be able to continuously and quickly adjust its behavior policy. In this thesis, we explore methods for RL and adaptive ANNs in order to address the following specific research hypotheses and questions:
Given that MARL environments are dynamic, how do simple algorithms such as Q-learning or SARSA perform in the simplest MARL environments? The simplest multiagent environments come from the area of game theory (Fudenberg and Tirole, 1991) and are called repeated games. Experiments with one of such games known as the Iterated Prisoner＊s Dilemma (IPD) have shown that agents that use the Q-learning algorithm (Watkins and Dayan, 1992) could not achieve large cumulative payoffs (Sandholm and Crites, 1996). Is there a way to motivate the agents to achieve high payoffs, without however, changing their learning algorithms or the rules of the game? Our hypothesis is that this could be achieved by optimizing the agents＊ reward function.
Repeated games do not have states as MDPs do, therefore, they are unstructured. Structured single-agent tasks could benefit from RL algorithms that utilize some method of hierarchically decomposing the task into simpler subtasks. Could structured multiagent tasks benefit from the same hierarchical RL algorithms that were used for single-agent tasks? If not, then can we simply combine the component that makes a RL algorithm hierarchical with the component that makes a RL algorithm better suited for MARL environments? Will the resulting combination perform better in such environments?
Moving away from multiagent environments to single-agent ones, we can have some control over the dynamics (nonstationarity) of the environment by varying the state transition function, T. As this makes the environment non-Markovian, it can be related to partially observable MDPs (Astr? om,“ 1965), for which recurrent ANNs, optimized using policy search methods such as evolutionary algorithms (Holland, 1962, 1975; Fogel, 1962; Fogel et al., 1966; Rechenberg, 1965, 1973; Schwefel, 1965, 1975), have shown very good results (for example, see Gomez et al., 2008). Motivated by the fact that synaptic plasticity, i.e., the strengthening or weakening of synaptic connections, is a major process that underlies learning and memory (Martin et al., 2000), researchers have also successfully created adaptive ANNs by designing or evolving synaptic plasticity rules (for example, see Floreano and Urzelai, 2000; Niv et al., 2002a,b; Stanley et al., 2003; Soltoggio and Stanley, 2012). Evolutionary experiments have shown that the rules could be optimized either for the whole network (Niv et al., 2002b; Soltoggio et al., 2008) or for every connection in the network separately (Floreano and Urzelai, 2000; Risi and Stanley, 2010). Is there a representation that can be used to approximate any learning rule? It is known that recurrent ANNs can approximate arbitrary dynamical systems (Funahashi and Nakamura, 1993). Since a learning rule can be viewed as a type of dynamical system, could ANNs be used to represent arbitrary learning rules? If yes, then how do such ANN-based learning rules compare to standard RL algorithms in both static and dynamic environments?
If we suppose that the state transition function, T, does not change, a different form of nonstationarity comes from varying the reward function, R, i.e., the goal of the agent. In such dynamic, reward-based scenarios, experiments have shown that the evolution of ANN architectures and modulated plasticity rules could foster the emergence of adaptive behavior (Soltoggio et al., 2008). However, designing the setting and the fitness function for evolving learning behavior is often hard (Soltoggio and Jones, 2009) and this problem presents a number of deceptive traps that can only be partially addressed, even with advanced evolutionary methods (Lehman and Stanley, 2008, 2011; Risi et al., 2009, 2010; Mouret and Doncieux, 2012; Lehman and Miikkulainen, 2014). Moreover, while ANN architectures and rules could be evolved in certain environments and display optimal adaptation (Soltoggio et al., 2008), they have the following undesired properties: (i) their function cannot be intuitively explained and (ii) in slightly modified versions of the initial environments, the architecture-rule combination needs to be usually evolved from scratch and the result looks nothing like the one of the initial environment (Soltoggio et al., 2008). This could possibly be an issue of the evolutionary algorithm itself, and evolving modular (Clune et al., 2013) and regular (Clune et al., 2011) architectures could offer some advantages in such scenarios. If the agent has already formed some internal neural representation of how the environment works, then we ask: are there any complementary mechanisms that can be used in order for the agent to be able to efficiently explore and find the goal when the reward function changes? One of our hypotheses is that these mechanisms take the form of single (artificial) neurons or (artificial) circuits. What principles from neuroscience could inspire the design of such mechanisms? Furthermore, assuming that we know of such mechanisms, could we utilize them to manually design adaptive ANNs whose function can be intuitively explained? If this could be done, then the ANN architectures between slightly different versions of environments would look similar.
Multiagent Reinforcement Learning (MARL) has attracted an influx of scientific work. The main problem of MARL is that the presence of multiple learning agents creates a nonstationary environment. Such an environment is affected by the actions of all agents, thus, for a system to perform well, the agents need to base their decisions on a history of joint past actions and on how they would like to influence future ones. In MARL there could be different kinds of situations which could be modeled using game theory (Fudenberg and Tirole, 1991; Leyton-Brown and Shoham, 2008; Shoham and Leyton-Brown, 2008): fully competitive or adversarial (which could be modeled with zero-sum games), fully cooperative or coordinative (which could be modeled with team games), and a mixture of both (which could be modeled with general-sum games).
Since different issues arise in each situation, a lot of algorithms were proposed to address them. Some of these algorithms are the following: (i) minimax-Q (Littman, 1994), which extends the classic Q-learning algorithm (Watkins, 1989) for single-agent RL by replacing the ※max§ operator in the update step of Q-learning with one that calculates each player＊s best response, and can be applied to two-player zero-sum games; (ii) Nash-Q (Hu and Wellman, 2003), which is an extension of minimax-Q to general-sum games; (iii) Joint Action Learners (Claus and Boutilier, 1998), which is an approach investigated in cooperative settings in which the agents maintain some beliefs about the strategies of the other agents and therefore learn joint action values; (iv) FoF-Q (Friend-or-Foe Q) (Littman, 2001), which can be interpreted as consisting of two algorithms, Friend-Q, which is suited for coordination games and Foe-Q for zero-sum games and is equivalent to minimax-Q; (v) WoLF-PHC (Win or Learn Fast - Policy Hill Climbing) (Bowling and Veloso, 2001), which uses an extension of Q-learning (Watkins, 1989) to play mixed strategies based on the WoLF principle that uses a higher learning rate when the agent is losing and a lower one when it is winning; (vi) WoLF-IGA (WoLF - Infinitesimal Gradient Ascent) (Bowling and Veloso, 2002; Banerjee and Peng, 2002), which combines gradient ascent with an infinitesimal step size (IGA) (Singh et al., 2000) with the WoLF method; (vii) CE-Q (Correlated Equilibria Q) (Greenwald and Hall, 2003), which learns correlated equilibrium (Aumann, 1974) policies and can be thought of as a generalization of Nash-Q and FoF-Q; (viii) FMQ (Frequency Maximum Q) (Kapetanakis and Kudenko, 2005), which is a heuristic method proposed for coordination in heterogeneous environments and is based on the frequency with which actions yielded the maximum corresponding rewards in the past; (ix) GIGA-WoLF (Generalised IGA - WoLF) (Bowling, 2005), which achieves convergence and no-regret (i.e., the algorithm performs as good as the best static strategy); (x) AWESOME (Adapt When Everybody is Stationary Otherwise Move to Equilibrium) (Conitzer and Sandholm, 2007), which is an algorithm that is guaranteed to converge to a Nashequilibrium (Nash, 1950) in self-play and learns to play optimally against stationary opponents in games with an arbitrary number of players and actions; (xi) WPL (Weighted Policy Learning) (Abdallah and Lesser, 2008), which assumes that an agent neither knows the underlying game nor observes other agents, and achieves convergence in benchmark two-player-two-action games, and (xii) M-Qubed (Max or MiniMax Q) (Crandall and Goodrich, 2011), which is a robust algorithm that was shown to achieve high degrees of coordination and cooperation in several two-player repeated general-sum games in homogeneous and heterogeneous settings. For a comprehensive coverage of MARL algorithms, see Bus?oniu et al. (2008) and references therein.
A lot of work is focused in deriving theoretical guarantees, based on different sorts of criteria such as rationality and convergence (Bowling and Veloso, 2001), targeted-optimality, safety and auto-compatibility (Powers et al., 2007) or security, coordination and cooperation against a wide-range of agents (Crandall and Goodrich, 2011). Theoretical work has also been done in analyzing the dynamics of multiagent learning. For instance, Tuyls et al. (2006) investigated MARL from an evolutionary dynamical perspective, while Iwata et al. (2006) approached the field from a statistical and an information theoretical perspective. Since the problem is not very well-defined, Shoham et al. (2007) attempted to classify the existing work by identifying five distinct research agendas. They argued that when researchers design algorithms, they need to place their work under one of these categories which are: (i) computational, which is aimed in designing learning algorithms that iteratively compute properties of games, (ii) descriptive, which is to determine how natural agents (such as humans, animals or populations) learn in the context of other learners and make decisions, (iii) normative, in which the learning algorithms give a means to determine which sets of learning rules are in equilibrium with one another, (iv) prescriptive cooperative, which is how to design learning algorithms in order to achieve distributed control and (v) prescriptive noncooperative, which is how to design effective algorithms, or how agents should act to obtain high rewards, for a given environment, i.e., in the presence of other (intelligent) agents. Subsequently some work did focus on specific agendas (for example see Erev and Roth, 2007), but more agendas were proposed (Gordon, 2007). In addition, the original agendas (Shoham et al., 2007) have been criticized that they may not be distinct, since they may complement each other (Fudenberg and Levine, 2007; Tuyls and Parsons, 2007). Stone (2007) extended the criticism by arguing that the game theoretic approach is not appropriate in complex multiagent problems. Despite these criticisms, our study lies in the original prescriptive non-cooperative agenda (Shoham et al., 2007).
As pointed out by Zinkevich et al. (2007) ※perfectly predicting the environment is not enough to guarantee good performance§, because the performance depends partly on properties of the environment. In our case, we believe that the property of the environment which plays a significant role in the CC outcome is the reward function (i.e., the payoff matrix), since it specifies the type and strength of the reinforcement the agents receive. Therefore, we introduce a method that evolves the payoff values of the IPD while satisfying its constraints, in order for simple RL algorithms to rapidly reach the CC outcome.
Reinforcement learning (RL) suffers from the curse of the dimensionality, where the addition of an extra state-action variable increases the size of the state-action space exponentially and it also increases the time it takes to reach the optimal policy. In order to deal with this problem, there exist three general classes of methods that exploit domain knowledge by: (i) using function approximation to compress the value function or policy and thus generalize from previously experienced states to ones that have never been seen, (ii) decomposing a task into a hierarchy of subtasks or learning with higher-level macro-actions, and (iii) utilizing more compact representations and learn through appropriate algorithms that use such representations (e.g., factored (Boutilier et al., 2000), object-oriented (Diuk et al., 2008), relational (Guestrin et al., 2003) and first-order (van Otterlo, 2009)). In this study, we focus on hierarchical methods that decompose a task into subtasks.
Hierarchical Reinforcement Learning
Hierarchical methods reuse existing policies for simpler subtasks, instead of learning policies from scratch. Therefore they may have a number of benefits such as faster learning, learning from fewer trials and improved exploration (Dietterich, 2000). This is an active area of research. Singh (1992) presented an algorithm and a modular architecture that learns the decomposition of composite Sequential Decision Tasks (SDTs), and achieves transfer of learning by sharing the solutions of elemental SDTs across multiple composite SDTs. Dayan and Hinton (1993) showed how to create a Q-learning (Watkins, 1989) managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them. Kaelbling (1993) presented the Hierarchical Distance to Goal (HDG) algorithm, which uses a hierarchical decomposition of the state space to make learning achieve goals more efficiently with a small penalty in path quality. Wiering and Schmidhuber (1998) introduced the HQ-learning algorithm, a hierarchical extension of Q-learning, to solve partially observable Markov Decision Processes (MDPs; Puterman, 1994). Parr (1998) developed an approach for hierarchically structuring MDP policies called Hierarchies of Abstract Machines (HAMs), which exploit the theory of semi-MDPs (Sutton et al., 1999), but the emphasis is on simplifying complex MDPs by restricting the class of realizable policies, rather than expanding the action choices. Sutton et al. (1999) considered how learning, planning, and representing knowledge at multiple levels of temporal abstraction, can be addressed within the mathematical framework of RL and MDPs. They extended the usual notion of action in this framework to include options, i.e., closed-loop policies for taking actions over a period of time. Overall, they show that options enable temporally abstract knowledge and actions to be included in the RL framework in a natural and general way. Dietterich (2000) presented the MAXQ decomposition of the value function, which is represented by a graph with Max and Q nodes. Max nodes with no children denote primitive actions and Max nodes with children represent subtasks. Each Q node represents an action/subtask that can be performed to achieve its parent＊s subtask. The distinction between Max nodes and Q nodes is needed to ensure that subtasks can be shared and reused. Andre and Russell (2001) extended the HAM method (Parr, 1998) and produced the Programmable HAM (PHAM) method that allows users to constrain the policies considered by the learning process. Hengst (2002) presented the HEXQ algorithm which automatically attempts to decompose and solve a model-free factored MDP hierarchically. Ghavamzadeh and Mahadevan (2004) addressed the issue of rational communication behavior among autonomous agents and proposed a new multiagent hierarchical learning algorithm, called COM-Cooperative Hierarchical RL to communication decision. Mehta et al. (2005) introduced the multiagent shared hierarchy framework, which generalizes the MAXQ framework and allows selectively sharing subtask value functions across agents. They also developed a model-based average-reward RL algorithm for that framework. Shen et al. (2006) incorporated options in MAXQ decomposition and Mirzazadeh et al. (2007) extended MAXQ-Q (Dietterich, 2000) producing a new algorithm with less computational complexity. Mehta et al. (2008) presented the HI-MAT (Hierarchy Induction via Models and Trajectories) algorithm that discovers MAXQ task hierarchies by applying dynamic Bayesian network (Dean and Kanazawa, 1989) models to a successful trajectory from a source RL task.
Multiagent Reinforcement Learning
Learning in the presence of multiple learners can be viewed as a problem of a ※moving target§, where the optimal policy may be changing while the agent learns. Therefore, the normal definition of an optimal policy no longer applies, due to the fact that it is dependent on the policies of the other agents (Bowling and Veloso, 2002). In many cases, the aim is for a multiagent learning system to reach a balanced state, also known as equilibrium, where the learning algorithms converge to stationary policies. Often a stationary policy can be deterministic, however, there are situations where we look for stochastic policies. An example from game theory (Fudenberg and Tirole, 1991; Leyton-Brown and Shoham, 2008), where pure strategy (i.e., deterministic policy) equilibria do not exist, is the famous Rock-Paper-Scissors game. In this game, the solution is to converge to a mixed strategy (i.e., stochastic policy) equilibrium where the agents play each action with probability 1/3. Therefore, in multiagent learning settings it is sometimes beneficial for an agent to explicitly keep an estimate of its stochastic policy (as in actor-critic methods, Witten, 1977; Barto et al., 1983; Konda and Tsitsiklis, 2003; van Hasselt and Wiering, 2007; Degris et al., 2012; Fremaux et al.,∩ 2013) and update it accordingly during the course of learning. This estimate of the stochastic policy is used by the algorithms Policy Hill Climbing (PHC, Bowling and Veloso, 2002), which is an extension of Q-learning, and Win or Learn Fast-PHC (WoLF-PHC, Bowling and Veloso, 2002), which is an extension of PHC (see Section 3.3.3). The WoLF-PHC was empirically shown to converge in self-play to an equilibrium, even in games with multiple or mixed policy equilibria (Bowling and Veloso, 2002).
In this study, our purpose is to accelerate learning in multiagent environments where (i) continuous adaptation is required and (ii) the task has some structure. Our motivation comes from the observation that (i) multiagent RL (MARL) algorithms are good for addressing the nonstationarity that comes through the adaptation of the other agents, and (ii) hierarchical RL algorithms work by decomposing tasks into subtasks which can be reused and this provides an intuitive approach for scaling to more complex problems. Therefore, we ask the following question: can we combine the mechanism that makes a RL algorithm hierarchical and the mechanism that makes a RL algorithm work well in multiagent settings and create a single algorithm that has the advantages of both? To answer this question we explore a novel combination of the hierarchical method MAXQ with the MARL algorithms PHC and WoLF-PHC. These methods were selected due to their simplicity. To evaluate our approach we specifically engineer a structured multiagent environment (shown in Figure 3.2c) where convergence to a stationary policy is not possible and the focus is on quickly adapting to the changing behavior of the other agent(s). Moreover, we evaluate the performance of Q-learning, SARSA (Rummery and Niranjan, 1994), PHC, WoLF-PHC, MAXQ-Q (Dietterich, 2000), as well as the two newly created algorithms MAXQ-PHC and MAXQ-WoLF-PHC, in two single-agent taxi domains (Dietterich, 2000; Diuk et al., 2008) (shown in Figures 3.2a and 3.2b) and the multiagent extension of the taxi domain we created (shown in Figure 3.2c). This comparison is made in order to establish whether our approach has advantages over existing methods.
 reinforcement learning (RL) agent can be seen as an entity that has two components: (i) the policy/controller, which is a mapping from past observations to actions, and (ii) the learning rule, which modifies the policy towards better performance. A way to address large or continuous state-action spaces that need generalization is by representing the policy using parametric approximators. Feedforward artificial neural networks (ANNs) are known to be universal function approximators (Hornik et al., 1989; Park and Sandberg, 1991), while recurrent ANNs are considered as universal approximators of dynamical systems and can potentially represent arbitrarily complex mappings (Funahashi and Nakamura, 1993). For this reason, a particular form of direct policy search known as neuroevolution (NE, Floreano et al., 2008; Yao, 1999), i.e., the evolution of ANNs, has demonstrated remarkable performance over the years, even though it does not explicitly address problems such as partial observability or the exploration/exploitation tradeoff (Gomez et al., 2008). For example, in partially observable environments or in nonstationary ones (where the state transition function changes over time), NE can find near-optimal nonstationary policies in the form of recurrent ANNs. While recurrent connections can be used to create adaptive agents, adaptation can also be achieved with the evolution of plastic ANNs, where a learning rule changes the synaptic weights online (Risi et al., 2010). The evolution of adaptive ANNs, either recurrent or plastic, provides a promising approximate solution to the problem of finding optimal policies in general partially observable MDPs; a problem that is known to be undecidable (Madani et al., 1999).
In this chapter, we turn our focus from the policy to the learning rule itself considering the latter to be a mapping as well. This mapping can potentially take as input the observations, the actions, the rewards and the parameters of the policy, and outputs a modified policy with the goal to improve (or maintain) performance. Our major aim is to investigate the emergence of general and robust learning rule mappings that could potentially be used for various classes of environments or tasks (e.g., generalized environments; Whiteson et al., 2011), as well as transferring learning rules (as opposed to knowledge transfer) between related and potentially unrelated domains. The motivation behind this goal comes from the observation that families of RL rules from the RL literature can be used in a variety of tasks. For example, Q-learning (Watkins and Dayan, 1992) is a well-known family of RL rules that has been used in many domains.
Before even trying to tackle such a difficult problem, we first need to test whether promising approaches can be effective in simpler problems. More importantly though, we need to identify the parts or aspects that make the problem vague and resolve any ambiguities that might arise from them. For this reason, we outline below several choices we made before developing the approach presented in this work. These choices (and consequently, this study) reflect the starting point from which we set out to explore the potential realization of our goal. Concretely, this study investigates an approach where:
The policy is represented only by (plastic) ANNs, due to their universal approximation properties. Alternative policy representations (e.g., decision trees), whilst interesting, fall outside the scope of this work.
The rules are not only optimized towards specific policy ANN architectures (Yao, 1999), but they are also coupled with the initial state of the ANNs that they modify. An alternative would be to develop rules that are optimized towards any ANN (topology and weights); this could not be done, unless the rules are manually designed with some guarantee in mind or with some external knowledge injected into the problem.
The rules require only local synaptic information and as a result make only local changes. An alternative would be to create global rules; local rules, however, are more biologically plausible and could have the same expressive power with the added advantage of requiring less information.
The rules do not modify the structure of the policy ANN, but only make synaptic modifications (synaptic plasticity). An alternative would be to allow structural changes as well (structural plasticity). Algorithms that modify both the structure and the parameters of ANNs, e.g., cascade-correlation (Fahlman and Lebiere, 1990), could be viewed as offering both structural and synaptic plasticity to the ANN. In this regard, even optimization algorithms could be viewed as learning rules, however, we prefer to make a distinction between learning rules and optimization algorithms.
The rules are optimized using gradient-free methods and more specifically, evolutionary algorithms. Other gradient-free methods could be used, while an alternative would be to use gradient-based methods. Using the latter, however, it is not clear to us at this stage how to formulate a cost function, estimate the gradient and subsequently derive a learning rule that trains the learning rule.
Only the parameters of the rules are optimized, not their internal structure, i.e., the ※equation§ of the rule does not change during optimization. Optimizing both the structure and the parameters clearly offers a lot of flexibility to the models. A drawback, however, is an expanded search space. While the (continuous) parameter space is considered to be infinite, structure learning makes the space combinatorial. Moreover, if the structure is allowed to vary, adding neurons increases the number of parameters and consequently the dimensionality of the problem. Algorithms that modify both the structure and the parameters of ANNs using evolutionary algorithms are sometimes called Topology and Weight Evolving ANNs (TWEANNs, see for example Stanley and Miikkulainen, 2002), but such algorithms are not used in this study.
Different RL rules are evolved for different RL tasks, i.e., the rule is overfit to the task. This is a starting point for this work as mentioned above. The proposed approach needs to be evaluated in simpler problems before moving to more complicated ones.
Adaptive organisms need to have the ability to adjust their behavior in response to changes in their environment. Such behavioral plasticity is believed to be linked with changes in the neural circuitry that produces the behavior. These changes are likely to be caused by mechanisms that go beyond the classical neurotransmission (of excitation or inhibition), such as neural plasticity (Churchland and Sejnowski, 1992; Binder et al., 2009) and neuromodulation (Katz, 1999). Neural plasticity refers to the capacity of neural circuits for functional or organizational modifications due to previous activity or damage (Churchland and Sejnowski, 1992; Binder et al., 2009). For example, synaptic plasticity, i.e., the strengthening or weakening of synapses, is a major process that underlies learning and memory (Martin et al., 2000) and has been validated through neural recordings (for example, see Kandel and Tauc, 1965; Bliss and L?mo, 1973; Markram et al., 1997; Bi and Poo, 1998). Neuromodulation refers to the process where a small number of neurons can influence (modulate) the intrinsic properties of multiple synapses or neurons, through the diffusion of certain neurotransmitters known as neuromodulators (Katz, 1999; Marder and Thirumalai, 2002; Binder et al., 2009). Neuromodulation and neural plasticity can be complementary. For example, the neuromodulator dopamine is believed to play a major role in operant conditioning (Thorndike, 1911; Skinner, 1938) as it was found to encode a reward prediction error (RPE; for example, see Schultz, 1998) similar to temporal difference (TD) error in reinforcement learning (RL, Sutton and Barto, 1998).
In this chapter, we consider nonstationary environments where the reward function, i.e., the goal of the agent, changes over time. Whenever this happens, the agent needs to be capable of exploring different ways of behaving (i.e., different policies), not in a random manner, but rather in a structured, ordered manner. As mentioned above, a mechanism that might be used for adaptation is synaptic plasticity and has been successfully used with ANNs in the past (for example, see Floreano and Urzelai, 2000; Niv et al., 2002a,b; Stanley et al., 2003; Soltoggio et al., 2008; Risi and Stanley, 2010; Soltoggio and Stanley, 2012). It is difficult, however, to devise synaptic plasticity rules that create structured exploration behavior, and most of the work in this front relies on evolutionary methods that optimize the ANN controllers and/or plasticity rules.
In this study, we make a step towards creating a mechanism for ANNs that endows them with structured exploration behavior. This mechanism does not rely on synaptic plasticity, but rather on neuromodulation.
While neuromodulation can be used to gate plasticity and synaptic transmission, a growing number of studies provide evidence that supports the existence of other types of synaptic gating mechanisms, capable of regulating information flow between various sets of neurons (see Gisiger and Boukadoum, 2011, and references therein). Such mechanisms should not be thought of as simply interrupting information flow, as they can also act as permissive gates (Katz, 2003). For example, certain neurons from an area of the brain called the nucleus accumbens (NAcc) were found to implement this type of gating. These NAcc neurons are bistable, meaning that they exhibit oscillations between two discrete states: an ※up§ state (where the membrane potential is depolarized) during which the neuron generates action potentials (spikes), and a resting, ※down§ state (where the membrane potential is hyperpolarized) during which the production of action potentials is absent (O＊Donnell and Grace, 1995; Grace, 2000). They were found to be part of a gating mechanism that controls whether information from the prefrontal cortex (PFC) is allowed to pass through to the ventral pallidum and further, to the thalamus. More specifically, input from PFC neurons arrives at NAcc neurons, but only those that are in their up state allow the input to propagate forward. What modulates the state of the NAcc neurons is an extra input from the hippocampus (HPC). That is, only the NAcc neurons that are stimulated by HPC neurons enter their depolarized state and subsequently, fire upon receiving input from the PFC (Grace, 2000). For this reason, these neurons are said to implement a type of AND gate, since they fire only if they receive input from both the PFC and the HPC (Gisiger and Boukadoum, 2011).
Gruber et al. (2009) used multichannel recordings to investigate rats during spatial exploration of an operant chamber  , and during reward-seeking afterwards in the same chamber. They observed that during spatial exploration, the activities of neurons in the NAcc core, i.e., the inner part of the NAcc which is located within the basal ganglia (Gerfen and Wilson, 1996, p. 372), synchronized with the activity of HPC neurons; however, during reward-seeking, they instead synchronized with the activity of PFC neurons. This suggested that the NAcc core can dynamically select its inputs according to environmental requirements, as it is able to switch its synchronization in a task-dependent manner (Gruber et al., 2009). It has to be noted that the basal ganglia is the structure believed to be associated with action selection (Redgrave et al., 1999) and RL (Barto, 1995; Montague et al., 1996; Schultz et al., 1997). For example, Redgrave et al. (1999) proposed that the action selection problem of vertebrates is solved by a central ※switching§ mechanism that reside in the basal ganglia. It has also been recently suggested that the release of a peptide called ※substance P§ in the striatum (i.e., the primary input nucleus to the basal ganglia), allows for rapid switching between actions in action sequences (Buxton et al., 2015).
Apart from bistable neurons, which were found to be abundant in the cortex, other gating mechanisms have been observed in theoretical or experimental data, that feature inhibitory neurons or even oscillations (see Gisiger and Boukadoum, 2011, and references therein). It has also been demonstrated computationally that neural circuits can implement various logic gates such as NOT, Switch, XOR, and Flip-Flop (Vogels and Abbott, 2005). In all observations and models, certain ※gatekeeper§ circuits influence synaptic transmission (Gisiger and Boukadoum, 2011). As mentioned above, it has been observed that for NAcc neurons the gatekeepers originate in the HPC. For cortex neurons, experimental evidence suggests that the gatekeepers could originate in the cortex, thalamus and basal ganglia (Gisiger and Boukadoum, 2011). Gisiger and Boukadoum (2011) present a theoretical model where a copy of the gating signal produced by one gatekeeper circuit, can be fed as an input to another gatekeeper circuit (see Figure 5 in Gisiger and Boukadoum, 2011). A key observation in that model is the existence of two types of neural pathways: the first implements normal information processing, whereas the second is formed by the gating mechanisms and ※could play various roles§. They hypothesize that such interacting gating circuits could create sequences of gating events that are responsible for the production of structured behavior.
Gating mechanisms can be seen as implementing a type of ※on-off§ switch by allowing or interrupting communication between brain regions. Inspired by these gating phenomena in the brain, which seem to play a significant role in various processes such as working memory (Williams and Goldman-Rakic, 1995), attention (Usher et al., 1999), and decision making (Cisek et al., 2009), we ask whether we could design an abstract computational model that can be used for the purpose of adaptive behavior. For this reason, we adopt the artificial intelligence agenda of rational decision making (Russell and Norvig, 2003), where an agent tries to maximize its reward intake (Sutton and Barto, 1998). The agent is controlled by ANNs, as they are very well suited for simulating adaptive behavior, due to the possibility of implementing memory through recurrent connections, and learning through plasticity rules. In this chapter, we do not focus on learning behavior per se, but rather on behavior exploration. More specifically, a central hypothesis of this work is that once some general neural circuits are established for certain behaviors through possibly neural plasticity mechanisms (or other methods), neuromodulation alone can be used to switch these behaviors by selectively gating various pathways accordingly. We implement this gating mechanism by introducing a novel type of an artificial neuron we call ※switch neuron§ that can be used in ANNs. Instead of implementing an on-off switch for certain connections, this unit selects which one of its incoming connections is allowed to propagate forward by opening its gate, while closing the gate of all others. We assess the switch neurons in nonstationary association tasks (Section 5.4.1) and discrete T-maze problems (Section 5.4.2). We further show that interactions between switch neurons can implement optimal and deterministic exploration behavior, therefore, providing a possible explanation as to what can be done when such gating mechanisms are linked in a sequence (see Gisiger and Boukadoum, 2011).
Note that the switch neurons of this work should not be viewed in a strict biological sense, but rather in a functional sense. They are inspired by biological phenomena, but they are artificially constructed to perform certain computations. Thus, throughout this study, we use the word ※neuron§ for the switch neuron, but note that this is a purely artificial unit. If mechanisms similar to the switch neuron we present in this chapter exist in the brain, they could either be in the form of individual cells, population of cells, or groups of interconnected neurons.
Understanding the underlying mechanisms of learning and memory emerging from a complex dynamical system like the biological brain and building intelligent systems inspired by such mechanisms, serves as one of the greatest pursuits of modern scientific research. The ability to learn and cognition are not merely the products of isolated neurons, but the properties that emerge from the underlying dynamics of a complex network of neurons in the brain. Despite considerable progress in neuroscience, computational sciences, and artificial intelligence, our understanding of such processes in the brain or emulation of biological like intelligence remains vastly constrained. The constantly changing nature of the environment we live in has resulted in exquisite evolutionary manipulation of the nervous system, leading to the ability to process and generate challenging spatial and temporal information. Imagine a scenario, where you are driving down the highway and someone tells you, ＊take the left turn at the next junction＊. To solve this seemingly simple statement the brain needs to perform a complex set of computations, within inherent dependence on the temporal aspects of the statement and any subsequent events. Not only you need to understand the meaning of the sentence and words such as, ＊take＊ and ＊turn＊, but also be able to hold this information temporarily till you reach the next junction and can perform the corresponding behavior of turning ＊left＊. Such temporary storage of available stimuli for the purpose of information processing, is referred to as working memory or temporal memory.
As such, it is obvious that timing and memory are intricately related in the brain. This is inherent in the brains ability to perform complex temporal information processing tasks like speech processing, motor processing, music perception, decision making for goal-oriented behaviors, working memory storage and processing, etc. Given that the brain is not static, but a highly adaptive 1 system, which processes can enable the initiation and execution of such temporal memory guided behaviors from neural activity ? We make the hypothesis that a combination of neural plasticity, homeostatic and adaptation mechanisms coupled with the presence of feedback loops (recurrency) in the neural circuitry give rise to such actions. Based on this hypothesis, the main focus of this thesis is to answer the question: How can we model such adaptation for brain like temporal information processing that in turn lead to memory-guided behaviors? The primary objective being not only to create a computational model of neural circuitry with inherent storage and processing of time varying stimuli, but also to use the same model to generate robust sensory-motor outputs and short-term memory guided behaviors in artificial
intelligent systems.
In nature, animals are capable of efficiently encoding space and time required for the learning and structuring of motor and cognitive actions. Specifically the mammalian brain processes temporal information over time scales spanning 10 orders of magnitude: from the few microseconds used for sound localization, to daily, monthly and yearly rhythms to sleep-wake, menstrual and seasonal cycles (Buonomano et al., 2009). In between, on the scale of milliseconds to few minutes, complex forms of sensory-motor processing leading to behaviors like speech recognition, motor coordination, motor prediction, and decision making for goal-directed learning, takes place (Ivry and Spencer, 2004),(Mauk and Buonomano, 2004),(Buhusi and Meck, 2005), (Buonomano, 2007). As such, we focus on this timescale of information processing and behaviors. Within this timescale, a number of different brain areas have been implicated as the key machinery behind the neural representation of time (Maniadakis et al., 2014). Among these, some of the most relevant are cerebellar event timing (Ivry and Spencer, 2004); generalized magnitude processing for time, space, and number in the right posterior parietal cortex (Bueti and Walsh, 2009), (Oliveri et al., 2008); time integration for working memory in right prefrontal cortex (Lewis and Miall, 2006),(Smith et al., 2003); coincidence detection in the fronto-striatal circuitry (Hinton and Meck, 2004) and time cells in the hippocampus computing the relation of time and distance (Kraus et al., 2013).
Such a wide spread participation of different brain regions for temporal information processing clearly advocates the key role of temporal perception in the brain as well as the intricate relationship of the different timescales that constitute the various cognitive aspects like decision making, planing, action selection, memory and recall (Rao et al., 2001),(Taatgen et al., 2007).
On a functional level, it is known that neuronal systems can adapt to the statistics of the environment over these wide range of timescales (learning, memory and plasticity) (Tetzlaff et al., 2012b), but the mechanisms for doing so are still largely unknown. Therefore, there seems to be an essential relationship between processing of temporal information and how the brain deals with the various timescales and generate relevant behaviors. In Fig. 1.2 we provide a succinct, schematic overview of the different timescales of temporal perception in the brain and their relationship to observed physiological processes, memory, behaviors and learning paradigms. In
this thesis we will primarily focus on the timescale of milliseconds to a few minutes and the behaviors, memory and processes corresponding to this scale.
Complex behaviors like memory guidance (also called delayed responses) and goal directed action planning involving temporal memory (short-term storage in the timescale of milliseconds to minutes) and learning can be observed not only in higher order mammals but also in insects.
For instance, cockroaches use their cercal filiform hairs (wind sensitive hairs) to elicit so called §wind-evoked escape behavior§ (Beer and Ritzmann, 1993); i.e., they turn and then run away from a wind puff to their cerci generated by a lunging predator. This action perseveres slightly longer than the stimulus itself. Once the action has been activated, it will be performed even if the activating stimulus is removed to ensure safely escaping from the attack. Thus, this action reflects not only a reactive response but also a simple memory-guided behavior (transiently  active internal drive) (Arkin, 1998). More complex examples can be found in mammals such as the one observed in the behavior of cats (McVea and Pearson, 2006). They use temporal memory of visual inputs in order to drive their front legs at the appropriate time to step over or around obstacles in their path at a time the obstacle is already invisible. There is also a unique form of predictive memory to guide the hind legs over obstacles that have already been stepped over by the forelegs. This can also be regarded as some form of predictive or forward modeling behavior (Kawato, 1999) which is a crucial aspect of temporal information processing. This type of processing can also be seen to occur even in invertebrates, allowing them to climb over large gaps almost twice the size of their body lengths (Blaesing and Cruse, 2004). Other sophisticated navigation and foraging studies with rodents have shown that they not only use spatial memory with reward learning, to navigate mazes and find food (Tolman, 1932),(Tolman and Honzik, 1930), (Olton and Samuelson, 1976), but they can also develop temporally structured behaviors, demonstrating some form of temporal memory to discriminate between long and short time intervals (Gouvea et al., 2014). As depicted in Fig. 1.2 such short-term memory guided behaviors are intricately related to the brains ability to process time or time varying patterns of activity. Furthermore, in order to understand such temporal processing, it is important to put it in a closed-loop perspective (Fig. 1.1). As such, in this thesis we use network models with inherent time processing that can lead to similar temporal memory guided behaviors, as evaluated on artificial agents as
abstractions of their biological counterparts (Arkin, 1998). Given that learning and memory is ultimately a consequence of a highly plastic brain (Dudai, 2004),(Martin et al., 2000), it is obvious that it should play a key role in the underlying temporal information processing. In the next section, we broadly explore the various facets of neuronal plasticity and put it in perspective of this thesis.
The inherently malleable and constantly adaptive nature of the nervous system was clearly noted by Cajal (1904) when he predicted that with the acquisition of new skills the brain changes via rapid reinforcements of pre-established organic pathways, which in turn lead to the formation of new pathways (Pascual-Leone et al., 2005). Although Cajal specifically mentioned neural pathways (synapses), recent experimental and theoretical studies have confirmed that nearly every brain region demonstrates such remarkable and flexible  reorganization. Widespread structural and functional alterations occur by processes of modulation of strength of synaptic connections between neurons (Abbott and Nelson, 2000), addition and deletion of connections (Holtmaat and Svoboda, 2009), changes in the intrinsic excitability of single neurons (Zhang and Linden, 2003), as well as, balancing homeostatic adaptation processes (Turrigiano and Nelson, 2004). Furthermore, the seminal studies of Merzenich and Kaas (Merzenich et al., 1983), (Merzenich et al., 1984) demonstrated that topographic reorganizations of cortical maps can be realized in an experience-dependent manner through neural plasticity, thus,  ighlighting the central role of brain plasticity in a lifelong learning process. Specifically, at the behavioral level, such adaptive mechanisms in the brain provides it with the crucial ability to learn and deal with environmental changes, capture and retain specific memories, process information critical for speech and motor functionality, etc. In general, neural plasticity can be divided into two broad types, namely (i) synaptic plasticity and (ii) homeostatic plasticity. As the main motivation behind this thesis is not to understand the biophysical machinery behind such plasticity mechanisms, but to use them as biological inspiration to adapt network models in order to deal with time varying stimuli and the processing of temporal information; in the next two subsections we briefly introduce the basic ideas of these two types of plasticity in the brain.
Synaptic plasticity can be defined in simple terms as the process of strengthening or weakening of synapses connecting different neurons, facilitating the transmission of electro-chemical signals (Citri and Malenka, 2007). Specifically, it refers to the activity-dependent modification of the strength or efficacy of synaptic transmission at pre-existing synapses, caused by the changes in the amount of neurotransmitter molecules at the synapse, or by the fluctuation in conduction of post-synaptic receptors. Synaptic plasticity is thought to play key roles in the early development of neural circuitry (termed as cell assemblies) (Hebb, 1949), (Dudai, 2004) and experimental evidence suggests that impairments in synaptic plasticity mechanisms contribute to several prominent neuropsychiatric disorders (Lau and Zukin, 2007). The encoding of external and internal events as complex, spatio-temporal patterns of activity within large ensembles of neurons is directly influenced by this type of plasticity of the pattern of synaptic weights that connect individual neurons comprising such ensembles or neuronal circuits. This forms the direct basis for the plasticity and memory hypothesis (Martin et al., 2000), (Martin and Morris, 2002), suggesting that activity-dependent changes in the strength of connections between neurons plays the key role towards the mechanism by which new information is stored or memory traces are encoded in the central nervous system.
The simplest theoretical foundations of such an idea was postulated by Donald Hebb, as early as 1940, where in he proposed that associative memories are formed in the brain by a process of synaptic modification that strengthens connections when presynaptic activity correlates with postsynaptic firing (Hebb, 1949) (Fig. 1.3). This has been popularly termed as Hebbian plasticity i.e. ＊cells that fire together, wire together＊ (Carla Shatz, 1992). The first experimental validation of Hebbian type of plasticity (showing an increase in synaptic efficacy) came from Bliss and Lomo in 1973 (Bliss and Lomo, 1973), with the report of the phenomenon of long-term potentiation (LTP). Subsequently in the year 1977, Lynch et al. (Lynch et al., 1977) found a reduction in synaptic efficacy called long-term depression (LTD). Later it was also noted that both LTP and LTD could be observed at the same synapse (Dudek and Bear, 1992). Unlike the basic Hebbian formulation of correlations between neuron firing activity, an influence of temporal signal order on plasticity was proposed by (Gerstner et al., 1996) and then experimentally validated by the findings of (Markram et al., 1997), (Magee and Johnston, 1997), (Levy and Steward, 1983), (Bi and Poo, 1998). As such this type of plasticity has been termed as spike-timing dependent plasticity (STDP). In this thesis, we model synaptic plasticity based on the correlations between the firing rate of the pre- and post-synaptic activity of the neuron, following the spirit of the basic Hebbian conjecture without delving deep into molecular or biophysical details (Dayan and Abbott, 2003). Furthermore depending on the type of learning paradigm, specific modifications of the original Hebbian learning rule will be used (see Tab. 1.1). We will primarily consider supervised and reinforcement learning in this thesis.
The word homeostasis or homeostatic stems from the Greek word homeo meaning ＊unchanging＊ and is a generic concept guaranteeing the ability of a system to reach the same internal state as prior to the application of an external perturbation. In neuronal systems homeostatic plasticity refers to the capacity of neurons and synapses to regulate their own excitability relative to the network activity, usually in response to an imbalance or external disturbances. It can be seen to balance the inherently unstable nature of purely Hebbian plasticity (correlations of pre- and post-synaptic activity) by modulating the activity of the synapse (Davis, 2006) or the properties of voltage-gated ion channels (Zhang and Linden, 2003). This regulates the total synaptic drive to a neuron and/or maintain the long-term average firing rate of a neuron at a critical level and therefore allows the stable operation of neuronal networks.
The two principle types of homeostatic mechanisms are namely synaptic scaling (SC) (Fig. 1.4 (a)) and intrinsic plasticity (IP)(see Fig. 1.4 (b)). SC is a mechanism that regulates the total synaptic drive received by a neuron while maintaining the relative strength of synapses established during learning (Turrigiano et al., 1998), (Turrigiano and Nelson, 2004). It has been found in several brain areas including the neocortex (Turrigiano et al., 1998), Hippocampus (Burrone et al., 2002) as well as at inhibitory synapses (Kilman et al., 2002). IP, on the other hand, is a homeostatic mechanism leading to the persistent modification of a neuron＊s excitability, mediated by the properties of ion channels in the neuron＊s membrane. It was noted that such intrinsic changes in a neuron＊s electrical properties, might function as part of the engram itself, or as a related phenomenon such as a trigger for the consolidation or adaptive generalization of memories (Zhang and Linden, 2003). Changes in neuronal excitability via IP lead to differentoutputs for the same synaptic drive. Furthermore it was experimentally observed that IP tends to reduce the intrinsic excitability of a neuron during long periods of stimulation and increase excitability during activity deprivation (Desai et al., 1999), (Zhang and Linden, 2003), (van
Welie et al., 2004) (Fig. 1.5 (a) and (b)).
In our network models, we primarily consider intrinsic plasticity at single neurons level and see its influence on homeostatic regulation as well as learning. Evidence that IP accompanies, and may help mediate, learning has been obtained in both invertebrates (Drosophilia - (Daoudal and Debanne, 2003), Aplysia - (Brembs et al., 2002) etc.) and mammals (Oh et al., 2003), (Saar et al., 1998), (Brons and Woody, 1980), specially with associative conditioning experiments (more details in (Zhang and Linden, 2003)). Furthermore along with its role in homeostasis, IP has been implicated directly in the formation of memory engrams (Gandhi and Matzel, 2000).
From an information transmission perspective (Fig. 1.5 (c) and (d)), IP can be seen to allow a neuron to exploit its full dynamic range of firing rates when coding for a given set of inputs and achieving exponential firing rate distributions in cortical neurons (Stemmler and Koch, 1999). It was also linked to information maximization and energy efficient coding at a single neuron level (Vincent et al., 2005). In this thesis, we model IP based on the same principles of information maximization (Triesch, 2007) for a recurrent network model which has shown to induce robust homeostatic effects on network dynamics (Steil, 2007), (Schrauwen et al., 2008), as well as increased performance for information processing and memory (Verstraeten et al., 2007), (Dasgupta et al., 2012), (Dasgupta et al., 2013a). Inspired by these approaches, in this thesis we take such an information-centric view of IP, such that neurons are enabled to maximize information transfer between its input and output, as well as matching the statistics of some optimal output distribution by modulating their activation functions in an input-dependent manner.
In the previous section we broadly discussed plasticity in biological brains which forms the basis of learning in living organisms. However the question of how do we model such learning? still remains unclear. In order to answer this question, we take a connectionists approach. Whereby we model the actual behavioral phenomenon as the emergent process or learning outcome of the dynamics of interconnected networks of simple units (artificial neurons). This type of network models have been termed as artificial neural networks where in, the fundamental computational unit of such networks although called neurons, they only very broadly resemble their biological counterparts. Here we typically consider artificial rate-coded neurons which compute their output as a non-linear transformation (activation function) of the sum of weighted inputs (incoming synaptic connections) it receives. 
There are two broad classes of neural networks that have been used in the past for handling time-varing input signals and solving specific temporal problems. These are namely feed-forward networks (Fig. 1.6 (b)) and recurrent networks (Fig. 1.6 (c)). Due to the lack of reverberating activity and a one directional flow of information in feed-forward networks, they have mostly been used to process non-temporal problems. Only in some cases, specific adaptations allowed feed-forward networks to incorporate in their structure an explicit representation of time (Elman and Zipser, 1988). However such explicit representation is computationally expensive as well as biologically unrealistic (Elman, 1990). Recurrent neural networks (RNN) on the other hand form the natural candidates for temporal information processing, due to their inherently dynamic nature and the existence of directed cycles inside the network, which allows reverberation of activity. As such, throughout this thesis we will concentrate on this type of neural network model. The first studies of RNNs started with the seminal works of Hopfield in 1982 and 1984 (Hopfield, 1982), (Hopfield, 1984), although Wilson and Cowan (Wilson and Cowan, 1972) originally developed the recurrent network in a biological context, a few years earlier. Using a RNN with a restricted topology of symmetric synapses, Hopfield demonstrated how to embed a large number of stable attractors into the network by setting the strengths of synapses to specific values. Trained with Hebbian plasticity this type of network could display auto-associative memory properties. However it did not consider time-varying input stimuli to drive the network, and it had very limited applicability to temporal problems. Despite the natural ability of RNNs to encode time, a universal computing ability and the subsequent development of a number of learning algorithms like Real-Time Recurrent Learning (Williams and Zipser, 1989), and Back- Propagation Through Time (Rumelhart et al., 1988), (Werbos, 1990), their usage on complex temporal problems remained restricted for a long period of time. This was largely due to the difficulty in training (Bengio et al., 1994) these networks. Furthermore, although the short-term storage of information is critical towards the ability of the brain (or a recurrent network model) to perform cognitive tasks like planning and decision making (Ganguli et al., 2008), previous models considered that the neural substrate for such memory arose from persistent patterns of neural activity, that were stabilized through reverberating positive feedback in the RNNs (Mongillo et al., 2008), (Seung, 1996) or at the single cell (Loewenstein and Sompolinsky, 2003). However, such simple attractor mechanisms are inherently incapable of remembering sequences of past temporal inputs.
Over the last decade, an alternative idea has tried to circumvent the training problem as well as the temporal memory issue, by suggesting that an arbitrary recurrent network could store information about recent input sequences in its transient dynamics, even if the network does not formally possess information-bearing stable attractor states. This was simultaneously introduced, both from a neurobiological perspective - Liquid state machines (Maass et al., 2002) and a machine learning perspective - Echo state networks (Jaeger, 2001a), (Jaeger and Haas, 2004). In this setup, a randomly structured RNN is used as a high dimensional projection space (＆reservoir＊) that transforms any time varying input signal into a spatial representation. Learning occurs only at the level of downstream readout networks, which can be trained to instantaneously extract relevant functions of past inputs from the reservoir, in order to guide future actions and solve spatio-temporal tasks. This type of RNN has been popularly termed as ＊Reservoir Computing＊ (RC) (Luko’ sevi’ cius and Jaeger, 2009). The basic idea of computation in a RC is analogous to the surface of a liquid. Even though this surface has no attractors, save the trivial one in which it is flat, transient ripples on the surface can nevertheless encode information about past objects that were thrown in (Ganguli et al., 2008). This provides the inherent property of fading memory (Jaeger, 2001b), (Boyd and Chua, 1985) crucial for temporal information processing. At each time point, the reservoir network combines the incoming stimuli with a volley of recurrent signals containing a memory trace of recent inputs. In general, for a network with N neurons, the resulting activation vector at any discrete time t, could be regarded as a point in a N-dimensional space or manifold. Over time, these points form an unique pathway (in an input or context-dependent manner) through this high-dimensional state space, also referred to as a §neural trajectory§. The readout layer can then be trained, using supervised learning techniques, to map different parts of this state space to some desired outputs. As a result, this same concept has also been referred to as transient dynamics (Rabinovich et al., 2008) or computing with trajectories (Buonomano and Maass, 2009). This idea of computing with neural trajectories is further exciting considering that, although there is some evidence that in higher-order cortical areas simple fixed-point attractors play a part in working memory (Goldman-Rakic, 1995),(Wang, 2001), few data suggest that they contribute to the pattern recognition of complex time-varying stimuli. Thus, it is possible that in early cortical areas discrimination of temporal signals could be extracted from such high dimensional neural trajectories.
Theoretically using the Stone-Weierstrass theorem (Stone, 1948), it can be proven that such liquid or reservoir computing networks can behave like universal function approximators (Maass et al., 2004), and can approximate any dynamical system under fairly mild and general assump- tions (Funahashi and Nakamura, 1993). This coupled with its ability to inherently represent time (Buonomano and Maass, 2009), makes such RNNs a suitable candidate for modeling of complex spatio-temporal tasks. They can display arbitrarily complex dynamics, including regular stable dynamics (Fig. 1.7 (c)), limit cycles (Fig. 1.7 (d)), as well as chaos (Fig. 1.7 (e)). Reservoir networks have been previously successfully applied for chaotic time-series prediction and signal correction (Jaeger and Haas, 2004), (Wyffels et al., 2008), (Wyffels and Schrauwen, 2010); speech recognition (Triefenbach et al., 2010); robot learning (Hartland and Bredeche, 2007), (Kuwabara et al., 2012); epileptic seizure detection (Buteneers et al., 2009), brain-machine interface applications (Sussillo et al., 2012) etc. Despite the apparent success in machine learning applications, the application of reservoir networks to more complex temporal-processing tasks has been lim- ited due to the large number of free parameters in the network, limited robustness to noise in
reservoir activity, effect of different non-linearities activation functions on the temporal memory capacity, as well as a largely non-plastic, non-adaptive recurrent layer. Specifically, just simply creating a reservoir at random is greatly unsatisfactory. Although it seems obvious that, when addressing specific modeling tasks, a specific reservoir design that is adapted to the task will lead to better results than a naive random creation, adaptation in RC has been a difficult problem. Most studies of adaptation in reservoir networks
in order to deal with these problems has been restricted to evolutionary learning strategies (Bush and Anderson, 2005), (Jiang et al., 2008), costly gradient decent methods (Jaeger et al., 2007), specific topologies for recurrent layer (Jarvis et al., 2010), (Xue et al., 2007), or mostly by careful empirical evaluations or manual design (Luko’ sevi’ cius and Jaeger, 2009). In 2009, Sussillo and Abbott (Sussillo and Abbott, 2009) introduced the ＊FORCE＊ learning algorithm which allowed a generic reservoir network working in the chaotic domain to be trained for complex time-series modeling tasks. In further extensions, they showed that using feedback from the
readout layer, it was possible to learn both recurrent as well as recurrent-to-readout weights (Sussillo and Abbott, 2012). Although this allowed for some level of plasticity in the network, no significant gain in performance was observed. More recently, Laje and Buonomano (Laje and Buonomano, 2013) were able to achieve coexisting stable and chaotic trajectories in a ratebased RNN model (Sussillo and Abbott, 2009) when the recurrent connections were tuned using a supervised plasticity rule, called ＊innate＊ learning. Using the concept of dynamic attractors, they demonstrated the ability of the network to deal with perturbations or noise. However, the model still remains strictly computational with limited application to complex spatio-temporal tasks (similar to the machine learning problems tested with non-adaptive reservoirs) or generating memory-guided cognitive behaviors.
From the perspective of information processing in the brain, extension of RNNs with the principles of self-organization is crucial as it constitutes the basic computational units in the cortex (Douglas and Martin, 2004). As such it is imperative to understand the interaction of different plasticity mechanisms in the brain and how they can lead to self-organization of recurrent network models, as well as improve the performance of non-adaptive, static reservoir networks. In the computational neuroscience community, only few attempts have been made in this direction in a successful manner (Lazar et al., 2007), (Lazar et al., 2009), (Toutounji and Pipa, 2014)
showing a self-organized network via the interaction of plasticity and homeostatic mechanisms.
However they have typically considered simplified binary neuron models with specific K-winner take all network topology, as well as restricted the computation of the reservoir network as linear classifiers without the requirements for cognitively relevant temporal processing. As such, there exists a large gap between the results obtained from the computational neuroscience approaches to RNN modeling as compared to the previously discussed machine learning based approaches or models. In this thesis, we primarily bridge this gap by introducing novel homeostatic mechanisms and adaptation of the RNN in an information-centric manner, which when coupled with synaptic plasticity can not only achieve a biologically plausible, temporal information processing model, but also provide superior performance in cognitively based spatio-temporal behaviors as compared to the state of the art with non-plastic networks.
Walking animals show diverse locomotor skills to deal with a wide range of terrains and environments. These involve intricate motor control mechanisms with internal prediction systems and learning (Huston and Jayaraman, 2011), allowing them to effectively cross gaps (Blaesing and Cruse, 2004), climb over obstacles (Watson et al., 2002), and even walk on uneven terrain (Pearson and Franklin, 1984), (Cruse, 1976). These capabilities are realized by a combination of biomechanics of their body and neural mechanisms. The main components of the neural mechanisms include central pattern generators (CPGs), internal forward models, and limb-reflex control systems. The CPGs generate basic rhythmic motor patterns for locomotion, while the reflex control employs direct sensory feedback (Pearson and Franklin, 1984). However, it is argued that biological systems need to be able to predict the sensory consequences of their actions to be capable of rapid, robust, and adaptive behavior. As a result, similar to the observations in vertebrate brains (Kawato, 1999), insects can also employ internal forward models as a mechanism to predicts their future (predictive feedbacks) state given the current state (sensory feedback) and the control signals (efference copies), in order to shape the motor patterns for adaptation (Webb, 2004).
In order to make such accurate predictions of future actions to satisfy changing environmental demands, the internal forward models (Fig. 4.1) needs memory of previous sensory-motor information. However, given that, such motor control happens on a very fast timescale, keeping track of temporal information is integral to such very short-term memory processes. Reservoir based RNNs (Maass et al., 2002), (Sussillo and Abbott, 2012) with their intrinsic ability to deal with temporal information and fading memory of sensory stimuli, thus provides the perfect platform to model such internal predictive mechanisms. Therefore we design SARN (Dasgupta et al., 2013a) (chapter 2) to act as the forward models that can work in conjunction with other neural mechanisms for motor control and generate complex adaptive locomotion in an artificial walking robotic system. Specifically, by exploiting the recurrent layer of our model it is possible to achieve complex motor transformations at different walking gaits, which cannot be achieved by currently existing simple forward models employed with walking robots (Manoonpong et al., 2013b), (Dearden and Demiris, 2005), (Schr?der-Schetelig et al., 2010). We present for the first time a distributed forward model architecture using six SARN-based forward models on a hexapod robot, each of which is for sensory prediction and state estimation of each robot leg. The outputs of the models are compared with foot contact sensory signals (feedback) and the differences between them are used for motor adaptation. This is integrated as part of the neural mechanism framework consisting of 1) central pattern generator-based control for generating basic rhythmic patterns and coordinated movements, 2) the reservoir forward models and 3) searching and elevation control for adapting the movement of an individual leg to deal with different environmental conditions.
Associative learning by way of conditioning, forms the main behavioral paradigm that drives goal-directed decision making in biological organisms. Typically, this can be further classified into two classes, namely, classical conditioning (or correlation-based learning) (Pavlov, 1927) and operant conditioning (or reinforcement learning) (Skinner, 1938) . In general, classical conditioning is driven by associations between an early occurring conditional stimulus (CS) and a late occurring unconditional stimulus (US), which lead to conditioned responses (CR) or unconditioned responses (UR) in the organism (Freeman and Steinmetz, 2011), (Clark and Squire, 1998). The CS here acts as a predictor signal such that, after repeated pairing of the two stimuli, the behavior of the organism is driven by the CR (adaptive reflex action) at the occurrence of the predictive CS, much before the US arrives. The overall behavior is
guided on the sole basis of stimulus-response (S-R) associations or correlations, without any explicit feedback in the form of rewards or punishments from the environment. In contrast to such classically conditioned reflexive behavior acquisition, operant conditioning provides an organism with adaptive control over the environment with the help of explicit positive or negative reinforcements (evaluative feedback) given for corresponding actions. Over sufficient time, this enables the organism to respond with good behaviors, while avoiding bad or negative behaviors. As such within the computational learning framework, this is usually termed reinforcement learning (RL) (Sutton and Barto, 1998). At a behavioral level, although the two conditioning paradigms of associative learning appear to be distinct from each other, they seem to occur in combination as suggested from several animal behavioral studies (Rescorla and Solomon, 1967), (Barnard, 2004), (Dayan and Balleine, 2002).
Behavioral studies with rabbits (Lovibond, 1983) demonstrate that the strength of operant responses can be influenced by simultaneous presentation of classically conditioned stimuli. This was further elaborated upon in the behavior of fruit flies (Drosophila), where both classical and operant conditioning predictors influence the behavior at the same time and in turn improve the learned responses (Brembs and Heisenberg, 2000). On a neuronal level, this relates to the interaction between the reward modulated action selection at the basal ganglia and the correlation based delay conditioning at the cerebellum. Although the classical notion has been to regard the basal ganglia and the cerebellum to be primarily responsible for motor control, increasing evidence points towards their role in non-motor specific cognitive tasks like goal-directed decision making (Middleton and Strick, 1994),(Doya, 1999). Interestingly, recent experimental studies (Bostan et al., 2010), (Neychev et al., 2008) show that the the basal ganglia and cerebellum not only form multi-synaptic loops with the cerebral cortex, but, two-way communication between the structures exist via the thalamus Fig. 5.1 A) along with substantial disynaptic projections to the cerebellar cortex from the subthalamic nucleus (STN) of the basal ganglia and from the dentate nucleus (cerebellar output stage) to the striatum (basal ganglia input stage) (Hoshi et al., 2005). This suggests that the two structures are not separate performing distinct functional operations (Doya, 2000a), but are linked together forming an integrated functional network. Such
integrated behavior is further illustrated in the timing and error prediction studies of (Dreher and Grafman, 2002) showing that the activation of the cerebellum and basal ganglia are not specific to switching attention, as previously believed, because both these regions were activated during switching between tasks as well as during the simultaneous maintenance of two tasks.
Based on these compelling evidences we formulate the neural combined learning hypothesis, which proposes that goal-directed decision making occurs with a parallel adaptive combination (balancing) of the two learning systems (Fig. 5.1 B) to guide the final action selection. As evident from experimental studies (Haber and Calzavara, 2009), the thalamus potentially plays a critical role in integrating the neural signals from the two sub-networks while having the ability to modulate behavior through dopaminergic projections from the ventral tagmental area (VTA)(Varela, 2014), (Garc∩?a-Cabezas et al., 2007). The motor thalamic (Mthal) relay nuclei, specifically the VA-VL (ventral anterior and ventral lateral) regions receive projections from the basal ganglia (inputs from the globas pallidus) as well as the cerebellum (inputs from the dentate nucleus) (Jones et al., 1985), (Percheron et al., 1996). This can be further seg-
regated with the ventral anterior and the anterior region of the ventrolateral nucleus (VLa) receiving major inputs from the globus pallidus internus (GPi), while the posterior region of the ventrolateral nucleus (VLp) receives primary inputs from the cerebellum (Bosch-Bouju et al., 2013). Recent studies using molecular markers were able to distinguish the VA and VL nuclei in rats (Kuramoto et al., 2009), which had hitherto been difficult and were considered as a single overlapping area as the VA-VL complex. Interestingly, despite apparent anatomical segregation of information in the basal ganglia and cerebellar territories, similar ranges of firing rate and movement related activity are observed in the Mthal neurons across all regions (Anderson and Turner, 1991). Furthermore some experimental studies based on triple labeling techniques found zones of overlapping projections, as well as interdigitating foci of pallidal and cerebellar labels, particularly in border regions of the VLa (Sakai et al., 2000). In light of these evidences, it is plausible that the basal ganglia and cerebellar circuitries not only form an integrated functional network, but their individual outputs are combined together by a subset of the VLa neurons which in turn project to the supplementary and pre-supplementary motor cortical areas (Akkal et al., 2007) responsible for goal-directed movements. We envision that such a combined learning mechanism may be driven by reward modulated heterosynaptic plasticity (neuromodulation by way of dopaminergic projections) at the thalamus.
In this study, input correlation learning (ICO)in the form of a differential Hebbian learner (Porr and W?rg?tter, 2006), was implemented as an example of delay conditioning in the cerebellum, while a self-adaptive reservoir network (Dasgupta et al., 2013a) (chapter 2) based continuous actor-critic learner (Doya, 2000b) was implemented as an example of reward based conditioning in the basal ganglia. Taking advantage of the individual learning mechanisms, the combined framework can learn the appropriate goal-directed control policy for an agent 1 in a fast and robust manner outperforming the singular implementation of the individual components.
Although there have been a number of studies which have applied the two different conditioning concepts for studying self-organizing behavior in artificial agents and robots, they have mostly been applied separately to generate specific goal-directed behaviors (Prescott et al., 2006), (Morimoto and Doya, 2001), (Hofstoetter et al., 2002), (Manoonpong et al., 2007), (Verschure and Mintz, 2001). In our previous work (Manoonpong et al., 2013a) we motivated a combined approach of the two learning concepts on a purely algorithmic level without any adaptive combination between the two. To the best of our knowledge, in this paper we present for the first time a biologically plausible approach to model an adaptive combination of the cerebellar and basal ganglia learning systems, where they indirectly interact through sensory feedback . In this manner they work as a single functional unit to guide the behavior of artificial agents. Furhtermore, with the use of the reservoir model for basal-ganglia circuitry, we show that it clearly outperforms earlier feed-forward models for reward learning, specifically in decision making situations with dependence on memory of past sensory inputs. We test our neural combined learning hypothesis within the framework of goal-directed decision making using a simulated wheeled robot situated in environments of increasing complexity designed as part of static and dynamic foraging tasks (Sul et al., 2011). Our results clearly show that the proposed mechanism enables the artificial agent to successfully learn the task in the different environments with changing levels of interaction between the two learning systems. Although we take a simplified approach of simulated robot based goal-directed learning, we believe our model covers a reasonable level of biological abstraction that can help us understand better, the closed-loop
interactions between these two neural subsystems as evident from experimental studies, display the use of reservoir based models from within the paradigm of reward learning (as opposite to the standard supervised principles) and also provide a computational model of such combined learning behavior which has hitherto been missing.
We now give a brief introduction to the neural substrates of the cerebellum and the basal ganglia with regards to classical and operant conditioning. Using a broad high-level view of the anatomical connections of these two brain structures, we motivate how goal-directed behavior is influenced by the respective structures and their associated neuronal connections. The individualcomputational models with implementation details of the two interacting learning systems are then presented in the Materials and methods section followed by results and discussion.











