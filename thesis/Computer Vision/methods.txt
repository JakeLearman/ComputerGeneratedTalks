The first step in creating accurate tridimensional models from Computer Vision photogrammetry is to
take good photos. Choosing the right camera and accessory equipment is essential to ensure that clear 
and properly colored images are captured. Although there is a range of acceptable solutions from which 
archaeologists can choose, the best results depend heavily on the equipment. (Zhukovsky, Kuznetsov, and 
Olkhovsky, 2013) discussed constraints in underwater photography, which include optical distortion and 
optical noise.

Optical distortion is generated when light passes through media of different densities, such as water 
and then the air in the underwater camera housing. Refraction causes distortion in images; it also 
narrows the coverage area of a single photo (Gietler, 2009). To minimize optical distortion, the best 
possible solution is a hemispheric dome-port (Zhukovsky, Kuznetsov, and Olkhovsky, 2013). The hemispherical 
shape of the dome allows light to enter the lens thought the underwater housing perpendicularly; therefore, 
it can minimize the refraction caused by the different densities of the water and air.

Optical noise can be minimized by the use of wider-angle lenses (focal length of 11mm – 18mm). Cameras with 
wider-angle lenses can get closer to the subject while keeping a wider coverage area. Simply put, an 11 mm 
focal length lens can capture twicethe area of a 22 mm focal length lens (Fig. 2-1). In other words, a camera 
with an 11 mm lens can be used at half the distance from the object as the same camera with a 22 mm lens, 
while covering the same area. This is an important factor in capturing crisp images for successful photogrammetry 
because underwater visibility introduces limitations for underwater photography and photogrammetry. Using a 
wide-angle lens minimizes the capture of optical noise, while still covering a large area, thus minimizing the 
number of photos required for effective Computer Vision Photogrammetry.

Another factor that affects the size of the coverage area and quality of the images is the sensor size of the 
camera (Crisp, 2013). Larger sensors can collect more pixel data (light information) and can cover a wider area 
in the same way as wider-angle lenses.

Technically speaking, underwater photogrammetry can be done by any type of camera that can be operated at the 
desired depth in water; nonetheless, more professional, or expensive, cameras tend to have larger sensor sizes 
and more precise exposure-control over the photos (Fig. 2-2). This is one of the main reasons that Digital Single 
Lens Reflex cameras (DSLRs) are recommended for underwater photography and photogrammetry.

Most manufactured DSLRs are categorized as either professional level or advanced-amateur level. As a result, 
DSLRs are more expensive than compact cameras. Another benefit of DSLRs is that they have greater options for 
lens types and accessory equipment, including underwater camera housings and strobe lights. Most manufactured 
compact cameras tend to come equipped with fixed-lenses which are not interchangeable, and the minimum focal 
length of those fixed lens are, in most cases, around 24 mm. Thus, DSLR cameras with larger sensor sizes and wider 
lens options are recommended for use in underwater photogrammetry.

Table 2-1 displays recently manufactured camera models of Canon, Nikon, and Sony, and their approximate retail 
sales price as indicated by their official company web pages (as of August 2015). The models in the table were 
selected based on availability of compatible underwater housings; in other words, only camera types that have 
available underwater housings manufactured by well-known companies (as of August 2015) were listed on Table 2-1.
 
The underwater housing companies selected by the author were Ikelite, Equinox Underwater Housings, Nauticam, Sea 
and Sea, and Aquatica. Tables 2-2 and 2-3 indicate coverage areas of one photo by each sensor size, based on 
different focal lengths, lens types, and distance from the object. In short, these two tables show sizes of 
coverage areas based on equipment and circumstance. For instance, when an advanced amateur camera with APS-C 
sensor size equipped 16 mm focal length lens was used from 1.5 m away from the subject, a photo image can cover 
2.20 m horizontally, and 1.47 m vertically.The creators of Agisoft PhotoScan recommend that two consecutive photos 
have an 80% overlapping area with the forward image, and a 60% overlapping area with the adjacent image for 
successful photogrammetry coverage (Agisoft LLC, 2014). This 80% overlap results in each specific location 
appearing in at least four separate photographs for successful photogrammetry coverage. Based on this 
recommendation, the values of Tables 2-4 and 2-5 display the progression of camera positions from the previous 
photo location to the following one. For instance, when a camera has a full-frame sensor size with a 24 mm focal 
length lens, and photos are taken 3.0 m away from the subject, photos are recommended to be taken every 60 cm 
forward, with 180 cm side overlap. Lastly, Tables 2-6 and 2-7 show the total recommended number of photos required 
to provide proper image overlap (80% forward and 60% side). For example, when a camera with an APS-C sensor 
equipped with an 11 mm focal length lens is employed at 1.0 m distance to cover a 5 m x 5 m area, 50 photos 
are required to cover the entire area with the recommended overlap. The tables above can be used as guidelines 
when choosing cameras and lenses based on the unique conditions and requirements of a given archaeological site, 
and the characteristics under which a camera is expected to be used during its lifetime. Archaeologists must choose 
cameras and related equipment based on cost, available lenses, accessories, and water-visibility of the 
archaeological sites to be recorded. The indicated total number of photos must be considered in order to allow a 
good choice of computing power and required storage capacity of the project’s computer.

Under-exposure environments are another relevant problem in underwater photography; photos taken under water tend 
to be underexposed and have a marked shift to the blue side of the color spectrum. Hence, lower lighting 
environments call for the use of strobe lights to ensure proper exposure and solve the blue coloration problem 
(Fig. 2-3). Artificial light also solves the light-column problem caused by shadows projected from the water 
surface in shallow water. Two strobe lights with long articulated arms create the best and most flexible equipment 
arrangement for Computer Vision Photogrammetry (Fig. 2-4). PhotoScan and other Computer Vison Photogrammetry 
software recognize pixel information to develop the depth in point clouds. Shadows cast by strobe lights must be 
avoided because they change position from image to image, and do not reveal a stable color for the materials of the 
objects being recorded. Care must be taken to manage the capacity and duration of strobe light batteries. This is 
typically a difficult problem to resolve because the number of pictures required is always high; batches of 1,000 
photos per dive may be necessary to ensure the proper coverage of a given site. If the battery of a strobe light 
is exhausted during a photogrammetry dive, the colors in the photos before and after the battery failure will vary, 
and the PhotoScan Align Photos function will be seriously compromised. Again, using DSLR cameras in conjunction 
with a strobe presents an added advantage, as their wider coverage reduces the total number of pictures required.

There is another practice related to underwater photography that increases the accuracy of Computer Vision 
Photogrammetry. As noted above, the proposed methodology in this dissertation uses Agisoft PhotoScan Professional 
Edition for Computer Vision Photogrammetry. PhotoScan has four main steps for the photogrammetric modeling: Align 
Photos, Build Dense Cloud (a point cloud), Build Mesh (in which the Mesh is the surface of the model made up of 
polygons overlaid over the point cloud), and Build Texture. The result can be improved using a number of relatively 
simple procedures. One of these is the use of coded targets, an array of simple, unique images that PhotoScan uses 
to help identify the area being recorded.

PhotoScan provides a collection of coded targets in PDF format, which can be printed at chosen sizes. Following a 
series of trials carried out at the Texas A&M Ship Reconstruction Laboratory (ShipLab) and in a nearby swimming 
pool in early 2014, the author found that targets with a center circle radii of between 5 and 10 mm worked best 
underwater. These values were selected for a camera with an APS-C sensor size, with an 11 mm lens, shooting at a 
distance of 1.5 m. The results of this laboratory testing were field-tested during the summer of 2014 when the 
author employed coded targets with a 7.5 mm center circle radius at the Gnalić Project in Croatia. The benefits 
of using coded targets to help the software align the photos were tremendous. On 18 July 2014, the author placed 
36 coded targets on the Gnalić shipwreck site prior to the photo-shooting session. That session required 1653 
photos, which covered a 16 m x 12 m area. In the subsequent Align Photos process, the total number of successfully 
aligned photos amounted to 100%, meaning that all 1,653 photos were successfully aligned (Fig. 2-5). When the 
author tried the function Align Photos using same images and settings, but without recognizing the coded targets, 
the total number of successfully aligned photos dropped to 804 (49% alignment). PhotoScan can also automatically 
detect coded targets in any photo. Using this function, coded targets are advantageous and can be used as reference 
points by recording known coordinates that are measured in situ, which are then input into the photogrammetry 
process in order to create 1:1 exact scaled photogrammetric models. 25

These results suggest that a coded target will be recognized successfully when the diameter of its outer circle is 
shown between 80 pixels and 300 pixels in a photo (Fig. 2-7 and 2-8). This success range for target size should be 
confirmed for a specific camera at a specific focal length by taking a photo of a coded target to check if its outer 
circle has a pixel diameter between 80 pixels and 300 pixels. 

As discussed above, photo alignment is the most important factor for successful photogrammetric modeling. In order 
to maximize the probability of creating successful photo alignment, the author developed a flight path with coded 
targets for photo shooting. Technically speaking, higher numbers of coded targets increase the accuracy of photo 
alignment; however, an excessive number of coded targets on shipwreck structures and artifacts may detract from 
the aesthetic appearance of the site, and may also cover important diagnostic features. To avoid placing coded 
targets directly on the wreck structure, the author recommends a flight path that places coded targets around a 
shipwreck structure to increase the accuracy of photo alignment (Fig. 2-9). Furthermore, the best results have 
been achieved by capturing surrounding coded targets first to Lock the site. Then, photographs are taken 
perpendicularly to capture a top view of the site with appropriate overlap. After complete photo shooting 
in both transversal and longitudinal paths, additional photographs must be taken with the camera tilted to 
capture vertical surfaces of rich tridimensional structures. Actual flight path of archaeological sites can vary 
depending on multiple factors; therefore, meticulous planning of a flight path is an important part of successful 
photogrammetric recording. Tying the site plan to a set of local coordinates is a key factor of this workflow 
process. There are two reasons that creating a local set of coordinates is important: to correct scale and 
distortion of the photogrammetric models and to geographically reference the site plan.

One of the advantages of Computer Vision Photogrammetry is that it does not require precise calibration of the 
camera. Although calibration is recommended, the software primarily uses pixel information to reconstruct a site, 
which means that the construction of the point clouds does not require manual calibration. Moreover, the software 
reads metadata from the camera and lens and minimizes the errors. Distortions are inevitable, however, and this 
factor alone argues for the necessity of establishing a set of precisely positioned datum points. The second 
reason to establish a number of datum points is that unless tied to a system of coordinates, models float in 
unspecified tridimensional fields. To fix the models in the correct position, local coordinates have to be included 
on the models. Without this, it is impossible to export the computer graphics files to mapping software with the 
correct position. Georeferenced information facilitates a straightforward workflow when models and orthophotos 
(high resolution photomosaics) are exported. 

The processing and rendering capacity of the PhotoScan software is limited by both software and hardware
configurations. This constrains the maximum polygon count for the mesh (which makes up the surface of the model), 
and the maximum number and resolution of UV mapped textures (‘UV’ is a XY coordinates for texture), which are 
composed photomosaic on surfaces of mesh. Large data sets can be divided into smaller Chunks (PhotoScan’s term), 
which are separately processed. These Chunks can be imported into other modeling and mapping software applications 
and merged without decimation (the reducing of the polygon count). When models and orthophotos are georeferenced, 
the merging process is automatic, and exported files are opened in their correct positions in other software. A set 
of local coordinates is paramount to ensure an accurate manipulation of models or orthophotos.

During the Gnalić 2014 field season, the author and ShipLab team created a control network using eight datum points 
solidly fixed to the site bedrock and shored with sand bags. These eight points were positioned using 3H Site 
Recorder, a mapping software that uses statistical adjustment of trilateration, also known as Direct Survey Method 
(DSM). Depths of all points were taken with a depth gauge in rapid sequence to avoid the effect of tide. Several 
measurements were taken and averaged (Fig. 2-10). The accuracy of DSM has been discussed and judged reliable by 
multiple sources (Atkinson et al., 1988; Rule, 1989; Green et al., 2002; Holt, 2003; Green and Gainsford, 2003).
 
As Holt (2003) pointed out, the acquisition of direct measurements and manipulation of the software are very simple, 
but can generate errors when handled by novice divers alone. Holt (2003) recommends the use of experienced divers 
in this phase. Measuring methods that will be discussed and compared in this chapter are: trilateration(DSM) using 
3H Site Recorder, creating an enclosure using rope that is cut to known distances, scale bars that are placed on 
the site, and coordinates taken by total station. To compare and examine the accuracy of these four methods, local 
coordinates and known distances of these four methods were applied to a photogrammetric model, and each different 
method was compared to control measurements. The control measurements were taken directly from the wooden model. 

For this test, the author used a 1/10 scale wooden model of a saveiro, a 20th-century Brazilian coastal sailing 
boat. In order to simulate an archaeological shipwreck site, the wooden model was laid down on its starboard side, 
and its floor timbers were pulled slightly out so that its starboard futtocks lay on the bottom (Fig. 2-11). An 
historical and structural description of the original saveiro ship shall be discussed in Chapter VI. The first 
method to discuss is trilateration, also known as Direct Survey Method (DSM). Trilateration (DSM) has been 
repeatedly used in underwater archaeological recordings. For this DSM, the author used 3H Site Recorder 
(Demo Version). In order to acquire coordinates of reference points, a control network first had to be established. 

The author placed eight control points around the wooden model, set all the control points to a congruent height 
(40 cm from the ground), and set this height as the surface of the water, or depth of 0 cm. To establish the 
positions of the control points, or a control network, 19 measurements were taken (Fig. 2-12). The tolerance 
established for errors was set at 0.3 cm; therefore, all distance errors bigger than 0.3 cm were shown in red 
(shorter) and blue (longer). Only one distance (CP5 – CP7) showed a + 0.39 cm statistical error after the 
adjustment of the control points. After the control network, or positions of datum points, was established, 
11 reference points were placed on the model. A total of 44 measurements were taken from the control points; 
each reference point was measured from the nearest four control points (Fig. 2-13 and 2-14). Two measurements 
indicated over 0.3 cm error; however, both errors were less 0.5 cm, which is within an acceptable range of error. 

After the local coordinate network of the saveiro model was established, XYZ coordinates of the reference points 
were extracted (Table 2-9). Then, these coordinates were plotted on the photogrammetric saveiro model. The next 
method to be examined was designated the enclosure method. This method is based on the idea of enclosing an 
archaeological site using ropes (using measuring tapes as ropes is recommended) that are cut in predetermined, 
controlled lengths; therefore, the distances between the four corners will be known without having to take a 
series of tape measurements. Using the known distances that surround a site, distortions and scales of a 
photogrammetric model can be fixed. The relative coordinates of the four corners of any given site can be 
calculated using simple trigonometric equations or existing tools in CAD software. Additionally, using the 
Pythagorean Theorem, the angles of the four corners of a rectangular area can be exactly set at 90 degrees 
(Fig. 2-15). To test the accuracy of the enclosure method, the author used the four corners of the base of 
the saveiro wooden model. These were measured: 101.2 cm at the stern, 101.5 cm at the bow, 182.8 cm at the 
starboard side, and 182.6 cm at the port side. The angle of each corner was measured at 90 degrees. The four 
distances were entered on the photogrammetric model (Fig. 2-16).Scale bars are often used when pictures of 
archaeological sites are taken. These scale bars can be extremely useful in Computer Vision Photogrammetry. 

Scale bars give accurate scale information in archaeological photography, and when a site is being mapped using 
Computer Vision Photogrammetry, they can be used to check measurements after other methods, such as DSM, were 
used to correct distortion and scale of a created photogrammetric model. However, distortions and dimensions can 
be fixed by using only scale bars, a feature that allows archaeologists to skip the time consuming and sometimes 
inaccurate DSM system. This may be one of the simplest methods used to record a site. To test the accuracy of 
this method, four scale bars were placed on the four sides of the saveiro model. In this particular case, 
considering the size of the model the four scale bars available were unnecessarily long. Therefore, seven 10 cm 
scale bars with markers were created on the existing scale bars. These smaller 10 cm bars were entered as fixed 
distances in the photogrammetric model (Fig. 2-17). On terrestrial archaeological sites, total stations are used 
as an effective and easy way to acquire local coordinates; total stations cannot, however, be used in underwater 
environments. Nonetheless, not all shipwreck sites are submerged, and total stations are a widely used recording 
method known for accuracy. To compare the accuracy of the different methods described above, the coordinates of 
the saveiro model were also collected with a total station. A series of coordinates of control points and reference 
points were taken with a total station, and these points were plotted in Rhinoceros 3D-CAD modeling software. Then 
all the points were re-oriented to match the coordinate system previously established, which has Control point 1 
as the origin of the coordinate system. Control point 1 (CP1) was attributed coordinates X=0, Y=0, Z=0, and 
control point 5 (CP5) was placed on the X-axis, or the easting line (Fig. 2-18). Coordinates of the reference 
points measured by the total station were shown in Table 2-12. To test the accuracy of the survey methods 
mentioned above, the measurements from all photogrammetric models of the saveiro were compared within a single 
set of control measurements. The control measurements were taken directly from the wooden model; 21 selected 
distances between reference points were taken for this purpose (Fig. 2-19). The control measurements were 
compared to the measurements obtained by the other methods, and corrected as indicated above (Table 2-13). 

The results suggest that trilateration (DSM) is not the most accurate method to correct scale in a 
photogrammetric model. The measurements taken from the photogrammetric model with trilateration (DSM) 
coordinates display an average error of 0.39 cm, although the 3H Site Recorder software used in the trilateration 
(DSM) method indicated that the measurements between control points and reference points had errors of less than 
0.3 cm. The second least accurate method in our experiment was the total station, but its 0.33 cm average error 
can be explained by the scale of the saveiro model. Total stations are designed to measure large distances and are 
less accurate when the measurements are this small. One must keep in mind that the nature of the total station 
suggests that this 0.3 cm error remains the same when the size of the site to be recorded increases, making its 
accuracy better as the site size increases. The more accurate results obtained were through the enclosure method 
and the use of scale bars; the enclosure displayed an average error of 0.2 cm, and the scale bars allowed an 
average error of 0.19 cm.

The results of the experiments discussed in this section show that the enclosure method and the scale bars 
method provided better results in fixing scales from Computer Vision Photogrammetry surveys. The main disadvantage 
of trilateration (DSM) is that it requires many measurements. For instance, the saveiro model required 19 
measurements to establish the control point network and 44 measurements to calculate the coordinates of reference 
points. On the other hand, the enclosure method only required four measurements (six measurements if you use the 
Pythagorean Theorem) plus the depths of the corners of the area. The scale bar method does not require any 
measurements. However, it is important to note that the enclosure and the scale bars methods cannot establish a 
local coordinates system. This is a disadvantage when compared to the traditional trilateration (DSM) method; 
the enclosure and scale bars can fix distortion and scale of photogrammetric models, but they need a complementary
method to establish local coordinates. Other surveying methods will be proposed in the following sections of this 
dissertation that take advantage of the accuracy and simplicity of the enclosure and scale bars methods while 
simultaneously allowing the archaeologist to acquire a local system of coordinates.

These surveying methods require only three steps. The first step is to create a 1:1 scale photogrammetric 
model using both enclosure and scale bars. The second step is to place the model in a 3D field using one of 
the corners as a datum point (X=0, Y=0, Z=0) and calculating the remaining corners using the measurements from 
the enclosure method. To add the depth of the site and correct the enclosure – if it turns out not to be an exact 
square or rectangle – simple trigonometry or CAD software can be used for minor adjustments. The third and final 
step is to extract the coordinates of the reference points marked inside the enclosure and over the structure to 
be recorded. Coded targets may help expedite this process, another reason to employ coded targets during the first 
photo-shooting session for the photogrammetry model. In the end, this method establishes a local coordinate system 
using a 1:1 scale photogrammetric model of the site.

Data collection as discussed in this methodology is conventional data collection by hand. Even when 
archaeologists can create an accurate 1:1 scale photogrammetric model and extract any desired measurements 
from the created model, these are just measurements. To truly understand the shipbuilding techniques employed 
during construction of the ship, sketches made by trained archaeologists are critical (Fig. 2-20). Archaeological 
training is crucial in the selection of hull areas of interest and specific components to be recorded; a profound 
knowledge of shipbuilding is invaluable in the field as well as during reconstruction in the laboratory.

Two prerequisite tasks in fieldwork site preparation are digging and subsequently cleaning the archaeological 
site (Fig. 2-21). A basic concept in the discipline of archaeology is that excavation results in the destruction 
of archaeological sites (with precise recording). Nonetheless, careful excavation supervised by trained 
archaeologists can minimize unnecessary damage to shipwreck sites. Additionally, good recording, including 
Computer Vision Photogrammetry, requires good cleaning; in other words, the results obtained in recording are 
largely dependent on traditional methods of excavation and cleaning archaeological sites; both tasks require 
experience and training.

At all shipwreck sites, archaeologists find various artifacts during excavation. When artifacts are removed 
from the site during the project, it is important to record the provenience of all artifacts and to take photos. 
Artifacts brought to a project’s base or a laboratory for conservation must be stored and recorded properly under 
the supervision of conservation specialists (Fig. 2-22 and 2-23). When there is no space, funding, or plans for 
proper conservation, it is unwise (and often illegal) to remove artifacts from the archaeological site. 

Preservation of artifacts in situ, while not ideal, is preferable to the artifact compromise and destruction that 
will be caused by removal and failure to conserve. Each artifact provides potentially valuable information; the 
excavating archaeologist has a duty to maximize the information obtained from each artifact while minimizing 
destruction and loss.

It must be noted that database development is one of the most important tasks in archaeological projects; future 
accessibility of the collected data depends completely on the database. This may be the most difficult task in an 
archaeological field project. Often inexperienced archaeologists and attending divers do not recognize the 
importance of data organization. Project directors must emphasize the importance of this task and assign dedicated 
attending archaeologists and students to managing data organization and storage. Without good organization, 
important information may be lost forever.


In summary, manual data collection and the subsequent development of a well-organized database in digital format 
are the most important steps in any archaeological project; without accomplishing these two tasks, the methodology 
outlined in this dissertation is useless.

The author was in charge of photogrammetric modeling of the Gnalić shipwreck during the 2014 field season, and 
this chapter is a reflection on that experience. During the excavation, several photogrammetric models of the 
Gnalić shipwreck site were created using Agisoft PhotoScan (professional edition). (in 2015) Photoscan professional 
edition costs $3,499 for a stand-alone license and $549 for an educational license. Photoscan was first released 
in the middle of 2010 as a mapping software based on aerial photography, but was soon appropriated by other 
disciplines, including architecture, the motion picture industry, and terrestrial and underwater archaeology. 

To reconstruct tridimensional structures of a given scenario or landscape, PhotoScan uses an algorithm that was 
developed in a discipline called Computer Vision. Computer Vision is a sub-discipline of computer science that was 
designed to help robotic devices to understand surrounding structures (Van Damme, 2015; Huang 1996; Szelski, 2010). 

Computer Vision algorithms find common features among images of a given target, taken from different angles, 
and process those images to derive depths of the structures mapped (Van Damme, 2015; Szelski, 2010). In the 
author’s opinion based on working with multiple software suites, off-the-shelf PhotoScan software is more 
accurate and user-friendly than other available photogrammetry software. The workflow of PhotoScan has four 
basic steps: Align Photos, Build Dense Cloud, Build Mesh, and Build Texture. This workflow is simple, but each 
step must be executed properly. The author has developed a number of additional tasks with associated sub-settings 
of basic workflow processes of PhotoScan to optimize the results of these processes. Unfortunately, the user manual 
of PhotoScan does not address the factors and influences of each sub-setting. In order to fully understand the 
meanings and functions of these sub-settings to optimize output results, the author conducted an experiment in 
which he produced photogrammetric models of two cubes with different surface patterns. Those photogrammetric 
models were processed using different sub-settings (Fig. 3-1). Although the user manual of PhotoScan does not 
fully explain the nuances of sub-settings, Photoscan programmers and professional users argue differences and 
advantages of various sub-settings to optimize PhotoScan output on Agisofts forum pages (Agisoft Online Forum, 
2015). 

The starting point for some of the specific sub-settings tested was selected by review of those forum pages. 

In all cases, the settings provided below were developed from testing, rather than from accepting forum 
discussions at face value. After uploading photos into the program, the first step of photogrammetric 
modeling is Align Photos. One of the forum discussions (Alignment Experiments by [handle name] Marcel) discusses 
Align Photos settings (Marcel, 2015). Align Photos has three different Accuracy settings: High, Medium, and Low. 

High Accuracy uses full resolution images, Medium uses 50% resolution, and Low uses 25% resolution. The next 
sub-setting is Pair Pre Selection; this sub-setting allows PhotoScan to perform a quick pre-scan before it begins 
full common feature detection in photo. This quick pre-scan can detect two photos that share similar views. Based 
on this pre-scan, PhotoScan can skip the whole image point detections to align each single photos. Instead, it only 
scans two pre-selected images each time to locate camera position. As a result, employing Pair Pre Selection 
reduces processing time. Pair Pre Selection has Generic and Disabled options. To activate pre selection, Generic 
must be selected.

The next Align Photos setting of interest is Key Point Limit. This setting specifies how many pixels PhotoScan 
uses from each photo to detect common features in images. The default setting of Key Point Limit is 40,000 (for 
reference, a typical 36 Megapixel photo has approximately 240,000 pixels, and a 21 Megapixel photo has approximately 
180,000 pixels). One Agisoft forum discussion stated that selecting a higher number for Key Point Limit will 
increase the processing time until the value reaches 240,000, supporting full processing of photos having 36 
Megapixels (240,000 pixels). However, the forum also noted that if the setting chosen for Key Point Limit is 
more than 40,000, there will be no improvement in the quality of Dense Clouds in subsequent processing. Thus 
if accuracy is the primary concern, projection error can be minimized to 0.3 pixel by increasing Key Point 
Limit to 120,000 (although an average projection error with Key Point Limit set to 40,000 is 0.7 pixels--already 
less than 1 pixel) (Marcel, 2015). The author tested the Align Photos process with Key Point Limit: 40,000 
(default) and Key Points Limit: 80,000 (Fig. 3-2 and 3-3). No significant difference was noted in the development 
of the created sparse points. The last sub-setting of the Align Photos stage is Tie Point Limit, which specifies 
how many detected common points are actually used to calculate the depth of the scene. For instance, when 40,000 
Key Point Limit and 1,000 Tie Point Limit are applied, 40,000 points of each image are used to detect common 
points to match images, and then the 1,000 highest quality common points from each image are used to calculate 
camera positions and to create sparse point clouds. Decreasing the Tie points setting decreases the processing 
time of photo alignment (Pasumansky, 2015). During the Align Photos stage the author used a setting of Tie Point 
Limits: 1,000 (default) and Tie Point Limit 2,000; doubling the number of tie point limits more than doubled 
the number of created sparse points, but at a cost of a significantly longer processing time (Fig. 3-4 and 3-5). 

Judging from the results, the Tie Point Limits value directly affects the total number of sparse points. The 
author elected not to perform experiments on the Quality sub-setting at this stage of processing because 
successful photo alignment largely depends on photo-shooting, not Quality sub-settings. Once photos are aligned, 
the next step is Build Dense Cloud. This process has five quality settings: Ultra High, High, Medium, Low, and 
Lowest. Ultra High quality uses full size image resolution, High uses 50% of the original scale, Medium uses 25%, 
Low uses 12.5%, and Lowest uses 6.75%. These percentages directly affect the total number of points in the dense 
cloud (Marcel, 2014) (Fig. 3-6, 3-7, 3-8, and 3-9). For this test, the author intentionally skipped a trial of the 
Ultra High sub-setting because once the computer began processing, it provided an estimated processing time of 
more than two weeks. Thus, this setting is not practical for typical archaeological projects, which are usually 
conducted without the services of a super computer. Another sub-setting is Depth Filtering. This filter has four 
selectable options: Disabled, Mild, Moderate, and Aggressive. The Aggressive option means that if the object is 
monotone, PhotoScan will recognize it as void space and will not create points within that space (Fig. 3-10). 

Selecting the Mild option prompts the program to recognize monotone areas as objects, and to create points within 
those spaces (Fig. 3-11). For example, the Aggressive depth filter does not recognize the surface of a white board 
as an object, and it creates a hole in its photogrammetric model; on the other hand, the Mild setting may recognize 
blue sky as a blue object, and thus create point clouds in what would otherwise be void space. Therefore, 
Mild works well on a monotone surface, and Aggressive works well with images that contain actual void spaces. 

Moderate filtering works between the previous two filtering options (Fig. 3-12). Disabled mode does not recognize 
depth; in other words, if an object has a monotone colored surface, PhotoScan does not regard it as a surface and 
creates void spaces (Fig. 3-13). In summary, each depth filtering selection has advantages and disadvantages, 
therefore the Depth Filtering setting must be chosen based on photography conditions. The next process is Build 
Mesh (mesh is the geometry of the created model based on the point cloud). Sub-settings for this process are 
Surface Type, Source Data, Face Count, Interpolation, and Point Classes. Surface Type has options Height Field 
and Arbitrary; when the subject of photogrammetric modeling is a flat structure, such as a level field (ex. 
aerial photography), Height Field is sufficient; however, if an object has 3D structure, Arbitrary must be 
selected (Fig. 3-14 and 3-15). From Source data, either Sparse Cloud or Dense Cloud may be chosen. 

In general, Dense Cloud is preferable since it generates more minute details. The Face Count (Face Count means 
total number of the created surface of mesh) sub-setting has High, Medium, and Low options; High creates faces, 
or surface of meshes, that are made of 1/5 of the number of points in the Dense point cloud, Medium creates 1/15 
of the point cloud (1/3 of the High setting), and Low creates 1/45 of the point cloud (1/9 of the High setting) 
(Fig. 3-16, 3-17 and 3-18).

The last sub-setting is Point Classes. This sub-setting is for GIS-based mapping software; it allows point 
clouds to be imported from mapping software. They can be classified into different categories, such as Ground, 
Vegetation, Building, Water, etc. This sub-setting allows PhotoScan to read the categories assigned by GIS 
mapping software, and then create meshes of chosen point types.

Another Sub-setting is Blending mode. To clarify, Mapping Mode is used to compose UV maps, and Blending mode 
is used to compose selected photo-images on created UV maps as textures. Blending mode has five options: Mosaic, 
Average, Max Intensity, Min Intensity, and Disabled. The default blending mode is Mosaic. This setting selects 
the closest photo to corresponding surfaces and uses that image without blending with other overlapping photos. 
(Fig. 3-26) The Average sub-setting calculates an average pixel value, or RGB value of colors, of overlapping 
photos and uses those pixel values as textures. (Fig. 3-27) The Max Intensity setting applies images that have 
maximum intensity, or bright pixels, to corresponding surfaces (Fig. 3-28). The Min Intensity setting is opposite 
of the Max Intensity setting; it uses minimum intensity images. The Disable setting is, again, for imported models 
that already have suitable textures (Fig. 3-29). In conclusion, based on the author’s experiences, Mosaic provides 
the best result among those sub-settings; therefore, Mosaic is preferred for creating textures on archaeological 
photogrammetric models.

In this chapter, the author will discuss various ways of sharing archaeological research with the public, which 
is a vital and ethically necessary part of archaeology. The first public outreach methodology that the author 
would like to introduce is Visual Tour Animation, which is a simple animation that uses photogrammetric models 
and a flying camera created in Autodesk Maya (this animation can be done in other modeling and animation softwares 
such as Blender). The author believes that making a Visual Tour Animation is much simpler and easier than 
creating conventional computer generated animations because it does not require computer modeling and Deformation. 

Since most of the modeling for the Visual Tour Animation can be done with Computer Vision Photogrammetry software, 
the required knowledge of computer animation in Maya is Key-Frame Animation for a created flying camera. The basic 
concept of computer generated animation is the same as that of video recording; one second of animation is 
composed of 24 – 30 rendered images so that the human eye and a brain see it as a movie instead of consecutive 
pictures. When the Animation Setting in Maya is set at 24fps (frames per second), a 20 second animation has 480 
still frame images. Key frame animation uses these frames as keys of timing (designated timing of actions of the 
object, such as positions, scales, and rotations of the selected object). When the position of a cube needs to be 
moved from point A to point B in 20 seconds, the first default key frame would be frame 1 and a cube would be 
located at point A. Then, the Set Key command is applied by pressing the S button. This command orders the cube 
to be located at point A at key frame 1. Then, the current frame on the Time Slider Tab that can be seen at the 
bottom of the Maya workspace needs to move from 1 to 480 to make time flow in Maya 20 seconds later (24 fps x 20 
seconds). Also, the position of the cube must be moved to point B, and then the Set Key frame command should be 
applied. Consequently, the cube travel from point A to point B in 20 seconds (Fig. 5-1).

In [TAN14] a computer vision system is introduced, including remarks on mechanical components, image capturing 
and algorithmic processing to grade the pearls shape and size. The complete system contains three main parts: 
pearl feeding, image capturing and pearl sorting. The feeding system is sketched briefly as a pipe system in 
which the pearls are routed through their own weight to the image capturing device. The image capturing is done 
with a single camera and four mirrors, an interesting approach that allows the capturing of 5 view angles of a 
pearl with only one camera. The pearls shape is determined by a fuzzy membership function calculated of the 
Fourier transform of the radius variation of the outer boundary of the pearl subject to its centroid. The size 
of a pearl is assigned by using the minimum and maximum diameter. According to the automatic grading, a rotating 
sorting device, situated under the image capturing part, routes the pearl to one of several concentrically 
arranged containers. Additional topics, such as lightning sources for the image capturing and control procedures 
are discussed as well. Besides the image capturing system, of which an experimental device was built, the 
remaining parts for pearl routing seem to be theoretical. Specifically the routing in pipes might need 
experimental validation if used for an equivalent processing of Tahitian pearls. If the diameter of the pipe 
is big enough to let pass large pearls, several small pearls might pass in parallel. The blocking of the pipe 
by pearls of irregular shape has to be considered as well. An alternative might be the routing of pearls via a 
mechanical arm that picks pearls separately by suction as presented in [BAI14].


In contrary to the previously mentioned use of Fourier descriptors for shape analysis ([TAN14], as well used 
in [CAO10]), Zernike moments of the radial variation of the pearls shape subject to its centroid are used in 
[LI07]. The membership to five predefined classes is assigned as well with fuzzy logic. Even though the shape 
grades used in the mentioned articles differ from those used for Tahitian pearls, the approach of using rotational 
invariant descriptors such as Fourier or Zernike for shape description and a classification with fuzzy logic to 
take into account the smooth transition between shape classes is a general approach. Accordingly, such an approach 
will likely be suitable for classifying the shape of Tahitian pearls.

The use of a monochrome camera with different filters to obtain images sensitive to specific wavelength bands to 
classify the pearls color according to human perception is proposed in [NAG94]. 100 pearl samples are graded by 
experts in 10 classes from white to cream. An Artificial Neural Network (ANN) is used to identify wavelength bands 
that allow a maximum correlation between human and artificial classification. Therefore, an ANN is trained with 
all images, and bands with a small influence on the classification result are removed iteratively. The optimal 
classification is reached with 9 filters with a classification success of 91% for the training data and 71% for 
the testing data. This approach has the advantage of covering a larger area of the pearls surface when compared 
to the use of a spectrometer. Additionally, the instrumentation is generally cheaper.

A more classical approach for pearl color classification can be found in [TIA09]. Regular RGB color images are 
transposed to the HSV color space. The images undergo median filtering and areas containing reflections are 
segmented by thresholding the Value channel. For the remaining region, the averages of Hue and Saturation channels 
are calculated. An Artificial Neural Network is trained with the obtained Hue of 800 pearl images, based on the 
human classification of the pearls, in three color classes red, purple black and white. The trained network is 
tested with 10 images showing one misclassification. The average Saturation is clustered with fuzzy C-means in 
four classes A-D. The approach is straight-forward but the averaging of Hue and Saturation is not applicable in 
the case of the Tahitian pearl, that is described by primary and secondary colors due to its multi-color appearance.

In this thesis computer vision based approaches to automatically measure the nacre thickness of Tahitian pearls 
as well as to classify their color are proposed. These two parameters are fundamentally different. The color is 
a perceptual parameter that depends on the perception of the observer, while the nacre thickness is a measurable 
parameter. Furthermore, the color can be visually evaluated by analyzing the surface of the pearl, while the nacre 
thickness depends on the inner structure of the pearl and has hence to be visualized for example with X-ray imaging.

Due to the fundamental difference of the nature of these two quality parameters, as well as the difference in means 
of required computer vision approaches, it was decided to divide the thesis in two parts. Part I presents our work 
to automatically measure the nacre thickness of Tahitian pearls out of X-ray images. Part II presents our work to 
automatically classify the color of Tahitian pearls based on color images of their surface. To underline the 
different character of both applications, each part has its proper numeration, introduction, bibliography, etc. 

Every reference within the parts refers to sections, literature or images of the same part, meaning there is no 
cross reference between the two parts. Accordingly, both Part I and Part II stand for themselves and can be read 
independently of any other part of this thesis. Some redundancy to the related work presented in the previous 
section will hence occur. In the following two subsections a general description of each part and the corresponding 
contribution will be given.

Optical flow is the two-dimensional vector field describing the motion of pixels in the image
plane due to the motion of the environment and the camera. Optical flow offers important
visual cues to a robotic vehicle moving in a dynamic environment. Applications such as visual
servoing Hamel and Mahony [2002], vehicle landing Herisse et al. [2012], height regulation
Ruffier and Franceschini [2005] and obstacle avoidance Srinivasan [2011a] use the estimated
optical flow computed by the onboard vision system of the robot.

In a robotics context, an important requirement of any optical flow algorithm is its capability
to run at real-time frequencies. In practice, this means that the vision processing system
should run at the same frequency, or preferably faster, than the vehicle controller. For a typical
high-performance autonomous vehicle, this translates to frame rates in the order of 200Hz
Bangura et al. [2015]. Furthermore, typical robotic platforms have weight limitations that constrain the amount of compute hardware they can carry. For example, small aerial vehicles use
similar embedded System on Chip (SoC) as those found in modern smart-phones. These chips
are equipped with multi-core processing units as well as mobile Graphics Processing Units
(GPU).
Real-time computation and embedded hardware constraints requires a different approach
to compute optical flow from the state of the art algorithms documented in the well known
benchmarks such as Baker et al. [2011]; Butler et al. [2012]; Geiger et al. [2013]. Typical
algorithms from the Computer Vision community use two frames to estimate dense optical
flow between the two images Baker et al. [2011]. In order to achieve highly accurate results,
modern algorithms use sophisticated mathematical models to extract the optical flow from two
frames. Typically, the most accurate algorithms are also the more computationally expensive,
as it can be verified in the runtime reported in benchmark datasets cited above.

Recall the properties of a Robotic Embedded Vision system (REV) described in chapter
1, in particular the high-speed video sampling of the environment to capture the dynamics of
objects in the world at high frequencies. This chapter proposes a new optical flow algorithm
capable of running at frequencies as high as 800Hz at 640  480 on a Desktop computer and
near 100 Hz on embedded GPU SoC at 320  240 pixel resolution.

The proposed algorithm follows a prediction-update filtering approach, where an internal
optical flow state is incrementally built and updated using the stream of image data from the
camera. An important component of the algorithms is the prediction stage, which is modeled
as a system of partial differential equations to integrate forward in time the current estimation
of image brightness and optical flow to create predictions for future time. Numerical solution
to these equations is implemented using an efficient finite difference method based on upwind
differences Thomas [1995]. This numerical method can be efficiently implemented on both
GPU and FPGA hardware.

Differential methods use spatio-temporal models of the image
brightness to recover the underlying optical flow from a sequence of images. These
methods are based on the brightness conservation assumption that states that the total
amount of brightness in the image is conserved over time. Intuitively, this means that
any change in the brightness value of a given pixel is due to the change in position of the
object in the scene in that pixel direction.

Early works in optical flow such as those of Lucas and Kanade [1981] and Horn and
Schunck [1981] are differential methods. These two algorithms marked a distinction between
local based (Lucas-Kanade) and global based methods (Horn-Schunck) for computing
flow. Local based methods use a small support window around each pixel in the
image to estimate optical flow, while global based methods impose a global constraint
over the optical flow field to improve the quality of the estimation. In general, local based
methods are faster than global based ones. A detailed explanation of these algorithms is
provided in Section 2.2.2.

Modern differential algorithms Brox et al. [2004]; Bruhn et al. [2005];Werlberger [2012]
utilize the same differential principles together with robust optimization frameworks to
create highly accurate flow estimates. In particular, global based methods often use the
L1 norm instead of the L2 (as in Horn and Schunck) to be more robust to outliers in the
estimation process.

Additionally, the survey article of Sun et al. [2014] offers a review of most recent algorithms
together with a quantitative analysis of different optimization frameworks used to
estimate optical flow. Also, the survey article of Chao et al. [2014] offers a list of optical flow
algorithms currently used in robotic applications.

Optical flow algorithms are too numerous to create an exhaustive classification and review.
Instead, the review provided in this chapter is focused on algorithms that have proven to work
at real-time frequencies, thus making them effective for real-life robotic applications. In particular,
differential optical flow algorithms are reviewed in detail, as such algorithm are closer
to the mathematical formulation used in this chapter. Additionally, bioinspired algorithms as
well as methods using alternative camera technologies such as Dynamic Vision Systems are
included in this review

This section describes real-time optical flow algorithms and systems. This separation from
mainstream optical flow algorithms is important in a robotics context, where algorithm’s runtime
is as important as its accuracy.

In general, real-time flow algorithms are local-based methods. Many of these methods
are some form of Lucas-Kanade method Lucas and Kanade [1981], region based method, like
Srinivasan [1994], or neuromorphic approaches Mueggler et al. [2014]. Image data is constrained
to a local support window around each pixel. Moreover, the algorithms are designed
such that the flow estimation is independent for each pixel, thus allowing one to perform computations
in parallel for each all pixels. This pixel independence allows algorithm implementation
on multi-core processors or Graphic Processing Units (GPU). Moreover, it is possible to
create digital circuit pipelines to compute optical flow directly in silicon.

There are alternative image sensors and algorithms inspired by nature from which it is possible
to extract optical flow. These technologies and algorithms are of particular importance to
robotics as they offer fast visual feedback to robotic vehicles.

One of such approaches comes from understanding the visual system of insects. Insects
such as flies and bees have compound eyes made of a grid of light sensors known as the
ommatidia. Each ommatidium captures a narrow field of view of the scene and is connected
to the visual cortex of the insect. The reader is encouraged to read the review paper by Borst
on the structure of insect’s eye Borst [2009]. It is possible to model the correlation between
neighboring ommatidium to extract motion information. This correlation model is known as
the Reichardt motion detector Reichardt [1987]. Systems such as those by Zhang et al. [2008]
and Plett et al. [2012] have demonstrated the effectiveness of this motion detector in real-life
systems.

The work by Srinivasan [2011a,b] offers an insight on how the motor control part of the
insects’ neural cortex connects to the visual system. In his experiments, he has demonstrated
how honeybees use optical flow for tasks such as navigation, obstacle avoidance and landing.
The understanding of insect vision has brought insight to robotics to solve the same tasks on
flying vehicles. Such is the case of the works by Ruffier and Franceschini on optical flow
regulation Ruffier and Franceschini [2005], the work of Mafrica et. al. on velocity and steering
angle control of a car vehicle Mafrica et al. [2016] and the work by Herisse et al. [2012] on
landing and take-off of an aerial vehicle from a moving platform using optical flow.
Another technology that has been proven effective for robotic vision are the Event-based
Cameras. These cameras are a complete paradigm shift compared to standard digital camera
hardware, with the principal difference lying in the information transmitted to the user.
As their name suggests, event cameras transmit events occurring in the scene instead of full
image frames. An event is triggered by a temporal change in pixel brightness. Current hardware7
transmits event location, timestamp and polarity (plus or minus) asynchronously. Consequently,
the bandwidth required between camera and processing units is significantly reduced,
as the camera only transmits the difference between two images. Applications of this technology
are emerging: Benosman et al. [2012, 2014] extract optical flow from the event stream
using a differential framework similar to that used on standard gray-scale images. Mueggler
et al. [2014] use the stream of events to track the 6-DOF pose of an aerial vehicle using the
on-board vehicle’s CPU.

The main concept in the development of the proposed real-time optical flow algorithm is that
of incrementally building a dense flow estimate over time. In this approach, one has an internal
optical flow state that is constantly refined using new image measurements from the camera.
Instead of computing dense flow fields from two images, as typical computer vision algorithms
do, an incremental approach will exploit the large availability of data one has in a real-life
robotic vision system to constantly estimate optical flow.

This incremental approach has two advantages over standard algorithms. First, the optical
flow field is temporally smooth. Thanks to the incremental nature of the algorithm, the
temporal evolution of the estimated flow field will show a smooth transition between two consecutive
image frames. Second, there are computational efficiencies in this approach. Instead
of using a complex algorithm to compute dense optical flow fields from two images, one can
design a simpler flow update algorithm considering new image data and the current flow state
estimation.

Intrinsic to the incremental approach to compute optical flow, is the concept of temporal
evolution of the flow field. At each time step, that is, when a new image arrives, one needs
to propagate forward in time the old flow state to align it to current time. Once the prediction
is completed, both the flow state and the measurements are temporally aligned, and a new
estimate of optical flow can be created using both pieces of information.

This process matches a filter architecture consisting of update and prediction stages. To the
best of the author’s knowledge, this architecture for computing optical flow was first described
in the PhD dissertation of Black [1992]. In his work, Black uses robust statistic methods in
the update stage to refine the predicted optical flow from previous time step. The prediction
is formulated as the warping of the optical flow field forward in time. This warping can be
implemented by adding the optical flow field to each pixel coordinate and then performing a
re-sampling of the resulting image.

The proposed algorithm in this chapter follows the same general update-prediction architecture.
However, a key novelty is the formulation of the prediction stage as system of partial
differential equations modeling the transport of optical flow, and any associated field, by the
optical flow. The prediction stage is implemented by numerically solving these transport PDEs
using a finite difference solver. In contrast to image warping, the finite difference solver does
work on a fixed grid of points and hence, it does not require a re-sampling post-processing stage.

As it will be shown throughout the chapter, this filter formulation leads to regular computations
that can be realized in both GPU and FPGA hardware, and can run at the frequencies
required by a robotic vision system.

The study of numerical methods for solving partial differential equations is a complete
branch of applied mathematics in itself (see LeVeque [2002] and Thomas [1995, 1999] for an
introduction to numeric PDE theory). Numerous numerical methods exists for different types
of applications such as computational fluid dynamics, aerodynamics and weather modeling. In
general, the choice of a particular numerical method is problem and application dependent.
Numerical methods can be classified according to their approach to derive a discrete version
of the underlying continuous PDE problem. Methods can be divided as finite volume
methods and finite difference methods.

Finite volume methods formulate the partial differential equation problem in terms of a
conservation law. First, the discrete volume, or area, element is defined. Simple examples of
these elements are a cube and a square. This elements act as reservoirs containing a certain
amount of “material” at any given time. Then, for the derivation of the conservation law, one
needs to define the flux of material entering and leaving the volume elements on each of its
sides or faces. These flux equations form the conservation law specific to the problem and can
be discretized to create a numeric implementation. The reader is encouraged to look at the
book of LeVeque [1992] for an introduction to finite volume methods.

Finite volume methods tend to be more physically accurate than finite difference methods.
One reason for this is the modeling process in terms flux processes, which naturally matches
the physical properties of the real-life phenomena. This gain in accuracy, as is usually the
case, comes at the price of computational complexity in order to actively balance the input and
output material flux at each discrete time step.

Finite difference methods offer a more direct approach to solve a partial differential equation.
Basically, one only needs to define the volume or area element (same as finite volume
methods), and define discrete operators for each partial derivative present in the PDE. Depending
on the type of equation (parabolic, elliptic or hyperbolic), one needs to carefully choose
the difference operators to avoid instability in the numerical scheme. Hyperbolic PDEs, which
model transport process (the optical flow state propagation), are very sensible to this choice of
operators. The books by Thomas Thomas [1995, 1999] are an excellent reference to study the
different types of PDE and their numerical solution.

Optical flow, is not a physical flow field, and discontinuities on both leading and trailing
edges have valid interpretations explained as motion boundaries typically originated at object
boundaries. It is desirable to consider a relaxation on the physical interpretation of the flow
that could be included explicitly in the numerical method to obtain a more accurate propagation
result. Such a task is, however, beyond the scope of this thesis and will be left as future work.

In the evaluation, optical flow algorithms capable of running at real-time frame rates are
used. The algorithms are: the pyramidal implementation of Lucas-Kanade method Bouguet
[2001], Farneback algorithm Farnebäck [2003] and Brox et. al. total variational method Brox
et al. [2004]. The implementation of these algorithms is available in the GPU library module
of OpenCV. The Massively Parallel Lucas-Kanade method by Plyer et. al. Plyer et al. [2014]
could not be included in the evaluation as there is no source code or executable available to test
the algorithm. The algorithm by Werlberger Werlberger [2012] could not be tested as his test
application requires a legacy version of Nvidia CUDA.

To analyze the error performance of the optical flow filtering algorithm, one needs an image
dataset that replicates the operating conditions in which the algorithm is expected to be
deployed. That is, for the purpose of this thesis, a dataset should provide images from a highspeed
camera moving in complex environments. Moreover, the length of the dataset should be
enough for the filter algorithm to converge to a dense optical flow field.

These two properties are not present on any of the standard datasets available for optical
flow evaluation. Those are: the Middlebury dataset by Baker et al. [2011], the Kitti dataset
by Geiger et al. [2013] and the synthetic Sintel dataset by Butler et al. [2012]. These datasets
either provide short image sequences for which the optical flow filter cannot converge (such is
the case of the Middlebury and Kitti datasets) or the pixel displacement between frames is too
large for the algorithm to be able to identify.

Considering this, a synthetic dataset using Blender 3D modeling software10 is created to
simulate a high-speed camera moving in a complex environment. The dataset, named Bunny,
is shown in Figure 2.9 as a panoramic view. The camera moves to the right with a constant
linear velocity of 0:5 ms􀀀1 during one second, thus generating a total of 300 images. The
ground truth optical flow is calculated from the rendered depth math using the camera velocity
and intrinsic parameters, as explained in Appendix A.

state is set to zero and flow starts to be identified at brightness discontinuities (k = 1; 2). The
estimated flow spreads to textureless regions as the camera moves and the smoothing filter is
applied to the updated flow. This can be observed at the interior of the floor tiles (k = 5).
After approximately 25 frames (or 0.08 seconds) the optical flow covers almost all regions of
the image. At this point, the filter state maintains a dense estimate constantly updated given
new image data. Last three rows of Figure 2.10 illustrates the trailing edge artifact of the filter
algorithm. Optical flow in the leading edge (left side of the bunny) shows a sharp transition
between background and foreground flow. At the trailing edge (right hand side), background
objects are being discovered and the optical flow goes from high to low. This transition is,
however, not immediate and needs several time steps for the algorithm to “forget” about the
foreground flow.

Table 2.12 shows a comparison of the runtime of the flow-filter algorithm running on the Nvidia
Tegra K1 embedded System on Chip and the GTX 780 desktop GPU card. The GPU in the
Tegra K1 SoC has 192 CUDA cores with Kepler architecture and approximately 12 GB/s
shared memory bandwidth11. For reference, the GTX 780 has 2304 CUDA cores with Kepler
architecture and 288 GB/s dedicated memory bandwidth12.

The flow-filter algorithm was run on the T-K1 SoC at a quarter VGA resolution (320240)
and varying the maximum flow allowed in the algorithm. This effectively increases the number
of iterations of the numerical scheme for the propagation stage of the filter. The number of
smooth iterations was set to 1 for all benchmarks.

For 1 pixel maximum flow, the flow-filter algorithm runs at 114 Hz on the T-K1 chip, and
frequency decreases to 74 Hz for flow values up to 8 pixels. It is believed this image resolution
and frame rates might be sufficient to provide visual cues to a small robotic vehicle, such as
quadrotor, equipped with an embedded computer. Robotic vehicles capable of carrying heavier
payloads, such a car, can take advantage of desktop computer components to run the optical
flow algorithm at much higher frame rates.

A filter formulation for computing dense optical flow in real time was introduced. The algorithm
is constructed as a pyramid of filter loops, where an optical flow field is incrementally
built and constantly refined using new image data and previous state estimation.
Each filter in the pyramid structure is made of an update and propagation stage. The
propagation stage takes previous optical flow estimation and propagates it forward in time.
This is implemented as a set of partial differential equations modeling the transport of image
and optical flow by the optical flow itself. A fast numerical implementation based on final
difference methods can efficiently be implemented both on GPU and FPGA hardware.

In the update stage, the predicted optical flow is corrected using new image data coming
from the camera. The formulation of the update follows the standard brightness conservation
assumption to create a cost function to recover flow. A temporal regularization terms is added
to include the predicted optical flow as a prior solution to this cost at current frame. This
makes the problem well defined for all pixels in the image regardless of their texture content.
A simple average filter diffuses information from highly textured regions to flat regions, thus
covering the whole image with dense flow estimation.

Currently, an open-source implementation of the algorithm is developed for Nvidia GPU
hardware using CUDA framework. This implementation is capable of reaching frame rates
above 800 Hz working on VGA images.

Experimental evaluation was provided using both ground truth image sequences and reallife
high speed video. A synthetic dataset was created using Blender to simulate a high-speed
camera moving in a photo-realistic environment. The ground truth optical flow is calculated
from the camera’s velocity, intrinsic parameters and the rendered depth map. Evaluation on this
sequences shows the convergence of the algorithm to dense flow fields after approximately 30
frames, after which the optical flow is maintained with error levels below standard algorithms
available for comparison. At the same time, the flow-filter algorithm outperforms the rest of
the algorithms in terms of frame rate performance.

A second set of ground truth evaluation experiments were carried out on the Middlebury
test dataset of Baker et al. [2011]. To make the evaluation fair, the image sequences were augmented
using the ground truth optical flow provided in the dataset. Results of these evaluations
show better error performance than some of the state of the art real-time algorithms.
Finally, a qualitative comparison between the flow-filter algorithm and the pyramidal implementation
of Lukas-Kanade method (OpenCV) is presented. For this comparison, real-life
300 Hz high-speed video recorded in urban driving scenes at the ANU Campus and Civic are in
Canberra was recorded and processed offline. The reader is encouraged to look at the compiled
videos publicly available at the URLs in Table 2.11.

There are several computer data structures used to discretize the sphere. Roughly, these data
structures can be classified by the type of surface element used to represent a small portion of
the sphere. This section reviews the most relevant sphere pixelation data structures.
The icosahedral subdivision proposed by Baumgardner and Frederickson [1985] uses the
regular icosahedron (Platonic solid made of 20 equilateral triangles) as reference for the subdivision.
The subdivision splits each triangle into four smaller ones by cutting each edge in the
middle point and connecting the new vertex. For efficient storage of the points, it is possible
to divide the original icosahedron into ten quadrants made of two triangles each and store the
subdivision points of each quadrant as 2D memory arrays. This allows for efficient access of
points in the data structure using (row, column) coordinates on each quadrant. To interpolate
data within each triangle, one can use barycentric coordinates to compute a weighted average
at a given position inside a triangle.

The HEALPix data structure from Górski et al. [2005] uses equal-area four-side polygons
to cover the sphere. This data structure is widely used in astronomy for the analysis of celestial
data such as microwave background radiation. The HEALPix coverage of the sphere is such
that the face elements create isolatitude rings. This property is essential for fast implementation
of spherical harmonics. The data structure consists of twelve grid patches: 4 patched in the
north pole cap, 4 in the equatorial belt and 4 in the south pole cap. Figure 3.2 illustrates the
patch layout. Within each grid, faces can be indexed using a ring indexing scheme where
neighbor index correspond to faces in the same isolatitude ring, or nested scheme, where the
indexing works as a quad-tree index scheme. Either indexing scheme can be used on the same
data structure. The ring scheme is useful for applying Fourier transforms to the spherical data.

Concerning the desired pixelation properties at the beginning of this chapter, HEALPix
satisfies properties 2 and 3. Regarding property 1, neighboring points in the pixelation do not
form a local orthogonal grid on which one can efficiently run image filters.

The cubic mapping of the sphere, proposed by Ronchi et al. [1996] subdivides the faces of
a cube and projects them to the sphere. There are several ways to create the subdivision and
mapping: equiangular, conformal, elliptic and gnomic. Each mapping produces pixelations
that satisfy different properties such as smooth angular transition between faces, the angle between
points or orthogonality of the local coordinate frame Putman and Lin [2007]. Moreover,
it is possible to apply a regularization procedure to improve the quality of the pixelation. In
their work, the authors use a system of compression and torsion springs to regularize the distance
and angle between neighboring points. An important difference between this work and
the Spherepix data structure is the fact that Spherepix allows overlapping between cube faces,
which further improves the regularity of the generated grid.

The cubic mapping of the sphere provides a simple representation of the sphere in terms
of six two-dimensional grids. Indexing of points in the cube data structure is straightforward.
This data structure produces a good initial condition for the Spherepix regularization procedure
to create equidistant and orthogonal grids.

The last type of sphere data structures relies on overlapping between surface elements to
improve pixelation properties. The Yin-Yang by Kageyama and Sato [2004] grid is an example
of such data structures. This grid is composed of two surfaces (Yin and Yang) that combined
cover the whole sphere, as illustrated in Figure 3.4. The Yin and Yang faces are geometrically
identical and consists of rectangular grids sweeping =2 +  in latitude and 3=2 +  in the
longitude direction, where  is a small extra angle to create face overlapping. The subdivision
scheme uses equiangular steps in both latitude and longitude directions to create equiangular
grids. Operations on each grid can be applied with relative ease. In the border region, one
needs to interpolate data from the other grid in order to avoid artificial border artifacts during
computations. One consequence of the overlapping is that there are regions of the sphere with
two solutions. One will expect that the difference between these solutions is negligible with
respect to the discretization step. Alternatively, one can run an extra post-processing algorithm
to reconcile the solution in the overlapping area.

One use case example of the Yin-Yang grid is the work of Hara et. al. Hara et al. [2015]
for the computation of SIFT feature points on the sphere.

The objective of the Spherepix data structure is to design a discretization of the sphere in which
residual error in orthogonality and equal spacing properties of the pixelation are minimized and
distributed across the data structure as evenly as possible. If these two properties are satisfied,
then the accuracy of low-level shift invariant image processing operations on spherepix will
not suffer significantly, opening the door to high-speed image processing of omnidirectional
images.

Image processing algorithms in the Spherepix patches comprise three distinct components.
First, camera images are mapped onto the Spherepix patches using the calibrated camera model
of the capturing system. Secondly, image processing subroutines run on the separate grid
patches using interpolated data near the boundaries. Finally, after each image subroutine, the
data from overlapping grid patches is reconciled to produce a homogeneous representation of
the output

In the recall–versus–(1 - precision) curve we observe the classical SIFT algorithm has
both a lower recall level for moderate precision (less than 90% precision, that is, greater than
10% false matches) as well as a poor degradation in recall as precision is increased and false
matches are eliminated. It is interesting to note that the spixSIFT and sSIFT algorithms have
comparable recall at moderate precision and spixSIFT has clearly superior recall for very high
precision, when the matching threshold is increased to ensure false matches are less than 10%.
It is believed that this is due to the fact that the Spherepix representation ensures minimal
distortion of the SIFT feature descriptors across the full image.

In terms of runtime performance, SIFT on the original image (1024 x768) takes 240 ms
on CPU. For spixSIFT, the mapping from the omnidirectional image to the six 256  256
Spherepix patches takes in total 0.085 ms on GPU plus 120 ms on CPU to extract the feature
points. The decrease in total computation time for spixSIFT is due to a reduction in the total
number of pixels required after mapping to Spherepix patches. This is because the full image
effectively over-samples in the peripheral zone of the image due to the catadioptric distortion
(see Figure 3.12d). For sSIFT, the reported runtime is 2:439 104 ms.

Local motion control of a robotic vehicle using visual remains a difficult task in robotics.
Optical flow has been widely used to incorporate visual information into the robot’s control algorithm.
Applications such as landing and obstacle avoidance have successfully been explored
in the past. Motion control algorithms that rely on raw optical flow are limited to specific
scenarios, for example: grazing landing Srinivasan et al. [2000], corridor centering Srinivasan
[2011b] and altitude regulation Ruffier and Franceschini [2005]. More sophisticated tasks such
as landing and obstacle avoidance require post-processing of the optical flow field to infer the
‘looming’ effect, or optical flow divergence, that occurs when an object is approached directly.
For example, Nelson and Aloimonos [1989] compute the divergence of the optical flow field
and then use this for obstacle avoidance. Hamel and Mahony [2002], Herisse et al. [2008] and
McCarthy and Barnes [2012] integrate the optical flow field divergence over the camera’s field
of view to compute the relative 3D velocity of the landing platform, effectively computing the
looming effect by integration over a large area.

This chapter introduces a new robo-centric spatio-temporal representation of motion, the
structure flow Adarve and Mahony [2016b]. Structure flow generalizes classical optic flow,
that measures translation of an image point across the image surface, by including an additional
normal component measuring the rate of angular ‘looming’ at each point in the image.
Structure flow will have a similar utility to optic flow for control of robotic vehicles where
in addition to existing methodologies, the normal component of structure flow directly yields
motion cues associated with approach to obstacles. Geometrically. structure flow is a 3-vector
assigned to each ‘pixel’ in the image comprised of the three-dimensional Euclidean velocity
between the robot and the environment (the scene flow Vedula et al. [1999]) scaled by the
inverse range of the scene.

Figure 4.2b illustrates the optical, scene and structure flow fields for a perspective camera
moving at 10ms􀀀1 in collision course with a building located approximately at 15m. Since
the scene is static, the calculated scene flow is equal for all pixels in the image, the negative
of the camera velocity. Scaling the scene flow by the inverse of the depth field, one obtains
the structure flow field. Note the clear identification between objects close to or far away
to the camera that is of direct importance in vehicle control. In contrast, the optical flow is
a divergent vector field with the focus of expansion located in the center of the image; the
direction of motion. Although the vehicle is moving fast, the optical flow in the central region
is small, and it is difficult to evaluate the time to contact for the vehicle to collide with the
building.

In addition to its relevance for robotic motion control, structure flow plays an intrinsic
role in understanding image kinematics and dynamics. Image kinematics are governed by the
well known constant brightness condition Barron [1994] which depends on optical flow, or in
the general case, on the projection of structure flow on the image plane. For reasonable assumptions
on the environment and camera motion, Partial Differential Equation (PDE) can be
derived for the evolution of the structure flow that depends only on exogenous acceleration and
rotation of the camera. These PDEs are naturally derived using a spherical camera model, and
as it will be shown, they can efficiently been implemented using the Spherepix data structured
developed in Chapter 3.

This chapter also introduces a filtering algorithm to compute structure flow in real-time
from stereo image data. The approach taken is a highly distributed predictor-update algorithm
implemented on GPU hardware. The prediction step is a numerical implementation of a PDE
integration scheme that propagates the current estimate of the structure flow forward in time.

Error in the structure flow estimate can then be estimated across a sequence of temporal frames.
A simple least-squares regularized update is used to iteratively correct errors in the structure
flow estimate. The resulting algorithm runs on a Nvidia GTX-780 GPU, processing 512 x512
at approximately 600 Hz for flow vectors up to 8 pixels in magnitude. The relative performance
of the algorithm far outperforms (10-20 times faster) all scene flow algorithms in the literature
in terms of processing speed. Although it is less accurate than some classical algorithms, its
advantages in speed, dense estimation, and robustness make it highly suitable for real-world
mobile robotic applications.

Stereoscopic algorithms use image sequences captured using a stereo camera to estimate the
3D scene flow. Early work by Patras et al. [1996] formulates the joint estimation of disparity
and motion (scene flow) from stereo sequences. Huguet and Devernay Huguet and Devernay
[2007] proposed a variational approach for the joint computation of disparity and optical flow.
This formulation of optical flow plus temporal disparity change is known as disparity flow
Gong and Yang [2006]. Wedel et al. [2011] decouple the variational formulation of to compute
flow and disparity in two separate stages while preserving stereo constraints. Vogel et al. [2015]
compute scene flow by means of a piece-wise planar segmentation of stereo images for which
a 3D rotation and translation (scene flow) is computed. The accuracy of recent two-pair stereo
based algorithms is compiled on the Kitti scene flow dataset by Menze and Geiger [2015].

In terms of real-time capable dense 3D motion estimation algorithms, most methods use Graphics
Processing Units (GPU) for implementing per-pixel level operations in parallel. Gong
[2009] reports 12 Hz optical and disparity flow estimation on QVGA image resolution (320 
240). Rabe et al. [2010] reported 25 Hz frequency on VGA (640 x 480) image sequences
using GPU. The algorithm of Wedel et al. [2011] reports frame rates of 20 Hz also at QVGA
resolution. RGB-D flow by Herbst et al. [2013] runs between 8-30 Hz at QVGA depending on
the amount of smoothing required.

Ranking plays a central role in many pattern recognition applications such as feature
selection, where the goal is to produce ranked lists of features to defy the curse
of dimensionality and improve machine learning, information retrieval (e.g., document/
image retrieval, person identification) where ranking of query results is one
of the fundamental problems, recommendation systems that produce ranked lists
of items according to what the user would probably be interested in, and so on.
This thesis examines the ranking problem from two different methodological perspectives:
ranking to learn, which aims at studying methods and techniques to sort
objects with the aim to enhance models generalization by reducing overfitting, and
learning to rank, which makes use of machine learning to produce ranked lists of
objects to solve problems in those domains that require objects to be sorted with
some particular criteria.

This chapter starts with ranking to learn and proposes, in Section 2.1, the related
literature of feature selection methods, since any set of hand-crafted features
may contain many features that are either redundant or irrelevant, and can thus be
removed without incurring loss of information. Indeed, in classification scenarios
the aim is to find useful features that separate well two or more classes, therefore,
ranking is used to individuate those features which are highly representative of a
class [87]. Section 2.2 starts with a task of learning to rank in the context of behavioral
biometric of keystroke dynamics. This section introduces the related literature
of users’ authentication, identification and verification in dyadic textual chat conversations.
In this context ranking is used to validate the identity of a user who
wishes to sign into a system by measuring some intrinsic characteristic of that user.
In section 2.3, we briefly review the main recommender system frameworks, where
their goal is to sort a set of items according to some criteria of preference, dictated
by the user.

Since the mid-1990s, few domains explored used more than 50 features. The situation
has changed considerably in the past few years and most papers explore domains
with hundreds to tens of thousands of variables or features. New approaches
are proposed to address these challenging tasks involving many irrelevant and redundant
variables and often comparably few training examples. In this work, the
term variable is used for the raw input variables, and features for variables constructed
from input variables. There is no distinction between the terms variable
and feature if there is no impact on the ranking and selection algorithm. The distinction
is necessary in the case of “feature learning” and “deep learning” methods
for which features are not explicitly computed (hand-crafted), but learnt from input
variables.

Two examples are typical of the feature selection domains and serve us as illustration
throughout this section on ranking to learn. One is gene selection from
microarray data and the other is image classification. In the gene selection problem,
the variables are gene expression coefficients corresponding to the abundance
of mRNA in a sample (e.g. tissue biopsy), for a number of patients. A typical classification
task is to separate healthy patients from cancer patients, based on their
gene expression profile. Usually fewer than 100 examples (patients) are available
altogether for training and testing. But, the number of variables in the raw data
ranges from 6,000 to 60,000. Some initial filtering usually brings the number of
variables to a few thousand. Because the abundance of mRNA varies by several
orders of magnitude depending on the gene, the variables are usually standardized.
In the image classification problem, the images may be represented by a bag-ofwords,
that is a vector of dimension the size of the vocabulary containing word
frequency counts. Vocabularies of hundreds of thousands of words are common,
but an initial pruning of the most and least frequent words may reduce the effective
number of words to 15,000. With the advent of deep learning, the number of
variables has grown to hundreds of thousands. Large image collections of 150,000
images with the presence or absence of 1000 object categories, are available for research
[55, 129, 221]. Typical tasks include the automatic sorting of images into a
web directory, the object localization, object detection, object detection from video,
scene classification, and scene parsing.

There are several benefits of selecting a subset of relevant features for use in
model construction. The central premise when using a feature selection technique is
that the data contains many features that are either redundant or irrelevant, and can
thus be removed without incurring much loss of information. This section surveys
the works proposed in last few years, that focus mainly on ranking and selecting
subsets of features that are useful to build a good predictor.

Generally, FS techniques can be partitioned into three classes [86]: wrappers
(see Fig. 2-2), which use classifiers to score a given subset of features; embedded
methods (see Fig. 2-3), which inject the selection process into the learning of the
classifier; filter methods (see Fig. 2-1), which analyze intrinsic properties of data,
ignoring the classifier. Most of these methods can perform two operations, ranking
and subset selection: in the former, the importance of each individual feature
is evaluated, usually by neglecting potential interactions among the elements of the
joint set [60]; in the latter, the final subset of features to be selected is provided.
In some cases, these two operations are performed sequentially (ranking and selection)
[90, 26, 84, 268, 147, 266]; in other cases, only the selection is carried
out [85]. Generally, the subset selection is always supervised, while in the ranking
case, methods can be supervised or not. While wrapper models involve optimizing
a predictor as part of the selection process, filter models rely on the general characteristics
of the training data to select features with independence of any predictor.
Wrapper models tend to give better results but filter methods are usually computationally
less expensive than wrappers. So, in those cases in which the number of
features is very large, filter methods are indispensable to obtain a reduced set of
features that then can be treated by other more expensive feature selection methods.
Embedded methods, Fig. 2-3, differ from other feature selection methods in
the way feature selection and learning interact. In contrast to filter and wrapper approaches,
in embedded methods the learning part and the feature selection part can
not be separated - the structure of the class of functions under consideration plays
a crucial role. For example, Weston et al.[257] measure the importance of a feature using a 
bound that is valid for Support Vector Machines only thus it is not possible
to use this method with, for example, decision trees.

Each feature selection method can be also classified as Supervised or Unsupervised.
For supervised learning, feature selection algorithms maximize some function
of predictive accuracy. Because class labels are given, it is natural to keep only
the features that are related to or lead to these classes. Generally, feature selection
for supervised machine learning tasks can be accomplished on the basis of the following
underlying hypothesis: “a good feature subset is one that contains features
highly correlated with (predictive of) the class, yet uncorrelated with (not predictive
of) each other [76]”. A feature which is highly correlated with the class may be
defined as “relevant”, whereas a feature which is uncorrelated with the others as not
“redundant”. Redundant features are those which provide no more information than
the currently selected features, and irrelevant features provide no useful information
in any context.

Nowadays, biometric technologies are used to secure several electronic communications,
including access control and attendance, computer and enterprise network
control, financial and health services, government, law enforcement, and
telecommunications. These systems can be logically divided into two, namely, enrollment
phase and authentication/verification phase. During the enrollment phase
user biometric data is acquired, processed and stored as reference file in a database.
This is treated as a template for future use by the system in subsequent authentication
operations. Authentication is the process of determining whether a user is, in
fact, who they are declared to be. For example, in the information security world,
user authentication is analogous to entering a password for a given username, which
is an authentication form based on something that the user have. Instead, Biometric
Authentication (BA) is performed via something the user is. In fact, BA uniquely
identifies the user through one or more distinguishing biological traits, such as fingerprints,
hand geometry, earlobe geometry, retina and iris patterns, voice waves,
keystroke dynamics, DNA and signatures. Among the many types of biometric
authentication technologies this thesis focuses on keystroke biometrics. Comprehension
of the statistical and structural mechanisms governing these dynamics in
online interaction (e.g., text chats) plays a central role in many contexts such as
user identification, verification, profile development, and recommender systems.
The behavioral biometric of keystroke dynamics uses the manner and rhythm in
which a user types characters on a keyboard [267, 5, 231]. The keystroke rhythms
of a user are measured to develop a unique biometric template of the user’s typing
pattern for future authentication. Data needed to analyze keystroke dynamics is
obtained by keystroke logging. Normally, all that is retained when logging a text
chat session is the sequence of characters corresponding to the order in which keys
were pressed and timing information is discarded. However, research is interested
in using this keystroke dynamic information to verify or even try to determine the
identity of the person who is producing those keystrokes. This is often possible because
some characteristics of keystroke production are as individual as handwriting
or a signature. In other words, keystroke dynamics is a behavioral measurement
and it aims to identify users based on the typing of the individuals or attributes such
as duration of a keystroke or key hold time, latency of keystrokes (inter-keystroke
times), typing error, force of keystrokes etc.

Many other feature extraction methods and features/attributes which make use
of keystrokes have been proposed in the last ten years. Gaines et al. [74] use statistical
significance tests between 87 lowercase letter inter-key latencies to check
if the means of the keystroke interval times are the same. Young and Hammon
[264] experimented with the time periods between keystrokes, total time to type
a predetermined number of characters, or the pressure applied to the various keys
and used it to form the feature template. Joyce and Gupta [118] propose two additional
login sequences: the user’s first name and the last name as the feature subset.
This improved the performance of the method considerably. Obaidat and Sadoun
[175] suggest inter-key and key hold times to be recorded using Terminate and Stay
resident (TSR) program in MS-DOS based environment. The standard keyboard
interrupt handler was replaced by a special scan codes to record the time stamp.
Lin [144] suggests a modified latency measurement to overcome the limitation of
negative time measure, i.e. when the second key is pressed before the release of
the first key. William and Jan [52] propose typing difficulty feature in the feature
subset to increase categorization. Robinson and Liang [202] use user’s login string
to provide a characteristic pattern that is used for identity verification. 

In [167], the authors examine the use of keystroke duration and latency between keystrokes 
and combine it with the user’s password. Monrose and Rubin [168] propose that users
can be clustered into groups comprising disjoint feature sets in which the features in
each set are pair wise correlated. In [258], box plot algorithm was used as a graphical
display with many features and the features extracted was normalized using
normal bell curve algorithm. Bergadano et al. [16] use timing information to obtain
the relative order of trigraphs. It is used to compare two different sets of sorted
trigraphs and to measure the difference in the ordering between them. In [154], the
users were asked to type the usual password, or passphrase twelve times to get the
digraph for further processing. Chen et al. [150] extracted features from the frequency
domain signal which include mean, root mean square, peak value, signal in
noise and distortion, total harmonic distortion, fundamental frequency, energy, kurtosis,
and skew ness. Nick and Bojan [11] incorporated shift-key pattern along with
keystroke inter-key and hold times. Kenneth [200] proposes motif signature which
is used for classification. In order to improve the quality of data Pilsung et al. 
and Sungzoon Cho and Seongseob Hwang [37] proposed artificial rhythms which
increase uniqueness and cues to increase consistency. Hu et al. [108] use trigraphs
(three consecutively typed keys) also known as Degree of Disorder as features and
normalize the distance to find the feature subset. Mariusz et al. [222] propose an
approach to select the most interesting features and combine them to obtain viable
indicator of user’s identity. Christopher et al. [137] used three stage software design
process along with data capture device hardware together with pattern recognition
techniques like Bayesian and Discrimination function for classification. They used
keystroke pressure and duration as features. Woojin [32] applies discrete wavelet
transformation (DWT) to a user’s keystroke timing vector (KTV) sample in the
time domain, and then produces the corresponding keystroke wavelet coefficient
vector (KWV) in the wavelet domain. In [77], a comparative study of many techniques
considering the operational constraints of use for collaborative systems was
made. Majority of the studies have identified three fundamental attributes: duration,
latency and digraph. Many statistical properties of the attributes such as mean,
standard deviation and Euclidean distance are measured and are used to construct
the user reference profile. Each method has its own pros and cons as the number of
test subjects differs.

No other technology penetrated our everyday life as quickly and ubiquitously as social
media. Billions of users are using these platforms every day, exploiting novel
communication means. Among the computer-mediated technologies, Facebook,
Twitter, YouTube, Google+, Skype, and WhatsApp are the most pervasive. In social
media recommender systems play an increasingly important role. The main reason
is that social media allow effective user profiling. For example, Facebook, MySpace,
LinkedIn, and other social networks use collaborative filtering to recommend
new friends, groups, and other social connections (by examining the network of
connections between a user and their friends). Twitter uses many signals and inmemory
computations for recommending who to follow to its users. The goal of
recommender systems is to estimate a users preference and deliver an ordered list
of items that might be preferred by the given user.

In other words, the recommendation problem can be defined as estimating the
response of a user for new items, based on historical information stored in the system,
and suggesting to this user novel and original items for which the predicted
response is high. The type of user-item responses varies from one application to the
next, and falls in one of three categories: scalar, binary and unary. Scalar responses,
also known as ratings, are numerical (e.g., 1-5 stars) or ordinal (e.g., strongly agree,
agree, neutral, disagree, strongly disagree) values representing the possible levels
of appreciation of users for items. Binary responses, on the other hand, only have
two possible values encoding opposite levels of appreciation (e.g., like/dislike or
interested/not interested). Finally, unary responses capture the interaction of a user
with an item (e.g., purchase, online access, etc.) without giving explicit information
on the appreciation of the user for this item. Since most users tend to interact
with items that they find interesting, unary responses still provide useful information
on the preferences of users. Ranking items, according to predicted responses,
and selecting a subset from the top of the ranked list produces a recommendation.

One approach to the design of recommender systems that has wide use is userbased
collaborative filtering [27]. These methods are based on collecting and analyzing
a large amount of information on users’ behaviors, activities or preferences
and predicting what users will like based on their similarity to other users. Many
algorithms have been used in measuring user similarity or item similarity in recommender
systems. For example, the k-nearest neighbor (k-NN) approach and the
Pearson Correlation as first implemented by Allen [56]. Other common approaches
when designing recommender systems are item-based and content-based systems.
The general principle of content-based (or cognitive) approaches [18, 19, 183] is to
identify the common characteristics of items that have received a favorable rating
from a user, and then recommend to the user new items that share these characteristics.
User-based collaborative approaches overcome some of the limitations
of content-based ones. For instance, items for which the content is not available
or difficult to obtain can still be recommended to users through the feedback of
other users. Furthermore, collaborative recommendations are based on the quality
of items as evaluated by peers, instead of relying on content that may be a bad indicator
of quality. Finally, unlike content-based systems, collaborative filtering (userbased)
ones can recommend items with very different content, as long as other users
have already shown interest for these different items.


In the context of biometric verification systems, performance evaluation is achieved
through estimating the FPR and the TPR and using these estimates to construct a
ROC curve, described above, that expresses the trade-off between the FPR and
TPR. This is used for the so called One-to-One (1 : 1) identification systems,
whereas the task is verification. Verification performance is defined as the ability of
the system in verifying if the biometric template that the probe user claims to be is
truly themselves. The ROC is a well-accepted measure to express the performance
of 1 : 1 matchers. On the other hand, a Cumulative Match Characteristic curve
(CMC) judges the ranking capabilities of biometric identification systems and it is
used as a measure of One-to-Many (1 : N) identification system performance [21].
Identification tasks involve a comparison against the entire biometric database. The
CMC is an effective performance measure for identification systems.
Specifically, when a system always returns the identities associated

Over last decade, successful results obtained in solving pattern recognition problems
(i.e., from vision to language) have been heavily based on hand-crafted features.
Obviously, performance of algorithms relied crucially on the features used.
As a result, progress in pattern recognition was based on hand-engineering better
sets of features. Over time, these features started becoming more and more
complex - resulting in difficulty with coming up with better, more complex features.
From a methodological perspective, there were principally two steps to be
followed: feature design and learning algorithm design, both of which were largely
independent. Meanwhile, research in the machine learning community has been focusing
on learning models which incorporated learning of features from raw data.
These models typically consisted of multiple layers of non-linearity. This property
was considered to be very important and lead to the development of the first deep
learning models (e.g., Restricted Boltzmann Machines [104], Deep Belief Networks
[105]). Nowadays, deep learning is expanding the scope of machine learning by
making algorithms less dependent on feature engineering. These techniques offers
a compelling alternative: the automatic learning of problem specific features.
In 2012 at the Large Scale Visual Recognition Challenge (ILSVRC2012), for
the first time, a deep learning model called “Convolutional Neural Network” (CNN)
[129] decreased the error rate on the image classification task by half for the first
time, outperforming traditional hand-engineered approaches. The network designed
by Alex Krizhevsky in [129], popularly called “AlexNet” will be used and modified
in the coming years for various vision problems.

Hornik in 1991 introduced the popular Universal Approximation theorem [107].
This well-known theorem states that a neural network with a single, hidden layer
is sufficient to model any continuous function. However, a few years later [14]
showed that such networks need an exponentially large number of neurons when
compared to a neural network with many hidden layers. It also showed that poor
generalization may be expected when using an insufficiently deep architecture for
representing some functions. With the introduction of greedy layerwise pre-training
by [105], researchers were able to train much deeper networks. This fact played a
major role in bringing the so-called Deep Learning systems into mainstream machine
learning.

While depth tends to improve network performances, it also makes gradientbased
training more difficult since deeper networks tend to be more non-linear. To
address these problems, [111] proposes stochastic depth. The principal idea is to
start with very deep networks but during training, for each mini-batch, randomly
drop a subset of layers and bypass them with the identity function. This simple
approach complements the recent success of residual networks. With stochastic
depth the number of layers increases beyond 1; 200 and the network still yields
meaningful improvements in test error.

Finally, a further improvement of ResNets leads to Dense Convolutional Networks
(DenseNets). They connect each layer to every other layer in a feed-forward
fashion [110]. These networks can be substantially more accurate since they contain
shorter connections between layers close to the input and those close to the
output. Moreover, DenseNets alleviate the vanishing-gradient problem, strengthen
feature propagation, encourage feature reuse, and substantially reduce the number
of parameters.

The back-propagation algorithm was introduced in the 1970s, but its importance
was not fully appreciated until a famous 1986 paper [220]. That paper shows that
back-propagation works far faster than earlier approaches to learning, allowing several
neural networks to solve problems which had previously been unsolvable. Today,
the back-propagation algorithm is a standard of learning in neural networks.
This section is more mathematically involved than the rest of the chapter, and it
assumes that the reader is familiar with traditional neural networks (see [20] for
further details).

Optimization methods, such as the limited-memory Broyden-Fletcher-Goldfarb-
Shannon (BFGS) algorithm, which use the full training data to estimate the next
update to parameters tend to converge conform well to local optima. However, in
practical applications the cost and gradient for the entire training set can be very
slow and sometimes intractable on a single machine. Moreover, a second limitation
of such methods is that they do not give a straightforward way to incorporate new
data in an “online” setting. Stochastic Gradient Descent (SGD) addresses both of
these issues by following the negative gradient of the objective after seeing only a
single or a few training examples. The use of SGD in the deep neural network setting
is motivated by the high cost of running back propagation over the full training
set, since SGD can overcome this cost and still lead to fast convergence.

Overfitting occurs when a model is excessively complex, for example, when having
too many parameters relative to the number of observations. A model that has
been overfit has poor predictive performance, as it overreacts to minor fluctuations
in the training data. Increasing the amount of training data is one way of reducing
overfitting. Another possible solution is to reduce the number of network layers L
so as to obtain far fewer parameters. However, large networks have the potential
to be more powerful than small networks, and so this is an option adopted reluctantly.
Other interesting techniques to reduce overfitting with fixed network depth
and fixed training data are known as regularization techniques. In this section we
describe one of the most frequently used regularization approaches known as L2
regularization or weight decay [174].

Dropout prevents overfitting and provides a way of approximately combining many
different neural network architectures exponentially and efficiently. Indeed, dropout
can be viewed as a form of ensemble learning. The term dropout refers to dropping
out units (hidden and visible) in a neural network (see Figure 3-7). By dropping
a unit out, we mean temporarily removing it from the network, along with all its
incoming and outgoing connections. In particular, given a training input x and corresponding
desired output y. Dropout, randomly and temporarily deletes the 7% of
the hidden units in the network, while leaving the input and output units untouched.
After this step, the training procedure starts by forward-propagating x through the
network (x as a mini-batch of examples), and then back-propagating to determine
the contribution to the gradient, therefore updating the appropriate weights and biases.
As shown in Figure 3-7.b, the neurons which have been temporarily deleted,
are still ghosted in the network. The described process is repeated, first restoring the
dropout neurons, then choosing a new random subset of hidden neurons to delete,
estimating the gradient for a different mini-batch x, and updating the weights and
biases in the network.

In this chapter, we have surveyed the use of modern deep learning networks, in
particular we provide an intuitive introduction to convolutional neural networks that
relies on both biological and theoretical constructs. We showed that CNNs enabled
complicated hand-tuned algorithms being replaced by single monolithic algorithms
trained in an end-to-end manner. We reviewed the basics of supervised learning
for deep architectures. Firstly, we looked briefly into the history of biologically
inspired algorithms exploring the connections between neuroscience and computer
science enabled by recent rapid advances in both biologically-inspired computer
vision and in experimental neuroscience methods. Secondly, we introduced the
convolutional neural network architectures, where we discussed the choice of the
cross-entropy cost function and presented the back-propagation algorithm. Thirdly,
the mini-batch stochastic gradient descent algorithm was addressed along with one
of its popular variants known as Momentum. Then, we addressed the regularization
methods (such as L2 and L1 regularization and dropout) which make the deep networks
better at generalizing beyond the training data, and considerably reducing the
effects of overfitting. The last part focused on transfer learning by fine-tuning deep
networks, which offers a way to take full advantage of existing datasets to perform
well on new tasks. In conclusion, we provided a discussion on the need to be deep,
highlighting pros and cons of very deep architectures.

The collection and organization of data are an integral and critical part of the research
process. A formal data collection process is necessary as it ensures that the
data gathered are both defined and accurate and that subsequent decisions based on
arguments embodied in the findings are valid. The process provides both a baseline
from which to measure and in certain cases a target on what to improve. Research
data is a set of values that is collected, observed, or created, for purposes of analysis
to produce original research results. Data can be generated or collected from interviews,
observations, surveys, experiments, or even from previous literature. Often,
data are irreplaceable. For this reason, it is important for researchers to be accurate
and precise with their data collection and storage techniques and processes.
Depending on the source and methodology involved, data collection may be time
consuming and expensive.

Under the perspective of pragmatics and social signal processing, it is interesting
to verify the presence of non verbal cues in chats. Non verbal signals enrich the
spoken conversation by characterizing how sentences are uttered by a speaker [125],
forging a unique style that characterizes the latter among many other subjects. In the
same vein, they express personal beliefs, thoughts, emotions and personality [54].
As a result, this dataset includes personality traits of the subjects involved in the
experiments. These traits, intended as individual differences that endure over time,
can be measured by well-known self-administered questionnaires such as 1) the 10
Item Big Five Inventory (Big5) [197, 198],2) the Barratt Impulsiveness Scale [182],
focused on 3 different impulsiveness factors (attentional, motor and non-planning
impulsiveness), 3) BIS-BAS [30], decoding human motivations to behavioral inhibition
(BIS) and activation (BAS), and 4) PANAS [255], analyzing positive and
negative affectivity traits.

The data collection has been based on a public website, where the text chatting
interface has been equipped with keylogging functionalities. At the moment of
the subscription, users have been informed that the chat platform was equipped
with a key logging application, but that the produced keylogs would be kept private
and would not be shared. The data collection has been realized on this platform
by setting up a campaign of chats; the subscribers who met certain requirements
have been invited to participate to an experiment involving a text chat session along
with the filling in of some questionnaires. The explanation of the experiment was
given in general terms, and the time required to complete the experiment was not
specified. The selected subjects are in the age interval of 18-30 years. The cultural
background of the participants is uniform. Most of the participants have a university
education (the most represented subjects are Master students in Computer Science)
and were recruited at the University of Verona.

The chats were conducted by a single operator, not having friendship ties with
the subjects. The chats were at least 20 minutes long, and the generic arguments
suggested by the operator were “holidays” and “friends”. The visual interface employed
in the dialogs mirrors the common instant-messaging platforms like Facebook,
Skype and the like. Due to the implementation of the chat software, the key
logging mechanism allows to get the timings of each button hit with a precision of
milliseconds, synchronized with the timing of the other participant. A total of 50
subjects participated to the experiment, with an average age of 24 years, standard
deviation p = 1:5.

The conversations revolve around the Winter Survival Task (WST), a scenario
that is often adopted in social psychology research [117], in which the subjects
are asked to identify items that increase the chances of survival after a plane crash
in Northern Canada [117] (see Appendix B for further details). In particular, the
subjects are given a list of 12 items (steel wool, axe, pistol, butter, newspaper, lighter
without fuel, clothing, canvas, air-map, whisky, compass, chocolate) and they have
to make a consensual decision for each of them (“yes” if it increases the chances of
survival and “no” if it does not). The main advantage of the scenario is that people,
on average, do not hold any expertise relevant to the topic. Thus, the outcome of the
conversations depends on social dynamics rather than on actual knowledge about
the problem. Moreover, the WST requires that the items are discussed sequentially
and that the participants do not move to the next item until agreeing on a decision
for the current one. The decision agreed on for any item cannot be changed after
moving to the next item.

A chat can be thought of as a stream of keys that are typed sequentially by the
subjects. The sequence can be split into tokens, i.e., strings of non-blank characters
delimited by blank spaces. The rationale behind this choice is that such strings
should correspond to semantically meaningful units. Overall, the data includes a
total of 33,085 tokens, 21,019 typed by the female subjects and 12,066 typed by
the male ones. In every dyadic conversation, one of the subjects is proactive (the
person that starts the chat) and the other is reactive (that person that joins the chat).
Every subject has been randomly assigned one of these two roles.

Recent soft-biometric approaches have shown the ability to unobtrusive acquire
these traits from social media [211, 205], or infer the personality types of users
from visual cues extracted from their favorite pictures from a social signal processing
perspective [251]. While not necessarily corresponding to the actual traits
of an individual, attributed traits are still important because they are predictive of
important aspects of social life, including attitude of others and social identity.
As a result, the proposed benchmark includes 1,200 spontaneously uploaded
images that hold a lot of meaning for the participants and their related annotations:
positive/negative (see Table 4.4 for further details). The images are personal (i.e.,
family, friends etc.) or just images participants really like/dislike. The motivations
for labeling a picture as favorite are multiple and include social and affective aspects
like, e.g., positive memories related to the content and bonds with the people that
have posted the picture. Moreover, they are provided with a set of TAGS describing
the content of each of them.

In an era where accumulating data is easy and storing it inexpensive, feature selection
plays a central role in helping to reduce the high-dimensionality of huge
amounts of otherwise meaningless data. Advances over the past decade have highlighted
the fact that feature selection leads to many potential benefits. Selecting
subset of discriminant features helps in data visualization and understanding, in
reducing the measurement and storage requirements, and in reducing training and
utilization times. Moreover, in many modern classification scenarios, the number
of adopted features is usually very large, and it is continuously growing due to several
causes. Data sets grow rapidly, in part because they are increasingly gathered
by cheap and numerous information-sensing mobile devices, aerial (remote sensing),
software logs, cameras, microphones, radio-frequency identification readers
and wireless sensor networks [58, 225]. The world’s technological per-capita capacity
to store information has roughly doubled every 40 months since the 1980s as
of 2012, every day 2.5 exabytes (2:51018)) of data are generated [103]. The management
of high-dimensional data requires a strong feature selection to individuate
irrelevant and/or redundant features and avoid overfitting [86]. In particular, feature
selection has become crucial in classification settings like object recognition,
recently faced with feature learning strategies that originate thousands of cues. Feature
selection is also used in mining complex patterns in data. The rationale behind
this fact is that feature selection is different from dimensionality reduction. Both
approaches seek to reduce the number of attributes in the dataset, but a dimensionality
reduction method do so by creating new combinations of attributes, where as
feature selection methods include and exclude attributes present in the data without
changing them. Examples of dimensionality reduction methods include Principal
Component Analysis (PCA), Singular Value Decomposition (SVD) and Sammons
Mapping. On the other hand, feature selection is itself crucial and it mostly acts as
a filter, muting out features that are not useful in addition to the existing features.

Ranking to learn is inserted in the above scheme, and it aids one to create an
accurate predictive model by firstly ranking all the the features available and then
selecting a subset of the most relevant ones. In particular, ranking to learn in classification
settings helps in choosing features that will lead to as good or better accuracy
of the system whilst requiring less data. Feature selection methods can be used to
identify and remove unneeded, irrelevant and redundant attributes from data that do
not contribute to the accuracy of a predictive model or may in fact decrease the accuracy
of the model. Fewer attributes is desirable because it reduces the complexity
of the model, and a simpler model is simpler to understand and explain. Therefore,
the objective of variable selection is three-fold: improving the prediction performance
of the predictors, providing faster and more cost-effective predictors, and
providing a better understanding of the underlying process that generated the data
[86].

In this chapter we propose two feature selection methods exploiting the convergence
properties of power series of matrices, and introducing the concept of
graph-based feature selection [215, 206, 214]. The most appealing characteristic of
the approaches is that they evaluate the importance of a given feature while considering
all the other cues, in such a way that the score of each feature is influenced
by all the other features of the set. The idea is to map the feature selection problem
to an affinity graph, and then to consider a subset of features as a path connecting
them. The cost of the path is given by the combination of pairwise relationships
between the features.

The first approach we are about to present (in Section 5.1) evaluates analytically
the redundancy of a feature with respect to all the other ones taken together,
considering paths of a length that tends to infinity. For this reason, we dub our
approach infinite feature selection (Inf-FS). The Inf-FS is extensively tested on 13
benchmarks of cancer classification and prediction on genetic data (Colon [246],
Lymphoma [62], Leukemia [62], Lung181 [80], DLBCL [232]), handwritten recognition
(USPS [33, 34], GINA [1], Gisette [88]), generic feature selection (MADELON
[88]), and more extensively, object recognition (Caltech 101-256 [133], PASCAL
VOC 2007-2012 [63, 64]). We compare the proposed method on these datasets,
against eight comparative approaches, under different conditions (number of features
selected and number of training samples considered), overcoming all of them
in terms of stability and classification accuracy, and setting the state of the art on 8
benchmarks, notably all the object recognition datasets. Additionally, Inf-FS also
allows the investigation of the importance of different kinds of features, and in this
study, the supremacy of deep-learning approaches for feature learning has been
shown on the object recognition tasks.

In the Inf-FS formulation, each feature is a node in the graph, a path is a selection
of features, and the higher the score, the most important (or most different)
the feature. Therefore, considering a selection of features as a path among feature
distributions and letting these paths tend to an infinite number permits the investigation
of the importance (relevance and redundancy) of a feature when injected into
an arbitrary set of cues. Ranking the importance individuates candidate features,
which turn out to be effective from a classification point of view, as proved by a
thoroughly experimental section. The Inf-FS has been tested on thirteen diverse
benchmarks, comparing against filters, embedded methods, and wrappers; in all the
cases we achieve top performances, notably on the classification tasks of PASCAL
VOC 2007-2012. The novelty of Inf-FS in terms of the state of the art is that it
assigns a score of “importance” to each feature by taking into account all the possible
feature subsets as paths on a graph, bypassing the combinatorial problem in a
methodologically sound fashion.

Our approach (EC-FS) is extensively tested on 7 benchmarks of cancer classification
and prediction on genetic data (Colon [246], Prostate [62], Leukemia [62],
Lymphoma [62]), handwritten recognition (GINA [1]), generic feature selection
(MADELON [88]), and object recognition (PASCAL VOC 2007 [63]). We compare
the proposed method on these datasets, against seven comparative approaches,
under different conditions (number of features selected and number of training samples
considered), overcoming all of them in terms of ranking stability and classification
accuracy.

Finally, we provide an open and portable library of feature selection algorithms,
integrating the methods with uniform input and output formats to facilitate large
scale performance evaluation. The Feature Selection Library (FSLib 4.2 1) and
interfaces are fully documented. The library integrates directly with MATLAB, a
popular language for machine learning and pattern recognition research.

In this section, we presented the idea of solving feature selection via the Eigenvector
centrality measure. We designed a graph – where features are the nodes – weighted
by a kernelized adjacency matrix, which draws upon best-practice in feature selection
while assigning scores according to how well features discriminate between
classes. The method (supervised) estimates some indicators of centrality identifying
the most important features within the graph. The results are remarkable: the
proposed method, called EC-FS, has been extensively tested on 7 different datasets
selected from different scenarios (i.e., object recognition, handwritten recognition,
biological data, and synthetic testing datasets), in all the cases it achieves top performances
against 7 competitors selected from recent literature in feature selection.
EC-FS approach is also robust and stable on different splits of the training data, it
performs effectively in ranking high the most relevant features, and it has a very
competitive complexity. This study also points to many future directions; focusing
on the investigation of different implementations for parallel computing for big data
analysis or focusing on the investigation of different relations among the features.
Finally, we provide an open and portable library of feature selection algorithms, integrating
the methods with uniform input and output formats to facilitate large scale
performance evaluation. The Feature Selection Library (FSLib 4.2 2) and interfaces
are fully documented. The library integrates directly with MATLAB, a popular language
for machine learning and pattern recognition research (see Appendix A for
further details).

Visual tracking remains a highly popular research area of computer vision, with the
number of motion and tracking papers published at high profile conferences exceeding
40 papers annually [49, 95, 4, 116]. The significant activity in the field over
last two decades is reflected in the abundance of review papers. Most of the tracking
systems employ a set of features which are used statically across successive
frames [102]. It is well known that tracking deals with image streams that change
over time [43], therefore, data will easily produce frames in which the object has
low-contrast or the target region is blurred (e.g. due to the motion of the target or
camera), in these cases to obtain robust tracking a great deal of prior knowledge
about scene structure or expected motion is imposed [10, 262, 272], and thus tracking
success is bought at the price of reduced generality. Selecting the right features
plays a critical role in tracking [43, 42, 81, 238]. Trackers which comprises feature
selection strategies into their inner engines can be flexible enough to handle gradual
changes in appearance in real scenarios [49]. The degree to which a tracker can discriminate
a target from its surroundings is directly related to this flexibility. Since
foreground and background appearance can also drastically change as the target
object moves from place to place, tracking features should also need to adapt.


Many feature selection methods used in off-line settings (e.g., bioinformatics,
data mining [186, 215, 268, 85]) have been so far largely neglected, to the best
of our knowledge, at the level of online visual tracking, especially under the hard
constraint of speed required for target tracking. This chapter demonstrates the importance
of feature selection in realtime applications, resulted in what is clearly a
very impressive performance. Figure 6-5.A presents tracking results in an environment
of illumination variations, occlusions, out-of-views, fast motion among other
challenging aspects. The example frames are from the Matrix and liquor sequences.
All the feature selection approaches performs favorably against the baseline ACT
tracker (red box). The contribution of the work presented in this chapter is three-fold. 

Firstly, we evaluate a pool of modern feature selection algorithms (among
filter, embedded, and wrapper methods) selected to meet the requirements of a realtime
application. Secondly, we investigate the strengths and weaknesses of these
methods for a classification task in order to identify the right candidates for visual
tracking. Finally, the selected candidates are embedded on the Adaptive Color
Tracking system [49] (ACT).We extensively test the solutions on 50 test sequences
from the Online Object Tracking [261] (OTB-50) benchmark. In section 6.2 our
solutions performance has been evaluated with the same protocol and methodology
provided by the OTB-50 benchmark. The baseline ACT and its variants, with
different feature selection mechanisms, have been also compared against 29 stateof-
the-art trackers (it is worth noting that 10 of them use feature selection as part
of their framework). In section 6.3 we present our contribution to the Visual Object
Tracking Challenge, VOT 2016, an initiative established to address performance
evaluation for short-term visual object trackers.

We present a collection of modern algorithms in Sec. 5.1.4, suitable to be embedded
on the ACT system. For all of them, the following steps are taken in the embedding
procedure. Firstly, the ACT appearances x are extracted for object and background
classes, and computed using samples taken from the most recently tracked frame.
Secondly, feature ranking is applied in a supervised/unsupervised manner depending
on the selection method. This important step can be interpreted as ranking by
relevance the dimensions of the feature vector x (10-D), where features in the first
ranked positions are tailored to the task of discriminating object from background
in the current frame (note: in Section 6.2.3 the first 4 features have been selected).
Finally, the most discriminative features are used to estimate the C covariance matrix
used to feed the ACT. This procedure enables ACT to continually update the
set of features used, which turns out to better separate the target from its immediate
surroundings.

In this first part we evaluated a collection of seven modern feature selection approaches,
used in off-line settings so far. We investigated the strengths and weaknesses
of these algorithms in a classification setting to identify the right candidates
for a real-time task. We selected four candidates who meet the requirements of
speed and accuracy for visual tracking. Finally, we showed how these feature selection
mechanisms can be successfully used for ranking features combined with
the ACT system, and, at the same time, maintaining high frame rates (ACTinffs
operates at over 110 FPS). Results show that our solutions improve by 3% up to 7%
their baseline. Moreover, ACTMI resulted in a very impressive performance in precision,
providing superior results compared to 29 state-of-the-art tracking methods.
We hope that this work motivates researchers to take into account the use of fast
feature selection methods as an integral part of their tracker systems. For the sake
of repeatability, the code library is posted on the project page (official VOT2016 
repository) to provide the material needed to replicate our experiments.

VOT2016 follows VOT2015 challenge and considers the same class of trackers.
The dataset and evaluation toolkit are provided by the VOT2016 organizers. The
evaluation kit records the output bounding boxes from the tracker, and if it detects
tracking failure, re-initializes the tracker. The authors participating in the challenge
were required to integrate their tracker into the VOT2016 evaluation kit, which automatically
performed a standardized experiment. The results were analyzed by the
VOT2016 evaluation methodology. In addition to the VOT reset-based experiment,
the toolkit conducted the main OTB experiment in which a tracker is initialized in
the first frame and left to track until the end of the sequence without resetting. The
performance on this experiment is evaluated by the average overlap measure.
Participants were expected to submit a single set of results per tracker. Participants
who have investigated several trackers submitted a single result per tracker.
Changes in the parameters did not constitute a different tracker. The tracker was
required to run with fixed parameters on all experiments. The tracking method itself
was allowed to internally change specific parameters, but these had to be set
automatically by the tracker, e.g., from the image size and the initial size of the
bounding box, and were not to be set by detecting a specific test sequence and then
selecting the parameters that were hand-tuned to this sequence. The organizers of
VOT2016 were allowed to participate in the challenge, but did not compete for the
winner of VOT2016 challenge title.

The advances of VOT2016 over VOT2013, VOT2014 and VOT2015 are the
following: (i) The ground truth bounding boxes in the VOT2015 dataset have been
re-annotated. Each frame in the VOT2015 dataset has been manually perpixel segmented
and bounding boxes have been automatically generated from the segmentation
masks. (ii) A new methodology was developed for automatic placement of
a bounding box by optimizing a well defined cost function on manually per-pixel
segmented images. (iii) The evaluation system from VOT2015 is extended and the
bounding box overlap estimation is constrained to image region. The toolkit now
supports the OTB no-reset experiment and their main performance measures.

Biometric technologies are used to secure several electronic communications, including
access control and attendance, computer and enterprise network control,
financial and health services, government, law enforcement, and telecommunications.
A biometric system consists of two different phases: the enrollment phase
and authentication/verification phase. During the enrollment phase user biometric
data is acquired, processed and stored as reference file in a database. This is treated
as a template for future use by the system in subsequent authentication operations.
In order to authenticate a user, at least two verifications are required. Indeed, verification
is the process of verifying if the information provided is accurate. Verification
does not verify the actual user (or their identity), just the information provided.
For example, in the information security world, user authentication is performed by
verifying username and password of a user, which turns out to determine whether a
user is, in fact, who they are declared to be. In Biometric Authentication (BA) a user
is uniquely identified through one or more distinguishing biological traits, such as
fingerprints, hand geometry, earlobe geometry, retina and iris patterns, voice waves,
keystroke dynamics, DNA and signatures.

Among the many types of biometric authentication technologies this chapter focuses
on keystroke biometrics. An interesting example of application is given by the
new authentication mechanism added to Coursera, a social entrepreneurship company
that partners with universities to offer free courses online. It introduced a new
feature aimed at verifying online student identity with their typing behavior. This
typing measurement, called keystroke dynamics, is the detailed timing information
that describes exactly when each key was pressed and when it was released as a
person types on a keyboard. Keystroke biometrics [267, 5, 231] refers to the art and
science of recognizing an individual based on an analysis of his typing patterns.
The concept of keystroke biometrics has arisen as a hot topic of research only in
the past two decades. Researchers at MIT [135] looked at the idea of authentication 
through keystroke biometrics in 2004 and identified a few major advantages
and disadvantages to the use of this biometric for authentication. For example, in
[135] the authors conclude that measuring keystroke dynamics is an accessible and
unobtrusive biometric as it requires very little hardware besides a keyboard. Also,
as each keystroke is captured entirely by key pressed and press time, data can be
transmitted over low bandwidth connections. The authors also identified disadvantages
to the use of keystroke dynamics as an authentication tool. First of all, typing
patterns can be erratic and inconsistent as something like cramped muscles and
sweaty hands can change a person’s typing pattern significantly. Also, they found
that typing patterns vary based on the type of keyboard being used, which could
significantly complicate verification.

adding other different cues related to the writing style of the user, known as
stylometric features. Stylometric cues have been introduced in literature in the Authorship
Attribution (AA) domain. AA is the science of recognizing the author of
a piece of text, analyzing features that measure the style of writing. Five groups
of writing traits have been proposed in literature that focus on lexical, syntactic,
structural, content-specific and idiosyncratic aspects of a document [2]. Earlier
computer-aided AA attempts focused on textbooks, exploiting mainly lexical cues
(statistical measures of lexical variation as character/word frequency) and syntactic
features (punctuation, function words) [106]. Later on, the diffusion of Internet
had a huge impact on the AA community, delivering novel authorship challenges,
since online texts (emails, web pages, blogs) exhibit distinctive qualities: structured
layout, diverse fonts etc. To this aim, structural features were introduced
[53]. Content-specific and idiosyncratic cues mine the text through topic models
and grammar checking tools, unveiling deliberate stylistic choices [6]. Table 2.1 is
a synopsis of the features applied so far in the literature.

In the last years, the rise of the social web requested a dramatic update of the
stylometry, especially concerning one of its most relevant communication channel,
i.e., the chat or Instant Messaging (IM). Standard stylometric features have been
employed to categorize the content of a chat [178] or the behavior of the participants
[277]; AA on chats (i.e., recognizing each participant of a dialog) is still at
its infancy. This is probably due to the peculiar nature of the instant messaging,
which mixes together multimedia communication aspects: a very short text, whose
lexicon is unstable, that follows turn-taking mechanisms inherited from the spoken
language realm.

In this chapter we focus on biometric identification and verification on text chat
conversations. We start with proposing a set of stylometric features aimed at measuring
the writing style of a user involved into a chat session. Then we move on
Keystroke biometrics, where we enrich the feature set by adding timing information
which describes exactly when each key was pressed and when it was released as a
person is typing at a computer keyboard. Experiments have been performed on the
datasets already introduced in Part I Chapter 4.

In this experimental section, we examine the C-Skype dataset. The dataset is made
of 94 users, we retain only 78 users from it, those users that have at least 3 conversations
with at least 10 turns each (note, it is needed for the multi-view learning
approach). The assumption used to split a full text chat in more conversations is
that the time interval that elapses between each pair of successive turns does not
exceed 30 minutes. For each person involved in a conversation, we analyze his
stream of turns (suppose T), ignoring the input of the other subject. This means
that we assume that the chat style (as modeled by our features) is independent from
the interlocutor - this assumption has been validated experimentally. From these
data, a personal signature is extracted, that is composed of different cues, written
in red in Table 7.2. Our approach differs from the other standard AA approaches,
where the features are counted over entire conversations, giving a single quantity.
We consider the turn as a basic analysis unit, obtaining T numbers for each feature.
For ethical and privacy issues, we discard any cue which involves the content of the
conversation. Even if this choice is very constraining, because it prunes out many
features of Table 2.1, the results obtained are very encouraging.

The analysis of the relations between personality and writing style has been an hot
topic in both computational linguistics and psychology fields; recently, the topic
has been renowned with the massive diffusion of the chats, where textual artifacts
and elements of the spoken interaction coexist. In this scenario, our long term goal
is to evaluate whether the influence of personality on chatting style is stronger than
in standard text, and in which measure this is manifested by non verbal signals,
that is, aspects that go beyond the semantics of the content. This way, algorithms
that recognize personality profiles of the speakers may be employed in a privacy
respectful manner, facilitating the conversation with diverse chatting layouts, or
encouraging contacts between “compatible” subjects. At the same time, we are
interested in discovering whether the manifestation of a very particular chatting
style, that is, very recognizable, may correspond to having also a distinguishable
personality profile. In this way we may understand which are the personality traits
that are more evident via chat, and in which measure chats can be considered a
transparent means of communication. This study goes in this direction, showing
that positive affectivity is connected with the usage few long words, slowly written
by following a particular rhythm and this turns out to be the most recognizable trait,
followed by non planning impulsiveness, behavioral inhibition, negative affectivity
and planning impulsiveness.

Deep Neural Networks (DNNs) have shown to be very effective for image classification,
speech recognition and sequence modeling in the past few years. Beyond
those applications, out recent research outcomes also show the power of DNNs for
other various tasks such as re-identification [181]. In this section we present a pilot
experiment which makes use of DNNs in order to transform raw data input to a representation
that can be effectively exploited in a re-identification task. The novelty
of this task stands in the fact that is does not obviate manual feature engineering
and allows a machine to both learn at a specific task (using a simple representation
of the features) and learn higher level features. Different from previous works, we
formulate a unified deep learning framework that jointly tackles both of these key
components to maximize their strengths. We start from the principle that the correct
match of the probe template should be positioned in the top rank within the
whole gallery set. An effective algorithm is proposed to minimize the cost corresponding
to the ranking disorders of the gallery. The ranking model is solved with
a deep convolutional neural network (CNN). The CNN builds the relation between
each input template and its user’s identity through representation learning from frequency
distributions of a set of data.

Vectorized data representations frequently arise in many data mining applications.
They are easier to handle since each data can be viewed as a point residing in an
Euclidean space. Thus, similarities between different data points can be directly
measured by an appropriate metric to solve traditional tasks such as classification,
clustering and retrieval. Unfortunately, many data sources cannot be naturally represented
as vectorized inputs. In our case, a text chat conversation can be seen as
a set of turns, where each turn is a sequence of characters and symbols. Many features
can be extracted from them, as we showed so far. On the other hand, text chats
also contain the detailed timing information which describes exactly when each key
was pressed and when it was released as a person is typing at a computer keyboard.
Such a kind of data, which describes keystroke dynamics, is represented by lists of
timestamps and key-pressed pairs. As a result, there is a need of combining these
heterogeneous data sources (text plus meta-data) to come up with meaningful and
stronger results. The basic assumption is that, once the vectorized representation is
obtained, the mining tasks can be readily solved by deep learning algorithms. To address
the aforementioned challenge, we present an interesting idea to representation
learning which learn from heterogeneous data. At the training stage, data a firstly
represented by histograms. According to our previous work (see section 7.3.2), we
extracted the set of 21 stylometric features, including the ones related to keystroke
dynamics, and then we represented these cues as 32-bins histograms which are
then used to feed the deep CNN (see Figure7-8). As a result from our findings, we
noted that some of the features extracted consisted of very smaller values, i.e., many
observations around zero, in those situations, we adopted exponential histograms.
Small-sized bin ranges are located around zero, and they increase their sizes while
going to higher values. It results to give a higher resolution in those locations where
most of the data is present.

In the identity recognition task, we performed the same experiments in [205, 203].
We built a training set, which for each person has a particular number of turns,
that is used by the 1D-CNN algorithm to train the system. After training, we applied
our approach on the testing set, which was composed of the same number
of turns as the training data. The 1D-CNN is used to classify the set of unknown
user templates xi by minimizing the objective cross-entropy cost function of positive
pairs < template; label >i. The recognition task is performed, then the CMC
curve and the related nAUC value calculated. We did the same with the comparative
approaches (i.e., [205, 203] simply calculate distances among features, and
computes the average distance among the probe and training conversations). All
the experiments were repeated 50 times, by shuffling the training/testing partitions.
The results are better with our proposal both in case on nAUC and rank 1 score.
In Fig. 7-10 is reported the CMC curves obtained from the Deep Ranking approach,
which gives the identity recognition performance. This curve is strongly
superior than all the others of our previous work, realizing an nAUC of 96:2% and
99:4% on TBK and C-Skype respectively. It is worth noting that, the probability
of guessing the correct user at rank 1 is slightly below 50% which is really encouraging.
Rank 1 of CMC curves tells us what is the capability of the re-identification
system in correctly recognizing the right identity from the gallery database. Interestingly,
in the case of the C-Skype we derive a 80% of accuracy performed by
deep learning on the corpus of 94 different users. This result strongly overcomes
the previous system performance where learning was not used.

Keypoints, also referred to as interest points, are points in an image or 3-D point
cloud that should be stable and distinctively describe an interesting region of a
point cloud. Typically, the number of interest points in a 3-D point cloud is only
a small subset of all 3-D points. The goal of keypoint detection algorithms is to
support the local feature descriptors, and thus a keypoint detection algorithm is
often developed in combination with a local feature descriptor.

The keypoint detection algorithm introduced by Gelfand et al. [26] in 2005 is an
adaptive method, that can be used with meshes. It determines the keypoint due
to the calculation of surface variations. The approach is based on the method
of Manay et al. [53] and adopts the concept of integral invariant signatures from
2-D to 3-D.

Both, depth images and point clouds require an approximation of the surface
to create a watertight model. In 3-D point clouds based on depth images, related
3-D points can be determined easily based on the corresponding depth pixels.
The watertight voxel grid will be computed filling all voxels along the z-axis in
ascending order beginning with the first voxel intersected by the approximated
surface.

For pure 3-D point clouds a closed model is assumed. One possible approach
to create a watertight model is the method by Adamson et al. [1]. They use
spheres around all points of the point cloud to dilate the points to a closed
surface. Another approach by Hornung et al. [33] is based on a combination
of adaptive voxel scales and local triangulations. The major drawback of both
methods consists in their long computation times.

For this reason, Garstka et al. have developed a fast and highly parallelizable
approach, with the minor disadvantage, that the algorithm may provide inaccurate
results in a few cases. Each voxel containing a 3-D point is initialized with 1.
All other voxel values are set to 0. For each dimension x, y and z the algorithm
iterates over the voxels. Beginning with the first occurrence of a voxel with a
value of 1, the following ones will be decreased by 1 until the next voxel of value
1 is reached. This steps will be repeated until the boundary of the voxel grid is
reached. If the boundary is reached while decreasing the voxel values by 1, there
is at least one hole in the surface. In this case all values of this iteration step will
be set back to previous values. Finally, all voxels with a value  −2, i. e., which
have been marked as inner voxels by at least two passes, will be treated as inner
voxels. These voxels will get a value of 1, while all other voxels will get a value
of 0. In a post-processing step all holes, i. e., tubes of size 1 will be filled. Both
scenarios are shown in Figure 3.3.

Dutagaci et al. [18] chose another approach to evaluate the quality of keypoint
detectors. They created a website and asked visitors of the website to mark
points of interest on the displayed 3-D objects. The results were transferred
into a ground truth, first by combining all markers within a radius r to a single
group. Then all groups selected by less than n users were discarded. Finally they
selected a representative point for each group with the minimum sum of geodesic
distances to all other points within the group. This ground truth was compared
to the results of 6 keypoint detection algorithms. However, all the methods used
in the article are intended for meshes.

In 2013 Filipe and Alexandre [19] compared four keypoint algorithms with
a focus on implementations available in the Point Cloud Library (PCL) [68].
The algorithms are 3-D extensions of the Harris corner detector [28] and SUSAN
[77], where the image gradients in the covariance matrix get replaced by
surface normals, as well as Thrift (see Section 3.1.5) and ISS (see Section 3.1.8).
Like Sali et al., Filipe and Alexandre prefer the repeatability as a quality measure.
They conclude that Thrift and ISS yielded the best scores in terms of
repeatability. They also note that ISS is the fastest of the tested algorithms.
Gou et al. [27] published 2014 a comprehensive survey paper with focus on 3-D
object recognition based on local features. Among other aspects of this application
area they present a total of 29 keypoint detection methods for meshes, depth
images and point clouds. The mostly short reviews of the proposed algorithms,
however, do not come from their own evaluations.

One of the main components of the basic 3-D classification pipeline is the computation
of a distinctive description of the point cloud. It must be considered
that the point cloud may be partially occluded. Accordingly, in most cases, local
feature descriptors are used, which are summarized to a global description
in a subsequent step. This chapter begins with numerous different algorithms
for local feature descriptions. Afterwards it is described how, in the context of
this work, a so-called bag of features approach can be used to combine the local
feature descriptions to get an aggregated representation that can be used as a
global description of the point cloud.

The goal of local descriptors is the description of particularly “interesting” local
areas of a 3-D object. In contrast to global 3-D descriptors not just one but
rather several signatures or histograms will be computed. The advantages of
local representations are, that they are robust with respect to noise, variability
in object shape and partial occlusions [27]. On the other hand they remain less
discriminating due to the limited scope of the local neighborhood. For this reason,
the subsequent comparison of feature descriptions is another challenging part of
a recognition pipeline which will be discussed later in this chapter.

The descriptor called 3-D shape context proposed by Frome et al. [23] in 2004
is a histogram based descriptor for 3-D point clouds. The spherical histogram
requires a normal vector for the alignment of its north pole, but it does not have
a unique reference frame.

As already mentioned in Section 2.1, many of the classification pipelines use a
bag of features model as a global description of a point cloud. The bag of features
model has its origin in a field of computer science named natural language processing.
There, a bag of words model is a histogram that counts the occurrences
of words in a text without regarding the grammar. In the same way a bag of
features is a histogram in which the occurrences of local feature descriptions are
counted.

Since the number of possible feature descriptions is, in contrast to a natural
language infinite, a defined subset of feature descriptions is required. Therefore,
the feature descriptions need to be quantized to a finite number of representatives.
This is often done with a k-means clustering.

Although invented by Lloyd in 1957 k-means was published accessible to the
public only in 1982 [47]. The aim of k-means is to cluster the data in k partitions
so that the sum of squared distances between each data element of a cluster and
the centroid of a cluster is at a minimum. In the bag of features context, the
problem can be formulated as follows.

Given these significant differences, it is difficult to generalize the number of
clusters. But there is another question in this context: is the Euclidean distance
the best choice in distinguishing the feature descriptions while performing a kmeans
clustering?

The last question can partly be answered by a work of Madry et al. [51]. They
compared, among others, different distance measures for a 33 dimensional fast
point feature histogram (see Section 4.1.6). The evaluation is performed with two
different test configurations. In the first configuration the original training objects
were rotated and used as test objects. The average classification rates differ only
slightly, i. e., a bag of features based on Jaccard distance for vectors [11] achieves
70%, while a bag of features based on Euclidean distance achieves 72%. 

In the second configuration Madry et al. use only objects where the test data differ
significantly from the training set. This leads to an average classification rate of
 36% for the jaccard and of = 46% for the Euclidean distance.

Therefore, the Euclidean distance provides at least better results than the
Jaccard distance for feature descriptions with a small number of dimensions (33
dimensions in this case).

Object classification is the correct assignment of an object to one of a given set of
abstract classes, e. g., cup, bottle, chair, and so on. In contrast to object recognition,
any object, even previously unseen objects shall be classified correctly.

As already mentioned in Section 2.1, a common way to identify a class based
on a given object description, i. e., a frequency histogram, is to use a classifier.
Most of the current 3-D classification pipelines as well as the basic classification
pipeline used in this thesis use a support vector machine. Hence, the support
vector machine is described in more detail.

The support vector machine (SVM) introduced by Vapnik and Chervonenkis
[96] is a learning algorithm that learns the relationship between a set of input
vectors to an output. Initially, SVMs handled only problems with two categories.
To show the principle of SVMs, Figure 5.1 depicts a simple initial example of
data points associated with two disjoint classes.

As shown, the data points in Figure 5.1 can be separated in 2-D by a straight line.
For this simple example, the classification problem could be described as follows:
find a straight line that separates the data points of the two classes maximizing
the points’ distances to the line. This approach corresponds, in essence, to the
approach of an SVM: identify a linear separator – a straight line in this case –
that separates the data points with a maximum margin.

Reinforcement learning is a machine learning method for sequential decisionmaking,
in which through trial and error an optimal behavior at each stage of
the sequence shall be learned. The goal is to learn the best possible way to solve
a given task, which may consist of a plurality of different decision dimensions.
First of all a reinforcement learning system consists of an agent that interacts
with an environment. Based on the current state of the environment the agent
decides with respect to the learned experience what action will be performed next.

The mentioned experience arises from consequences within the environment, i. e.,
a positive or negative reward, which reflects whether the action was appropriate
to bring the agent closer to its goal.

Before we consider how an appropriate policy can be determined using the rewards,
the following few lines give a brief overview of how reinforcement problems
and rewards are often modeled. Typically the issues of constructing or modeling
a reward signal are not part of a reinforcement learning problem, but they help
to enhance the understanding of reinforcement learning.

Figure 6.2 shows an exemplary Markov decision process of a recycling robot
as introduced in [83]. The task of the robot is to collect empty soda cans in
an office environment. The robot has two battery states, low and high. Based
on these states the agent decides between three possible actions: searching for
empty cans, waiting for someone to bring a can, and return to the base station
to recharge the battery. In the latter case a recharge makes sense only when the
battery level is low. Therefore, a recharging transition from state high to state
high makes no sense. The wait action can be quickly explained, too. If the robot
waits, it does not run down the battery power. Accordingly, the transitions for
this action keeps the state.

In connection with the curse of dimensionality it was already mentioned that a
reinforcement learning method needs to visit each state-action pair sufficiently
often to ensure convergence. This means that in every state, all actions should
be used as often as possible. This could be done systematically. However, since
the values of different states and actions can relate to each other, a systematic
approach can influence the results. Therefore, the actions are usually selected at
random. Hence this will be called a random policy. Since this kind of exploration
is needed due to uncertain action values, a random policy seems to be a preferable
strategy to select actions.

However, if the current knowledge a reinforcement learning system has learned
shall be applied to solve its goal, a random policy is inappropriate. Instead a
policy that maximizes the expected reward in accordance with the objective of the
reinforcement learning task is required. This can be achieved selecting the action
with the greatest Q-value for each state. This is a greedy policy called maxQ
policy. Selecting an action this way means exploiting the current knowledge.

This chapter can be seen as an overview over of the remaining chapters of this
thesis. It briefly describes the problems when using local 3-D feature descriptors
for classification tasks as well as the approaches that will be used to improve the
classification results gradually.

This focus is on local 3-D feature descriptor algorithms for point clouds without
additional structural information like triangle meshes and surfaces. These
local 3-D feature description algorithms differ considerably with respect to descriptiveness
and computation times. However, in general the computational
costs of calculation and comparison are high.

Note that a recognition or classification task of objects which are represented
exclusively as a 3-D point cloud of the surface of an object is a problem even for
a human observer. This is particularly clear when one bears in mind how objects
like apples, oranges or tennis balls look like without additional color information
and without a continuous surface: they look equal.

A comparison of recent algorithms [4] and a survey of local feature based
approaches for 3-D object recognition [27] show, that there is not a single best
algorithm in the domain of 3-D object recognition. Concerning these two publications
it must also be taken into account that they use local 3-D feature descriptors
for object recognition and not for object classification tasks. That means, that
they typically look at most for a hand full of objects which are usually very distinctive.
Accordingly, it can be assumed that the classification rate of objects
from large scale data sets with many similar object classes is much worse.

The aim of this work is to show that a reinforcement learning system is able to increase
the classification rate of 3-D objects by a skillful selection and combination
of algorithms for local 3-D feature description.

The keypoint detection algorithm used within all experiments has to be taken in
advance. But considering the fact that there are still current pipelines that rely
on sparse sampling (see Section 2.1), it was decided that sparse sampling will be
initially included in the baseline experiments.

Secondly, a keypoint detection algorithm has been selected which was introduced
by Zhong [105] in the context of intrinsic shape signatures (see Section
3.1.8). This algorithm is chosen, because both Salti et al. [72] as well as
Filipe and Alexandre [19] (see Section 3.2) conclude that the intrinsic shape signature
yields the best scores in terms of repeatability and is the fastest of the
tested algorithms.

of the visual words is to a certain extent independent of the objects used, only
the first 10 (’apple’ to ’cellphone’) as well as the first 20 (’apple’ to ’foodjar’)
of the 51 categories in alphabetical order from the RGB-D Object Dataset (see
Section 7.3.1) will be used initially. To have a reference to compare to, the visual
words will also be determined on the basis of all object categories.

In relation to the second question, Section 4.3 provides a commonly used
combination: k-means clustering with an Euclidean distance. The initial centers
of the clusters are chosen at random from the point cloud by using a variant
proposed by Arthur and Vassilvitskii [5] in 2007, where they weigh positions of
points according to the squared distance to the closest 3-D point already chosen.
They show that their approach (named k-means++) leads to about 20%
better results and is up to 70% faster in comparison with the standard k-means
algorithm.

The third question can not be answered directly. The vocabulary should be
large enough to represent relevant changes, but not so large as to distinguish
between irrelevant variations caused by noise. The methods mentioned in Section
4.3 use values of k that differ by orders of magnitude, i. e., k = 7 versus
k = 300. For this reason, the classification results for 7 different vocabulary and
histogram sizes will be determined in this work.

As already stated in Section 5, most of the mentioned classification approaches
use support vector machines as underlying technique. Rusu et al. [70, 65] state,
that support vector machines have already been used for a classification based
on frequency histograms of feature descriptions for color images with great success.
In the referenced work, Rusu et al. test support vector machines, k-nearest
neighbor searches and k-means clustering in different configurations against each
other. The algorithm used in context of their classification is the point feature
histogram.

Once the main parameters of the basic pipeline are determined, the pipeline
will be successively fused with the reinforcement learning environment. To fuse
the basic classification pipeline with a reinforcement learning framework, the
pipeline is modified in a way, that a local 3-D feature description algorithms will
be selected by a reinforcement learning agent, while all other parameters of the
pipeline remain unchanged.

Initially, it is assumed that the environment of the proposed reinforcement
learning framework is only defined on the class candidates. Hence, the computation
of the feature descriptions, their conversion into a bag of features histogram
and the classification are those components of the pipeline, which may lead to a
change of the class candidates and of the environment, respectively. The keypoint
detection is not taken into account, because the algorithm is given and fixed, and
is only carried out once per input cloud. This is shown in Figure 7.2.

Therefore, it has to be clarified in which situations (states) the agent receives
which amount of reward. For this purpose the states in which the reinforcement
model should terminate must be defined in advance. For some of these cases,
a special value is required, namely the sum that is calculated for each object
class with the prediction values which have been determined for an object by
the corresponding support vector machine while applying a sequence of local 3-D
feature description algorithms.

On closer inspection, it turns out that the previously presented model is neither
adaptive nor uses the characteristics of the point cloud to influence the selection
of the local 3-D feature description algorithm. The latter is the case, because the
first state will always be the same, since the state is only given by the set of class
candidates and it consists always of all 51 objects classes.

The preparation of 3-D point clouds is an important step toward a successful
application of 3-D feature descriptors. Especially the point cloud resolution in
terms of an approximated mesh resolution is a significant value, which is required
by many of the subsequent algorithms. Therefore, the experiments start with a
cleanup of the raw data and with the determination of the point cloud resolution
to match a corresponding mesh resolution as good a possible.

The histograms for different values of n, e. g., for values between 4 and 9
nearest neighbors of the point cloud from ’Happy Buddha’ shown in Figure 8.2
are illustrative for all results. It shows, that 7 nearest neighbors lead to the best
approximation of the mesh resolution of the corresponding triangle mesh.

Furthermore, it can be seen that the spread of approximated mean values at
a sample size of 50 randomly selected points gets smaller. Since in the following
experiments the local neighborhood of keypoint detection and feature description
algorithms will have a size of at least 6 times the mesh resolution, these changes
of < 10% of the mesh resolution (see the Table 8.3 for details) is small enough
that the approximated mesh resolution can be used as a “stable” value for a point
cloud resolution (pcr).

As mentioned in Chapter 7 two different sets of keypoints will be used in comparison:
keypoints based on sparse sampling and keypoints based on the intrinsic
shape signature keypoint algorithm.

The number of keypoints per object appears not to significantly affect the overall
computation time for objects with a size similar to those used in this experiment.
This relation is shown in Figure 8.6. It cannot be expected, however, that this
holds for objects of any size. But for the objects of the RGB-D object dataset
from the University of Washington the overall computation time seems to be
dominated by a mostly constant overhead, e. g., method calls, instantiation of
objects, or routines for memory allocation.

The local 3-D feature descriptors used within this thesis require some parameters.
Their values are taken from the publications of the respective algorithms where
possible and subsequently discussed.

The classification results that can be achieved with this reinforcement learning
model are illustrated in Figure 9.3. Only the episodes from 80 to 100 million are
shown, since the classification results of a purely random selection (< 90 million
episodes) are all identical. Figure 9.3 (a) shows the six different terminal states.
One can see, that the classification results increase with a decreasing x-value. To
get a better picture, all relevant values are shown in Table 9.1.

The classification results that can be achieved without the use of global information
in the initial state and with a “normal” usage of support vector machines,
i. e., with the separation of the classes at a prediction value of 0.0 are shown in
the following sections. Instead of three algorithms with two configurations, all
six algorithms will be used.

At first it can be noticed, that the number of 1640769 actual states is almost
four times as high as shown in Figure 9.2 (a). This can be explained as follows:
While the set of the class candidates for an algorithm with a higher prediction
limit must always be a subset of the class candidates of the same algorithm with a
limit of zero, there isn’t a similar relationship between the 6 different algorithms in
the current case. Thus, the variety of permutations within the class candidates is
correspondingly higher. Apart from that, the graphs differ only marginally from
those in Figure 9.2.

The enhancement of the overall results that can be achieved using the global
properties are quite small, but show that the method basically works. However,
the significance of the simple global parameters used here is higher than expected.
It could be shown that the fail states subsequent to the first applied algorithm
could be more than halved using the global properties.

All in all, it must be assumed that better results can only be expected with
a larger variety of local 3-D feature description algorithms and/or a more informative
global description. The latter could be, for example, a very simple global
descriptor, like the rotation invariant shape descriptor of Suzuki et al. [85], where
they fit the point cloud into a unit cube, divide the cube into a coarse grid and
count the points in each grid cell.

The last three chapters have shown that it is possible to increase the classification
results by a skillfully combination of several local 3-D feature description algorithms,
without exceeding a defined time limit. The results show on the one hand
how difficult a classification of noisy low-resolution 3-D point clouds is. But they
also show that it is generally possible, and that proper classification rates can be
reached for object classes which are to a certain degree disjoint. Therefore, the
results are summarized in the following sections.

To achieve a maximum classification rate for every single 3-D local feature description
algorithms as well as for the reinforcement learning framework, the basic
classification pipeline was analyzed and evaluated in a comprehensive examination
of numerous parameters. Besides the optimization of various parameters
of the entire classification pipeline, the classification rate of the support vector
machines has been maximized. This step should especially reduce the number
of false-negative results, i. e., the results that exclude an object from an object
class, although the object belongs to the object class. Due to the design of the
system this is crucial, since it is not possible to revise an exclusion.

The latter leads more or less inevitably to low precision values, i. e., a high
number of false-positive results. In many cases, even when combining multiple
algorithms, this does not lead to a unique assignment, but to a set of class
candidate. In order to be still able to evaluate the result, the prediction values
are summed up for each object class that remains in the set of class candidates.
Finally, this sum is used to identify the object class with the highest sum of
prediction values, which is then used for a mapping between the object and
an object class.

optimized basic classification pipeline lead to the following results: Exact
classification results were neither possible with all object classes nor with the
reduced set of 10 object classes. The algorithm, which assigns the most correct
classes due to the prediction values of its support vector machines is in both cases
the fast point feature histogram (FPFH) by Rusu et al. [71]. In case of all object
classes a rate of 9.4% could be achieved, while a rate of 65.0% could be achieved
for the reduced set of object classes.

Following the experiments of the basic classification pipeline the results of
the reinforcement learning frameworks were determined. For this purpose, it
was first examined whether an increase of the classification limit and thereby
the prediction value of a support vector machine leads to better classification
results. However, the analysis showed that an increase of the classification limit
precisely had the opposite effect, because the increase of the classification limit
led to an increased number of false-negative results, which should be avoided as
noted above. For this reason, this approach was rejected.

But even without changing the classification limits it was possible to increase
the rate of positive terminal states which would allow a correct assignment to
the object class. While this classification rate for 10 object classes increased
slightly from 65.0% for FPFH to 73.5% for the learned combination of different
algorithms, the rate increased considerably from 9.4% to 21.7% for all object
classes. Additionally, the rate of exactly assigned object classes could be increased
from 0.0% to 15.3% and 4.9% respectively. This is a substantial improvement
especially for the large data set.

Since the first state of the reinforcement learning framework in the approaches
so far always was the state in which the set of class candidates consists of all
object classes, the reinforcement learning agent was not able to select different
algorithms in a first step. Therefore, a differentiation of the first state based
on a few global properties of a point cloud was introduced. This approach was
especially intended to reduce the relatively large number of fail states after the
application of the first algorithm. Even if the reduction of the amount of fail
states from 17.9% to 16.3% is relatively low, it has been shown that the basic
principle works. In addition, it was shown that the amount of fail states after
the first applied algorithm could be more than halved using the global properties.
This is quite a satisfactory result, even if it is not reflected in the overall result.
In addition, the classification rate increased from 73.5% to 75% in the case of 10
object classes.

One of the essential features of a reinforcement learning frameworks is the continuous
adaptability to a changing environment. While other machine learning
methods usually need to be retrained due to changes of the environment and
the parameters, a reinforcement learning framework usually reacts on changes in
an ongoing process, which affects the actual task of the reinforcement learning
framework only slightly.

With an adaptive, randomized choice between -greedy and random episodes
the proposed reinforcement learning framework demonstrated impressively how
effectively this approach works. This enables the framework to successively add
or remove local 3-D feature descriptors or to modify their parameters, e. g., the
classification limits to observe the influence of the changes directly on the running
system.

The role of this chapter is to provide the fundamentals of Markov random fields and
review some state-of-the-art optimization algorithms. In Section 2.1, we formally
introduce Markov random fields. Section 2.2 discusses binary MRF optimization and
introduces the basic concepts on pseudo-boolean optimization and the min-cut/maxflow
(graph-cut) algorithm. In Section 2.3, we turn to the multi-label MRFs. In this
section, we first review a graph-cut algorithm for multi-label submodular MRFs, and
then formalize move-making algorithms and discuss some useful special cases. Later,
we consider the linear programming relaxation of the discrete MRF problem. In this
case, we discuss a tree-based decomposition algorithm where each subproblem (tree
MRF) is optimally minimized using the message passing algorithm.

Broadly speaking, there are three different kinds of maxflow
algorithms: those relying on finding augmenting paths [Ford and Fulkerson,
1962], the push-relabel approach [Goldberg and Tarjan, 1988] and the pseudo-flow
techniques [Chandran and Hochbaum, 2009]. The polynomial time guarantee of
max-flow was first proven in [Edmonds and Karp, 1972] by modifying the augmenting
path algorithm, to always find the shortest augmenting path. There are numerous
max-flow implementations available for general purpose as well as specific to computer
vision applications. Among them, the specialized implementations are significantly
faster in practice. In particular, the BK method [Boykov and Kolmogorov,
2004] is arguably the fastest implementation for 2D and sparse 3D graphs arising
from computer vision applications. Recently, for dense problems, the EIBFS algorithm
[Goldberg et al., 2015] was shown to outperform the BK method.

On tree structured MRFs, there is a natural order (from leaves to root) to pass the
messages and in two passes (called forward pass and backward pass) the algorithm converges
and yields the optimal labelling [Pearl, 1988]. In particular, for an arbitrarily
chosen root node r 2 V, in the forward pass, the messages are passed (reparametrization)
from leaf nodes to the root node. In the backward pass, the messages are passed
from root to leaf nodes. This message passing algorithm can be better explained using
the multi-label graph. See Figure 2.19.

Furthermore, the optimal labelling x is computed by back-tracking through the zero
weight edges and nodes (backward pass). This two pass procedure is optimal for tree
structured MRFs and, for a graph with loops, it is not guaranteed to find the optimum.
In fact, it has been shown that, for multi-label submodular MRFs, one can find
an ordering of optimal message passing that would guarantee optimality [Werner,
2007, 2010]. However, in general, the algorithm may not even converge. Despite that,
the optimality on tree MRFs made it an ideal choice to tackle the LP relaxation, in a
dual-decomposition framework [Wainwright et al., 2005; Komodakis et al., 2011].

In addition to the LP relaxation, there are many other continuous relaxations have
been studied in the literature. This includes Quadratic Programming (QP) [Ravikumar
and Lafferty, 2006], Second Order Cone Programming (SOCP) [Muramatsu and
Suzuki, 2003] and Semi-Definite Programming (SDP) [Torr, 2003] relaxations. Among
them, the LP relaxation was shown to provide a better approximation than a large
class of QP and SOCP relaxations (see [Kumar et al., 2009]) and the SDP relaxation
was shown to scale poorly [Olsson et al., 2007]. Furthermore, researchers have studied
approaches to tighten the LP relaxation [Sontag et al., 2008; Komodakis and
Paragios, 2008].

In this chapter, we have studied the important theories on Markov random fields and
some state-of-the-art optimization algorithms. These algorithms can be categorized
into three groups: 1) max-flow for submodular MRFs; 2) move-making algorithms;
and 3) continuous relaxations. In the subsequent chapters, we describe our contributions
in these three categories. First, in the next chapter, we discuss a memory efficient
variant of the max-flow algorithm for multi-label submodular MRFs. This algorithm
constitutes the method of choice for Ishikawa type graphs when the complete
graph cannot be stored in memory. Next, in Chapter 4, we explain a move-making
style algorithm for multi-label MRFs with a certain class of robust non-convex priors.
This approach is effective in obtaining significantly lower energy solutions than
other move-making algorithms for MRFs with such robust non-convex priors. Later,
in Chapter 5, we describe a block-coordinate descent algorithm for the LP relaxation
of fully connected CRFs. This constitutes the first LP relaxation algorithm for dense
CRFs that has linear time iterations.

We now introduce our polynomial time memory efficient max flow algorithm, which
minimizes multi-label submodular MRF energies with pairwise interactions. Our algorithm
follows a similar procedure as the standard Edmonds-Karp algorithm [Edmonds
and Karp, 1972], in that it iteratively finds the shortest augmenting path and
then pushes the maximum flow through it without exceeding the edge capacities.
However, instead of storing the residual graph, we store exit-flows as proposed in
Section 3.3, which, at any stage of the algorithm, would allow us to compute the
residual graph. Below, we discuss how one can find an augmenting path and update
the exit-flows, i.e., perform augmentation, without storing the full Ishikawa graph.

We follow the time complexity analysis given in [Cormen et al., 2001] for the standard
Edmonds-Karp algorithm to derive a polynomial time bound on our algorithm.
In particular, the analysis first proves that the shortest path distance from the source
(node 0) to any node is monotonically increasing with each flow augmentation. Then,
it derives a bound on the number of augmentations. In fact, the number of augmentations
of our MEMF algorithm also has the same bound as the Edmonds-Karp
algorithm.

In the previous section, we have provided a general purpose polynomial time maxflow
algorithm that is also memory efficient. However, for computer vision applications,
the BK algorithm [Boykov and Kolmogorov, 2004] is shown to be significantly
faster than the standard max-flow implementations, even though it lacks the polynomial
time guarantee. The basic idea is to maintain a search tree throughout the
algorithm instead of building the search tree from scratch at each iteration.

Motivated by this, we also propose doing search-tree-recycling similarly to the
BK algorithm. Since we lose the polynomial time guarantee, for increased efficiency,
we further simplify the Ishikawa graph. In particular, we find an augmenting path in
a block-graph, that amalgamates the nodes in each column into blocks. Since an augmenting
path in our block-graph corresponds to a collection of augmenting paths in
the Ishikawa graph, our algorithm converges in fewer iterations than the BK algorithm.

Furthermore, similarly to the BK algorithm, we find an augmenting path Pb using
BFS and maintain the search tree throughout the algorithm, by repairing it whenever
the block-graph is updated. However, since the block-graph needs to be reconstructed
after each augmentation, for simplicity, we maintain a single tree.

The most popular method to minimize a multi-label submodular MRF energy is to
construct the Ishikawa graph [Ishikawa, 2003] and then apply a max-flow algorithm
to find the min-cut solution. Broadly speaking, as mentioned in Section 2.2.4, there
are three different kinds of max-flow algorithms: those relying on finding augmenting
paths [Ford and Fulkerson, 1962], the push-relabel approach [Goldberg and Tarjan,
1988] and the pseudo-flow techniques [Chandran and Hochbaum, 2009]. Even
though numerous implementations are available, the BK method [Boykov and Kolmogorov,
2004] is arguably the fastest implementation for 2D and sparse 3D graphs.
Recently, for dense problems, the IBFS algorithm [Goldberg et al., 2011] was shown
to outperform the BK method in a number of experiments [Verma and Batra, 2012].

Furthermore, for multi-label submodular MRFs with convex unary potentials an efficient
algorithm is presented in [Hochbaum, 2001]. For arbitrary unary potentials, all
the above-mentioned algorithms, however, require the same order of storage as the
Ishikawa graph and hence scale poorly. Two approaches have nonetheless been studied
to scale the max-flow algorithms. The first one explicitly relies on the N-D grid
structure of the problem at hand [Delong and Boykov, 2008; Jamriška et al., 2012].
The second one makes use of distributed computing [Shekhovtsov and Hlaváˇc, 2013;
Strandmark and Kahl, 2010; Vineet and Narayanan, 2008]. Unfortunately, both these
approaches require additional resources (disk space or clusters) to run max-flow on
an Ishikawa graph. By contrast, our algorithm lets us efficiently minimize the energy
of much larger Ishikawa-type graphs on a standard computer. Furthermore, using
the method of [Strandmark and Kahl, 2010], it can also be parallelized.

We compare our results with two max-flow implementations: the BK algorithm [Boykov
and Kolmogorov, 2004] and Excesses Incremental Breadth First Search (EIBFS) [Goldberg
et al., 2015] (which we ran on the Ishikawa graph), and three LP relaxationbased
algorithms: Tree Reweighted Message Passing (TRWS) [Kolmogorov, 2006],
Subgradient-based Dual Decomposition (DDSG) [Komodakis et al., 2011] and the
Adaptive Diminishing Smoothing algorithm (ADSal) [Savchynskyy et al., 2012]. For
DDSG and ADSal, we used the Opengm [Andres et al., 2012] implementations. For
the other algorithms, we employed the respective authors’ implementations.

In practice, we only ran the BK algorithm and EIBFS if the graph could be stored
in RAM. Otherwise, we provide an estimate of their memory requirement. For LP
relaxation-based methods, unless they converged, we ran the algorithms either for
10000 iterations, or for 50000 seconds, whichever occurred first. Note that the running
times reported for our algorithm include graph construction. All our experiments
were conducted on a 3.4 GHz i7-4770 CPU with 16 GB RAM.

In this section, we empirically analyze various properties of our algorithm. First,
note that, at each iteration, i.e., at each augmentation step, our algorithm performs
more computation than standard max-flow. Therefore, we would like our algorithm
to find short augmenting paths and to converge in fewer iterations than standard
max-flow. Below, we analyze these two properties empirically.

In Figure 3.10, we show the distribution of the lengths of the augmenting paths
found by our algorithm for the Tsukuba stereo instance. Note that the median length
is only 5. As a matter of fact, the maximum length observed over all our experiments
was 1073 for the KITTI data. Nevertheless, even in that image, the median length was
only 15. Note that, since our algorithm finds augmenting paths in the block-graph,
the path lengths are not directly comparable to those found by other max-flow-based
methods. In terms of number of augmentations, we found that our algorithm only required between 
35% and 50% of the total number of augmentations of the BK algorithm.

We parallelized our algorithm based on the dual-decomposition technique of [Strandmark
and Kahl, 2010] and evaluated it on the same six stereo instances from the Middlebury
dataset [Scharstein and Szeliski, 2002, 2003]. The relative times tm/ts, where
tm stands for the multi-thread time and ts for the single-thread one, are shown in
Figure 3.13 for two and four threads. In this experiment, for all problems, the image
grid was split vertically into two and four equally-sized blocks, respectively. Note
that this splitting strategy is fairly arbitrary, and may affect the performance of the
multi-threaded algorithm. In fact finding better splits may itself be a possible future
direction.

In the previous chapter, we discussed a memory efficient max-flow algorithm to
optimally solve multi-label submodular MRFs (e.g., MRFs with convex priors). In
this chapter, we present an approximate graph-cut algorithm for multi-label MRFs
with a certain class of non-convex priors. We show that, by iteratively minimizing a
multi-label submodular energy function, we can approximately minimize MRFs with
robust non-convex priors. Furthermore, we discuss its relationship to the majorizeminimize
framework. This chapter is based on our work [Ajanthan et al., 2015] with
some extensions.

In this chapter, we introduce an algorithm to minimize the energy of multi-label
Markov random fields with non-convex edge priors. As discussed in Chapter 2, in
general, minimizing a multi-label MRF energy function is NP-hard. While in rare
cases a globally optimal solution can be obtained in polynomial time, e.g., in the
presence of convex priors [Ishikawa, 2003], in most scenarios one has to rely on an
approximate algorithm (see Chapter 2 for a review). Even though graph-cut-based
algorithms [Boykov et al., 2001] have proven successful for specific problems (e.g.,
metric priors), there does not seem to be a single algorithm that performs well with
different non-convex priors such as the truncated quadratic, the Cauchy function
and the corrupted Gaussian, which are widely acknowledged as highly effective in
computer vision.

Here, we propose to fill this gap and introduce an iterative graph-cut-based algorithm
to minimize multi-label MRF energies with a certain class of non-convex
priors. Our algorithm iteratively minimizes a weighted surrogate energy function
that is easier to optimize, with weights computed from the solution at the previous
iteration. We show that, under suitable conditions on the non-convex priors, and as
long as the weighted surrogate energy can be decreased, our approach guarantees
that the true energy decreases at each iteration.

More specifically, we consider MRF energies with arbitrary data terms and where
the non-convex priors are concave functions of some convex priors over pairs of
nodes. In this scenario, and when the label set is linearly ordered, the solution
at each iteration of our algorithm can be obtained by applying the Ishikawa algorithm
[Ishikawa, 2003]. Since the resulting solution is optimal, our algorithm guarantees
that our MRF energy decreases. Furthermore, our MEMF algorithm described
in Chapter 3 can be applied instead of the standard Ishikawa method to tackle large
scale problems.

Note that, since our algorithm iteratively approximates the true multi-label MRF
energy using a surrogate energy, it can be categorized as a move-making algorithm.
Here, the Ishikawa graph construction at each iteration defines the associated movefunction
(see Section 2.3.3 for details on move-making algorithms). Moreover, our
method is inspired by the Iteratively Reweighted Least Squares (IRLS) algorithm
which is well-known for continuous optimization. To the best of our knowledge, this
is the first time that such a technique is transposed to the MRF optimization scenario.
We demonstrate the effectiveness of our algorithm on the problems of stereo correspondence
estimation and image inpainting. Our experimental evaluation shows
that our method consistently outperforms other state-of-the-art graph-cut-based algorithms
[Boykov et al., 2001; Veksler, 2012], and, in most scenarios, yields lower energy
values than TRWS [Kolmogorov, 2006], which was shown to be one of the bestperforming
multi-label approximate energy minimization methods [Szeliski et al.,
2008; Kappes et al., 2015].

In this section, we introduce an iterative algorithm for the case of multi-label MRFs
with pairwise node interactions. In particular, we propose to make use of the
Ishikawa method [Ishikawa, 2003] at each iteration of our algorithm. The Ishikawa
method yields an optimal solution under the following two conditions2: 1) the label
set must be ordered; 2) the pairwise potential must be a convex function of the label
difference. In practice, such convex priors have limited power due to their poor
ability to model noise. In contrast, while still relying on the first condition, our algorithm
allows us to generalize to non-convex priors, and in particular to robust norms
that have proven effective in computer vision. Furthermore, our MEMF algorithm
described in Chapter 3 can be used at each iteration to tackle large scale problems.

So far, we have discussed an optimal max-flow algorithm and an approximate movemaking
algorithm for sparsely connected MRFs. In this chapter, we present an efficient
LP relaxation-based algorithm for fully connected CRFs1. Specifically, we
present a block-coordinate descent algorithm to minimize the LP relaxation of a
dense CRF with Gaussian pairwise potentials. We show that each iteration of our
algorithm is linear in the number of pixels and labels. To this end, we also discuss
a modification to the permutohedral lattice based filtering method [Adams et al.,
2010], which enables us to perform approximate Gaussian filtering with ordering
constraints in linear time. This chapter is based on our work [Ajanthan et al., 2017a]
and the extended version is available in [Ajanthan et al., 2017b]. The work presented
in this chapter was conducted under the supervision of Prof. Philip Torr and As.Prof Pawan
Kumar, during my visit at the Torr Vision Group at the University of Oxford, from 4th July
2016 to 4th December 2016.

In the past few years, the dense Conditional Random Field (CRF) with Gaussian
pairwise potentials has become popular for multi-class image-based semantic segmentation.
At the origin of this popularity lies the use of an efficient filtering
method [Adams et al., 2010], which was shown to lead to a linear time mean-field
inference strategy [Krähenbühl Philipp, 2011] (see Section 2.3.5 for a brief review on
mean-field). Recently, this filtering method was exploited to minimize the dense CRF
energy using other, typically more effective, continuous relaxation methods [Desmaison
et al., 2016a]. Among the relaxations considered in [Desmaison et al., 2016a], the
Linear Programming (LP) relaxation provides strong theoretical guarantees on the
quality of the solution [Kleinberg and Tardos, 2002; Kumar et al., 2009].

Note that the LP relaxation-based algorithms discussed in Section 2.3.4 exploit
the sparsity of the CRF via the tree decomposition technique. In the fully connected
case, they would yield quadratic complexity in the number of pixels per iteration.
Clearly, this would lead to prohibitively large running time. In [Desmaison et al.,
2016a], the LP was minimized via projected subgradient descent. While relying on
the filtering method, computing the subgradient was shown to be linearithmic in the
number of pixels, but not linear. Moreover, even with the use of a line search strategy,
the algorithm required a large number of iterations to converge, making it inefficient.
We introduce an iterative LP minimization algorithm for a dense CRF with Gaussian
pairwise potentials which has linear time complexity per iteration. To this end,
instead of relying on a standard subgradient technique, we propose to make use of
the proximal method [Parikh and Boyd, 2014]. The resulting proximal problem has a
smooth dual, which can be efficiently optimized using block-coordinate descent.

We show that each block of variables can be optimized efficiently. Specifically, for one
block, the problem decomposes into significantly smaller subproblems, each of which
is defined over a single pixel. For the other block, the problem can be optimized via
the Frank-Wolfe algorithm [Frank and Wolfe, 1956; Lacoste-Julien et al., 2012] (often
referred to as conditional gradient descent). We show that the conditional gradient required
by this algorithm can be computed efficiently. In particular, we modify the
filtering method of [Adams et al., 2010] such that the conditional gradient2 can be
computed in a time linear in the number of pixels and labels. Besides this linear complexity,
our approach has two additional benefits. First, it can be initialized with the
solution of a faster, less accurate algorithm, such as mean-field [Krähenbühl Philipp,
2011] or the Difference of Convex (DC) relaxation of [Desmaison et al., 2016a], thus
speeding up convergence. Second, the optimal step size of our iterative procedure
can be obtained analytically, thus preventing the need to rely on an expensive line
search procedure.

We demonstrate the effectiveness of our algorithm on the MSRC and Pascal VOC
2010 [Everingham et al., 2010] segmentation datasets. The experiments evidence that
our algorithm is significantly faster than the state-of-the-art LP minimization technique
of [Desmaison et al., 2016a]. Furthermore, it yields assignments whose energies
are much lower than those obtained by other competing methods [Desmaison et al.,
2016a; Krähenbühl Philipp, 2011]. Altogether, our framework constitutes the first efficient
and effective minimization algorithm for dense CRFs with Gaussian pairwise
potentials.

Our goal is to design an efficient minimization strategy for the LP relaxation in (5.7).
To this end, we propose to use the proximal minimization algorithm (see Section 5.2.5).
This guarantees monotonic decrease in the objective value, enabling us to leverage
faster, less accurate methods for initialization. Furthermore, the additional quadratic
regularization term makes the dual problem smooth, enabling the use of more sophisticated
optimization methods. In the remainder of this chapter, we detail this
approach and show that each iteration has linear time complexity. In practice, our
algorithm converges in a small number of iterations, thereby making the overall approach
computationally efficient.

The dual problem (5.22), in its standard form, can only be tackled using projected
gradient descent. However, by separating the variables based on the type of the feasible
domains, we propose an efficient block-coordinate descent approach. Each of
these blocks are amenable to more sophisticated optimization, resulting in a computationally
efficient algorithm. As the dual problem is strictly convex and smooth, the
optimal solution is still guaranteed8. For b and g, the problem decomposes over the
pixels, as shown in Section 5.3.2.1, therefore making it efficient. The minimization
with respect to a is over a compact domain, which can be efficiently tackled using the
Frank-Wolfe algorithm (see Section 5.2.6). Our complete algorithm is summarized in
Algorithm 5.2. In the following sections, we discuss each step in more detail.

There are two ways to relax the integer program (5.6) to a linear program, depending
on the label compatibility function: 1) the standard LP relaxation [Chekuri et al.,
2004; Werner, 2007]; and 2) the LP relaxation specialized to the Potts model [Kleinberg
and Tardos, 2002]. There are many notable works on minimizing the standard
LP relaxation on sparse CRFs. This includes the algorithms that directly make use the
dual of this LP [Kolmogorov, 2006; Komodakis et al., 2011; Wainwright et al., 2005]
(see Section 2.3.4 for a review) and those based on a proximal minimization framework
[Meshi et al., 2015; Ravikumar et al., 2008]. Unfortunately, all of the above
algorithms exploit the sparsity of the problem, and they would yield an O(n2) cost
per iteration in the fully-connected case. In this work, we focus on the Potts model
based LP relaxation for dense CRFs and provide an algorithm whose iterations have
time complexity O(n). Even though we focus on the Potts model, as pointed out
in [Desmaison et al., 2016a], this LP relaxation can be extended to general label compatibility
functions using a hierarchical Potts model [Kumar and Koller, 2009].

To ensure consistent behaviour across different energy parameters, we ran the same
experiments for the parameters tuned for MF. In this setting, all versions of our algorithm
again yield significantly lower energies than the baselines. For this parameter
setting, the respective timing plots and segmentation results are given in Figs. 5.6
and 5.7, and the quantitative results are summarized in Table 5.3.

In order to support users in dealing with the onerous task of ground truth
generation, several tools have been developed to provide them a set of simple and
intuitive graphic interfaces to detect and draw object features such as contours,
bounding boxes, membership class, tracking information and, more in general,
object metadata. As we already said, one of the most common applications
designed for these purpose is ViPER [17] which provides a standard XML file
containing object metadata information inserted by the user. Starting from thisassumption, 
the application described in the following sections, called GTTool.

To evaluate the performances of our approach in Chapter 4 we show a
comparison between GTTool and ViPER concerning the generation of ground
truth for a video file. To be more precise, our evaluation approach is based on the
assessment of time required to label the video with each tool and on the accuracy
analysis of the generated contours, compared with those obtained from a higher
resolution version of the videos.

The proposed tool relies on a modular architecture (Fig. 2.1) which allows
users to define the ground truth by using an easy graphical user interface (GUI).
The developed application integrates a number of computer vision techniques,
with the purpose of enhancing the ground-truth generation process in terms of
both accuracy and human effort. In particular, Active Contour Models (ACM) are
integrated to automatically extract objects’ contours; object detection algorithms
and state-of-the-art edge detection techniques are employed in order to suggest to
the user the most interesting shapes in the frame. Moreover, by using a twowindow
GUI layout, the application enables the user to generate tracking ground
truth through straightforward drag-and-drop and context-menu operations. The
user can also open previous ground-truth XML files in order to add new objects
or edit the existing ones and save the performed improvements to the same or a
new file.

As in nearly every common ground-truth generation application, the
developed tool allows the user to draw ground truths manually by using the pencil
tool or the polygon tool to trace the contour of an object of interest.
Though slow and tedious to the user, the usage of these tools is often
necessary, because the automatic contour extraction methods may fail to segment
correctly the objects of interest.

In conjunction with the GMM algorithm, CAMSHIFT [62] is used to
generate automatic object tracking ground-truth data. The algorithm takes as input
the objects identified in the previous frames and suggests associations with the
objects localized (either manually or automatically) in the current frame (Fig. 2.3).
As in the case of automatic object detection, the user is always given the choice to
accept or refuse the suggested associations.

Starting from the previous work, we subsequently propose a web-based
collaborative approach for video annotation, which is based on the same
architecture of GTTool, but it has been mainly conceived to integrate effective
method for quality control of the collected data and for the combination of
multiple users’ annotations.

This tool, currently, is being used to collect large scale ground truth on
underwater video footage gathered for the Fish4Knowledge project1 which aims
at developing automatic video and image analysis methods to support marine
biology research.

The strengths and limitations of the existing video and image annotation
approaches have been largely discussed in the previous sections, in the following
subsections we first describe the proposed framework, highlighting functionalities
and improvements with respect to the state-of-the-art, then the collected content
on the aforementioned underwater environment is presented. The performance
analysis is instead given in Chapter 4 where a detailed analysis concerning the
accuracy of the generated ground truths, the efficiency of the platform in
collecting annotations, its learnability and user satisfaction is given.

Given an input video stream, our platform extracts video frames and
provides a set of utilities to annotate each video frame and to follow objects
across frames. It is a rich internet application, based on standard client-server
architecture: the client is implemented in Silverlight while the server’s logic and
the communication with the database are developed in C#.

Once the user identifies the videos she wants to create ground truth for, she
can initiate the labeling process by launching the annotation application. This part
of the platform permits to create annotations by using multiple windows. Each
drawing window (Fig. 2.9, top left) shows one image and, by using the available
toolbox (Fig. 2.9, bottom), annotations can be drawn on it.

After an object is drawn, the user can further annotate subparts of it (Fig. 2.9,
Top left) by employing the same tools described above. Furthermore, from the
same window, the user is able to add textual metadata to the object (that are
included in the exported XML file) that can be useful in other image processing
contexts (object recognition, image segmentation, information retrieval etc.).

In the proposed tool, the ground truth generation for object tracking exploits
the capabilities of multiple windows applications in order to implement an easyto-
use and intuitive way to follow objects across consecutive frames. In particular,
to be able to annotate multiple instances of the same object in consecutive frames,
the user must arrange side-by-side multiple drawing windows. When the user
places two windows with their boarders in direct contact, they become, what we
call, a “drawing chain”. While chained, the Next and Previous buttons and the
sliders of all the drawing windows are disabled except from the last one’s (the
rightmost), which serves as a control to navigate through the image sequence.
Moreover, all the chained windows maintain all the drawing functionalities as if
they were unchained. When an adequate, for the user’s needs, chain is formed the
user must draw an object and bring up the context menu by right clicking on it,
then select the voice “Tracking” and select an object from the previous frames she
wants to assign the clicked object to (Fig. 2.11).

When used in high resolution desktop setups, the application can create
multiple long chains (as shown in Fig. 2.11) of successive frames in the same
annotation instance (about 3 chains of 6 windows using a 1920×1080 resolution,
more on a multi-monitor setup).

Figure 2.16 shows the histogram of the total number of annotated images
with respect to the percentage of labeled pixels. In particular, 10.034 frames have
less than 10% of pixels labeled and no image has more than 60% of pixels labeled.
The histogram of the number of images per the number of objects in these
images (see Fig. 12), instead, shows that there exists a high number of images with
only one annotation (a little more than 11.000).

In line with the growing number of users which use the Web and considering
the need of relieving the operators from the onerous task of gathering annotation
for testing computer vision algorithms, in the following subsections a set of
crowdsourcing methods are proposed.

We first start by exploiting the peculiarities of the “Flash the Fish” online
game [66], where the user is shown videos from underwater environment and has
to take photos of fish by clicking on them. The collected “clicks” are then used to
drive some computer vision techniques for automatic image segmentation based
on Gaussian Mixture Model (GMM), exploiting visual features around the
gathered points.

The game consists of 7 different levels of progressively increasing difficulty.
Every time a game session starts, a list with 7 random selected video segments,
taken from our repository that contains more than 600.000 10-minute videos, is
generated. The first level serves the role of assessing the skills of the player (see
next section) and has an initial frame rate of 5 FPS and the time available is 35
seconds.

In order to make the game more appealing, we adopted a scoring system that
rewards users according to the quality of their annotations. In other words, the
more precise the user is, the more points she earns and climbs up the final
classification. Of course, in order to be able to assign scores, it is necessary that
each video segment comes with a reference ground truth. If, for the specific
video, there exists a hand-made ground truth, it will be used. Otherwise, if the
video is not a new one (i.e. several players have already played it, meaning that
several annotations exist), the reference ground truth is given by the combination
of all the existing annotations (see paragraph 3.2.2). If, instead, the video is a new
one (i.e. no one has played a session with this video yet) then the detection
algorithm’s [67] output is used as reference ground truth.

A reference ground truth is also used to compare the annotations provided
by the users against it. For each object in the reference ground truth a 2D
Gaussian Distribution is placed, centered on the object’s bounding box center. If
a player clicks on this point, she gains the maximum score bonus she can get,
while the bonus awarded is reduced as the clicked point gets more distant from
the center.

The contribution of each user playing the game cannot be equal. In fact, there
exist casual players that dedicate a little time playing, achieving, usually, low scores
and on the other extreme, hardcore players can be found. Assessing user quality is
of key importance for generating a ground truth based on the weighted
contribution of users. The weight is the quality score itself, meaning that the
higher a player’s score is, the more influential her annotations will be in
determining the final ground truth.

Once the users obtain a quality score, their annotations can be integrated in
order to build the best ground truth representations. In order to identify the
locations that users clicked the most, we apply iteratively an unsupervised
clustering algorithm. In particular, initially, a K-means analysis is applied with a
predefined number of clusters (set to 10 or to the number of fish in the existing
ground truth, if it contains more). The clustering result is further refined by
iterating through each point (clicked by the user) and determining whether it fits
well in the assigned cluster or not, by calculating the Euclidean distance from the
cluster’s centroid.

In this section we present the approach employed to label images by using
the click points collected with the online game described in the previous section.
In detail, the users’ clicks drive the image segmentation approaches, namely the
Region Growing and Grab Cut, which allow us to derive meaningful annotations
(Figure 3.4 show the basic schema of the proposed approach). By comparing the
results obtained with this approach against hand-labeled ground truth data, in
Chapter 4 we demonstrate that the proposed method constitutes a valid alternative
to the existing video annotation approaches and allow a reliable and fast collection
of large scale ground truth data for performance evaluation in computer vision,
relieving the users from the noisy work of annotating images and videos manually.

More specifically, the user who plays the game has to take photos of objects
through the game’s levels. In order to assess the accuracy of our approach, we
adopted two classic techniques for image segmentation which require an initial
labeling that may be either a point within the object (seed) or some region (or line)
outside the object to be segmented. Needless to say, this initial labeling is the
single most important parameter that influences the performance of image
segmentation algorithms: if the initial labels are not positioned accurately either
the result will contain undesirable information (a segment that contains the object
and part of its surroundings) or it will omit desirable information (a partial result).

Starting from the raw users’ clicks taken while playing Flash the Fish,
unsupervised K-Means cluster analysis [68] is performed in order to extract the
locations of the most clicked areas. Since the game’s purpose relies on the belief
that the most clicked areas represent actual objects, the resulting clusters will be
devoid of the influence of noisy clicks, because clusters with low numbers of
clicks are discarded.

So, the output of the game are the clusters with their associated points which
can be also represented with heatmaps showing where the majority of the clicks
are located.

Then we resort to image segmentation approaches to generate annotations on
moving objects. In detail, we used two approaches: the classic region growing that
works by identifying the differences between objects in the image according to
their color characteristics, and the Grabcut that performs image segmentation by
means of a probabilistic approach.

Grabcut operates in a different manner: instead of using a single pixel of the
image as seed for determining the part of it that belongs to the desired segment or
not, Grabcut uses an area where the object should be located.
Describing the exact theory behind Grabcut is not in the scope of this
dissertation, but the reader can find more information in [70]. What this work
addresses, instead, is the definition of the initial labeling for Grabcut to start the
segmentation process.

In detail, by using the game data, the initial labeling is derived by processing
the players’ clicks, in order to define a region large enough to contain the whole
object, but also small enough in order not to include unnecessary information. In
our case, this region is computed as the convex hull containing all the points
belonging to the same cluster.

A labeling mask is then created, where white points (foreground) are all the
points inside the convex hull, and the black ones (background) are the points
outside. This mask constitutes the initial labeling for Grabcut.
The application of the Grabcut algorithm is shown in Fig. 3.9. The same
figure also shows that Grabcut performs well even when both the background and
the object have similar colour and texture characteristics.

To be more precise, starting from the collected clicks we first apply a shiftback
of the points obtained for the current frame in order to consider the delay
with which the user clicks after seeing the object of interest. Then a uniform
random point distribution on the source image is generated for constructing the
background model, which does not include the users’ clicks. For each point
belonging to the two images, a features vector is created containing information
about position, color and texture. As concerns the texture features, the Histogram of
Oriented Gradient (HOG) descriptors [72, 73] are used. The collected feature
vectors are then assembled to build a matrix for which the Expectation
Maximization for Gaussian Mixture Model is calculated to obtain the model
related both to the object and the background. Finally, for each pixel of the source
image, the features vector is also constructed and the probability that each feature
vector belongs either to the object model or the background model is computed.
Then, according to the log-likelihood ratio, each pixel is classified as background
if the probability value is greater than a given threshold, as foreground otherwise.
In Figure 3.10 the basic schema of the proposed method is shown.

Experimental results (see Chapter 4 for the details) show that the proposed
system, when compared against the hand labeled ground truth dataset, is able to
generate reliable annotations starting from big-noisy data, providing a valid
alternative to the existing ground truth generation methods.

Afterwards, the model for the objects and the background can be built. To do
this, for each point a feature vector is created containing information about position,
color and texture features. As concern the position, x and y coordinates are extracted
from each point, while for color features the values of H, S and V channels are
taken into account. The V channel could not be considered because it does not
contain any specific information about color, but only with respect to the
brightness.

To provide a consistent description of texture characteristic of the considered
points, we refer to the Histogram of Oriented Gradient theory [73]. Generally the
HOG descriptor is used to determine local object appearance and shape within an
image by computing the distribution of intensity gradients or edge directions. In
our case, the same concept is applied not to the entire image, but to a given
windows which strictly surround each point located in the source images.

The system starts by computing the model both for object and background
which are then used to obtain a set of probability maps. Each map is associated to
a known object and contains the probability for every pixel of the source image to
be classified as the current object.

To be more precise, we compute the Gaussian distribution, not only for the
points which represent the users’ clicks and the surrounding background, but also
for each pixel of the image. Then we use the models acquired from the learning to
calculate the probability that a pixel belongs to the object model rather than the
background model.

This chapter is fully devoted to describe the methodologies adopted to
evaluate the performance of the approaches proposed in Chapters 2 and 3.
The performance of the methods for semi-automatic ground truth generation
is assessed in terms of i) the accuracy of the generated ground truth by also
referring to the VIPER-GT tool for a comparison of the obtained annotations, ii)
the efficiency of the proposed platforms, as measured by time needed to generate
annotations and iii) the learnability and user satisfaction.

Learnability represents the ease of learning the usage of the tools, while
satisfaction represents the subjective feelings of the users about their experience
with each tool; both values range from 1(worst) to 10 (best). The results show that
their experience with GTTool was more satisfactory than with ViPER, mainly,
according to most comments, because of the two-window layout (which avoids
having to go back and forth through the video to check one’s previous
annotations) and of the integrated algorithms (which drastically reduced the
number of frames and objects which had to be manually analyzed).

In this section we describe the method used to evaluate the performance of
the image segmentation approaches discussed in paragraph 3.3 (i.e. region growing
and grabcut) by comparing the obtained results against hand-drawn ground truth.
This ground truth contained 4140 objects and it was generated with PerLa [8].
Also in this case, we exploit the same outlines of the Facebook event organized
for testing Flash the Fish game (4 days duration, more than 80 users, about 1300
game sessions and more than 260000 clicks).

From the same table, it is also possible to notice how the performance of the
segmentation algorithms shows a different behaviour. In particular, region
growing performed better when the number of clicks available was low. In fact, in
the lower levels (levels 1 and 2) the region growing based approach achieved, on
average, 25% in precision and 36% in recall. The precision score was so low
because of the large number of inaccurate clicks which, therefore, resulted in
clusters whose centroids were outside the object’s boundaries. On the contrary, in
the highest levels (i.e. 6 and 7), where only few motivated and reliable users were
able to get score, both precision and recall achieved, on average, 64%. In this case,
even if the number of clicks was considerably lower than those of the first levels,
they were extremely accurate as obtained by the best performing users.

Home care is one solution for easing the stress for the health care system. It is a supportive
care provided to people with special needs at their home. Depending on the need, home
care can be provided multiple times a day and in many countries it is available around
the clock. (Sanerma 2009) The concept has grown considerably. The practice is helping
the society by minimizing hospital care by moving the action to the home of the patient.
Home care can also be assumed to increase the patients quality of life and maintenance
of independence. (Thome, Dykes & Hallberg 2003; Sanerma 2009)
The home care term is used for formal care; home care which is provided by professionals
and informal care; home care which is provided by non-professionals. Formal
care can also be defined as home health care. Informal care can also be defined as domiciliary
care, non-medical care or custodial care. Informal care is usually provided by family
or friends. The definitions is dependent on the countries health care system and their
common practices. (Sanerma 2009) Further in this research the terms informal care and
formal care will be used. The term home care will be used as a umbrella for both informal
care and formal care.

Formal care can be divided in four different categories. Practical - Preparing food, cleaning
and shopping. Personal - Personal hygiene and helping the patient get dressed. Monitoring
/ supervision - Monitoring patients suffering from dementia. Care / Case management
- Coordinating the patients services. The needed formal care is determined by measuring
activities of daily living (ADL) and instrumental activities of daily living (IADL). ADL
contains eating, personal hygiene, putting on clothes and transferring (in the house). IADL
contains preparing meals, housework, managing medications and managing finance. (Sanerma
2009) Patients usually rate their abilities to function at home (ADL/IADL) higher
than the professional care takers would. Only the fear of falling is a bigger concern for
patients than the professionals would rate it. (Morrow-Howell, Proctor & Rozario, 2001)

Sanerma (2009) and Stengård (2011) both agree that one of the big problems in home
care with is that care takers do not have enough time for each patient. Automatic monitoring
could free resources for the care takers.

Kangasniemi and Andersson (2016) made a cautious estimation that 20% of the care takers
work, in Finnish hospitals, could be automatically performed by currently available
technology. They estimated that some monotonous tasks could be performed by robots or
by automation and would give more time to spend on tasks that can only be performed by
humans. They estimated that a this would not affect care takers employment because of
the rising amount of elderly population. Implementation of the technologies needed would
take 2-3 years.

European Union (EU) has noticed the growth of life expectancy and the problems it will
bring to the health care system. EU/European Commission have established funding
programs to support research and global competitiveness since 1984, currently in the European
Research Area (ERA). The funding programs are called Framework Programmes
for Research and Technological Development and currently run at phase eighth. Horizon
2020 or Framework Programme 8 (FP8) is currently in progress with an estimated budget
of 80 billion euros. (Grove 2011) One of the funded projects is Active and Assistive Living
(AAL).1 With a 700 million euros budget, AAL is trying to enhance the quality of life for the
older people while improving the industry in Europe with the help of technology. (AAL -
Active and Assistive Living programme 2016) Industry and research has shown growing
interest in video based solutions for AAL (VAAL). Technology has evolved to a state that
products are being commercialized. (Cardinaux et al. 2011)

One of the methods to automatically monitor persons and their activities is utilizing sensors.
These can be body-worn sensors or embedded sensors installed to the environment.
Embedded sensors generally need a large network of sensors. They could be placed
on every door to detect when it does open or close. (Cardinaux et al. 2011) Or as
Ropponen (2012) suggested sensors could be mounted in to the floor. These embedded
systems are usually costly to maintain and the installation or relocation of them can be
time-consuming. Embedded sensors are also highly sensitive to the performance of the
sensor. Body-worn sensors could monitor falling, activity, vital signs, etc. and they do it
quite effectively. In body-worn sensors user compliance is poor and they do not work if the
patient forgets to wear them. Help buttons can be useful but are useless if the person is
unconscious or is not able to move. Fall detection could also be made with floor vibration
sensors, sound monitoring or video monitoring. Sound monitoring could detect when something
hits the floor and also identify speech to identify a cry for help. These methods
could also be combined to get the advantages of all systems. (Cardinaux et al. 2011; Lin
& Ling 2007; Rougier et al. 2007)

Video based monitoring has some advantages, one single camera could monitor one
room and pick up most of the activities in that room. Also the installation of a robust system
could be easy. There are still some barriers to overcome, mostly around the patients
concerns about their privacy. (Cardinaux et al. 2011)

Traditional video surveillance systems need an operator to track activities with a video
displaying multi-monitor setup. All the recording occur centralized and there is a complete
lack of privacy. There need to be a paid operator watching the screens over the surveillance
period. On top of this there is a study showing that after 22 minutes the operator will
miss 95% of the activities on the screen. (Fleck & Straßer 2010)

There are different technologies available for processing video material in a secure way.
The video can be filtered and obscured so that identities or activities can not be recognized.
A silhouette can be placed over the humans in the video so that they can not be identified.
(Cardinaux et al. 2011)

For the reasons presented above the proposed solution will be a smart camera system. A
smart camera system is a privacy respecting solution. In the system there are multiple
camera nodes. Each camera node can analyze the video automatically and detect falls.
The cameras will not transfer the video to any centralized system and thus preventing loss
of privacy. (Fleck & Straßer 2010) In the proposed solution no human is able to see the
video. If the camera detects a fall it will alarm the personnel or family. The alarm can be
made with different solutions and are discussed further later in this thesis.

Video analysis can be made with a lot of different methods. Usually the interesting part of
a scene is not the background but the objects in the foreground. These objects of interest
could be any object, e.g. humans, cars, animals etc. Foreground detection is a method
where objects of interest are separated from the background in a video stream. This method
can also be called background subtraction. Further in this thesis both terms will be
used. Background subtraction works by thresholding the error between the current image
and the estimate of the image without the objects of interest. (Xu, et al. 2016; Chan, Mahadevan,
Vasconcelos 2010; Stauffer & Grimson 1999) Faster computers have enabled
researchers to propose robust models that are more complex than the previous methods.
Backrounding methods can be categorized into pixel-based, region-based, hybrid methods
and also into parametric and non-parametric methods. Every method has its own
strength and weakness. (Xu, et al. 2016; Vacavant & Sobral 2014; Stauffer & Grimson
1999) In this thesis we can not cover every available algorithms but we will discuss the
most significant. Basic terminology of the methods will be discussed next.

Image segmentation is a process where the frame is divided into multiple segments. These
segments are created with labeling each pixel according to their shared characteristics,
such as value or position. These methods can be divided in to region-based and pixelbased
methods. More specifically these labeling techniques can use e.g. inter-pixel relations,
edge detection or histograms. With the help of the labels it is easier to further process
a single or multiple frames. (Srinivasan & Shobha 2007, Xu et al. 2016)

If the background of a scene remains unchanged the detection of foreground objects
would be easy (Vacavant & Sobral 2014; Langanière 2011, 266-277). Let us assume that
each frame is converted to a grayscale image before it is processed. Basically a frame (I),
at the time (t), when there is no foreground objects in the scene (empty room, road without
cars) is declared as the background model and then each pixel value (P) is compared to
the pixel value at the same coordinate (x, y), in the frame, at a specific time. Each pixel
that is different from the background model would be declared as foreground (F). (Tamersoy
2009) In Fig. 1 let us assume the first frame (t=0) has no foreground objects. This
method can be tested with Fig. 1b.

Non-adaptive backgrounding has other challenges too, it needs re-initialization (updating
of the entire background model) or otherwise changes in the background is detected as
foreground. These problems make non-adaptive backgrounding only useful in highlysupervised
tracking applications. (Stauffer & Grimson 1999) The re-initialization could be
avoided by using the previous frame as the background model (Fig. 6), but this fails if the
foreground object suddenly stops (Vacavant & Sobral 2014). This can be tested with Fig
6b.

Successful early work in the field on human tracking was made by Wren et al. (1997) who
proposed PFinder ("person finder"). In this, pixel-based method, the background model is
a single Gaussian per pixel and the tracked object have a multi-class statistical model
(Wren et al. 1997). This has been proved to be a good background subtraction method
(Vacavant & Sobral 2014; Stauffer & Grimson 1999). However a single Gaussian per pixel
is not able to adapt quickly to a dynamic background (swaying trees, waves in the ocean).

Because the background can change, short and long term, Stauffer and Grimson (1999)
proposed the Gaussian mixture model (GMM). The GMM models every pixel with a mixture
of K Gaussians function. GMM is able to quickly adapt to a dynamic background and it
has become a very popular background subtraction method. However GMM is not able to
handle sudden illumination changes (turning on/off lights, clouds blocking sunlight) and
shadows very well. (Chan, Mahadevan, Vasconcelos 2010; Vacavant & Sobral 2014; Xu
et al. 2016) Zivkovic (2004) proposed improvements to GMM with Adaptive Gaussian mixture
model (AGMM). These improvements did not include a solution for the shadow problem
but reduced processing time by automatically selecting the number of components
needed for each pixel. This research was funded by EU Framework Programme 6 (FP6
2002-2006). (Zivkovic 2004)

As mentioned earlier the biggest challenges in backgrounding would be illumination, dynamic
background, shadows and video noise. One of the challenges is that the RGB color
space is sensitive to illumination changes. For this reason SOBS uses the HSV color space
and SACON normalized color space. CodeBook separates color distortion and brightness
distortion with a color model. Rapid illumination changes can occur when someone
turn on a light or sunlight is blocked by some clouds. KDE combines a short and long term
background model to handle rapid illumination changes. (Xu et al. 2016)

Usually every video feed has noise. The source of this noise can be e.g. sensor noise,
compression artifacts or camera shaking. One technique to minimize the effects of noise
is to adapt a Gaussian or median filter to the video stream. Another technique is to construct
the background model from noisy pixel values so that the model will automatically
adapt to noise. Vibe uses this method to construct the background model. Robust backgrounding
methods should be able to cope with noise disturbance. (Xu et al. 2016)

Foreground object often cast a shadow. A shadow from a foreground object will affect the
backgrounds illumination. Usually shadowed areas have a minor color variation, but a
significant illumination variation. This creates challenges for backgrounding methods and
a robust method should be able to handle shadows. (Xu et al. 2016)

One of the reasons why one would like to detect foreground objects from a video is to be
able to determinate what activity the person in the video is performing. In the static analysis
the persons posture is analyzed at a specific time. A posture is a good indicator of
what the person is doing e.g. lying, standing or sitting. This information alone is not very
useful. That is why in the dynamic analysis the outcome of the static approach is combined
to the earlier static approach outcomes. In this way we can analyze movement patterns.
(Cardinaux et al. 2011) If the person was standing in the last frame and in the current
frame is detected as lying, the person probably have suffered from a fall (Cardinaux
et al. 2011; Kroputaponchai & Suvonvorn 2013).

Because the static approach alone is not very useful a dynamic approach is used. In the
dynamic analysis the results from the static analysis are used and saved to a register.
With the register the system can identify if a lying person did fall or intentionally lying. The
time since a person detected as standing to the time that the person is detected lying is
used to identify falls. (Cardinaux et al. 2011) If the person was standing 0.4-0.8 seconds
ago and is now detected as lying, the person probably have suffered from a fall. Also an
audio analysis could be utilized with video-based methods to make the detection more
robust. (Cardinaux et al. 2011; Kroputaponchai & Suvonvorn 2013)

Nasution & Emmanuel (2007) proposed usage of a stripped GMM to detect foreground
objects. To detect the activity of a person they then trained the system with sitting, standing,
bending, lying and lying against the camera. After the training phase they adapted a
KNN algorithm to calculate equality with each posture from the training phase. After this
they adapted an evidence accumulation technique to only change the posture (from last
frame) if the equality was high enough to exceed a threshold. (Nasution & Emmanuel
2007)

Many researches define a fall as a person having a greater acceleration than in their normal
daily activities (Gjoreski, Lustrek, Gams 2012; Kroputaponchai & Suvonvorn 2013;
Nasution & Emmanuel 2007). Fall-like activities can have the same acceleration as normal
activities such as lying down on the bed quickly or quickly sitting down. Additionally,
all falls does not include a fast acceleration. (Gjoreski et al. 2012; Rubenstein 2002; Rougier
et al. 2007) Gjoreski et al. (2012) proposed that Rubenstein (2002) showed that 22%
of the falls could be "slow falls", falls that does not have a greater acceleration than normal
activities. That is why approaches that only rely on acceleration can be problematic
and could cause many false detections. A robust system should detect slower falls and be
able to separate quick normal activities from falls.

While posture analysis is a good way to detect the persons state it is hard for it to detect
what activity, more specific than just sitting, standing or lying, the person is performing.
That is why the persons position could be used to determine what ADL or IADL the person
is currently performing. With this technique the daily routines could be monitored and
taught to the system and if something abnormal is detected, it could create an alarm
(Rougier et al. 2007; OMASeniori; Lin & Ling 2007).

The persons position analysis with an overhead camera could extract the following information:
position, velocity and orientation. This could be done by tracking the persons
head. The head tracking can be made with an approximated ellipse to extract information
of height changes i.e. how close to the camera is the person. Motion and position information
is useful for making decisions of activities. (Cardinaux et al. 2011) Unusual inactivity
could also be detected (Lin & Ling 2007). Position based limitations could be defined to
turn off the fall detection on zones where falls could be normal (Rougier et al. 2007). If a
person is detected as falling on to the sofa and then detected as sitting, the person could
be intentionally sitting. Another example could be that it is not normal behavior for a person
to be lying on the floor but the person can lye down on the bed.

The first version of the fall detector utilizes sort of a dynamic approach. It will detect if a
person is not moving or is moving too little in a specific time period. Detections could be
configured so that there are different detection times for e.g. sitting on the sofa or lying in
the bed and if the person is lying on the floor not moving the detection could be triggered
in a few minutes. When the detection is made it will send an alarm to a RESTful web service.
This web service can trigger different functions from a centralized alarm center. These
functions include information to professional care takers and family.

The idea of the project is to lower health care costs and provide safer home care for the
aging generation. The presented system is just a raw prototype and should be further developed
for commercial use. The methods presented in this paper were not systematically
tested, these methods could be tested in further research. The features that are not yet
implemented in the system but were presented earlier should be implemented. This paper
did not include cost comparison of available systems, but a quick look at the market field
suggest that this is a thing that should be researched. How much the solution could save
is not yet relevant and was not be in the scope of the paper.

Coronary angiography is a definitive, gold standard and low cost diagnostic
procedure for determining the presence and severity of CA disease. The introduction
to this image modality and the catheterization procedure was already discussed in
section 1.4.2. Hence, the main focus of this chapter is to elaborate some of the
significant aspects of coronary angiography for CA disease diagnosis. The first
section of this chapter introduces about the hardware organization of the
catheterization laboratory (cath lab), fluoroscopy function and the aspects of contrast
agent used for the catheterization comprehensively. Typically, the coronary
angiograms are taken in different views to emphasize the various CA segments in
order to clearly detect the diseased areas. The next section of the chapter contains
information about these angiographic views with sufficient visual illustrations to
easily recognize them separately. The angiography based stenosis grading methods
have been presented as the third section of this chapter. In the subsequent section, the
problems of this image modality have been elaborated with the literature evidence.
Finally, this chapter is concluded by mentioning the recent research attempts carried
out to improve the angiography image modality under three different areas namely;
the enhancement of angiograms, vessel segmentation and quantitative coronary
analysis.

The objective of this section is to elaborate how the cardiac catheterization hardware
components function together to produce the CCA for medical diagnosis. Therefore,
initially, some important facts about the catheterization laboratory setup, main
hardware components and the installation of them have been emphasized. After that,
the attention is given to discuss about the fluoroscopy imaging system, which is the
X-ray image modality used in the cardiac cath labs to generate the CCAs. Finally,
this section explains about the features of the contrast material used to opacify the
vessel structures during the angiography procedure.

The cath lab is setup with digital imaging equipment and computers for fluoroscopy
and CCA, which is used for assessment of CA disease. Fluoroscopy is an imaging
technique used to obtain real-time moving images of internal structures of the human
body [32]. The fluoroscope is the device used to obtain such medical images. It
shows a continuous X-ray image on a monitor, which is akin to an X-ray movie [33].
In cardiac catheterization, the fluoroscopy assists the doctor in guiding the catheter
into a specific location in the heart. Moreover, it produces CCAs, which are the
fluoroscopic motion-picture, recording of a blood vessel or of a portion of the
cardiovascular system obtained after injecting a patient with a non-toxic radio
opaque medium (contrast agent) [34]. The cine X-ray camera attached with the
fluoroscope records these CCAs.

The c-arm is the largest part of the cath lab system. It is installed either as a floormounted
or ceiling suspended arc and consists of the X-ray source tube and the
detector (Figure 2.1 label 1). According to Figure 2.1, it is clear that the X-ray source
tube (label 2) is placed at the bottom of the c-arm and detector (label 3) is placed on
top of it. As shown in Figure 2.2, the patient is laid on the cardiac catheterization
table by positioning the detector of the c-arm above the patients’ chest and the X-ray
source tube below the patients’ table (Figure 2.1 label 2). Moreover, the c-arm could
be rotated to the left or the right of the patient. These are known as the Left Anterior
Oblique (LAO) and RAO views respectively. In addition to that, the c-arm can be
rotate towards (cranial) and away (caudal) from the patients head. All these
movements are achieved with the help of a control handle (Figure 2.1 label 7) that is
situated near the hand of the operator. In addition to the c-arm movements, controls
are also available to raise and lower the patients’ table, raise and lower the camera
attached with the detector, change the magnification and to increase and decrease the
size of the shutters.

The two pedals that activate fluoroscopy and cine filming lie on the floor, near the
operator's feet and each mode is turned on by stepping on it (Figure 2.1 label 8). For
example, the operator will first step on the fluoroscope pedal and confirm a position
(during test injection of a small amount of contrast agent) and then switch over and
press the cine pedal during the contrast agent injection.

The X-ray tube attached with the c-arm generates powerful X-ray pulses during the
catheterization procedure. It should be a heavy-duty tube with high-heat capacity and
improved heat dissipation. The X-ray generator, which is attached to the cath lab
system energizes the X-ray tube by delivering the full current capacity of the X-ray
tube in cine mode (Figure 2.1 label 5). The detector detects the X-rays, which pass
through the patients’ body in order to form the X-ray images. In older cath lab
systems an X-ray image intensifier and a video camera are placed in the detector and
in latest models a flat panel detector replaces the X-ray image intensifier.

The camera attached with the detector records the fluoroscopy images and those
recorded images are visualized through the monitors, which are placed in the cath lab
(Figure 2.1 label 6). A computer unit, which is installed with specialized digital
image processing programs is located in the control room of the cath lab and is useful
to diagnose and make decisions about the severity of the detected lesions (Figure 2.1
label 9).

The X-ray source tube of the fluoroscope consists of an X-ray generator, X-ray tube,
collimator and filter. The high-voltage generator and X-ray tube are used in most
fluoroscopy systems to generate X-ray pulses in order to record the anatomical
structures of the human body. It is similar in design and construction to the X-ray
tubes used in general radiographic applications. Fluoroscope used in angiographic
systems produces clear images of moving vessels. In order to produce that, the
fluoroscope needs short, powerful X-ray pulses and to achieve that it consists of a
high-power X-ray generation system. Moreover, in cardiac studies, exposure time
ranging from 1 to 10 milliseconds is required and the radiation output can be either
continuous or pulsed [37]. Automatic exposure rate control maintains the radiation
dose per frame at a predetermined level, adapting to the attenuation characteristics of
the patient’s anatomy and maintaining a consistent level of image quality throughput,
which is required for the examination [37]. In order to limit the geometric extent of
the X-ray field, either circular or rectangular shape collimator shutters are assembled
in the X-ray tube. The shape of the collimation shutter depends on the shape of the
image receptor of the fluoroscope. The beam hardening filters are placed in between
the X-ray tube exit port and the collimator for filtration of the X-ray beam path,
providing flexibility to manage the low dose and higher dose modes according to the
conditions dictated during a fluoroscopic procedure. Further, it is important to note
that the patients’ table of the fluoroscope system does not absorb much radiation to
avoid shadows, loss of signal and loss of contrast in the image.

The X-ray image intensifier or the image receptor of the fluoroscopy system is an
electronic device that detects X-rays that passes through the patients’ body to form
X-ray images. It converts the detected X-ray beam intensity pattern into a visible
image, which is suitable for capturing by a video camera and displaying on a video
display monitor [32]. Further, it provides both real-time imaging capability, which
allows patient positioning, catheter manipulation and recording of the angiographic
injection. In order to achieve this, the X-ray image intensifier consists of four major
components namely; an input phosphor layer, a photocathode, several electron optics
and an output phosphor layer. Figure 2.4 depicts the comprehensive schematic of the
organization of these components in the X-ray image intensifier for further
clarifications.

The function of the X-ray image intensifier begins by converting the detected X-ray
image into a visible light image. It is done by the input phosphor layer equipped with
the X-ray image intensifier. As the next step, the photocathode which is placed in
close proximity to the input phosphor layer releases electrons directly proportional to
the amount of visible light from the input phosphor that is incident on its surface.
These released electrons are steered, accelerated and multiplied in number by the
electron optic components and finally contact the surface of the output phosphor
layer in the X-ray image intensifier. The coated phosphor material in the output
phosphor layer glows visibly when struck by high energy electrons and converts
them into light. After that, a video camera, which is optically coupled to this
phosphor screen observes the intensified image and forwards it to a computer for
post-processing. Finally, these post-processed signals are rendered using the display
devises to visualize the detected anatomical structures. The brightness of the images
produced by the X-ray image intensifiers are achieved by increasing the electron
energy produced by the accelerating potential and decreasing the size of the image as
it is transferred from the input to the output phosphor [32].

As shown in Figure 2.4, output phosphor layer of the X-ray image intensifier is
coupled to a video camera. Further, a film camera is also assembled in angiography
devises, which is used for the cardiac imaging to produce the CCAs. This film
camera is a 35 mm motion picture camera and is optically attached to the image
intensifier output phosphor (Figure 2.4). In order to record the fluoroscopic image
with the video camera and film camera, the X-ray image intensifier should be able to
divide the light coming from the output phosphor layer and direct it in two separate
paths. This system of reflecting light in two directions is called beam splitting and it
is done by a semi transparent mirror (beam splitter), which is placed between the
image intensifier and the video camera tube. Ninety percent of the light is reflected to
the film camera while the remaining light passes through the mirrors and is received
by the video camera [39]. The video cameras used in X-ray image intensifier systems
were originally analog devices borrowed from the broadcast television industry but
later systems equipped with digital cameras based on charge-coupled device image
sensors or complementary metal oxide semiconductor technology were used.

The modern angiographic suites are produced with the digital image processor unit,
which is used for real time processing of the captured images by the video camera.
Spatial filtering, temporal filtering, image-subtraction and integration algorithms are
implemented with this image processor unit to post-process the images before
rendering.

The process of generating CCAs and its visual representations have been discussed
comprehensively in Chapter 02. Even though these CCA frames are produced
directly by the fluoroscope, past researches and empirical results have shown that
those produced CCA frames are degraded by various types of artifacts. Moreover,
those negative artifacts result in severe obstructions for segmentation the vessel
structures and quantitative analysis of stenosis. Hence, it is worth analyzing those
artifacts and to implement an effective process to enhance the CCA frames to obtain
the optimal quality required for the subsequent processing.

Initially, the chapter emphasizes the complete design of the proposed quantitative
coronary analysis method of this research study for improving the CCA for objective
diagnosis. Later, the chapter is focused to discuss about the first phase of this
proposed method, which is known as pre-processing. Within this section, it
elaborates the effect of already recognized negative visual artifacts in CCAs with
visual illustrations. Finally, the main implantation stages of the proposed preprocessing
phase such as frame enhancement, frame alignment and mask creation
will be detailed comprehensively.

In order to achieve the study objectives stated in section 1.7, a novel method has
been proposed as depicted in the flow chart of Figure 3.1. According to the
flowchart, the direct CCA is input to this proposed method. An individual frame
extracted from this input CCA is processed at a time by passing it through the four
main processing phases namely; pre-processing, segmentation, vessel tracking and
calculation. Although this is an iterative process, the same processing phases will be
implemented on each and every frame of the input CCA continuously. Within
iteration, it logs the diameter of the selected vessel segment and length of the vessel
skeleton with respect to the current frame to be processed. These log data can be
used to generate visualizations of the vessel diameter to determine the suspected
regions.

The objective of the pre-processing phase is to enhance the visual quality of the CAs
shown in the CCA frame to be processed. It is achieved by reducing the recognized
visual degradation artifacts from the CCA frame such as noise, non-uniform
illumination and global motion. Moreover, in this phase, some image processing
techniques have been applied to emphasize the blood vessels by reducing the
background details shown in the frame. This enhanced frame is input to the
subsequent segmentation phase to extract the main CA depending on the
angiography view. Subsequently, this segmented frame is input to the vessel tracking
phase to track the vessel skeleton starting from the catheter engaged point visualized
in the frame to be processed.

The vessel diameter and length of vessel skeleton calculations are done in the last
implementation phase of this proposed method, which is known as quantitative
analysis. Meantime the calculated vessel diameter and length of vessel skeleton are
logged for both result analysis and detection of stenosis regions in the processed
main CA segment. This chapter broadly discusses about the implementation steps of
the pre-processing phase and the remaining implementation phases of this proposed
method will be elaborated comprehensively in the following chapters.

Ethics Review Committee of the Faculty of Medicine, University of Colombo has
granted the ethical clearance to extract the CCAs, which are required for validating
the results of this proposed method. Hence, the direct CCAs recorded under the three
standard angiography views were selected for data validation namely; LAO Cranial
in RCA catheterization and AP Caudal and AP Cranial views in LCA catheterization.
The main reason for selecting the aforementioned angiogram views for creating the
dataset is that those views provide excellent visualizations for the main CAs namely
RCA, CX and LAD respectively.

It is necessary to obtain clear X-ray images for CCAs because, spatial indistinctness
of CCA frames cause incorrect assessments in subsequent quantitative approaches
followed in this study. The quality of angiographic X-ray images is determined in
terms of contrast and spatial resolution. Moreover, it has been reported that these
angiographic quality factors depend on the following effects: geometric distortions,
the resolution of the detector, scatter and veiling glare, non-uniform opacification,
noise and motion [30]. Fluoroscope with a flat panel detector has some inbuilt
mechanics to evade the geometric distortions, scatter and veiling glare from the
angiography images. However, non-uniform illumination, noise and motion are still
present in the angiography images and cause some visual degradation in recorded
angiography images. Following sections will emphasize the characteristics and
consequences of those artifacts for the CCAs in detail.

Non-uniform illumination in CCAs degrades the visual quality of the angiography
and formulates some incorrect vessel segments in computer based vessel recognition
or segmentation procedures. This phenomenon visualizes an individual vessel
breaking into several segments. Moreover, it makes some hindrances to clearly
recognize which branch segment belongs to which CA in the frame to be processed.
Hence, it makes some difficulties to automatically segment the CAs accurately [25].
Figure 3.2 (a) depicts this artifact for further clarifications. In this figure, the affected
area is circled and within that area, it is hard to identify which vessel branch belongs
to which CA apparently.

It is reported that the poor contrast opacification of the vessel may lead to a false
impression of an angiographically significant lesion or lucency, which could be
considered a clot [40]. Further, inadequate mixing of contrast material and blood
could be seen as a luminal irregularity. It occurs when there is a strong presence of
blood and contrast agents in the vessels, the thick vessels have more contrast to noise
ratios than the small narrow ones [25]. Figure 3.2 (b) clearly depicts this artifact.
Contrast of the vessel depicted in the circular area is extremely poor when compared
to the main blood vessel. In order to overcome this issue, a bolus injection of contrast
material must be delivered continuously until the adequate opacification level is
reached and the angiogram could be interpreted correctly. Moreover, it is possible to
enhance the delivery of contrast material by using a larger catheter or a power
injector. Even though the injection of contrast agent is controlled mechanically still
there is a possibility of getting low contrast angiography images. Hence, it is
necessary to find a method to overcome this problem automatically.

In order to determine the global motion, it is required to calculate the GMV among
the consecutive CCA frames and is calculated based on the template matching Image
Processing technique. Further, this calculated GMV presents the displacement of the
blood vessels (frame content) among the two consecutive frames as depicted in
Figure 3.4. Subsequently, the frame reconstruction step will be performed to adjust
the frame contents according to the calculated GMV. As a result of that, visual
alignment among frame content can be achieved and it visualizes the spreading of
contrast agent within the vessel structure starting from one fixed point.
In conventional template matching, the given template is matched with the reference
image to find the region of interest based on the spatial contents coherence [95].
However, when considering about angiogram images the conventional template
matching will fail in some situations because, the arteries recorded in angiogram
frames are different from frame to frame due to vessel structure deformation due to
motion artifact and non-uniform distribution of contrast agent within the CAs. This
issue has been successfully addressed by the proposed frame alignment stage. The
next sections of this chapter elaborate the implementation steps of this frame
alignment stage based on its’ five implementation steps namely; (i) template
selection, (ii) template matching, (iii) false matching correction, (iv) calculating the
GMV and (v) frame reconstruction.

The proposed content alignment begins with the template selection step. In this step,
the initial template is selected interactively from the first frame of the input preprocessed
CCA. As shown in Figure 3.8(a), this selected template is located at Fx,y
point of the initial frame and it has w width and h height. The values of w and h
depend on pixel width and height of the arbitrarily selected template. In addition to
the template selection, it is necessary to select another region around the selected
template as the search window in order to set the boundary for the subsequent
template matching step. Figure 3.8(b) depicts the template search window located at
Gx,y point of the initial frame. It has m width and n height where m˃w and n˃h. The
matching will provide elegant results if the selected template consists of a clear
object within it. Therefore, selecting a template around the catheter engaged area
visualized in the first frame of the CCA provides successful results during the
template matching steps because the catheter engaged area is clearly visualized in all
recorded frames of most CCA cases. Hence, it is recommended to select the initial
template from such an area in the first frame of the CCA to be processed.

The implementation of HOG descriptor can be achieved by dividing template image
into small regions, called cells, and for each cell compiling a histogram of edge
orientations for the pixels within the cell. Hence, in this study, it has been selected
4×4 cells and 8×8 blocks to calculate the histograms of gradient orientation and there
are four cells in one block. Figure 3.13 depicts a sample implemented HOG
descriptor for a scaled template image (64×8 size). It consists of 16 ×2 total blocks.
Further, it represents the computed histogram of gradient orientation in each block.
Moreover, each of these histograms has 9 bins to represent orientation of voted
pixels from 00- 1800. The length of the bin in each histogram indicates the dominant
direction of gradient magnitude. Eventually, the combination of these histograms
forms the ID for the HOG descriptor.

Initial step of mask creation is done after the frame reconstruction step of content
alignment. Though the content alignment is an iterative procedure, the accumulated
foreground image created by mask creation is also performed iteratively. The
accumulated foreground image creation was done by executing three operations to
the copies of two consecutive frames, which are processed within each repetitive step
of the content alignment. As the first operation, the frame difference between the
copies of two consecutive frames of the CCA to be processed is computed. This
frame difference depicts the intersection area where the foreground objects (catheter
and CA tree) are moving within the CCA frame. Figure 3.16 (c) depicts the frame
difference image of the two consecutive frames visualized in (a) and (b) for further
clarifications of the aforementioned operation. This frame difference operation
generates some noise blobs in addition to the foreground intersection area. Those
noise blobs highlighted by Figure 3.16 (c) should be minimized significantly to
obtain the best mask image. As the second operation, thresholding is applied to the
frame difference image to obtain clearly separated background and foreground
regions of the frame difference image as depicted in Figure 3.16(d). Setting the
threshold is done manually and it depends on the visual contents represented in the
CCAs. The amount of noise blobs in the frame difference image is reduced mostly
due to the application of thresholding operation.

The proposed vessel segmentation method is mainly based on a spatial filtering and
region growing approach. It follows Frangi’s filter for vessel enhancement, which
was widely accepted for vessel enhancement in recent past research studies [63][98].
The main objective of this phase is to extract the foreground area from the CCA
frames. As mentioned in section 3.6, CAs and parts of the catheter recorded in each
frame of CCA are considered as foreground objects. Moreover, this segmentation
method does not depend on any kind of prior knowledge about vessel regions of
CCAs.

This chapter has elaborated the proposed method for segmentation of the CCAs to be
processed. The catheter engaged point and the complete CA vasculature depicted in
each CCA are extracted into a separate image as a result of this proposed method.
Initially the chapter discussed about the proposed segmentation method for
foreground extraction followed in this study. It consists of three main
implementation stages namely foreground enhancement, structure filling and
foreground extraction. Latter sections of the chapter have discussed these
implementation stages comprehensively with visual illustrations for further
clarifications. The results provided in this segmentation phase are directly applied for
the next phase of the study, which is known as vessel tracking and luminal
information extraction and it will be discussed in the next chapter.

Subjective stenosis analysis and lack of quantification methods for determining the
severity of stenosis are considered as major drawbacks in angiography image
modality. Thus, it is crucial to suggest an accurate quantification method to improve
this medical image modality for objective clinical assessments. Therefore, the main
aim of this chapter is to elaborate the design and implementation stages of the
proposed vessel tracking and quantitative analysis phases of this research study. In
here, the segmentation image denoted as f8(x,y) is further processed to obtain the CA
luminal information such as blood vessel boundary, skeleton and diameter along the
vessel skeleton. The proposed vessel path tracking method has been explained
clearly in the first section of this chapter. Consequently, the methods proposed for
quantitative analysis and representation of the results have been elaborated with
visual illustrations.

In order to start the execution of SPT algorithm, it is necessary to set a seed point and
mark the tracking direction. Seed point is a pixel point, which is on the detected
skeleton of the input image fs(x,y) and is marked interactively. According to the
Figure 5.3 (a), (c) and (e), it is apparent that this detected skeleton consists of a
catheter portion skeleton and a vessel portion skeleton. Hence, two possible tracking
directions occur starting from the seed point namely forward and reverse direction.
Moreover, forward direction means tracking towards the vessel skeleton portion
starting from the seed point and reverse direction means tracking towards the
catheter portion starting from the seed point. Even though there are two tracking
directions available, forward tracking will be the desired approach in this study.
Hence, it is required to always mark forward direction as the tracking direction and
essential to deny reverse tracking.

Figure 5.4 emphasizes important steps of marking the tracking directions. Figure 5.4
(a) depicts a sample skeleton image (fs(x,y)) and assume that it is required to track
this skeleton starting from the given seed point. A 10×10 pixel region around the
seed point is marked in the same image and image (b) of Figure 5.4 expands this
marked region for better visualizing the pixel positions. On pixels in Figure 5.4 (b)
represent the skeleton pixels and off pixels represent the background. Moreover, the
given seed point is colored in gray, in the same image to clearly identify its position.

This is an important aspect of this study because the last skeleton point of the
catheter skeleton signifies the catheter engagement point of the blood vessel. Hence,
this pixel point can be used to automatically set the initial seed point for tracking the
vessel skeleton of the subsequent skeleton image of the CCA to be processed.
Further, the position of the catheter engagement point has been fixed during the
frame alignment stage of the pre-processing phase. As a consequence of that, the
variation of the placement of catheter engaged point throughout the frame sequence
has become lower. Figure 5.5 has provided visual illustrations of the execution of
proposed SPT algorithm on four consecutive skeleton images of a certain CCA.
Image (a) of Figure 5.5 depicts the first frame of the selected CCA and it visualizes
the skeleton of the catheter. Seed point is marked on this catheter skeleton and last
track point of the catheter skeleton is also highlighted in the same image. Moreover,
this last track point is automatically set as the initial seed point of the subsequent
skeleton images and it is emphasized in images (b), (c) and (d) of Figure 5.5.
Furthermore, the SPT algorithm executes automatically based on this initially set
seed point and perform tracking continuously until it satisfies the case 1.

Vision-based hand gestures interaction is a challenging interdisciplinary research
area, which involves areas such as computer vision and graphics, image processing,
data mining, machine learning, and informatics. To make use of such techniques in a
successful working system, there are some requirements which the system should
satisfy [10] such as for example robustness to variations in illumination and to
occlusions. The system must be computationally efficient, so it can be used in realtime
situations, and it must error tolerant, giving users the possibility to repeat some
actions. Also, it should be flexible enough so that it can be adapted to different scales
of applications, i.e., the core of vision-based interaction should be the same for
desktop environments and for robot control, for example.

Vision based gesture recognition has the potential to be a natural and powerful tool
supporting efficient and intuitive interaction between humans and computers. Visual
interpretation of hand gestures can help achieving the ease and naturalness desired
for HCI (Human Computer Interaction). Also, vision has the potential of carrying a
wealth of information in a non-intrusive manner at a low cost. Therefore, it
constitutes a very attractive sensing modality for developing hand gesture
recognition, and can be divided into two categories: 3D model based methods and
appearance based methods [11, 12].

3D model-based methods are used to recover the exact 3D hand pose. Such models
however have a disadvantage which is computationally intensive, making such
methods less suitable for real-time applications. On the other side, although
appearance-based methods are view-dependent, they are more efficient in terms of
computation time. They aim at recognizing a gesture among a vocabulary, with
template gestures learned from training data [10, 13, 14].

Appearance-based models extract features that are used to represent the object under
study and must have, in the majority of cases, invariance properties to translation,
rotation and scale changes.

Given this, the approach typically used for vision-based hand gesture recognition
interfaces can be divided into the following three steps: hand detection and
tracking, hand feature extraction and hand gesture classification.
There are many studies on gesture recognition and methodologies well presented in
the literature [10, 13-16].

The following sections present some background and the state of the art related with
the areas of hand feature selection and extraction, static or hand posture classification
and dynamic gesture classification. The importance of proper feature selection and
extraction will be addressed and a comprehensive description of some of the
techniques and algorithms used in the area will be given.

The essential thought behind the Histogram of Oriented Gradient descriptors is that
local object appearance and shape within an image can be described by the
distribution of intensity gradients or edge directions. Since the HoG descriptor
operates on localized cells, the method is invariant to geometric transformations and
illumination changes.

Pixel intensities can be sensitive to lighting variations, which can lead to
classification problems within the same gesture under different light conditions. So,
the use of local orientation measures avoids this kind of problem, and the histogram
gives us translation invariance. Orientation histograms summarize how much of each
shape is oriented in each possible direction, independent of the position of the hand
inside the image [30]. This statistical technique is most appropriate for close-ups of
the hand.

The algorithm first calculates the image gradients on line 11 and 12 using one of
many possible operators. In the proposed solution implement the Sobel operator was
used [32-34]. This operator is used to find the approximate absolute gradient
magnitude at each point in an input grey scale image. The grey average from the
obtained contrast image is used to define a contrast threshold used to eliminate low
gradient values during histogram calculation. Also, the number of histogram bins, or
orientation values, was defined to be 36, so, the contrast image values are mapped to
the proper histogram range by dividing by the step value defined in line 23. After
histogram calculation, the respective histogram blur is performed with the defined
filter initialized in line 39.

Instead of using the original image representation in the spatial domain, feature
values can also be derived after applying a Fourier transformation. The feature vector
obtained this way is called a Fourier descriptor [39]. This is another feature normally
used to describe a region boundary [25, 33], and considered to be more robust with
respect to noise and minor boundary modifications.

As explained in section 2.3 no single approach for classification is “optimal”,
depending for example on the nature and type of extracted features or the type of
application. Consequently, as seen before, it is sometimes a common practice to
combine several modalities and classifiers for the purpose of pattern classification. In
practice, the choice of a classifier is a difficult one and it is often based on which
classifier(s) happen to be available or known to the user.

Artificial Neural Networks (ANNs) have also been widely used in the area of visionbased
gesture recognition with very good results. Maung [88], Tasnuva Ahmed [89]
and Mekala et al. [90] applied Artificial Neural Networks (ANNs) to the problem of
static hand gesture recognition in a real-time system. Maung applied it to a system
able to recognize a subset of MAL (Myanmar Alphabet Language) static hand
gesture in real time. His method extracted HoG features that were used to train an
ANN classifier. The paper includes experiments with 33 hand postures, and the
author claims that the method is simple and computationally efficient. The results
showed that the system was able to achieve an accuracy of 98% on a single hand
database. One drawback of the method is, that it is efficient as long as the data sets
are kept small. Tasnuva Ahmed trained the ANN with a database of records
composed of 33 different features extracted from grey scale and binary hand images,
that the author claims are rotation, scaling, translation and orientation independent.
The ANN architecture used had an input layer with 33 inputs, an hidden layer with
85 nodes or neurons, and an output layer composed of 4 nodes, and used the backpropagation
learning algorithm. The proposed approach was able to achieve an
accuracy of 88,7% with a database composed of four static gesture types. The system
presented however some difficulties in varying light conditions and hand tracking.
Mekala et al. on the other side, used an ANN in a vision-based system for static hand
Sign Language recognition, based on a HW/SW co-simulation platform. This
approach intends to increase the speed of execution, while maintaining the
flexibility. As feature vectors they used the extracted hand shape and the orientation
magnitude. The training phase is done on the software platform and the testing phase
is done on the hardware platform. This is based on a new technique presented in the
paper called co-simulation neural network. In their method, a part of the neural
network is designed on the hardware with dedicated ports. 

The network is built of 16 input neurons in the input layer, 50 neurons in the first hidden layer,
50 neurons in second hidden layer, and 35 neurons in the output layer. The number of neurons
selected resulted from the analysis of the selected features for image representation.
The authors claim that the co-simulation platform was able to reduce the recognition
times, with a 100% success rate for all the sign language alphabets (A to Z). Haitham
Hasan et al. [91] presented a novel technique for static hand gesture recognition for
human-computer interaction based on the hand shape analysis and an ANN for hand
gesture classification. Their main goal was to explore the utility of a NN approach
for the recognition of hand gestures. The system was divided into three parts: (1) preprocessing,
(2) feature extraction and (3) classification. The main efforts were made
in the feature extraction and classification. As hand features they used complex
moments (CMs) introduced by Abu-Mostafa [92] as a simple and straightforward
way to derive moment invariants. The CMs are very simple to compute and quite
powerful in providing an analytic characteristic for moments invariant. They are a set
of values extracted from the image that have the property of invariance to image
rotation. Moment invariant can be used as a feature for object classification and
recognition. They tested a Neural Network (NN) with the extracted hand contour and
a Neural Network with the complex moments. The system was able to achieve a
recognition rate of 70,83% using the contour but suffered from invariance to
translation, while the second method achieved an accuracy of 86,38%. Nevertheless,
the system presented some problems to recognize the same gesture under different
light conditions, which is a serious limitation.

As analysed in the previous chapter, vision-based hand gesture recognition is an area
of active current research in computer vision and machine learning [88]. Being a
natural way of human interaction, it is an area where many researchers are working
on, with the goal of making human computer interaction (HCI) easier and natural,
without the need for any extra devices [74, 119].

As Hasanuzzaman et al. [120] argue, it is necessary to develop efficient and real time
gesture recognition systems, in order to perform more human-like interfaces between
humans and robots.

Although it is difficult to implement a vision-based interface for generic usage, it is
nevertheless possible to design this type of interface for a controlled environment
[10, 121]. Furthermore, computer vision based techniques have the advantage of
being non-invasive and based on the way human beings perceive information from
their surroundings [8].

In order to build a solution that could meet all the previous requirements, three
modules were implemented. These modules allow you to train, in a supervised way,
all the necessary gestures that will be part of future vision-based hand gesture
recognition systems, for human / computer interaction.

The implemented modules are: the Pre-Processing and Hand Segmentation (PHS)
module, the Static Gesture Interface (SGI) module and the Dynamic Gesture
Interface (DGI) as shown in the diagram of Figure 6. From the diagram, one can see
that user hand must be detected, tracked and segmented on each frame. The
segmented hand is passed as an argument to the SGI module to extract hand features
that are saved into a features dataset. This dataset is later used for model training,
and the resulting model is also for future use. The detected hand is passed as a
parameter to the DGI module used to extract the hand centroid which is used for
hand path construction. Each hand path is labelled according to a defined alphabet
resulting in a feature vector that is saved into a gesture dataset. This dataset is later
used for model training and the corresponding obtained model is saved. The user has
also the possibility to define the final commands that the system will be able to
interpret in the command language definition module.

The following sections describe in detail each one of the proposed modules. First, it
is described the PHS (Pre-Processing and Hand Segmentation) module, where the
problem of hand detection and tracking is addressed, as well as the problem of hand
segmentation. Secondly, it is described the SGI (Static Gesture Interface) module,
responsible for training static gestures and learn the model for a set of predefined
hand postures, and finally it is described the DGI (Dynamic Gesture Interface)
module, which is responsible for the dynamic gesture training, creating one model
for each one of the predefined gestures to be used.

The Static Gesture Interface module is responsible for hand feature extraction and
system training for static gesture classification. Static gestures, also sometimes
designated hand postures, are considered a static form of hand pose [139, 140].
For the problem of static hand gesture recognition, first it is necessary to extract
meaningful features from the hand image, as explained in section 2.2.1, to train the
system to recognize the required hand postures. Training implies the use of the
extracted features to learn models, with the help of machine learning algorithms, that
can be used in real-time human computer interaction interfaces.

Careful hand features selection and extraction are very important aspects to consider
in computer vision applications for hand gesture recognition and classification for
real-time human-computer interaction. This step is crucial to determine in the future
whether a given hand shape matches a given model, or which of the representative
classes is the most similar. According to Wacs et al. [141] proper feature selection,
and their combination with sophisticated learning and recognition algorithms, can
affect the success or failure of any existing and future work in the field of human
computer interaction using hand gestures.

The resulting model is used during the classification process as shown in Figure 19.
The user’s hand is detected and an instance feature vector is extracted. The feature
vector is normalized, by the z-normalization method (equation 23), and the SVM
model is used to predict the instance class as shown in the two examples of Figure 20
and Figure 21, where the command CLOSE, corresponding to a close hand and the
command FIVE, corresponding to an open hand with all the fingers spread, are
correctly classified.

The Dynamic Gesture Interface Module is responsible for hand feature extraction
and model train for each one of the gestures we want the system to learn. Dynamic
gestures are time-varying processes, which show statistical variations, making
HMMs (Hidden Markov Models) a plausible choice for modelling the processes
[146, 147]. In this way, a human gesture can be understood as a HMM where the true
states of the model are hidden in the sense that they cannot be directly observed.
HMMs have been widely used in a successfully way in other areas, like for example
in speech recognition and hand writing recognition systems [75].

As explained before, the design of any gesture recognition system essentially
involves the following three aspects: (1) data acquisition and pre-processing; (2)
data representation or feature extraction and (3) classification or decision-making.
Taking this into account, a possible solution to be used in any human-computer
interaction system is represented in the diagram of Figure 31. The system is generic
enough, and can be easily implemented using the previous described modules. For
that, it uses the learned models for online human gesture recognition and
classification. As can be seen in the diagram, the system has also a gesture sequence
module that is responsible for building a sequence of hand gestures (static and
dynamic), classify the built sequence, and transmit a command to any system
interface that can be used to control a robot/system.

As explained in section 3.1, the proposed system is designed to use only one camera
and is based on a set of assumptions. As it can be seen in the diagram, the system
first detects and tracks the user hand, segments the hand from the video image and
extracts the necessary hand features. The features thus obtained are used to identify
the user gesture, using the previous learned models and loaded during system
initialization. If a static gesture is being identified, the obtained features are first
normalized and the obtained instance vector is then used for classification. On the
other hand, if a dynamic gesture is being classified, the obtained hand path is first
labelled according to the predefined alphabet, giving a discrete vector of labels
which is then translated to the origin and finally used for classification. Each
detected gesture is used as input into a module that builds the command sequence,
i.e. accumulates each received gesture until a predefined sequence defined in the
Command Language is found. The sequence thus obtained is classified into one of a
set of predefined number of commands that can be transmitted to the GSI (generic
system interface) for robot / system control.

This chapter discussed the importance of developing efficient and able to do realtime
gesture recognition applications, in order to achieve human-computer
interaction systems that are more intuitive and user-friendly. We saw that this type of
systems have a wide range of possible applications like virtual reality, robotics and
telepresence, desktop and tablet PC applications, games and sign language
recognition. They also must satisfy a number of requirements in order to be
successfully implemented as robustness, computationally efficiency, error tolerance
and scalability. The need to have easily configurable systems was addressed, in order
to ensure the necessary flexibility and scalability. Also, since the proposed solution is
based on a single camera, a set of assumptions that the system must obey were also
described.

This chapter discussed the importance of developing efficient and able to do realtime
gesture recognition applications, in order to achieve human-computer
interaction systems that are more intuitive and user-friendly. We saw that this type of
systems have a wide range of possible applications like virtual reality, robotics and
telepresence, desktop and tablet PC applications, games and sign language
recognition. They also must satisfy a number of requirements in order to be
successfully implemented as robustness, computationally efficiency, error tolerance
and scalability. The need to have easily configurable systems was addressed, in order
to ensure the necessary flexibility and scalability. Also, since the proposed solution is
based on a single camera, a set of assumptions that the system must obey were also
described.

As explained in section 3.3.1, the features used for posture classification were
obtained from the centroid distance. Algorithm 9 implements the centroid distance
calculation technique. The first step is to extract the hand contour from the hand
blob. For that, the OpenCV function findContours() is used with the contour retrieval
mode set to external contour (CV_RETR_EXTERNAL), and the contour
approximation mode set to store all the contour points
(CV_CHAIN_APPROX_NONE).

In the proposed implementation, the number of histogram bins (nrBins), or the
number of equally spaced points extracted from the hand contour, was set to 32. This
value gives us a good compromise between number of features and recognition
ability with the chosen operator. For all the given contour points, it calculates the
distance from the hand centroid and saves the value in the instance variable. The
vector thus obtained, which represents the centroid distance signature, is returned.

In this phase, as explained in section 3.4.1, the hand path sequence of points must be
labelled according to the minimum distance to a set of predefined centroids, using
the Euclidean distance metric. Algorithm 14 implements this procedure and returns
a discrete feature vector containing the labels that are added to the “trainset” dataset.
The obtained dataset will then be used for learning the HMM model parameters.
The following algorithm implements the add gesture technique, which is called every
time a new user gesture is detected. It receives as parameter a set of points that
represent the hand path, verifies if the size of the sample vector is valid and calls the
toObservation function for gesture labelling.

Algorithm 18 implements the proposed solution for the vision-based hand gesture
recognition system. When the system starts tracking the user, it switches between the
three possible states. A sequence of dynamic and static gestures can be modelled as
possible commands which can then be used in any robot/control system interface.
During user tracking, the current right hand position is retrieved and the system tests
if the left hand is inside the left hand viewport (control viewport), which activates
gesture recognition. If during gesture recognition before a gesture is recognized the
user removes its hand outside the control viewport, the gesture information is
cleared.

If the finite state machine is in a DYNAMIC state then a particular dynamic gesture
is classified whenever the tracked hand has stopped moving, controlled by the
minDistance variable, and the gesture is only considered valid if the distance
travelled by the hand has a certain size, as explained in section 4.4. Whenever a
dynamic gesture is detected, the state machine information is updated with the
gesture number and the corresponding gesture name and the gesture information are
cleared. If on the other hand, the finite state machine is in a STATIC state, the
tracked hand is segmented and features are extracted. The features are used to predict
the hand posture class with the model obtained in section 4.3.2, and if the predicted
class is within the predefined number of classes, the state machine information is
updated with the gesture number and the corresponding gesture name. The PAUSE
state is entered every time a gesture or hand posture is found, and exited after a
predefined period of time or when a command sequence is identified, as can be seen
in the state transition table (Table 3).

One of the first issues faced, was the problem of detecting and tracking the hand.
When it was decided to start using the Kinect it was possible to explore some of its
technologies in order to try to answer that issue. From those experiments it was
implemented an application that gave a user the possibility of remotely control a
robot in terms of direction and speed of movement with a set of simple hand
commands. For these experiments an MSL robot [150] from the Laboratory of
Automation and Robotics at the University of Minho [151] was used, which allowed
testing the effectiveness of the proposed solution and the final implementation and
which gave rise to the final prototype called Vision-based Remote Control Robot
Hand.

In a second stage it was necessary to start identifying hand features, which could be
used with machine learning algorithms in a supervised way for teaching a computer /
robot understand a set of human gestures. During that study, some tests conducted by
other authors were found in order to be able to detect fingertips as possible hand
features. It was decided to test those features in order to verify whether they could be
used robustly in real-time systems for human-robot interaction. From those
experiments came a prototype called Vision-based Hand Robotic Wheel Chair
Control, that allowed a user to drive a robotic base wheelchair, developed at
Laboratory of Automation and Robotics in the University of Minho [152], through a
finite number of finger commands. The experiments were carried out with the aid of
a MSL robot soccer player, since the base used on those robots is equal to the chair
base on which the system was intended to be deployed.

The Vision-based Remote Hand Robot Control is an application that enables a user
to remotely control a robot with a number of simple hand commands committed in
front of a Kinect camera. The Kinect camera is used to gather depth information and
extract the user’s hand in order to use that to calculate control information to transmit
to the robot. The depth image is used to detect and track the hand by the nearest point
approach as explained in the following section (Hand Segmentation). After hand
detection, two planes are defined as minimum and maximum thresholds for hand
segmentation. The extracted hand blob is used to calculate the hand centroid (relative
position), direction of turning, direction of movement, and the linear velocity that are
transmitted to the robot.

Using this minDistance value, two parallel planes in the Z-direction are defined with
values equal to minDistance - margin and minDistance +margin, in order to extract
the hand blob and the hand contour. The constant value margin, added and
subtracted from the minDistance, was defined as 5 which is sufficient to cover all the
depth pixels of interest.

The following algorithm implements the nearest point calculation, giving the depth
image and its size, and returns a point that represents the minimum distance to the
camera and the resulting segmented hand binary blob.

In order to validate the prototype with the proposed method, a MSL (Middle Size
League) soccer robot from Minho team was used to carry out a series of experiments.
A computer connected to a Kinect camera grabs hand movements and communicates
the calculated information (heading, angular velocity and linear velocity) through
Wi-Fi to the robot on-board computer, as it attempts to show the image form Figure
37. The robot motion speed transmitted to the robot is proportional to the vector
length that connects the hand centroid to the camera image centre point (red vector in
Figure 38), and hand orientation gives us robot-heading direction (blue line in Figure
38).

The system main algorithm implementation is shown below. Every frame the depth
image is updated and the hand blob and corresponding contour are extracted. From
the obtained hand blob, the hand centroid is extracted, for direction of movement
computation, and the hand orientation is calculated (Algorithm 20.). The values for
the linear velocity, angular velocity and direction of turning must be updated every
frame and transmitted to the robot.

The main goal of the proposed system consists of giving the user the capability to
control a robotic based wheelchair without the need to touch any physical devices.
With this kind of technology, we expect that people with disabilities can gain a
degree of independence in performing daily life activities, being at the same time an
alternative to some of the already existing solutions.

The Human-Computer Interface (HCI) for the prototype (Figure 45) was
implemented using the C++ language, and the openFrameworks toolkit with the
OpenCV and the Kinect addons, ofxOpenCv and ofxKinect respectively, under
Ubuntu. OpenCV was used for some of the vision-based operations like extracting
the hand blob contour, and the Kinect addon was responsible for the RGB and depth
image acquisition. In the HCI interface image it is possible to see below the
segmented hand, the direction of movement, all the values given for linear velocity,
angular velocity, and direction. We have also the number of fingers found and the
angle obtained as explained in section 5.3.4. The fingertip extraction algorithm used
was a reimplementation of a non-working add-on for the openFrameworks, where all
the modifications were given back to the community.

The main algorithm implementation for the prototype is given in Algorithm 25.
Every frame the depth image is updated and the hand blob and corresponding
contour are extracted. From the obtained hand blob, fingertips and the number of
fingers are calculated and the respective values and commands are set and
transmitted to the robot.

The Referee CommLang Prototype is a real-time vision-based system that is able to
interpret a set of commands defined for the MSL (Middle Size League) referee, and
send them directly to the RefereeBox [157] (referee’s assisting technology), which
transmits the proper commands to the robots. The commands were defined in a new
formal language described in section 5.4.2 and given in Table 4.
With the proposed solution, there is the possibility of eliminating the assistant
referee, thereby allowing a more natural game interface. The application uses a finite
state machine, as the one described in section 4.5, for referee command construction.
As stated before, the system is always in one of three possible states: DYNAMIC,
STATIC and PAUSE. On start-up, the system enters the DYNAMIC state and waits
until a dynamic gesture is correctly classified. When this happens, the system enters
the PAUSE state for a predefined period of time, necessary to model the transitions
between gestures. After that time, the system transitions to the STATIC state and
remains in this state until a valid command sequence, composed of one or more hand
postures, is found. At this point the system returns to the DYNAMIC state, waiting
for a new command sequence.

connection between its various modules was described. Some of the requisites that a
system based on vision for human-robot interaction must satisfy, in order to be
successfully implemented, were listed. It was also mentioned that, to implement such
a system, it is necessary to be able to learn models that can be used in real-time
gesture classification situations.

The aim of the proposed system is to enable the recognition of static or dynamic
gestures or a combination of both. Thus, in order to select a set of hand features that
could meet the requirements of robustness, computationally efficiency, error
tolerance and scalability, a set of experiments were performed with hand features
collected from a set of users who executed the pre-defined gestures in front of a
Kinect camera. The extracted features were used alone or combined, in order to find
which of them behaved better within a pre-established set of parameters. Those
experiments were performed with the help of the RapidMiner Community Edition
[161], in order to select machine learning algorithms that would achieve the best
classification results for the given datasets. RapidMiner is an open-source data
mining solution that enables to explore data and at the same time simplify the
construction of analysis processes and the evaluation of different approaches. It has
more than 400 data-mining operators that can be used and almost arbitrarily
combined. This way, RapidMiner can be seen as integrated development
environment (IDE) for machine learning and data mining, and a valuable tool in this
field.

Experiments were also performed with dynamic gestures features in order to learn
HMM parameters, and build classifiers able to do real-time dynamic gesture
recognition. For the HMM model learning and implementation the Dlib library
[145], a general purpose cross-platform C++ library, was used. This is a library for
developing portable applications dealing within a number of areas, including
machine learning.

The following sections describe in detail each of the experiments and present the
results in order to prove that the objectives of this work (section 1.2) were achieved
in order to validate the approach and the adopted methodology.
The presentation will be carried out in accordance with the various aspects and
developments of the implemented prototypes. With this approach, it is intended to
show an evolution of the choices made and the results achieved during the work
progress

After dataset creation, the obtained files are converted to Excel files in order to be
imported to RapidMiner for algorithm performance testing, parameter optimization
and learner selection. As explained before, the four chosen algorithms (k-NN, Naïve
Bayes, ANN, SVM) were applied to the two datasets, and the experiments were
performed under the assumption of the k-fold cross validation method. The k-fold
cross validation is used to determine how accurately a learning algorithm will be
able to predict data not trained with [68]. In the k-fold cross-validation, the dataset X
is divided randomly into k equal sized parts, Xi : i =1,…, k. The learning algorithm is
then trained k times, using k-1 parts as the training set and the one that stays out as
the validation set. A value of k=10 is normally used, giving a good rule of
approximation, although the best value depends on the used algorithm and the
dataset [66, 77]. As explained by Ian H. Witten et al. [77], “extensive tests on
numerous different datasets, with different learning techniques, have shown that 10
is about the right number of folds to get the best estimate of errors”.
Prior to learning and model application, the data was normalized as explained before.
Finally, a performance test was carried out, based on the number of counts of test
records correctly and incorrectly predicted by the model.
Since classifier settings and parameters used are important aspects to take into
account, for all the algorithms a parameter optimization analysis was carried out.
For the simplest of the algorithms, the k-NN, a value of k=1 (number of neighbours
used) for the two datasets was obtained with an Euclidean distance metric.

The following images show RapidMiner process setup for the k-NN dataset 1
analysis with the corresponding setups. The first image has represented the file
import phase, data normalization and cross-validation configuration, with the number
of validations set to 10, as explained before. For the sampling type there are three
possible options: linear, shuffled and stratified. Linear sampling simply divides the
dataset into partitions without changing the order, i.e. subsets with consecutive
samples are created. The shuffled sampling builds random subsets with the dataset,
i.e., examples are chosen randomly for making subsets. Stratified sampling builds
random subsets and ensures that the class distribution in the subsets is the same as in
the whole dataset, i.e. each subset contains roughly the same proportions of the
number of classes. This parameter has not been changed during the tests, where the
default value was accepted.

The achieved results, permit to conclude that the dataset 1 features resulted better as
possible solutions for hand gesture identification, and that further experiments should
be carried out with them, or even possibly try them separately. From the obtained
results, it can be easily proven that feature selection and data preparation phases are
important ones, especially with low-resolution images, which is the case of depth
images captured with the Kinect camera.

For that, seven datasets with different features extracted from the segmented hand
were used. The hand features used in the current experiment, and described in
section 2.2, were: the Radial Signature (RS), the Radial Signature Fourier
Descriptors (RSFD), the Centroid Distance (CD), the Centroid Distance Fourier
descriptors (CDFD), the Histogram of Gradients (HoG), the Shi-Tomasi Corner
Detector and the Uniform Local Binary Patterns (ULBP).

For the problem at hand, two types of images obtained with the Kinect camera were
used during the feature extraction phase. The first one, the hand grey scale image
was used with the HoG operator (Figure 71), the LBP (local binary pattern) operator
(Figure 72) and the Shi-Tomasi corner detector (Figure 73). The second one, the
binary hand blob, was used in the Radial Signature (Figure 74), the Radial Signature
Fourier Descriptors (Figure 75), the Centroid Distance Signature (Figure 76) and the
Centroid Distance Signature Fourier Descriptors (Figure 77).

For all the operators, as explained in section 3.2, the hand is detected and tracked as
shown on each of the images, where the hand being tracked is surrounded by a white
square labelled with the hand world position. For the ones that use the grey image,
like the HoG operator, described in section 2.2.3, it can be seen in Figure 71 both
extracted hand images below the RGB image and to the right the respective
orientation histogram image. For the Local Binary Pattern operator, described in
section 2.2.4, it can be seen in Figure 72 the obtained LBP image below the depth
camera image.

It has been found that the radial signature and the centroid distance were the best
shape descriptors tested in this experiment in terms of robustness and computation
complexity. The simplicity of these results follows the Occam’s razor principle
which states that “simpler explanations are more plausible and any unnecessary
complexity should be shaved off”.

Better results were expected from the Fourier descriptors, after having analysed
related work on the area with this type of features. In this case, even with the
algorithm that achieved the best results, the k-NN with k=1, the results fell far short
from the expected.

For the case of the Local Binary Patterns although the obtained accuracy was 89,3%
with a SVM, some more experiences should be done in the future in order to try to
achieve better results. Some authors used combinations of LBP features, like
geometric moments used by Marek Hrúz et al. [51] with a combined accuracy of
99,7% for signer dependent tests but with only 57,4% for signer independent tests.
Others, like Jinbing Gao et al. [53], used a combination of LBP features with HoG
features to train a SVM classifier and were able to achieve an accuracy of 95,2% in
real-time situations.

The first part of the experiment, as explained in the previous section, implied the
analysis of the obtained features in order to find the best learner for the given data,
and the best parameters through a parameter optimization process.
In order to find the best learner for the data under analysis, a process was built in
Rapid Miner that iterates through the following three possible learning algorithms:
the SVM, the ANN and the k-NN, and select the best result. The following images
show the Rapid Miner configurations for the process setup, i.e., from the main screen
setup with a parameter optimization definition, through the specification of the three
learning algorithms to test, in the final one. By opening the “Optimize Parameters
Grid” object by double-clicking it, we get to the Validation setup (Figure 96). Here,
the parameters and type of cross-validation to be used are defined, which in the
current experiment were performed with a 10-fold cross validation with a stratified
sampling type. The same way, by opening the “Validation” object, goes next to the
Training and Testing setup (Figure 97), where is selected in the training phase a
Select Sub-Process object for learner selection and in the testing phase the Apply
Model object and the Performance object that will give the accuracy for each one of
the chosen algorithms. The final setup window (Figure 98) is opened by selecting the
“Select Sub-Process” object. This is where the learners to test are defined in order to
obtain for the given data, the best one.

This chapter provides a summary of the datasets used in the experiments in this thesis.
There are three sets of data: tunnel datasets, wide-baseline matching datasets and change
detection datasets. In Section 3.1, the methods used to acquire datasets from tunnels
and the details of the trial sites are described, together with a discussion of the practical
issues. Section 3.2 describes the datasets that are used to evaluate the proposed feature
matching algorithms in Chapter 6. In Section 3.3, the beam datasets, which are used to
evaluate the proposed change detection system in Chapter 7, are described.

The datasets are obtained from four sites, referred to as Bond Street, Aldwych, Mustek
and Sagrada Familia. The datasets are named after the associated metro stations, and
are obtained manually using a standard digital camera. Specialised equipment could have
been used to speed up the acquisition time, such as a robotic system (Ahrary et al., 2007),
but the aim of this project is to develop an inspection system that can readily be employed
by inspectors without the need for any specialised equipment. The attractiveness of the
proposed system is that it requires minimal equipment, which can easily be obtained from
any camera shop.

There are three main pieces of equipment: a standard digital camera, a tripod and a
portable spotlight. Various camera models are used for the different trial sites, as summarised
in Table 3.1. The tripod can rotate 360# in any direction such that the full
coverage of a tunnel ring can be obtained. The portable light is a diffuse light, designed
to ensure that the lighting is uniform on the tunnel linings. An external flash may be
used, although it is not used in the datasets presented here due to the problems related
to battery power, as the flash can drain the battery quickly.

As a rule of thumb, there should be some degree of overlapping for every three images;
therefore, an overlap of more than 50% for every two images was chosen, as shown in
Figures 3.1(a) and (b). The procedure is as follows. For each tunnel ring, a camera
mounted on a tripod is rotated to cover approximately 270#, as shown in Figure 3.1(a),
and the overlap between the images should be at least 50%. The camera is then moved
along the tunnel by approximately half of the width of the ring, as shown in Figures
3.1(d) and (e). The amount of distance moved along the tunnel is flexible, depending on
the width of the tunnel lining. In some datasets, extra sets of images are obtained, as
shown in Figures 3.1(b) and (c), as redundancy datasets. The camera is rotated to cover
the entire ring of a tunnel lining, but the rail tracks at the bottom section of the ring are
omitted to allow a more accurate 3D model of the tunnel to be reconstructed.

For the image datasets, each image is taken using a self-timer function (e.g. 3-10s can
be used) that releases the camera shutter to avoid blurring. The F-stop and shutter speed
were set to the P mode when the flash was not used, and the Auto mode was employed
when the flash was used. These modes are standard in modern digital cameras. The P
mode lets the camera calculate both the shutter speed and aperture to obtain automatic
image exposure, whereas the Auto mode allows the camera to alter all of the settings (e.g.
flash, exposure compensation, etc.). An external light source is used when the flash is
turned off (i.e. in the P mode). For image datasets, a camera is locked into position on
a tripod when the images are taken to avoid vibration, which can cause image blurring.

Mustek Station is a station on Prague Metro, Czech Republic. There are a total of
three metro lines, as shown in Figure 3.5, which connect 57 stations across an area of
nearly 60km. In August 2002, the metro suffered disastrous flooding that struck parts
of Bohemia and other areas of Central Europe. Nineteen stations were flooded, causing
the partial collapse of Prague’s transport system. The stations affected by the floods
are shown in blue in Figure 3.5 (Jakoubek, 2007). Because of the flooding, Mustek has
been subjected to extensive monitoring and inspection since then. The inspections are
especially concerned with the ingress of surrounding water into the linings, in which many
water patches are visible on the tunnel surfaces. The linings at this trial site are made of
concrete and so prone to water ingress when cracks occur. 3D scanned data were obtained
at this site, which are then compared with a 3D model obtained from the reconstruction
algorithm, as discussed in Chapter 4. The images are collected at Ring no. 705-710
(approximately 5 meters), and Ring no. 740-750 (approximately 10 meters).

Apart from the data collected from Aldwych Station, all of the trial sites required the images
to be collected during engineering hours in the underground, which are from the time
after the last train stops at the terminal station and before the first train operates, which
is approximately 00.30-4.30a.m. However, the actual working hours are approximately
3 hours maximum, since time must be spent conducting safety checks before access to
the tunnels is allowed and also for preparation before the first train operates. This time
constraint meant that only a small section of tunnel images could be collected per session.
With the demand for train services to operate for even longer hours, this inspection time
could become even shorter. An image acquisition system should be able to collect images
as fast as possible.

In this chapter, an overview of the proposed inspection system is described and the
results from the preliminary experiments are presented and discussed. The system is divided
into three separate modules: registration, reconstruction and recognition. In section
4.1, an outline of the proposed system is explained. Section 4.2 provides the background
of a registration system, and the results of the preliminary experiments from state-of-theart
registration software. Section 4.3 presents the background to a reconstruction system
and shows the results from the experiments. The final section discusses the results and
the subsequent research focus based on the obtained experimental results.

Figure 4.1 presents an outline of the proposed system. The first component is image
acquisition (see Chapter 3, Section 3.1). Following the image acquisition step, the system
can be subdivided into three modules as follows.

One application of registration is image mosaicing. The aim is
to stitch images together to provide a wider view of a scene that a single image cannot
capture. The registration module in the proposed system is composed of a number of
components, including Structure From Motion (SFM), surface estimation with a support
vector machine, and warping and mosaicing, the details of which are explained later in
Chapter 5. However, in this chapter, commercial mosaicing software packages are used to
process the tunnel datasets (see Chapter 3) in order to explore their pros and cons, and
allow us to develop further improvements for the registration process in Chapter 5.

Image registration is the process of transforming image sets from varying and unknown
co-ordinate systems into a single system. A well-known application is panoramic stitching,
which is a popular technology embedded in many modern digital cameras. It allows users
to produce a wide-angle panoramic mosaic of a scene from a number of images that are
overlapped. Image registration is also applied in other areas of study, such as medical
imaging and remote sensing (Zitová and Flusser, 2003). To create a stitched image, image
alignment and image stitching algorithms are applied. The former algorithm recovers
the correspondence relationships among images with varying degrees of overlap. Image
stitching algorithms take the alignment estimates from registration algorithms and blend
them to create a seamless mosaic, in which visible seams usually arise from blurring,
ghosting caused by parallax and scene movement, and varying image exposures (Szeliski,
2006). The pipeline of most registration algorithms is shown in Algorithm 4.1 and Figure
4.2.

Before images can be registered and aligned, the mathematical relationship that maps
the pixel coordinates from one image to another must be chosen. A variety of parametric
models exist for this purpose, from simple 2D transformation to planar perspective models,
3D camera rotations, lens distortions and mapping to non-planar (e.g. cylindrical
coordinates). Two models, which are used in the experiments in this study, are explained.

A feature has an image pattern which differs from its immediate
neighbourhood; this allows the matching of similar features in other images. For example,
a corner corresponds to a point in the 2D image with high curvature, and can be
found at various types of junction. A blob is a region in an image that differs in terms
of properties, like brightness or colour, compared to its neighbourhood. Different types
of feature have different properties, and choosing which features to use depends on the
application. Some feature detection algorithms produce features that have a high degree
of invariance, but they may not be computationally efficient. Currently, choosing the type
of features for applications still remains an active area of research (see Tuytelaars and
Mikolajczyk (2008) for further discussions). Once features are detected, feature descriptors
are applied to create descriptor vectors to represent the appearances of the features.
The feature descriptors are matched by feature matching algorithms to establish putative
correspondences, which are then filtered by robust matching algorithms. The reader is
referred to Chapter 5 for more details on all of the steps involved in finding matches
between images.

Once global alignment is performed, local adjustments, such as parallax removal, often
need to be applied to reduce blurring between images due to local mis-registration causing
blur and ghosting in a stitched image (Szeliski, 2006). Local mis-registration is caused
by a variety of factors, including unmodelled radial distortions and 3D parallax due to
a failure to rotate the camera around its optical centre. The approaches to alleviating
these problems differ. Radial distortion can be removed during a pre-processing stage.
The parallax problem may be alleviated by estimating a full 3D structure of a scene, or
by averaging the intensities from all associated pixels (Shum and Szeliski, 2000).
If an unordered set of images is given, panorama recognition can be performed to
cluster similar images to speed up the stitching algorithms.

This stage aims to determine how to represent the final image. For a small number of
images, a simple approach is to warp all of the images into the coordinate system of a
chosen reference image. However, for a large number of images, this approach results in a
severely distorted panorama, especially near the border of the image, where the pixels are
excessively stretched. The alternative approach is to warp images using a cylindrical or
spherical coordinate projection, as explained in Section 4.2.1.1. The choice of the surface
is application-dependent and involves a trade-off between keeping the local appearance
undistorted and providing a uniform sampling of the environment. Once the surface is
selected, other issues for consideration include, view selection (selecting which part of the
scene will be centred), coordinate transformation (computing the mappings between the
input and output pixel coordinates), and sampling (if the panorama has a lower resolution
than the input images, pre-filtering can prevent aliasing) (Szeliski, 2010).

In this experiment, images from the Prague datasets (see Chapter 3) are input into commercial
panorama software called Microsoft Image Composite Editor (ICE) (Microsoft,
2011a). Figure 4.4(a) shows the result of a mosaic using a planar motion model. The
mosaic is constructed from 79 out of 89 images. Some images are not used by the software,
which may be removed during the global registration stage. It can be seen that there are
many areas in the image with misalignment, where the mosaic appears to diverge more
towards the edge, as labelled in the figure.

At the time of writing, the prominent state-of-the-art commercial image stitching software
available are Autopano (Kolor, 2011) and ICE. The experiments are only conducted on
ICE, since it is freely available. In principle, the two software packages are very similar.
Currently, both can operate based on the cylindrical and spherical projection models, and
the planar projective model. They are automatic and also equipped with manual tools to
allow the making of further refinements to a panorama. The automatic process is mainly
made possible by a better feature matching algorithm, such as SIFT keypoint matching.
This is the opposite of traditional image stitching software packages, in which the control
points must be manually specified by the users. The global registration in these programs,
as explained in Section 4.2.1.3, also helps to improve the quality of a mosaic by minimising
registration errors. Additionally, these programs are good at compositing images to create
a seamless mosaic by robust blending schemes.

However, one of the limitations in the software is the motion model. Images are related
by a planar projective model. Therefore, the mosaic is best when the scene of interest
is planar. However, this model is extended to mosaic a general scene by estimating the
homographies using only camera rotations, as shown in Section 4.2.1.1. This restricts the
movement of the camera to simply rotating at a fixed point. The result of this camera
movement is that one panoramic strip is created from one sweep of the camera movement.
Stitching multiple panoramic strips results in distortion in the final stitched image. The
images of tunnels from the datasets are obtained by multiple sweeps of the camera as it is
moved along the tunnels. The poor quality of the resulting mosaics is observed in Figure
4.4(a) and (b), because the software tries to stitch multiple strips together.

Special equipment can be used to obtain images with restricted camera movements
to allow the stitching of multiple panoramic strips. An example of this is Panogear, as
shown in Figure 4.5(a), which is a motorised head on a tripod that allows a camera to
rotate in a full 360#. The head allows multiple sweeps of images to be taken to create a
set of images, as shown in Figure 4.5(b). Because the camera centre is fixed at one point,
all images can be related by homographies and a larger panorama can be created. This
type of mosaicing is applied in Jahanshahi et al. (2011). However, this method does not
allow image stitching when the centre of the camera has been translated to a different
location, as in the datasets in this study, because the images can no longer be related by
homographies.

Mosaicing by a cylindrical projection model for the tunnel datasets also poses some
limitations because the model relies on a camera rotating at a fixed point and requires the
camera to be levelled in order to create good results. This model cannot be applied to a
set of images obtained from a camera with translations. These restrictions do not apply
to the tunnel datasets and, therefore, a poor quality mosaic in Figure 4.4(b) is produced.
With the advancement of reconstruction systems, such as Structure from Motion algorithms,
camera parameters and a 3D model can be recovered from photographs effectively.

The SFMsystem offers the possibility of creating a mosaic image that allows unconstrained
camera movement. This is similar to the work of Agarwala et al. (2006), where a streetview
mosaic of architectural buildings is created using an SFM system. The background
and preliminary experiments of an SFM system are explained in the following sections.

A reconstruction system aims to recover the 3D model of a scene from images or videos.
The simplest reconstruction systems return a set of sparse point clouds. More sophisticated
reconstruction systems are able to produce a watertight 3D surface model. Examples
of applications of reconstruction systems are the reconstruction of archaeological
sites (Moons et al., 2009) and museum collections (Vogiatzis et al., 2005).

If the intrinsic parameters of a camera are known, it is possible to estimate directly
the extrinsic parameters (e.g. rotation and translation) and a metric 3D model. Many
systems prefer this approach because the process of camera calibration is simpler and,
also, the initial estimation of a 3D model is reasonably accurate. Snavely et al. (2006)
create an interactive system that allows the browsing and exploring of large unstructured
collections of photographs using a novel 3D interface. The system consists of an imagebased
modelling front-end that automatically computes the viewpoint of each photograph
and a sparse 3D model from images. The intrinsic parameters for each photograph are
estimated based on initialisation from the Exchangeable Image File Format (EXIF) tags
of photographs. This system relies on a Bundle Adjustment algorithm to refine a 3D
model and camera parameters.

The intrinsic parameters of the cameras are unavailable in this case. Without these parameters,
one can only hope to recover up to a projective 3D model. The projective model
may be sufficient for some applications, such as in a robotic system, but is insufficient
for visualisation purposes. Nevertheless, it is possible to upgrade a projective model to
a metric one by a series of transformations. Hartley and Zisserman (2000) show how to
upgrade from a projective model to an affine model and then to a metric model. The
most popular technique for removing projective ambiguity is by self-calibration. Pollefeys
et al. (2008) present a self-calibration system using linear self-calibration. Moons et al.
(2009) present a system called ARC3D, which reconstructs a sparse 3D model from images
through a web service. Robertson (2004) presents a method for camera calibration
using vanishing points. This method requires a moderate amount of user interaction to
specify the geometrical primitives, such as lines, points and planes, in the scene.

Given a minimum of two camera views and camera parameters, 3D coordinates can be
recovered using the epipolar geometry and triangulation method. The following section
explains the methods used to solve the structure from motion problem for an arbitrary
number of views. The final stage in the structure from motion system is bundle adjustment,
which is used iteratively to refine the structure and motion parameters via the
minimisation of an appropriate cost function. However, bundle adjustment depends critically
on a suitable initialisation; otherwise, the algorithm may converge to a local rather
than global minimum of the cost function. The initialisation schemes to extend from
two-view geometry to multi-view geometry are grouped into sequential, factorisation and
constraint-based reconstruction algorithms (Robertson, 2004). The former is explained
here and the reader is referred to Robertson (2004) for a review of the other two algorithms.

Bundler, which is a non-commercial state-of-the-art SFM software package developed by
Snavely et al. (2006), was applied to the Prague dataset to obtain a 3D model. The
software successfully reconstructed the model with the statistics shown in Table 4.1. All
173 images are registered in the model. Note that the computational time in the table
does not include the feature matching and preparation stages. Figure 4.8(a) shows the
result of a 3D model from the Prague dataset and Figure 4.8(b) shows a close-up view of
the camera poses. It can be seen that most of the 3D points as well as the camera poses
are reasonably accurate. The final mean reprojection error of this model is 0.4799 pixels,
which is low and corresponds well to the observed figures. However, another view of the
model, as shown in Figure 4.9, reveals that many points lie outside the geometry of the
model, due to either incorrect tracks or incorrectly recovered camera projection matrices.
The experiment is performed on an Intel Core i3 3.06GHz with 4GB of memory.

SFM systems offer the possibility of creating a mosaic with unrestricted camera movement.
The state-of-the-art system, Bundler, can create a 3D model from photographs
successfully. However, inaccuracy can be observed in the reconstructed 3D point cloud in
the Prague dataset, which arises due to incorrect tracks or bad camera projection matrices.
This conclusion is based on that of Snavely (2008), who states that Bundler can fail
into 4 distinct modes (as will be explained in Chapter 6).

In this chapter, an outline of the proposed system is introduced. The system consists
of three main modules: a reconstruction system, a registration system and a recognition
system. The backgrounds of the reconstruction and registration systems are given in this
chapter together with some preliminary experiments. The experiments are conducted
to discover the limitations of the state-of-the-art systems, including the Microsoft Image
Composite Editor (ICE) for the registration system, and Bundler and PMVS for the
reconstruction systems.

From the experiments using the ICE software, it is found that the software capability
is limited by the motion models. These models require restricted camera motion, which
do not apply in most of the datasets in this thesis. One way to deal with free camera
movements for mosaicing is to recover a 3D model and camera poses using an SFM system.
Therefore, image mosaicing for the tunnel datasets is studied and improved by using the
SFM system, as explained in Chapter 5.

The experiments on the reconstruction system show that, currently, the accuracy of
a reconstructed 3D model is not comparable to the LiDAR data. The reconstruction
system may be preferred due to its lower cost. However, the experiments show that some
incorrect 3D points can be observed in SFM systems due to incorrect tracks. Therefore,
Chapter 6 focuses on the development of a novel feature matching algorithm, which will
ultimately improve the accuracy of the tracks for the SFM system.

Panoramic images provide wide-angle visualisation of a scene, which cannot be achieved
with a single image. The field of view of the human eye spans almost 180-degrees in
the horizontal direction, and it is natural for humans to see wide-angle images of large
scenes. For tunnel inspection, a single image does not give inspectors a spatial sense;
hence, it is difficult to indicate the locations of any observed anomalies based on a single
image. Typically, this problem is solved by a non-intuitive referencing system used in
an inspection report to relate images to their location inside a tunnel. A more elegant
solution is to stitch images together to increase the field of view, which then facilitates the
localisation of the anomalies. There are, however, problems associated with the current
commercial stitching software. Most software can only stitch images in one direction,
which may prove less useful for tunnel inspection because the increase in the field of view
is only marginal. Another problem is the severe perspective distortion caused by the
limitations of the image alignment models employed in the commercial software. Usually,
mosaics from commercial software are distorted and less intuitive for humans to visualise,
as shown in Figure 5.1(top). In the tunnel datasets described in Chapter 3, commercial
stitching software cannot produce a high quality mosaic because the datasets do not
comply with the assumptions made for the software to work.

In this chapter, a system which constructs a mosaic image of the tunnel surface with
little distortion is presented. The system can deal with general camera motion, unlike
the existing mosaicing algorithms that have to constrain camera motion (i.e. a camera
rotating around its optical axis) to achieve a high quality mosaic. The system is based
on a Structure from Motion (SFM) system, which allows images to be stitched in two
directions (i.e. radial and longitudinal), which increases the field of view in a mosaic
significantly. A low level of distortion in a mosaic is achieved by exploiting the properties
of the developable surfaces that are assumed to be the geometry of tunnels. The mosaic
results, as shown in Figure 5.1(bottom), are intuitive for human, as they preserve the
physical attributes, such as line parallelism, collinearity, line straightness and angles.

The system relies on the accurate estimation of tunnel geometry to achieve a high
quality mosaic. As shown in Figure 5.1(middle), the mosaic looks incorrect when an
inaccurately estimated surface proxy is used. In the proposed system, a Support Vector
Machine (SVM) classifier is used in order to estimate the surface proxy automatically.

The visual presentation and quality of a panorama depend largely on the accuracy of
the image alignment step (Chapter 4, Section 4.2.1), which in turn relies on an accurate
motion model. There are many motion models, though they can be broadly grouped into
single-viewpoint and multi-viewpoint. A single-viewpoint panorama is the most common
type in the commercial stitching software. This type of panorama is created with the
assumption that all images share the same centre of projection (Haenselmann et al.,
2009). This can be achieved by rotating a camera around its optical centre (Agarwala
et al., 2006). One model is based on planar projective transformation, in which all images
are aligned using homographies. The final panorama is created by warping other images
based on a homography to a chosen reference image. One drawback of this method is that
the panorama appears to diverge towards the edge and only a small number of images can
be combined without causing severe distortion (Peleg et al., 2000). To avoid this problem,
one can first warp each image to cylindrical or spherical coordinates and then relate each
image using a translational model (Chen and Klette, 1999). This model, however, distorts
a panorama by making all of the straight lines appear curved. This model also requires
a level camera for the cylindrical projection or a level camera with known tilt angles for
the spherical projection. The drawback of the single-viewpoint panorama is that it does
not work well with images with general camera motion.

Carroll and Seitz (2009) create multi-viewpoint panoramas using developable surfaces,
which can be unwrapped onto a plane without distortion. The result is a mosaic without
any perspective distortion. The system estimates a camera pose for each frame and then,
with known camera poses, an inverse projection of each frame can be performed onto a
developable surface. This system, however, requires tracking pixels in the pose estimation
algorithm; hence, the camera path has to be smoothed. Therefore, it may only be suitable
for cameras with forward motion, as shown in Figure 5.2. Forward motion is unsuitable
for scenes taken inside tunnels due to the large size of the tunnels, since only a small
portion of the tunnel surface for each panel is visible in the images, as shown in Figure
5.2. This is inadequate for inspection purposes.

Agarwala et al. (2006) develop a system that creates multi-viewpoint panoramas from
digital images taken from a standard SLR or a fish-eye lens camera. The system uses
Structure From Motion, which explicitly recovers a sparse 3D point cloud of a scene and
camera poses. The texture from each camera is projected onto a dominant plane, which
corresponds to the facade of the buildings along the streets, to create the final panoramas.
In this work, a dominant plane is chosen automatically by Principal Component Analysis
(PCA). PCA is able to find a direction with the largest variance of data. In this work,
PCA is applied to a 3D point cloud and the direction of the largest variance in the point
cloud is used to form the parameters for a plane. The PCA method works when a street
scene is not curved. If it is slightly curved, a dominant plane is selected manually. For
a scene with a large curvature, such as a tunnel, a geometry proxy, such as a cylindrical
surface, can be used as a dominant plane, as demonstrated later in this chapter. In
Agarwala et al. (2006)’s work, when a distance between a scene and a camera is large,
only a slight distortion in the mosaic is observed for objects that do not lie on a dominant
plane, such as cars. Therefore, standard stitching and blending algorithms are sufficient
to cope with these objects. Figure 5.3 shows an example, in which a panorama appears
as an orthographic projection perpendicular to the scene as a person walking along the
street would see it in the real world. The author claims that the algorithm should work
for long and tall scenes, although only the panoramas of the long scenes are demonstrated
in the paper.

The method developed in this study is similar to Agarwala et al. (2006), which relies on
an SFM system to create multi-viewpoint panoramas. The SFM system allows stitching
for a general camera motion, which eases the process of image acquisition, and also enables
images to be stitched in both the radial and longitudinal directions for tunnels. Since
the tunnel surface is a developable surface, the panoramas can be distortion-free. The
main challenge of the current work, also faced by Agarwala et al. (2006), is how to
extract a geometry proxy for mosaicing. Agarwala et al. (2006) identify a dominant plane
automatically by the use of PCA. In this thesis, the automatic extraction of the tunnel
proxy is achieved by Support Vector Machine classification, as explained in Section 5.3,
and the manual method is used when the automatic method fails, as explained in Section
5.2.1.

Sometimes, SVM may fail to classify keypoints and subsequently lead to incorrect estimation
of a tunnel surface geometry. User input is used to obtain the initialisation of the
surface estimation in Equation 5.2. A user is required to specify an initial estimate of the
cylinder parameters including a radius r0, a directional vector a0 and a cylinder centre x0,
and the algorithm then optimises over the initial estimates. A summary of the algorithm
for the manual method is shown in Algorithm 5.1.

The 3D coordinates reconstructed from tunnel images are composed of two regions: points
lying on the tunnel panels (i.e. surface points), and points not lying on the panels (i.e.
off-surface points). Figure 5.8(left) shows an example of a typical image in which the
points in red are surface points and the points in black are off-surface points. The image
patches of the keypoints from these regions have a distinctive appearance, as shown in
Figure 5.4. In this figure, the patches of the non-surface points or class 1 contain apparent
edges, unlike those of class 2. Hence, these two classes can be separated based on the
distinctive appearance of image patches, and the 3D point cloud can be classified into the
surface region and the non-surface region, based on their 2D associated keypoints. The
classified point cloud is then input into the algorithm in Section 5.2.1 to estimate the
geometry of a tunnel surface.

An experiment is carried out using the Aldwych dataset (see Chapter 3). There are 23
images in this dataset, 14 of which are labelled to separate the non-surface region from the
surface region, as shown in Figure 5.8(a). Eight labelled images are used for training an
SVM classifier, in which 1369 SIFT keypoints are collected as training samples, and the
remaining 6 labelled images with 635 keypoints are used as validation samples. Samples
of the keypoints are shown in Figure 5.8(b).

Figure 5.10(a) and (b) show the sparse 3D reconstruction of a tunnel with and without
applying the Bundle adjustment (BA) algorithm, respectively. The tunnel linings are
clearly seen after the BA algorithm is applied. The convergence graph from the BA
algorithm as seen in Figure 5.9(b) quantitatively shows a significant improvement in
global registration as the cost function converges.

Figure 5.10(c) shows the result of the 3D reconstructed points that are classified by
SVM as marked in red for the surface point, and the non-surface points are marked
in black. From Figure 5.10(d), the estimated surface, shown in yellow, is without the
use of SVM (i.e. all points are used for the estimation), whereas the blue surface is
estimated using the SVM classifier. From Figure 5.1, in the middle figure, the mosaic
is created from the yellow surface (i.e. the surface without SVM), whereas the bottom
figure is constructed from the blue surface (i.e. an accurate surface). It can be seen
that the quality of the final mosaics depends largely on the accuracy of the estimated
surfaces. From Figure 5.1(middle), the mosaic results in curvature, while the mosaic from
the bottom figure preserves all physical geometries, such as line straightness, parallelism
and a 90 degree angle between horizontal and vertical lines. The curvature is caused by the
skewness of the estimated surface, which is induced by the non-surface points. In contrast
to the mosaic produced from the commercial software shown in Figure 5.1(top), which
exhibits strong perspective distortion, the mosaic generated from this study is almost
distortion free. This improvement in the mosaic quality is desirable and suitable for a
tunnel inspection report.

From Sections 5.2 and 5.3, a surface proxy is estimated to fit onto a 3D reconstructed
point cloud. This proxy is a developable surface whose properties allow the surface to be
flattened onto a plane without distortion, such as stretching or compressing. These special
properties enable a mosaic from tunnel images to be created with little or no distortion.
In this section, the process of obtaining the final mosaic image is explained. Section
5.4.1 explains how each image is warped using the estimated surface from Section 5.2. The
warped images can then be stitched together using standard panoramic stitching software,
as explained in Section 5.4.2. The results and discussion are presented in Sections 5.4.3
and 5.4.4.

In the final stage of mosaicing, it is necessary to decide how to produce the final stitched
image. This involves different processes, including selecting a compositing surface, selecting
pixel contributions to the final composite, and blending these pixels to minimise any
visible seams, blur and ghosting.

Most commercial stitching software packages already contain the above algorithms for
producing a final composite. However, these packages only work well with images related
by planar projective transformation, and do not work well with the images as shown in
Figure 5.12(top). However, in the proposed system, the input images are transformed
using actual camera calibrations and a real surface, so they can be warped accurately.
Once warped, the images can be modelled using a translational model and then stitched
using a standard stitching algorithm. The warped images are mosaiced using ICE to
obtain the final mosaics from the experiments

Figure 5.12 shows an example of the result from the Bond Street dataset. The top figure
shows the results from ICE, in which a strong curvature can be observed. Quantifying distortion
is a relatively new topic for a multi-viewpoint image, as discussed in Swaminathan
et al. (2003), who introduced methods for evaluating image distortion. The degree of distortion
is, therefore, not quantified in this research and only a qualitative evaluation is
performed. Visually, the result from the proposed system does not contain perspective
distortion, and a mosaic from the proposed system preserves all of the physical entities,
such as parallelism, line straightness and a 90# angle between vertical and horizontal lines.

Figure 5.13 shows another example from the Bond Street dataset. This result demonstrates
that the proposed system allows images to be stitched in the longitudinal direction.
This means that the mosaic can be created to cover a much larger section of a tunnel. The
ability to stitch in the longitudinal direction is impossible in standard stitching software.
Misalignment can be observed near the rail tracks in this result. This is because the rail
track lies far away from the true geometry of the tunnel, which violates the assumption
made in the proposed system. The proposed system works well for the areas that are
close to the tunnel geometry. The further the scene is away from the surface, the more
violated the assumption becomes. As a result, greater inaccuracy can be observed in the
places that lie further away from the tunnel surface.

The proposed system relies on the accuracy of an SFM system, especially for the estimations
of the camera poses. For each image, an estimated camera pose and an estimated
tunnel proxy are used to compute the warping of the input images. Hence, warping inaccuracy
can arise from the incorrect estimation of the camera poses. The SFM system
used in the proposed system is sequential, and as discussed by Snavely (2008) and Nister
et al. (2004), error accumulation and propagation are common in the sequential methods.
If a camera pose is inaccurate, it will then cause an incorrect estimation of a 3D point
cloud. Such inaccuracy in the camera pose will be propagated to other camera poses, and
the reconstruction system may fail altogether. The error in one camera will propagate
through the sequence. This problem may be alleviated by loop-closing, which is used in
Borrmann et al. (2008) to bound the registration error of the camera poses. However,
loop-closing is not implemented during image acquisition for the tunnel datasets in this
thesis, and reducing errors in the reconstructions is impossible using this solution. Another
solution for improving an SFM system is by employing a better feature matching
algorithm. Better algorithms can provide more accurate tracks and, hence, errors caused
by incorrect matches can be alleviated, which will improve the overall accuracy of an SFM
system. Novel feature matching algorithms are proposed in this study, as explained in
Chapter 6.

The results shown in this chapter are derived from the tunnel datasets, whose geometries
are cylindrical. For other non-cylindrical datasets, such as the Barcelona dataset,
a different geometry is required for the surface estimation process. This can be done
by modifying Equation 5.1, such that other types of geometries can be estimated. For
example, the Barcelona datasets, which are semi-circular in shape, can be estimated as a
combination of planes and cylinders. These datasets require multiple types of geometry,
which is a non-trivial problem, and further research is required. The work by Schnabel
et al. (2007) is a good example of an algorithm that can be used to extract geometries
from a point cloud automatically.

For the datasets with a cylindrical shape, the geometry is assumed to be a perfect cylinder.
This assumption is not entirely correct as real tunnels are not perfectly cylindrical
in shape due to the deformation of tunnels. However, the deformation can be assumed
to be negligible in relation to the tunnel sizes in the mosaicing system presented in this
thesis. The amount of deformation is insufficient to cause the effect of 3D parallax in mosaicing.
Parallax can occur if the deformation is of the same magnitude as the distance
from the tunnel wall to other objects, such as cables. Since the deformation is generally
many times smaller than such a distance, the effect of parallax due to tunnel deformation
can be ignored.

A cylindrical tunnel is assumed to be parameterised using only a uni-directional vector.
This assumption may be true for a small section of a tunnel. However, for a longer section,
a tunnel will be bent, and the assumption is no longer valid. Subsequently, it is necessary
to obtain an actual directional vector for an entire length of a tunnel, which can be done
by concatenating the shorter directional vectors, but this lies beyond the scope of the
study.

Some degree of variation in lighting can be observed in the mosaic images presented in
this study. The input images are obtained using a single directional external light source
whose intensity is strongest at the point on which the light focuses. Obtaining images
with the same degree of illumination is somewhat difficult in the tunnel environment. One
solution to this problem is to use multiple light sources to illuminate an entire section of
tunnel surface.

The mosaicing system presented in this chapter is validated for the tunnel datasets
with a cylindrical shape. Cylindrical tunnels are commonly found in man-made tunnels
although additional datasets are required to verify the system with other types of tunnel.
The types of tunnel surfaces validated in this chapter are cast iron and concrete. These
types of surfaces are commonly found, although additional datasets for other types of
surface would provide a more complete validation. Note that more complete datasets for
all types of tunnel shape and surface were not obtained since the datasets can only be
obtained from the sites provided by participating research partners.

Another source of inaccuracy may be the commercial software used in the final stage of
the proposed system. The image alignments computed by the software may be inaccurate
due to the incorrect matching of images. Nevertheless, since the algorithms in the software
are unknown, it is difficult to determine which stages in the software cause the inaccuracy.

Feature matching is one of the most important and fundamental problems in computer
vision. Its purpose is to match landmark features (e.g. corners, blobs and edges) in one
image with the same features viewed in other images. It is usually the first module of
many systems and it can determine the success or failure of the system. It is commonly
applied in many areas of computer vision, including automatic 3D reconstruction, object
recognition and registration. Considerable progress has been made towards improving the
algorithms of feature matching so they can achieve desirable properties, such as scalability,
accuracy, robustness and efficiency. However, the feature matching problem still remains
an active area of research.

As illustrated in Figure 6.1, failure during the feature matching stage can lead to an
unsuccessfully reconstructed model. In this figure, the 3D model of a tunnel, reconstructed
from the Bond Street datasets, fails due to the ambiguity of the generated tracks—tracks
are features that are matched and linked across multiple images and are formed by matching
features in pairs of images. Snavely (2008) concludes that his reconstruction system
(Bundler) can fail in 4 distinct modes: (1) insufficient overlap or texture; (2) ambiguous
and repeating texture; (3) poor initialisation; and (4) cascading errors. An example of
failure due to repeating textures is shown in Figure 6.2, in which Bundler is unable to
distinguish between two similar sections of the building. Inevitably, texture repetition is
common in buildings in the urban environment. An example of cascading error is shown
in Figure 6.3(a). Cascading error refers to the propagation of reconstruction errors due
to inaccurate camera positions or the uncertainty of recovered 3D coordinates due to the
narrow baselines between cameras. A similar error is found in the 3D model obtained
from the tunnel datasets in this thesis, as shown in Figure 6.3(b). Essentially, however,
these different modes of failure are largely due to a failure during the feature matching
stage, which can be solved by improving the feature matching algorithm.

This chapter describes the methods intended to improve the feature matching stage,
which will lead to the improved performance of the overall reconstruction system. This
is achieved by applying a spatial consistency constraint in addition to matching based on
an appearance similarity constraint. The spatial consistency constraint helps to reduce
ambiguity in the matching features. The proposed methods are evaluated using widelyused
datasets by Mikolajczyk and Schmid (2005) as well as the tunnel datasets created
in this thesis (see Chapter 3, Section 3.2). The tunnel datasets are unique because they
have repetitive textures in low brightness, which is common in the tunnel environment,
whereas the Mikolajczyk datasets are obtained in an environment where lighting is not
an issue.

This chapter is focused on the wide-baseline feature matching problem, which is applied
only to images. Feature matching in video data requires a different approach to
the method required for image data. This is because the viewpoint changes between the
video frames are significantly smaller. Image data are chosen in preference to video data
for the following reasons.

Figure 6.4 shows the typical stages involved in a feature matching system. The input
is a pair of images, each of which is processed separately. For each image, the Detector
is first applied to identify local features that are distinctive, such as corners and blobs.
The Descriptor then computes multi-dimensional descriptor vectors to describe the local
appearance of the regions around the features. The descriptor vectors from the image pair
are passed to the Feature Matching algorithm to select putative correspondences. These
correspondences are further processed by a RANdom SAmpling Consensus (RANSAC)
algorithm to verify whether they are geometrically consistent according to a chosen geometrical
transformation. The output from the image matching system is a set of features
that are geometrically consistent. Advances have been made in each of the components
in the feature matching algorithms, as reviewed below.

Lepetit and Fua (2006) formulate keypoint matching as a classification problem. This
approach allows keypoints to be matched in real-time, though a classifier must be trained,
and the computation time required to be spent on training may be high. The algorithm
works by selecting a set of prominent keypoints in an image which are then given class
labels. The patches centred on the keypoints are used as descriptors to train a classifier
and, at run time, the classifier can give a label to the input patches with the likelihood of
the patches belonging to particular classes. The Randomised Forest (RF) classifier is used
in this work as it is very fast to train and classify. The main drawback of this approach
is that, although descriptors may not be required, choosing which keypoints to use for
training the classifiers is somewhat ad hoc.

So far, all of the literature discussed largely involves matching descriptors or patches
solely based on the similarity of the local appearance of features. The descriptors, however,
can fail to account for the global context in an image, which can help to reduce the degree
of ambiguity in the matching task. Mortensen et al. (2005) add a global context descriptor
to a standard SIFT descriptor and this method shows improvements in the matching
result. This approach works especially well in the case of images containing deformable
objects as well as those with many local regions with similar appearances, such as a chessboard
pattern. Similarly, Carneiro and Jepson (2007) and Deng et al. (2006) apply Shape
Context descriptors (Belongie et al., 2002) to create global context descriptors based on
the distribution and spatial arrangement of the neighbourhood keypoints. Torresani et al.
(2008) match keypoints based on the spatial arrangement of the features and formulate
the matching problem as a graph matching problem. This method works particularly well
for matching images with deformable objects.

The SIFT matching algorithm searches for correspondences based on the local appearances
of features alone, whereas the proposed method utilises the structural information
in images. Structural information or spatial neighbourhood considers that matched features
should lie within a close neighbourhood under a rigid global transformation. This
assumption is true in most cases for the tunnel datasets where there is not a significant
amount of rotation and scaling between a pair of images.

The proposed method is similar to Sattler et al. (2010). This work demonstrates
that the matches whose feature locations are not within their neighbourhood should be
discarded, and the method results in speeding up the RANSAC algorithm. In scenes
with repetitive textures, such as the tunnel datasets, many features have a similar local
appearance, and the structural information should be used to help to reduce the degree
of ambiguity in the matches. The filtered correspondence set also improves the accuracy
of the transformation models and the speed of the RANSAC algorithm, as demonstrated
in the experiments.

This algorithm clusters a set of features into different groups such that the features within
the same groups are thought to have similar characteristics to qualify to be in the same
cluster. If features are not in the same cluster, they are considered to have a different
appearance, and to be unmatched. If a feature from one image resides in the same cluster
as a feature in the other image, these features are defined as a matching pair. An example
is shown in Figure 6.9; all of the features shown in blue (i.e. they have the same label) on
the left image are matched with all of those features shown in blue in the right image. It
can be seen that each feature can have multiple potential matches since only one tree is
used for the matching; hence multiple trees are applied in order to improve the accuracy
of the matching, as explained in Section 6.4.2.2.

Spatial relations between features can help with reducing ambiguity to improve matching
accuracy. This is similar to the human visual system. Humans are able to recognise
and distinguish an object by using the spatial relationships between multiple objects in a
scene to help to disambiguate regions accurately (Mortensen et al., 2005). In Mortensen
et al.’s work, the global context descriptors are added to the SIFT descriptors to provide
an additional constraint, using the spatial relationship between features to improve the
matching accuracy over the original SIFT matching algorithm.

The evaluation of the proposed matching algorithm is conducted on two datasets. The
first dataset is provided by Mikolajczyk and Schmid (2005). This dataset is commonly
used in evaluating the performance of feature matching algorithms, as described in Section
3.2.3. The performance of the proposed algorithm is evaluated using a toolbox provided
with the dataset, downloadable from the Visual Geometry Group (2011). An explanation
of the evaluation method found in the toolbox is briefly given in Appendix A.1, and the
reader is referred to Mikolajczyk and Schmid (2005) for full details.

Two matching schemes are considered in the evaluation, the Nearest Neighbour or NN
matching scheme and the Similarity or Sim scheme. For the Sim scheme, a pair of features
is matched if their similarity score exceeds a specified threshold. Therefore, for the Sim
scheme, it is always possible for a feature to have a 1-to-many matching relationship.
For the NN scheme, only the pairs with the highest similarity scores are considered as
matched. Therefore, the NN scheme results in a 1-to-1 matching relationship. A full
explanation of the different types of matching schemes can be found in Appendix A.1.
The second dataset is constructed from the beam datasets, as described in Section 3.3.
Six pairs of images are used for the evaluation, as shown in Table 6.2 and Figure 6.14.
Various viewing angles and focal lengths are covered by these image pairs. Furthermore,
various stages of crack propagation in the beam are also present in these images in order to
evaluate the algorithm performance when an object is non-rigid. The method of evaluation
is identical to that shown in Section 6.3.2.1.

In this experiment, the proposed algorithm is compared against the SIFT matching algorithm
using the beam dataset (Section 6.4.4.1). The images in this dataset are widebaseline
images and contain a deformable object (i.e. the beam). The performances of
the algorithms are evaluated based on visual comparison. In addition, the RANSAChomography
algorithm is applied in order to evaluate quantitatively the performance of
the proposed and SIFT matching algorithms. The RANSAC-homography algorithm iteratively
chooses a model from a subset of input matches to obtain the best transformation
model for a pair of input images, and the set of inliers or correct matches is obtained
based on the chosen transformation model. Note that homography transformation is not
the true transformation model of the images in the beam datasets, because the beam is
deformable and cannot be accurately modelled by a rigid body transformation such as a
homography. However, the homography transformation is a good approximation for the
beam datasets. The RANSAC-homography algorithm is repeated 1000 times to obtain
statistically meaningful results; the mean and standard deviation are plotted as shown in
Figure 6.21. Table 6.2 summarises the image pairs used in this experiment.

Figure 6.21(a) shows the plots of the means of the inlier ratios in each image pair.
The proposed algorithm performs better than SIFT matching for all image pairs. The
trial counts, as shown in Figure 6.21(b), are also significantly improved by the proposed
algorithm, with up to a 50% improvement in pairs 3 and 6, when comparing with the SIFT
matching algorithm. As mentioned earlier, homographies are not a true transformation
of the images in this dataset; hence, only small improvements can be observed in the
inlier ratios. However, based on visual comparison, the proposed algorithm (shown in
Figure 6.22(b)) contains many more accurate matches than the SIFT matching algorithm
(Figure 6.22(a)). The proposed algorithm works better than SIFT matching because the
SC algorithm can cope with some degree of deformation. The SIFT matching algorithm,
on the other hand, can only make matches based on the local appearances of features,
which clearly does not work well in this dataset.

The goal of change detection is to identify the regions of changes between multiple images
of the same scene taken at different times. Figure 7.1 illustrates typical images taken from
a real site. Inspectors visually compare the images to determine whether any anomalies
have arisen between inspections. In tunnel inspection, being aware of any changes occurring
on the tunnel surfaces is useful as it allows an appropriate repair regime to be
devised. For example, crack changes may suggest tunnel deformation, and additional
reinforcement may be required to strengthen the tunnel linings and so prevent further
deformation. Another example, illustrated in Figure 7.1, is the change in staining due to
an ingress of saline through cracks in the tunnel linings. If the change is occurring rapidly,
an urgent repair regime must be undertaken to prevent further ingress and harmful structural
damage but, if the change is slow, the regime may involve merely a cosmetic repair.

Figure 7.3 shows an outline of the proposed change detection system. The system starts
with the pre-processing module. A query image is input into Bundler to register it onto
the existing 3D reference model. Then, the reference images are synthesised such that
they have the same viewpoints as the query image, based on Surface Estimation. The
synthesised reference images and the query image are then processed by a photometric
adjustment algorithm so that any variations due to lighting are removed. These images are
then input to the Change Detection module to produce a binary change mask. Suitable
algorithms in each step are chosen, as discussed below.

The proposed system tackles the problem of change detection by accounting for variations
in viewpoint and illumination changes occurring in real images. Not only can the system
be used to detect changes due to crack growth, it can also be applied to detect changes
due to other anomalies, such as stains due to water ingress. Additionally, the proposed
change detection system accounts for variations in the non-rigid deformation of structural
components. The proposed system attempts to create an algorithm that is invariant to
changes in viewpoint, illumination and non-rigid deformation.

The chapter is organised as follows. Section 7.2 provides a background and reviews the
previous work on change detection. Section 7.3 explains the pre-processing techniques for
rectifying the variations in images caused by different viewpoints and illumination. Section
7.4 provides details about the change detection algorithms applied after pre-processing.
The experiment setups, together with the results, are shown in Section 7.6, and the
conclusions are drawn in Section 7.7.

Change detection is an important preliminary task for visual interpretation. It involves
the identification and detection of change regions of the same scene taken at different
times. Change detection has a large number of applications in various fields of study,
such as video surveillance (Collins et al., 2000), remote sensing (Bruzzone and Prieto,
2002), medical diagnosis and treatment (Bosc et al., 2003), civil engineering (Nagy et al.,
2001, Landis et al., 1999) and underwater sensing (Edgington et al., 2003). The goal of
change detection is to identify a set of pixels that are significantly different between the
last image of the sequence and the previous images, highlighting them for further attention
by outputting a change mask. A typical change mask may result from a combination of
underlying factors, including the appearance or disappearance of objects, the motion of
objects relative to the background, shape changes in the objects, or stationary objects
that undergo changes in brightness or colour. It is important to detect significant changes
while rejecting unimportant ones, such as those induced by camera motion, sensor noise,
illumination variation, non-uniform attenuation, or atmospheric absorption (Radke et al.,
2005).

In general, geometric and radiometric (i.e. intensity) adjustments are applied in the
pre-processing steps to suppress or filter out common types of unimportant changes before
making change decisions. In terms of geometric adjustment, accurate image registration is
a pre-requisite, allowing several images to be automatically aligned, warped and stitched
into a common coordinate frame. Pixel changes due to camera motion should never be
detected as real changes. If the scenes of interest are rigid and the degree of camera
motion is light, image registration can often be performed using spatial transformations,
such as similarity, affine, or projective transformation. However, for wide-baseline images
or those containing deformable objects, a non-global transformation (e.g. optical flow,
tracking, or structure from motion algorithms) may be required. Radiometric adjustments
remove intensity variations in images caused by changes in the strength or position of the
light sources. Various techniques are used in radiometric adjustments, such as intensity
normalisation, homomorphic filtering and illumination modelling (Radke et al., 2005).
After the pre-processing steps, the decision rules are applied to determine if the change is
significant or not. Examples of decision rules are the significance tests and the likelihood
ratio tests, both of which are based on statistical hypothesis testing.

A query image is registered on the current reference 3D coordinate frame using Bundler.
The registration steps are as follows. Firstly, the SIFT features from the query image are
extracted and their descriptor vectors matched to all descriptors in the existing reference
images. The camera pose of the query frame is then estimated using the RANSAC pose
estimation algorithm and then optimised by the least squares method. The camera pose
is then refined via Bundle Adjustment. Figure 7.6 shows an example of a query image
registered to the reference 3D point cloud in the beam dataset. The existing camera poses
are shown in black and the query camera in blue.

Some years ago in 2010, I visited the Biennale for International Light Art that took place
in the context of the European Capital of Culture RUHR 2010. The Biennale was titled
‘open light in private spaces.’ The exhibition project presented works of art by
internationally renowned artists in 60 private spaces belonging to inhabitants of
different cities and towns in the German Ruhr district. Next to impressive light art
installations that caught my eye, the most rememberable piece of art was somewhat
different. I happened to walk into one of the exhibition venues, an opthalmologist´s
surgery, and took a seat in the waiting room. After a while, I was called into the
consulting room. When the door of the room closed behind me I did not see anything at
all; it was totally dark. I waited and expected some form of light to appear, at least
something that my eyes could see, but suddenly a voice began to speak. It started to
state clearly in German, words that could be translated as ‘blood-orange,’ ‘basalt-grey,’
‘curry-yellow,’ ‘tobacco-brown,’ and so on. It was a fascinating experience for me,
because with every different word, the description of a colour with a prefix from
everyday life that specified the respective colour, I was able to imagine exactly what kind
of colour was meant. In most cases, I saw amazingly clear pictures in front of me that
were associated with the colour and the entity that described the colour. This artwork
with the title ‘dunkellampe’ (‘dark lamp’) by the Reykjavík-based artist Haraldur
Jónsson invited me to contemplate what it does mean to see and to recognise. What do
we as a society actually mean if we speak about seeing or the ability to see and to
recognise? The involvement with this question is the basis for understanding what it
really means if the ability to see is to be transferred to computers and Image Processing
Algorithms. In what follows, I first engage with some lessons learned from Blindness
Studies and lead on to what can be termed Visual Culture Studies. Because images play
such a crucial role in our contemporary visual culture, I also deal with a concept—the
Social Studies of Scientific Imaging and Visualisation (SIV)—that analyses scientific
images and their role in society from a sociological perspective. Because the ability to
see, understood in sociological terms, is very much the ability to recognise something
meaningful in a specific entity, the second part of this chapter engages with recognition.

Firstly, I deal with the history of recognition by referring to the history of identification
practices and techniques and secondly, I connect this history with current technologies
of facial and pattern recognition.

An interesting and promising approach to this question is an involvement with
blindness which would seem to be the opposite of seeing at first glance. But, considering
my own experience in the dark room of the opthalmologist´s surgery, I started to
question this contrast. Physiologically, I did not see anything with my eyes, but was still
able to see clearly the colours and things mentioned. Did I draw on some kind of “saved”
images in my mind that I automatically associated with the stated colours in the
opthalmologist´s surgery? Also, blindness studies show that the physiological ability to
see is not necessarily a prerequisite for participating in visual-aesthetic discourse
(Mraczny 2012: 193). Mraczny interviewed blind people to learn about seeing and visual
culture. He found out that practices of seeing that always include some form of visual
meaning, are guided and produced through social institutions in the framework of a
dominant visual culture. Visual culture acts as an ordering device that simultaneously
defines practices of seeing and of blindness. It constitutes standards and instructions
how and what has to be seen at what time (ibid.: 197). Simultaneously, in creating
“normal” and “deviant” visual subjects it constructs blindness as a “disability,” which
also leads to discriminating effects (ibid.: 189). Länger (2002: 8f.) reported that blind
speakers use the same visual language as non-blind people. This use of visual vocabulary
shows the expertise of blind people in their involvement with visual situations and
semantics (Mraczny 2012: 190), meaning that although blind people do not see
physiologically, they see and think in visual categories and participate in the visual
world. That is, amongst other things, due to the communication and interaction with
non-blind members of society, who talk with blind people about optical and visual
phenomena. Insofar, the visual becomes a highly relevant part of blind people’s
everyday lives. Especially in public situations when it comes to encounters with
strangers, visual expertise is an essential condition for “correct” behaviour. For example,
in German speaking countries where there are two different forms of second person
singular pronouns (generally “du” for younger people or contemporaries and a more
polite “Sie” for elders or people with a visibly higher status), the visual inspection of
somebody in order to estimate his or her relevant status, age or sex, is the basis on
which the decision on how to address the respective person correctly is made (ibid.:
195). For most non-blind individuals, seeing is a quasi-natural practice, taken for
granted. It seems there is a continuously ongoing defence and reproduction of a visual
order, because seeing does count as the “natural” and “realistic” way to perceive the
environment (ibid.: 197).

Next to Mitchell’s definition of divine purpose there can be a discussion on whether the
sense of sight is preferable to the other senses, being as it is, a reflection of broader
visual culture. What would visual culture then be seen to be and how did such a
dominant visual culture come about? Is it because sight and speech count as more public
than the more personal senses of touch, taste and smell, or because pictures, graphs,
and diagrams make it easier to show results and to present and create evidence (Burri,
Schubert & Strübing 2011: 4) in areas such as the sciences.

However the answer might be, today images represent social realities and shape the
ways people think, interact (Burri 2012: 46) and see. Images do have different logics,
one of them being that “images allow social actors to perceive visual information
simultaneously” (ibid.: 49), independent from being together in one place. Another
advantage is that images enable things to be seen at a glance, which is often set in
contrast to other forms of perceiving social realities that can only be processed in linear
sequence. Think about reading a text or listening to someone speaking. However, also
images can be read in a nearly sequential way. The eye moves over the image on a
structured path, in a not entirely coincidental way. The eye moves from one point to the
next, though chronology can vary at different times and also differs in individuals
(Breckner 2003: 37).

Nevertheless, images and visual information play a highly important role in the (re-)
production of society. According to Burri, images can be defined in different ways. They
can be seen either as ‘inner images’ or mental imagination or as physical expressions
such as a bodily performance or, in a third perspective as ‘the world as image’ (Burri
2012: 46). Burri herself defines images as artefacts that are both visual and material
objects and can be conceptualised as technical objects. In her work on the sociology of
images, she suggests the concept of ‘visual logic’ to analyse images sociologically (ibid.:
45) from the perspective of social practices. Here, three different visual dimensions of
images play a role: First, the visual value refers to the non-discursive characteristics of
images and can be seen as the surplus value of images that makes images different from
auditory, taste or tactile signs. “A picture is worth more than a thousand words” is a
popular saying that describes the visual value of images quite well. The most important
characteristic of the visual value of images for my understanding of seeing and images is
“that images cannot entirely be transformed into textual or nummerical signs without
losing some of their advantages” (ibid.: 50). In the context of ‘computers and the ability
to see’ one has to ask, whether the view that human perception of a large amount of
information by the means of images happens “at once” (ibid.: 50), is still maintainable,
because what does “at once” actually mean? Does it, in fact, mean “extremely quick?”
Does “extremely quick” mean at such a high speed that for most humans it appears to
happen “at once?” But what actually goes on inside our heads when we see something
like an image? How long does it take from seeing something to processing this
something in our heads? A metaphor used many times for seeing is the photographic
metaphor. It has its origins in the observation that our eyes are, in many respects, like
cameras as both are equipped with lenses.

Collins distinguishes a formal (or pattern recognition) model of seeing from an
enculturational model (Collins 2010:11). The formal model of seeing “involves
recognizing what an object really is by detecting its distinguishing characteristics.” The
enculturational model of seeing stresses that the same object may be seen as many
different things. For example, as Goodwin notes (Goodwin 1994: 606), “an archaeologist
and a farmer see quite different phenomena in the same patch of dirt.” The farmer
would be more interested in the quality of the soil, but the archaeologist in “artifacts
that provide evidence for earlier human activity.” For Goodwin, seeing emerges through
the interplay between a domain of scrutiny (e.g. patch of dirt) and a set of discursive
practices (e.g. highlighting) that are being deployed within a specific activity (e.g. planting
crops). In the article Professional Vision Goodwin investigates seeing “as a socially
historically constituted body of practices through which the objects of knowledge which
animate the discourse of a profession are constructed and shaped (ib.: 606).” He
attached importance especially to three practices of seeing: Firstly, Coding Schemes that
are used to transform materials of the world into objects of knowledge, into categories,
classifications and events that are relevant to the work of a specific profession (ibid.:
608). Secondly, Highlighting, that is, “making specific phenomena in a complex
perceptual field salient by marking them in some fashion.” (ibid.: 606). This can happen
through coloured markers, post-it notes or handwritten annotations, for example. As a
consequence this highlighting activity does not only shape one’s own perception, but
also that of others (ibid.: 610). Thirdly, The Production and Articulation of Material
Representations such as diagrams, maps, graphs and photographs. Here, Goodwin refers
to the central importance of ‘inscriptions’ in the organisation of scientific knowledge (cf.
Latour & Woolgar 1979, 1986). According to Latour and Woolgar inscriptions can be
summarised as all traces, spots, points, histograms, recorded numbers, spectra, peaks
and so on, meaning everything more basic than writing (Latour & Woolgar 1986: 88).
Inscription devices such as figures or diagrams transform pieces of matter into written
documents, which are directly usable by one of the individulas working with these pieces
of matter (ibid.: 51). They are especially useful in ‘rhetorical situations,’ because they are
easy to transport and remain immutable and can be reproduced and altered in size (cf.
Burri 2012: 48).

Goodwin exemplifies this with the so called ‘King Trial,’ in which four white policemen
were charged with beating Rodney King, an African-American motorist, who has been
stopped for speeding in the US in 1992 (ibid.: 606). The incident had been videotaped
and for the prosecutor it was absolutely clear, objective evidence showing uncontrolled
and brutal violence against Rodney King. However, the lawyers defending the policemen
did not treat the tape as evidence that spoke for itself. Rather, they were able to
transform the perception of the tape, in so far as it was evidence, into “a very disciplined
and controlled effort to take Mr. King into custody” (ibid.: 617). With the help of a
coding scheme delivered by experts that showed how police usually work, a ‘perceptual
transformation’ had been accomplished (ibid.). Goodwin concludes that “the
perspectival framework provided by a professional coding scheme constitutes the
objects in the domain of scrutiny that are the focus of attention” (ibid.: 622).

Tomomitsu shows that it is not only discursive practices that shape seeing, but also a
highlighting of the embodied material practices of seeing when analysing practices of
scientific imaging (Tomomitsu 2011: 20), which I would identify as a specific form of
professional vision. She describes three practices of how scientists enact ‘seeing.’ The
first practice is purification that refers to “how scientists contain their objects through
sample preparation in the laboratory.” The second practice is tinkering, describing “the
various ways that instrumentation is adjusted and fiddled with.” Finally, the third
practice is touch that refers to “how objects require physical handling or alteration to see
and make visible” (ibid. 18).

During the production of images, aesthetic criteria are also applied to images and
visualisations (Burri 2012: 50). This insight contradicts the view that images and
especially scientific images are the quintessence of objectivity and that they have
nothing to do with aesthetics. On the contrary, one has to ask, how far are aesthetics
permissible in maintaining a status of objectivity? In the wake of the emergence of
photography, possibilities of compensating for the weaknesses of the human eye
occurred. For example, photography facilitated the permanent and objective securing
and preservation of evidence in police work. Because photography replaced the practice
of local inspection it acquired the label of being the most immediate and direct
experience of truth (Heßler 2006: 26). Currently, we seem to live in a culture in which
image evidence is crucial. The last bastions that struggle against image evidence are
beginning to totter, exemplified by the decision to apply goal-line camera technology in
world championship football matches as announced by the International Federation of
Association Football (FIFA) and the International Football Association Board (IFAB) in
February 2013.

Returning to the discussion of scientific images, Lynch (1990), in his
ethnomethodological study of a laboratory demonstrated how visual representations are
fabricated through processes of mathematisation and the selection of visual elements.
Both of these modifying interventions aim at making the visual object more useful for
the researcher by transforming, neglecting, or boosting visual signs (Burri 2012: 50).
What is seen in an image also depends on how the image is interpreted. Interpretative
practices are shaped by cultural traditions of seeing and by professional skills in reading
images (ibid.: 51). How an image is interpreted is also dependent on the social status of
the respective actors involved in the interpretation processes. At this point, we can
propose the question of which social status, computers, machines or Image Processing
Algorithms have achieved when they are involved in interpretation processes.

Human vision is inevitably historically and culturally specific in all of the presented
conceptions. These conceptions should provide a really interesting challenge to
assumptions about human vision in computer vision projects that emphasise the formal
or pattern recognition model of seeing much more. Burri puts her case for a sociology of
images that must investigate the processes by which image interpretation is
interactively negotiated in social practices. This means that the focus is not only on the
images alone, but on social practice, and contexts of image production, interpretation
and use have to be taken into account (ibid.: 53f.). This focus is similar to what Burri
and Dumit developed in their concept of the Social Studies of Scientific Imaging and
Visualisation (SIV) (Burri & Dumit 2008). The development of SIV has been a
consequence of a general practice turn in social theory (Schatzki, Knorr-Cetina & von
Savigny 2001) and they strongly refer to STS laboratory studies (Lynch, Knorr Cetina,
Latour and Woolgar). However, their focus goes beyond the scientific laboratory and
community. One major question is what happens when scientific images leave academic
territories and extend or travel to other arenas and contexts? They talk about the “social
life of images” that includes the total lifespan of images from production to use (Burri &
Dumit 2008: 299f.). Mitchell also dedicates an individual “inner life” to images. He even
conceptualises images as living organisms due to the possibility of lifetimes of images
ending; images die when they are not used any longer (Mitchell 2005). Burri and Dumit
discuss the social life of images on three thematic clusters that are artificially separated:
production, engagment and deployment of images. It is important to note that because
seeing is so often believing, one main SIV concern is to demonstrate how the making
and using of images come together with seeing and believing in practices of scientific
truthmaking and perceptual convention (Burri & Dumit 2008: 300).

Regarding the production of images Burri and Dumit ask the question “how and by
whom images are constructed by analyzing the practices, methods, technology, actors,
and networks involved in the making of an image” (ibid.: 300). The example of magnetic
resonance imaging (MRI) shows that the production of images is dependent on a series
of decisions concerning the machines, data, parameters, scale, resolution, and angles.
These decisions and selections “do not depend on technical and professional standards
alone but also on cultural and aesthetic conventions or individual preferences” (ibid.:
301). The production process of scientific images is far from being a neutral process, but
is shaped by sociotechnical negotiation. This is also dependent on local variation in the
question of who is able to read images and who is allowed to read them, because visual
expertise is its own form of literacy and specialisation (ibid.: 302).

Once images have been produced, the question arises how images are used and how they
are talked about or talked to. This is what Burri and Dumit are interested in when they
conceive the notion of an engagement with images. The focus here is on the process of
making data meaningful and thus, their becoming meaningful images. As a
consequence, images should be treated as actors that are actively involved in scientific
practices. In these practices the question arises of what impact images have on the
objectivation of knowledge by reducing uncertainties that occur during observations
(ibid.: 302).

What seems to be retrospectively, rather an arbitrary technology of classification in the
case of race classification in apartheid South Africa, but nevertheless had enormous
discriminating consequences, could also be transferred to what seems like an innovative
and sophisticated system of race classification. In theory even if there were the
possibility of estimating the curliness or thickness of hair automatically, by fictional,
sophisticated “smart,” advanced technology, one still has to question whether this is the
right method of evaluating race or ethnicity of a person. In this case, it becomes clear
that this fictional, sophisticated, advanced technology used for recognising the curliness
or thickness of hair is not “better“for evaluating the race category of a person than is the
out-dated pencil test described in the final part of chapter two. As real and far-reaching
the arbitrary pencil test was in apartheid South Africa, so too would any fictional, smart,
advanced technology be today. It depends on how much authority, power and truth is
allocated to the method or theory and by whom. In this regard the widespread view of
technical authority and neutrality (cf. Gates 2010: 10) should not be underestimated.
Technology and machines commonly count as “more accurate and objective” and “less
subject to prejudices” (ibid.) than humans. Technologies like Facial Recognition or other
Pattern Recognition methods that purport to possess human vision and recognition
abilities are in line with what Gates calls the “digitizing impulse”. It is just “another way
of standardizing images and eliminating individual human judgement in their
interpretation” (ibid.). Thus, the digitizing impulse is a re-articulation of the
mechanizing impulse in the 19th century (Daston & Gallison 1992 cit. in Gates 2010:
10). When Gates identifies digital technologies as “being used to make newly
empowered claims to the truthfulness of visual images” (Gates 2010: 203) she argues
against Mitchell who sees digital technologies as subversive to traditional notions of
photographic truth (Mitchell 2001: 223 cit. in Gates 2011: 203). Also, Borck argues for
digital computer technology to be in line with the tradition of 19th century image
production and engagement where the interference of human involvement was thought
to have been eliminated (Borck 2001). He notices a “paradox of technical transparency”
(ibid.: 388): On the one hand in the production of images, more and more digital
technology is used, whereas exactly this increased use of digital technology leads to the
view that images are extraordinarily real and represent reality exactly as it is. But—as
Donna Haraway famously noticed—knowledge is always situated and perspectives are
always partial (Haraway 1988). That means that we cannot assume that there is “a
universal, disembodied, objective form of vision, outside of any particular vantage point
or subject position” (Gates 2010: 10), but that humans as well as machines “must
necessarily embody particular ways of seeing” (ibid.). Gates questions the view that
“objective, all-seeing machines that function much better, more efficiently, and more
powerfully than human perception alone” can exist. Instead, she promotes the view that
“computers ‘see’ only in a metaphorical sense, only in highly constrained ways, and only
with a significant investment of human effort.” (ibid.: 11). So for example, for a machine
as is also the case for a human being, it is not always clear how to differentiate between
men and women. Both machines and humans have to master the recognition of this
difference and have to learn that there is a difference and that this difference is of
importance in most societies. Both machines and humans also have to learn what the
characteristics and features that indicate or determine the gender or sex of a person are.
Questions of resemblance and difference—in this case which characteristics exemplify
resemblances within a group of men or within a group of women? Which characteristics
differentiate these groups: are key in constituting what is real? (Suchman 2008: 140)
However, these resemblances and differences are not set in stone, they have to be
continuously enacted (ibid.: 141). What seems to be clear and obvious for most adults in
everyday life when located in familiar surroundings, is a real challenge for machines; a
challenge that cannot be solved without significant investment of human and societal
effort.

From today’s perspective it is especially in the fields of computer science and computer
vision and their connections to research in Artificial Intelligence (AI) that is engaged in
this challenge and in research activities. Thus, in this chapter I first deal with what has
been famously called “The Sciences of the Artificial” by Herbert Simon in 1969. The
term was taken up by Lucy Suchman (2008) and thus, brought into the realm of
(feminist) Science and Technology Studies (STS). In this regard I will bring together
computer science and AI literature with STS exploration into these areas. One of the
most popular and recognised actors and the main reference point in the discussions
about AI is the fictional character HAL 9000 from Kubrick’s movie 2001: A Space
Odyssey. Therefore, HAL will be of special interest when I discuss visions, imagination,
expectations and promises connected to AI and computer vision, and connected to the
larger societal transformation processes of digitalisation, automatisation and
smartisation.

Subsequent to this analysis of the interconnections between science fiction and science
fact, I engage with the essentials of computer science and computer vision history in
brief and argue that these endeavours cannot be regarded as universal and global.
Moreover, local differences and particularities must be considered as well. This is of
great importance as the special focus of my study is on the relatively small geographical
and cultural area of the nation-state Austria that often positions itself as a “Gallic
village” when it comes to the deployment of certain new technologies (Felt 2013: 15).
Therefore, I will also elaborate in brief on the history of computer vision in Austria in,
by referring to the formation of the most important umbrella organisation, the Austrian
Association for Pattern Recognition (AAPR). The national techno-political identity of
Austria (Felt 2013) and in addition to this, the techno-political identity of Europe as a
whole, is connected to the current state of computer vision and pattern recognition in
Austria which will also be addressed in this section.

In Science and Technology Studies (STS) the automation of human abilities, tasks and
procedures such as human vision has been discussed and analysed under the term
‘Sciences of the Artificial’ (Suchman 2008) until now. The term was coined by Nobel
Prize Laureate, economist, computer scientist, psychologist, and management theorist
Herbert Simon in 1969. Suchman, from her feminist perspective, contrasts “Simon’s
conception of relations of nature and artifice” with his effort to overcome this boundary
by exploring the historically relevant practices that created this boundary between
nature and culture (Suchman 2008: 141). By doing so, she questions “antecedents and
contemporary figurings of human-technology relations” (ibid.: 139). Suchman especially
was concerned with questions of “what understandings of the human, and more
particularly of human action, are realized in initiatives in the fields of AI and robotics”
(ibid.: 144). Projects within the Sciences of the Artificial that aim at humanlike
machines bring up the question of what it actually means to be human or humanlike.
What is characteristic of being human and acting like a human? Adapting this to my
research interest, I need to ask the question: what is then characteristic of human vision
and recognition?

Questions on how boundaries between humans and non-humans are drawn, re-drawn
and modified in these projects ensue after asking initially what is characteristic for
human vision and recognition. Research in AI and robotics always contain repetition or
mimicry. The machine is becoming a powerful ‘disclosing agent.’ Assumptions about the
human and what it means to be human come to light. Thus, a way to break down these
assumptions is to explain how computer scientists and engineers, the very people that
are extensively working on constructing robots and human-like machines, imagine
being human (Suchman 2007: 226) and imagine human vision and its relationship to
computer vision. Referring to Donna Haraways notion of ‘figuration’ (Haraway 1997:
11), Suchman observed that the prevalent figuration of the human-like machine in
Euro-American ‘imaginaries’ is one of an autonomous, rational agency. AI projects have
simply reiterated these culturally specific assumptions (Suchman 2007: 228). In these
Euro-American imaginaries the figuration of the child is of importance as well. In this
conception the learning child is confronted with a specific trajectory of development
that brings with it unavoidable periods and stages (ibid.: 237). With her book Plans and
Situated Actions (1987) Suchman has given the most significant social scientific
contribution to the field of computer science so far (Collins 1995: 292). Suchman
showed that plans of action prescribed by computer programmes and machines can be
applied to human action only in retrospect. Persons react moreover, to set actions in
everyday life in an unrestricted manner because everyday situations are just too
unpredictable for plans. This “emphasis on sociality” to be found in Suchmans work
stands, as one might expect, “in strong contrast” to the “fixation on the individual
cogniser as the origin point for rational action” (Suchman 2008. 144).

Knowledge is one of the central resources of the ‘sciences of the artificial’ in its project
to build human-like machines with the capability to see, as it is also in STS. From the
Sociology of Scientific Knowledge (SSK) standpoint, knowledge is always social, therefore a
computer cannot show the whole range of human abilities, but only that kind of
knowledge that can be computerised (Collins 1995: 298). From this perspective, the key
to understanding the possibilities and limits of intelligent machines is ‘tacit knowledge’
(Collins 2010). In his book Tacit and Explicit Knowledge Collins (2010) extends the term
‘tacit knowledge’—that was introduced by Michael Polanyi29—and shows how it
consists of three elements: ‘relational’ (contingencies of social life), ‘somatic’ (nature of
human body/brain) and ‘collective’ (nature of human society). In my view especially
interesting and the “irreducible heartland” of the concept of tacit knowledge (ibid.: 119)
is collective tacit knowledge (CTK). In Collins’ argumentation the individual can acquire
this specific kind of knowledge only by being embedded in society (ibid.: 11) and by
having what Collins calls ‘social sensibility’ (ibid.: 123)30.

Diana E. Forsythe explored the creation and development of knowledge in the context
of expert systems in computer scientist laboratories (Forsythe 1993). Thus, her writings
are highly relevant for approaching the question on knowledge categories and how this
knowledge is used in the creation of Image Processing Algorithms. Forsythe has been
interested in the ways knowledge is conceptualised by specific groups of scientists and
how these concepts are realised in the practice of knowledge production. Her research
showed that computer scientists considered knowledge acquisition as problematic,
because “inefficient” humans had to be involved in this process (ibid.: 454). This
problematisation of the human was due to the specific conception of knowledge
inherent in the computer scientists she followed. In this conceptualisation, knowledge
was understood as formal and codified. By contrast “what everybody knows knowledge“
was not defined as knowledge per se (ibid.: 458). Following this insight one might ask
how computer scientists deal with rather more informal, fluent and changing forms of
knowledge that could be called ‘tacit’ or ‘non-explicit’ knowledge. To give an example
from my fieldwork observations, the ability to recognise whether something is machine
written or hand written might be clear for most literate people that are used to both
types of writing. There might also be tacit agreement about this recognition task,
meaning that any form of expert knowledge for this specific recognition task would not
seem to be needed. The ability to recognise and differentiate between machine written
and hand written texts does not appear as something specific, but as something selfevident
(“what everybody knows”). In this regard, coming back to Forsythe, she
observed introspection as a method of research in the process of engineering (ibid.:
458). That means engineers of expert systems relied on their own unproblematically
perceived views instead of seeking out empirical data. She identified an engineering
ethos with a clear technical orientation in problem solving practices. In this regard trial
and error as “practically rather than theoretically problem solving” was preferred (ibid.:
456). It became apparent that it was “better to build a rapid prototype and see how it
turns out than to map it out exhaustively beforehand” (ibid.). However, these
conceptualisations and engineering assumptions and the common knowledge connected
to it, might be biased in a particular direction. For example, they might be
predominantly gendered – meaning from a specific male viewpoint, as was also observed
by Forsythe—whereas a female view is neglected. This would not be as problematic, as
such gendering would be open to scrutiny and perceived as one particular perspective or
one “particular way of seeing” (Gates 2010: 10). On the contrary, it might be the case
that the specific male view is perceived as universal und neutral. Furthermore, from a
social scientific perspective, knowledge is never self-evident, but must be interpreted
(Forsythe 1993: 453) in different social constellations and situations. Instead of the
conceptualisation of knowledge in expert systems being static, formal and codified,
Forsythe’s anthropologically informed social scientific view is that individuals’ beliefs
“are modified through negotiation with other individuals” in everyday life (ibid.: 466). A
view I share with her.

Two other characteristics of knowledge in expert systems observed by Forsythe worth
mentioning here were first, the brittleness of background knowledge that has been
taken for granted, and second, its narrowness following the involvement of only one
expert view (ibid.: 467). In conclusion, Forsythe notes that any knowledge-based system
necessarily involves selection and interpretation. In reference to Bourdieu (1977) she
argues that knowledge engineers exercise power, because “the ability to decide what will
count as knowledge in a particular case is a form of power” (Forsythe 1993: 468).
Consequently, “the exercise of this power is to some extent invisible” (ibid.: 469). This
means that the engineers’ specific (e.g. male, western etc.) ‘situated’ view with all its
tacit values and assumptions is being black-boxed and stabilised over time.
Nevertheless, it is perceived by the user of such a system as being ‘correct’ and ‘true’ in
every sense.

Giving computers the ability to see is a complex sociotechnical process. As elaborated
upon earlier, all attempts at this, are in fact attempts at producing, processing and
understanding (digital) images algorithmically. Therefore, it makes sense to understand
the process of giving computers the ability to see as the sociomaterial process in which
Image Processing Algorithms are developed, produced and implemented in devices or in
larger systems; advertised, used, talked about, criticised, configured, in short; materially
and semiotically negotiated and formed in varying sites and in different situations. This
makes clear that computer science laboritories in university or industrial settings are
not the sole sites of importance when analysing the construction of Image Processing
Algorithms. Making IPAs can be understood as “…a practice of configuring new
alignments between the social and the material that are both localized and able to travel
…” (Suchman, Trigg & Blomberg 2002: 164). They “…take their shape and meaning not
in any single location but through their incorporation across diverse milieu” (ibid.).
Thus, the focus on technology (invention) in the lab and on technoscientific experts (cf.
Pinch 1993) was continuosuly broadened by STS scholars in the last years. For example,
Nelly Oudshoorn, in her influential book The Male Pill (2003), analysed the testing of
technology also in the media. In her understanding, journalistic and scientific texts are
equally important for analysing the technology testing (ibid.: 192). In her area of
interest, male contraceptive technology, “... journalists have played an active role in
articulating and demarcating the identities of the potential users of this technology-inthe-
making.” Thus, journalists as well as scientists played an important role in the
assessment of this new emerging technology. When tracing the path of a scientific
report to a press release to media reports, Oudshoorn shows how the media accounts
differed from the original scientific ones. While the scientific report stressed the
prototype character of the technology as being far from the finished product, the press
bulletin reported on a major breakthrough (ibid.: 205). The British and Dutch media
accounts analysed by Oudshoorn, presented the technology in a significantly different
way, namely as painful and problematic for its users. In doing so, the newspapers
shaped the scientific claims, contesting them “by providing an alternative testing of the
new technology” (ibid.: 206). The media articles did not question the technical, just the
cultural feasibility of the technology (ibid.: 207). This was exceptional, as Oudshoorn
notes, because more often it is the case that journalists shape scientific claims by
uncritically replicating what scientists tell them. This often leads to a “simplified and
overly optimistic picture of what has been claimed” (Fox and Swazey 1992 cit. in
Oudshoorn 2003: 207). What follows from this insight is the recognition of a gap
between different groups of people in their ability to know what specific technologies
consist of and are able to do. This connects to what was elaborated upon in Chapter Two
when referring to the power gap that arises between law enforcers and prisoners about
the role of DNA evidence (cf. Prainsack 2010): whereas the law enforcement side (those
with specialist insider knowledge) showed “rather nuanced understandings of what
forensic DNA profiling can and cannot do”, the prisoners (those without specialist
insider knowledge) “tended to regard DNA profiling as infallible and true” (ibid.: 171).
That means, those people that only perceived a simplified and overly optimistic picture
of DNA profiling via the media were not able to develop a more nuanced
understanding—and this includes a critical assessment—of what the technology of
forensic DNA profiling was able to accomplish. Thus, they were put in a position of less
power because of less knowledge in comparison to the law enforcement side. I have
identified this power gap between different groups (e.g. those surveilling and those
being surveilled) as a constitutive element in the promotion, diffusion and public
understanding of identification technologies.

During my field work I realised that in contrast to my expectations, in Austria most of
the relevant image processing projects in the area of pattern recognition, especially
those of Behaviour Pattern Analysis are at the best still at the prototype or field test
stage. I asked myself, if there actually are projects that are already in operation as it
would make sense to analyse and learn from these projects in operational conditions. As
it is a characteristic of ethnographic field work it was by default rather than by design,
but also as an outcome of my deeper understanding of computer vision and image
processing over time that I came across a nationwide system in operation that contains
at its heart, Image Processing Algorithms that are designed to recognise patterns: the
so-called ‘Automatic Toll Sticker Checks’ (“Automatische Vignettenkontrolle”, in short:
AVK) on Austrian motorways and expressways. My first experience of AVK was in a
newspaper article and I started to find out more about it, searching and reading
different articles in the press and publicly available documents. As my questions about
AVK grew the more press accounts I read, I quickly realised that it would be necessary to
talk to ASFINAG52, the operator of AVK. In March 2012, I wrote an email with a request
for a scientific interview about AVK to a press spokesman who I had seen mentioned
regularly in newspaper articles on AVK. I explained that following the information about
AVK in the press, I would like to gain a comprehensive, objective picture from the
operator ASFINAG and outlined that I was interested in both the ‘history’ of the
implementation of AVK and in the technical and practical mode of operation. The email
remained unanswered until two weeks later I wrote the same email to a press
spokeswoman, also mentioned in many of the articles. This time I got a quick reply with
the information that my request had been forwarded to the executive office responsible.
As I did not get a reply for two and a half weeks I sent a short reminder and again got a
quick, friendly reply stating that on behalf of the executive office I would receive the
most important data and facts about AVK in the following days, but that an interview
would not be possible.

After a short time of disappointment about the information on AVK I had received
being reminiscent of a press bulletin, I replied to the email and thanked the press
spokeswoman for sending me the information. But I also mentioned that I had known
most of the facts from articles in the media. I asked again about the possibility of an
interview and also attached a PDF to the email with my questions about AVK in order to
give a better impression of what exactly I was interested in. I requested a written answer
to my questions should an interview still not be possible. To date, the email has
remained unanswered.

These developments made the case even more interesting for me and I decided not to
delegate it to the realm of unpublished academic work. ASFINAG’s information and
secrecy policies should be seen as a result in themselves, confirming Kammerer’s
insights experienced in Germany (cf. Kammerer 2008) and even going beyond what
Monahan (2010) reported in the US. Under these conditions I decided to focus on the
detailed analysis of all publicly available documents about AVK, concentrating especially
on newspaper reports. Therefore, the aim of the case study went in the direction of
understanding and reconstructing the incremental introduction and implementation of
AVK on the basis of publicly available documents and to analyse how it is described,
framed, imagined and tested or non-tested in the Austrian news. I am particularly
interested in what stories are told in the news about the history, the relevant actors
involved, the mode of operation, the capabilities and limitations, and the implications of
AVK.

Certainly, the heading of this chapter anticipates some of the results presented in what
follows. It is the camera that is the focus of attention in the newspaper articles. The
camera is positioned as the central actor in AVK. It is the camera that recognises the
presence and validity of toll stickers, whereas Image Processing Algorithms are widely
neglected and blackboxed in comparison to the ‘automatic’ and ‘innovative’ camera.
Another central theme in many of the newspaper articles is the presentation of AVK as a
ready-made and autonomous camera system. This is in contrast to the necessity of
double-checking the AVK results manually in an enforcement centre by human
operators. Error rates, probabilities, uncertainties, false positive or false negative cases
are not made a subject of the discussion. Instead, AVK is mainly evaluated and tested by
its economic success. In this context a recurring theme is the presentation of exclusively
provided detection numbers and sales figures.

Regarding the mode of operation, the article described it the following way: cameras
capture the windscreen of a vehicle including an enlargement of the toll sticker (if the
toll sticker exists). The only data saved derives from cars that raise suspicion of not
having a toll sticker on the windscreen. It is described as a mobile system, which can be
installed in different places. It is however, planned for implementation especially on
heavily trafficked motorways in the greater Vienna area. The plan is to operate the
system on 80 days a year. In the article, the problem of very few regular checks carried
out on the heavily trafficked motorways of the greater Vienna area was described. Here,
in the future, automatic checks are planned to replace the ASFINAG toll monitoring and
enforcement unit.

Next to ASFINAG, other relevant social groups mentioned in the article were ‘toll sticker
offenders’ (“Vignettensünder”), ‘car drivers’ (“Autofahrer”) and ‘toll sticker offenders
from abroad’ (“Vignettensünder aus dem Ausland”). Toll sticker offenders not showing a
valid toll sticker on the windscreen had risen by 20 percent in the previous year (2006).
80 percent of the offenders is from abroad. It has to be noted here that it is not clear
from the article if the 80 percent relate to the total number of toll sticker offenders, or
to the 20 percent increase that took place in 2006.

Text had consistent color and high contrast that leads to high intensity profiles which
makes the MSER feature detector to work well in finding text regions from the images [4]. In
the image processing toolbox of MATLAB, detectMSERFeatures function is used to discover
all text regions within the image and results are plotted. Many non-text regions are also
detected alongside the text.

Many stable regions in the image which are not actually text is also detected alongside the
text regions even though the MSER algorithm is highly robust. Non-text regions are removed
by a rule-based approach. In our work, by using simple thresholds the geometric properties of
text are exploited to filter out non-text regions [51]. On the other hand, a machine learning
approach to train a text against non-text classifier [34]. Classically, these two approaches are
combined to produce better results [3].

Stroke width is one of the most common metrics used to differentiate text and nontext.
Stroke width is a measure of the width of the curves and lines that make up a character.
Non-text regions have larger variations in their stroke width while text regions tend to have
less stroke width variation.

By using a distance transform and binary thinning operation [2] the stroke width of the
detected MSER regions is estimated. It is noticed that over most of the region the variation in
the stroke width image is very little. The curves and lines all having similar widths make up
this region being the typical characteristic of human readable text, indicates that the region is
a text region.

In order to use stroke width variation to remove non-text regions using a threshold value,
the variation over the entire region must be quantified into a single metric. Tuning this
threshold value is required for images with different font styles. Then, a threshold can be
applied to remove the non-text regions.

The detected results comprises of discrete text characters at this point. The discrete text
characters are merged into strings or words and then text lines in order to use these results for
recognition tasks, such as OCR. This recognition of the actual words is enabled in an image
that convey more sensible information than just the discrete characters. For example,
recognizing the word 'PLATE' against the set of individual characters {'P','L','A','T','E'},
where the meaning of the word is vanished without the exact arrangement.

Find neighboring text regions and then forming a bounding box around text regions is one
of the approaches for merging discrete text regions into strings or words then text lines.
Expanding the bounding boxes computed previously with regionprops is used to find
neighboring regions. This makes the bounding boxes of neighboring text regions overlap
such that text regions that are part of the same word or text line form a chain of overlapping
bounding boxes

Now, the overlapping bounding boxes can be combined together to form a single
bounding box around discrete strings or words. The overlap ratio between all bounding box
pairs are computed in order to do this. Finding groups of neighboring text regions by looking
for non-zero overlap ratios is made possible by quantifying the distance between all pairs of
text regions. A graph is used to find all the “connected components” text regions by a nonzero
overlap ratio after the pair-wise overlap ratios are computed.

In the image processing toolbox of MATLAB, bboxOverlapRatio function is used to
compute the pair-wise overlap ratios for all the expanded bounding boxes. Further a graph is
used to find all the connected regions. The output of conncomp are indices to the connected
text regions to which each bounding box belongs. The indices are used to combine multiple
neighboring bounding boxes into a single bounding box by computing the minimum and
maximum of the individual bounding boxes that construct each connected component.

cylinder can be considered as a stack of circles with equal radius arranged such that the
centre of all the circles are placed on the axis of cylinder. Here, depth estimation is carried
out and analyzed assuming a single view point. The same method can be implemented for
any number of cameras (viewpoints) though this work is restricted to two cameras because of
stereo vision.

The proposed method involves measurement of the depth of points without vanishing
points. As shown in figure 2.4, only one circle from the cylinder that is mapped tangentially
is taken and then the distance from the camera centre is measured by the following method.
Let the radius of circle in the real world be rb; radius of image be rc and the distance between
the centre Ob and Oc is K. If a circle is imaged perpendicularly, then it appears as a single line
in the image with its length equal to the scaled value of diameter of the circle and thus these
circle are formed with that line as diameter (D).

In this chapter we present and discuss the basic concepts, techniques, and mathematical
background that we use in this thesis. We also provide symbols, image notations, and the
equations that will be consistently used in the following chapters. The chapter introduces
image notations in spatial domain, basic statistics, the concept of integral image, colour space
conversions, and Hough transform. We also discuss the concepts behind Haar-wavelets,
classification and boosting algorithms. Kalman filter and object tracking are also topics
that is discussed in this chapter. More details and references could be found in the relevant
chapters.

Parts of this chapter have been published and presented at 3 conferences: Computer Analysis
of Images and Patterns (CAIP 2011), Asian Conference on Computer Vision (ACCV
2012), and Arts and Technology (ArtsIT 2011). Being the first author, the papers [2],[5],
and [6] were joint work with Reinhard.

Classification is a process that uses a set of features or parameters to recognize an
object. In this thesis we use supervised classification techniques, which means that
an expert defines the classes of objects (e.g., face, eye, vehicles), and also provides
a set of sample objects for a given class which is called training set. Regardless of
the chosen classification technique (e.g., neural networks, decision trees, or nearest
neighbour rule), we have two phases to construct a classifier: a training phase and an
application phase.

Based on the provided training dataset, a classifier learns to use which set of
features, parameters, and weights to be combined together in order to distinguish
objects from non-objects. In the application phase, the classifier applies the already
trained features and weights to an unknown query image to detect similar objects,
based on what is has previously learned from the training set.

Regardless of the classification technique, the performance of a classifier can be
evaluated based on the detection efficiency. Let TP denotes the number of truepositives
or hit, when a classifiers can correctly detect the objects. Also let FP be
the number of false-positives or miss, when a classifier wrongly detects a non-object
as an object. Similarly, we can define true-negatives (TN) and false-negatives (FN) to
describe correct classification of non-objects and the missing objects, respectively.
Although we always can measure (count) the number of FN, we can not simply
define the number of TN for an application such as vehicle detection in a road scene.
That is because the background of an image in basically uncountable. Therefore, for
performance evaluation of a classifier we mainly rely on evaluations using TP, FP,
and FN.

By convolution of a Haar-features to the query image, the classifier tries to find
those “adjacent” dark and bright regions in the image that closely match the appearance
of the given Haar-feature. Figure 2.9 shows convolution of two line and edge
features that match the eyes and forehead regions. The figure indicates that if the
sliding window falls in an actual face region, we can expect some particular Haarfeature
matches, as there is always a darker region of eyes compared to the forehead
region, and also a brighter region of iris compared to the eye sclera.

Typically, an object detection algorithm such as a Haar-like classifier can locate the
objects anywhere within the input image. However, there are many cases of temporary
object occlusion, as well as impossibility of object detection, e.g., due to a harsh
sunlight, reflections, back-lights, or strong shades 3.

Furthermore, even in case of ideal condition and possibility of easy object detection,
it may suffice that we only search for the object in a specific region of the
image, instead of searching in the whole image plane. This can be considered with
respect to the temporal information, and the location and size of a detected object in
previous frames.

In this chapter we propose a method to assess driver drowsiness based on face and eye-status
analysis. The chapter starts with a detailed discussion on effective ways to create a strong
classifier (as “training phase”), and it continues with our optimization methods for the “application
phase” of the classifier. Both together significantly improve the performance of our
Haar-like based detectors in terms of speed, detection rate, and detection accuracy under
non-ideal lighting conditions and for noisy images. The proposed framework includes a preprocessing
denoising method, introduction of Global Haar-like features, a fast adaptation
method to cope with rapid lighting variations, as well as implementation of a Kalman filter
tracker to reduce the search region and to indirectly support our eye-state monitoring system.
Experimental results obtained from MIT-CMU dataset, Yale dataset, and our recorded
videos and comparisons with standard Haar-like detectors show noticeable improvement.

Parts of this chapter have been published in 5 journals and conference proceedings: Journal
of Image and Data Fusion (IJIDF 2011), Computer Analysis of Images and Patterns
(CAIP 2011), ACM Image and Vision Computing New Zealand (IVCNZ 2012), Journal of
Arts and Technology (IJART 2012), Image and Video Technology (PSIVT 2013). Being the
first author, the papers [1],[2],[7],[8], and [9] were joint work with Reinhard Klette, Hossein
Ziaei Nafchi, Sandino Morales, and Juan Lin.

For driver facial analysis, we use the standard Haar-like face detector provided by
Leinhart [84]. However, we trained our own eye-state classifiers, in order to perform
a further research on the parameters that may affect the performance of a Haar classifier.
We trained and created two individual cascade classifiers, one for open-eye
detection and one for closed-eye detection.

There is a general belief that any increase in the number of positive and negative
images in the training dataset can improve the performance of the trained classifier.
However, after several experiments we noticed that by increasing the number
of training images there is also an increasing risk of feature mismatching by the
boosting algorithms. The process of selecting positive and negative images is an
important step that affects the overall performance considerably. Thus, a careful
consideration for number of positive and negative images and their proportional
ratio was essential. The multi-dimensionality of the training parameters and the
complexity of the feature space also defines further challenges. Here we review the
specifications of our robust dataset as well as the optimized training parameters we
obtained.

In the initial negative image database, we removed all images that contained any
objects similar to human eye (e.g., animal eyes such as tiger eyes, dog eyes). We
prepared the training database by manually cropping closed or open eyes from positive
images. Important questions needed to be answered were: how to crop the eye
regions and in what shapes (e.g., circular, isothetic rectangles, squares). There is a
general belief that circles or horizontal rectangles are best for fitting eye regions (as
it is also used in OpenCV eye detector); however, we obtained the best experimental
results by cropping eyes in square shape. We fit the square enclosing full eye-width;
and for the vertical positioning we select equal portions of skin area below and
above the eye region. We cropped 12,000 open and closed eyes from online images
plus seven other databases including Yale dataset [79], FERET database sponsored
by the Department of Defense (DoD) Counterdrug Technology Development Program
Office [184][119], Radbound face database [76], Yale B face database [191], BioID
database [60], PICS database [182], and the Face of Tomorrow (FOS) dataset [195].
The positive database includes more than 40 different poses and emotions.

The adaptation module is detailed in three sub-sections, addressing the weakness of
a Viola-Jones detector, statistical analysis of intensity changes around the eye region,
and dynamic parameter adaptation to overcome inefficiency of Haar-feature based
detectors under non-ideal conditions.

We examined five well-recognized and publicly available Haar classifiers developed
by Castrillon, Lienhart, Yu, and Hameed [197], for our nominated application,
driver monitoring. Although they work well for non-challenging and normal lighting
scenes, we realized that due to frequent on-off shades and artificial lighting in
day and night, those Haar-like classifiers are likely to fail. The situation becomes
even more complicated when a part of the driver’s face is brighter than the other
part (due to light falling in through a side-window), making eye status detection
extremely difficult.

Decreasing MNN causes increase of detection rate; however, it increases false
detection rate as well. Larger values for MNN lead to more strictness to confirm a
candidate for face or eye, thus a reduced detection rate.

Figure 3.6 shows potential issues related to the MNN parameter for an eye classification
example. Left image shows 10 initial eye candidates before trimming by the
MNN parameter. Detections are distributed in 5 regions, each region shows 1 to 4
overlapping candidates. In order to minimize this problem, it is common to assign a
trade-off value for the MNN parameter to gain the best possible results. Figure 3.6,
top right, shows one missed detection with MNN equals 3 or 4, and Figure 3.6, bottom
right, shows one false detection with MNN equals 2. MNN equals 1 causes 3
false detections, and any MNN greater than 4 leads to no detection at all; so there is
no optimum MNN value for this example.

To cope with the above mentioned issues, we propose that Haar-classifier parameters
have to be adaptive, varying with time depending on lighting changes. Figures
3.7 and 3.8 illustrate that we cannot measure illumination changes by simple
intensity averaging over the whole input frame: In the driving application, there
can be strong back-light from the back windshield, white pixel values around the
driver’s cheek or forehead (due to light reflections), or dark shades on the driver’s
face. All these conditions may negatively affect the overall mean intensity measurement.
Analysing various recorded sequences, we realized that pixel intensities in
eye region can change independently from its surrounding regions. Focusing on
a detected face region, Figure 3.7, right, shows very dark and very bright spots in
two sample faces (in a grey-level range of 0-35 and 210-255, respectively).

It also shows a considerable illumination difference for the left and right side of a driver’s
face. Apart from the iris intensity (for dark or light eyes), the surrounding eye intensities
play a very crucial role in eye detection. Thus, proper classifier parameter
adjustment based on proper intensity measurement in the region can guaranty a
robust eye detection. Following this consideration, we defined two rectangular
region around eyes (as shown in Figure 3.7, right) which not only can provide a good
approximation of both vertical and horizontal light intensities around the eyes, but
they are also very marginally influenced by the outer green or blue (very bright or
very dark) regions.

The final step of the detection phase is classifier parameter optimization based on the
measured Ir and Il values, to make our classifier adaptive for every individual input
frame. Now we need to find optimum parameters (SWS, SF, MNN) for all intensity
values in the range of 0-255, which is a highly time-consuming experimental tasks.
Instead, we defined optimum parameters for 10 selected intensities, followed by a
data interpolation method to extend those parameters to all 256 intensity values

A simple tracking around a previously detected object can easily fail due occlusion
or fast change in both size and moving trajectory of the object (Figure 3.10). Therefore,
in order to minimize the search region we need to perform a dynamic and intelligent
tracking strategy. We considered three trackers as the candidates of choice:
Particle filter, Kalman filter, and unscented Kalman filter. While Particle filter can
generally perform better in case of multi-object tracking and complicated scenarios,
the Kalman filter performers more accurately for single-object tracking [92]. Furthermore
Particle filter has significantly more computational cost than the Kalman
filter [125], which is in contradiction with our policy to develop a real-time ADAS.
In our application the driver’s face can not move fast or very irregular, so we assume
it as a linear movement or with a little non-linearity. Altogether our filter of
choice is standard Kalman filter.

Different than a basic variance normalization, this section proposes a further preprocessing
step to overcome the problem of noisy images and that of images contaminated
with illumination artefacts. We use the phase-preserving denoising method suggested
by Kovesi in [73] which is able to preserve the important phase information of
the images, based on the non-orthogonal and complex-valued log-Gabor wavelets.
The method assumes that phase information of images is the most important
feature and tries to preserve this information, while also maintaining the magnitude
information.

We apply the technique of denoising in conjunction with a novel version of the Haarlike
features method which together lead to an outperforming result. As one of
the main contribution of this chapter, we propose Global Haar-like (GHaar) features
which complement the commonly used standard Haar-like features. With global
Haar-like features we introduce a new point of view to gain benefit from the intensity
information of the whole sliding query window. This is beyond the standard
Haar-like features that only look through adjacent dark-bright rectangular regions.

more weak classifiers need to reject 50% of non-face samples taking much more
time than earlier stages. The inclusion of these late-stage weak classifiers can highly
increase the computational cost. Global features are an efficient alternative for two
reasons. Firstly, they are faster to calculate and secondly, due to adding a new level
of information (by extracting different patterns than the local features), they can
offer a better classification in early stages without need to go to the higher stages.

In this section, a cascade of weak classifiers is designed by considering the global
features. It is common approach that each stage of a cascade rejects 50% of nonobject
samples while the true detection rate remains close to optimal (e.g., 99.8%).
When global features are considered, it is important to decide which of the local
features should be considered as being global. One approach is to temporarily keep
a current global feature and continue search for the next local feature, without considering
the effect of the current global feature. If a candidate global feature shows
a better rejection rate, then it is efficient to choose the candidate as an appropriate
feature, and then searching for next local features again. Also, even if their rejection
rate becomes equal or near to equal, the global features are preferred

This section reports about the experiments done for the proposed adaptive classifier,
the tracking platform, and the global Haar-classifier.
Two video sequences and two datasets were used in the experiments. Figures 3.15
to 3.18 show the results of eye-status detection using standard classifier and the implemented
adaptive classifier. The images have been selected from various recorded
video sequences with extremely varying lighting. Tables 3.2 and 3.3 provide the details
of TP and FP detection rates performed on the two most difficult videos (5
minutes each), and two face datasets (2,000 images each).

Figure 3.19 illustrates partial face tracking results and error indices in x-coordinates,
for 450 recorded sequences while driving. Using adaptive Haar-like detectors, we
rarely faced detection failure for more than 5 consecutive frames.

This chapter mainly contributes to propose a comprehensive method for detecting driver’s
distraction and inattention. We introduce an asymmetric appearance-modelling method and
an accurate 2D-to-3D registration technique to obtain the driver’s head pose, yawing detection,
and head-nodding detection. Chapter 3 and this chapter present the first part of
our research which focuses on “driver behaviour” (i.e. driver drowsiness and distraction
detection). The final objective of this thesis is to develop an ADAS that correlates driver’s direction
of attention to the road hazards, by analysing both simultaneously. This is presented
in Chapter 6.
Parts of this chapter have been published and presented at IEEE Conference on Computer
Vision and Pattern Recognition (CVPR 2014). Being the first author, the paper [4] was a
joint work with Reinhard Klette.

Using monocular vision only, we keep the system as low computation as possible,
while the high accuracy of the proposed methods enables us to compete with stateof-
the-art techniques. To the best of our knowledge, no previous research has jointly
addressed all of the above mentioned subjects in one integrated real-time solution.
Jian-Feng et al. [61] propose driver-fatigue detection using the same standard
active appearance model (AAM) introduced by Cootes [30] by fitting it to an eye region,
followed by head-gesture detection based on the face-centroid. The method appears
to be too basic to be applicable in highly-dynamic real-world scenarios.

Chutorian and Trivedi [105] propose a method to monitor driver’s activity by using
an array of Haar-wavelet AdaBoost cascades for initial face detection, and by
applying localized gradient orientation (LGO) as input for support vector regressors.
The method uses a rigid facial-mesh model to track the driver’s head. There
is a general weakness here as the tracking module may easily diverge for the face
shapes that are very different from the reference “mesh model”.
Visage Technologiesr provides a state-of-the-art head tracker [196] based on a
feature-point detection and tracking of nose boundary and eye regions. Despite of
accurate results under ideal conditions, their tracking fails in the presence of noise
and non-ideal conditions.

Kr¨ uger and Sommer use Gabor wavelet networks [74] for head-pose estimation.
Claimed advantages cover, for example, independence to affine deformations and
high-precision of the algorithm for any desired input; also the input may range from
a coarse representation of a face to an almost photo-realistic subject. Nevertheless,
the experimental results are not backed-up by a validation or comparison with other
techniques.

Appearance models (AM), as originally introduced by Cootes et al. [30], are widely
used for object modelling, especially in the context of face processing. In order to
define a face-AM, we need to train a variation of face shapes (as shape model) and
variation of face intensities (as texture model). In other words, we need to combine a
model of shape variation with a model of intensity variation. Rather than tracking
a particular object (or a particular face) with a particular pose (or expression), an
Active Appearance Model (AAM) iteratively tries to refine the trained model with the
target image, in order to reduce the matching errors and to lead to a best model
match. Current research addresses the underlying optimization problem (to find an
improved fitting algorithm) and the reduction of matching errors.

Reviewing the Cootes et al. method [30], the Active appearance model (AAM) refers
to an active search and refinement process to adapt a previously trained face-AM
into an unknown face; and with asymmetric AAM (ASAAM) we process a face as an
asymmetric object.

Let us assume we like to interpret the facial features of an unknown driver. After
finding the initial position of a 2D face in the input frame we aim to adjust the model
parameters c to match the trained AM to an unknown input face as close as possible.
Model matching is a crucial step in our algorithm, as all the next stages including
head-pose estimation, head-nodding and yawning detection, can directly be affected by
the accuracy of our model matching methodology.

Before proceeding with model initialization, we need the driver’s face location.
We use our proposed GHaar features and classifier in Section 3.8 and in [9]. The
classifiers can return a robust face detection and localization even under challenging
lighting condition. Having model parameters c, and shape-transformation parameters, 
the rough position of model points F can be calculated on the image frame,
which also represents the initial shape of the face patch.

In the previous section we described tracking of the driver’s face-shape model by
localization of the key feature-points.
This section introduces driver’s pose and gaze estimation in six degrees of freedom,
based on mapping 2D feature-points into a 3D face model. The section also
introduces the Fermat-point transform, for the first time.

We describe a conical gaze-estimation method as the preliminary step to analyse
driver’s direction of attention and its correlation with the location of detected hazards
(e.g., vehicles) on the road.

This section contributes on both issues: Selection of an optimum generic 3D
model, and proposing a solution to minimize the 2D to 3D matching error; therefore
a more accurate and faster pose estimation can be achieved.
In the next two sub-sections, we provide a trigonometry-based solution to reduce
pose-estimation errors that happen due to differences between the 3D face model
and the 2D face-shape variation among different people (drivers).

This section introduces the selection of appropriate facial-feature points and a registration
technique for matching of the driver’s 2D face-image into a generic 3D facemodel.
As part of our algorithm to obtain roll, yaw, and pitch angle of the driver’s
face, we use the state-of-the-art method of EPnP by Lepetit et al. [80], in conjunction
with a novel pre-processing step to minimize the mismatch errors of 2D-to-3D
corresponding points

Some of the works that use the Perspective-n-Points (PnP) method, normally consider
four points around the nose boundary [48, 104]. This may, in general, simplify
the case to a planar problem, as the sampled feature points around the nose boundary
have almost the same Y-depth in the camera-coordinate system. However, a
weakness is that those feature points cover only a limited area of the face region
which might be also affected by noise. This causes larger error values for matching
the driver’s face and the 3D model.

Although four sets of points in both world-coordinate, and image-coordinate systems
are sufficient to solve a PnP problem, we use five points (ears’ top, nose tip,
and mouth corners) in order to maintain both redundancy and robustness towards
image noise, and to reduce the impact of potential errors from the ASAAM step.
Furthermore those five points potentially cover a wider region of the face, also with
different depth values.
Before proceeding further with the EP5P solution, we propose a new point-set
transform and normalization to minimize the 3D model’s matching error with the
actual face shape. The objective is to gain a more accurate pose estimation, and to
avoid model-matching divergence and failure of the EP5P, due to residual errors.
After solving Eq. 4.12, we rescale the shape of the driver’s face (obtained from
the ASAAM stage) to match the points p4 and p5 to known corresponding points F4
and F5 in the 3D model (Figure 4.5). However, due to face-shape variation, we can
expect that the three remaining points (i.e., p1, p2, and p3) do not exactly match to
the corresponding points in the 3D model (F1, F2, and F3).

This chapter reviewed the latest research on head pose estimation. We identified
three important weaknesses among the proposed methods including failure of face
shape modelling due to challenging lighting conditions and intensity variation in
face halves; unstable pose estimation and pose divergence due to residual errors
from mismatch of the face model to the generic 3D model; and slow operation of
previous work due to need to model refinement within several iterations (up to 10
iterations). We proposed methods and solutions for all the three identified issues.

We introduced the ASAAM to solve the problem of intensity variation in face
halves which led to more accurate face shape modelling. We also selected a generic
face model and a 5-point facial feature point analysis followed by a Fermat-point
transform that led to both a faster and more accurate 2D to 3D matching with lower
residual errors, 3 times faster operation, and no model divergence. The method was
performed in real world driving conditions and was tested on different subjects; the
result proved a considerable step forward in the context of driver pose estimation.
Empirical test and analysis confirmed the accuracy and robustness of the proposed
method, over a wide range of recorded videos and different subjects compared
with standard AAM. However, further analyses was not conclusive as our
asymmetric 2x32 point annotation approach was not comparable with other (e.g., a
72 point land-marking) methods.

crashes present the highest rate of injuries (30.9%), fatality rate of 5.6%, and also the
highest percentage of property loss (32.9%) among all types of vehicle accidents in
the USA, at the reported time.

By maintaining early vehicle detection and warning, it is possible to provide more
time for a distracted driver to take an appropriate safe action to resolve driving
conflicts, and consequently, decrease the possibility of rear-end crashes.
Many researchers have already developed computer vision based algorithms to
detect and localize vehicles on the road. However, most of the methods suffer from
either lack of robustness in complicated road scenes, or from very expensive computational
cost. This makes many of the developed methods non-realistic and inapplicable
as a driver assistance system.

Santos and Correia [132] use a symmetry-detection technique after having background
subtraction based on an already known estimated background, using a static
surveillance camera. The approach is effective for detecting vehicles in cases such
as a parking lot with already analysed parking background; it is not suitable for
unknown environments or roads.

One of the most important points that has been neglected in the reviewed research
is that the appearance of a vehicle, can highly vary depending on the distance between
the observer and the vehicle. This challenge cannot be solved even by scaleinvariant
methods, as the shape and details of a vehicle at close distance (a few
meters) is completely different to a vehicle’s appearance at a distance of, e.g., 100 m
(Figure 5.2).

Thus, relying on a one-dimensional solution for robust vehicle detection for both
short and long distances appears to be hard and unrealistic to achieve. As discussed,
there are many publications on general object detection or tracking approaches that
are based on LBP or Haar wavelet classification; however, not many of them can be
suitable for highly dynamic and real-time applications such as vehicle surveillance
or monitoring. We actually need to incorporate domain specific information from
road conditions or vehicles’ characteristics to prevent false alarms or missing true
detections

road scene and initial detections will be further processed by feature detection, taillight
symmetry analysis and a final data fusion as shown in Figure 5.3. The overall
method eliminates many false-positive detections as well as retrieval of missed
detections (false-negatives). We also provide an accurate distance estimation technique
using a single monocular vision sensor.

To assess the road-scene intensity, we cannot simply use the mean intensity of the
pixels in the input frames. Figure 5.5 illustrates how we deal with intensity analysis
by segmenting a road scene into two parts- “sky” and “road”.
After analysing mean intensity and standard deviation of road and sky regions
from 280 samples of road scenes, taken under different weather and lighting conditions,
we noticed that the top 5% of the sky region, and the bottom 5% of the road
region normally provide acceptable intensity estimation about the whole scene, in
which the measured values also fall within the scene-intensity standard deviation.

In the shown example of a night scene (Figure 5.5, bottom left), despite of an
expectation of dark pixels, some bright pixels (due to street light) fall into the Sl
region; this influences our mean-intensity measurement for the left patch of the sky;
consequently, a dark blue segmentation (bottom, right) shows regions around the
street lights, instead of showing the sky region as part of the light-blue segment.
However, on the other hand, the measurement in Sr supports an “acceptable”
segmentation of the sky, shown as a light-blue segment.

The mode pixel value (the pixel value with the highest frequency of repetition in
Sl[Sr) determines which of the resulting segments (light blue or dark blue) is a better
representative of the sky intensity. By assigning  equals to 0.66 2, we consider
a double importance factor for the detected mode intensity compared to a standard
mean; this consequently reduces the negative impact of any inappropriate segmentation.
In other words, for the night scene shown at the bottom of Figure 5.5, the
final value of Is() is automatically much closer to the intensity of light blue segments
rather than to that of the dark blue (actual sky) segments. A similar approach
is applied for road background intensity evaluation, Ir(), which is shown by dark
and light green segments.

Using the same training dataset, we created three vehicle classifiers using LBP, Standard
Haar, and AGHaar. Samples of vehicle detections are shown in Figure 5.6. The
proposed AGHaar classifier provides more accurate initial vehicle detection, clearly
outperforming LBP and standard Haar classifiers. However, we still consider those
initial detections by AGHaar as being vehicle candidates or ROIs only. In order to
have even more accurate results (i.e., less false-positives and less false negative) we
continue our evaluation by analysing line and corner features before confirming that
an ROI is a vehicle.

Test image data from an experimental setup is used. Digital images are sensed with a 11
megapixel CCD sensor and analog images are captured using a small format camera with
several types of film. Note that the image plane are of equal size, since the used CCD
sensor has also a format of 36 × 24mm. The focal lengths of the used lenses are chosen in
a way, that a 9μm digital pixel (native CCD pixel size) represents the same object area as
a pixel from film scanned at 20μm. With this constellation it is possible to show that the
quality of a 9μm CCD pixel is superior to a 20μm film pixel, and that this advantage is
unaffected by a reduction of the size of film pixel achieved by scanning at higher resolution.

The main disadvantage of analog film is its granularity that causes grain noise. To measure
the impacts of grain noise to image processing tasks, the following algorithms on artificial
and natural images are performed

To evaluate the geometric accuracy of images, a stereo image matching setup is proposed.
Homologous points of two images taken from the same camera from different locations are
taken. The epipolar geometry is computed first using relative orientation. This employs
standard yet recent computer vision technology. The distance of each match point from
its according epipolar line is a quality measure for image matching. Image matching starts
with a reference image and the computation of points of interest using the Harris operator

The quality of an imaging system may be evaluated using the amount of blurring at edges.
The edge spread function of a 1D signal is the response of the system to an ideal edge.
The first derivative of the edge spread function, called the point spread function (PSF),
is usually used to describe the quality of an imaging system [Luxen and F¨orstner, 2002].
Two different measures to characterize the edge response function have been implemented.

Then all strong magnitudes in the image get plotted according to the edge orientations
in an orientation plot. Luxen proposed to describe the PSF by the parameters of the enclosing
ellipse, which is in case very sensitive to noise. Some large magnitudes produced
by image noise can change the parameters arbitrarily. That is why in the used implementation
instead of the enclosing, the fitted ellipse in the least squares sense is used. This
process is robust to a certain amount of outliers. The width of the ellipse is normalized
within the interval [0, 1] where a width of 1 stands for an ideal step edge from black (0
DN) to white (255 DN) in the 8 bit case.

All testing methods were performed on all film images scanned with 5, 10, 15 and 20μm and
on the ccd images with 9μm which covers the same area as the 20μm scans, irrespective of
the distance to the object area. If using an ideal film image and an ideal scanner the image
could be scanned with e.g. double resolution and the same results for e.g. the Siemens
star test are obtained. Since real film images are used, it is obvious that the geometric
performance will not stay the same when scanning with very high resolution, because the
granularity of the film causes image noise to cover up the object.

The digitally sensed images are equal to the film image when using the edge response
and Siemens star tests, however they outperform the film images in stereo matching
accuracy and noise levels. Stereo matching results in a 2.5 times smaller noise level
and it is possible to match in homogenous areas because of the absence of noise. The
conclusion of the work is that digital sensors should support a more accurate and robust
photogrammetric process than film can.

The idea of using multiple cameras to enlarge the field of view is not new. In the 1930s
aerial cameras of the time were restricted in the format size and in angular coverage due
to lens distortion and fall off illumination toward edges. So two or four RMK-13.5 were
mounted in such a way that the optic axes were oblique, exactly as the novel concept by
Z/I Imaging used in their DMC camera (see figure 3.1).

The alternative approach is to use linear single line CCD arrays deployed in a pushbroom
scanner. The basic technology has been developed over a long period by various
space agencies and are implemented in satellite remote sensing such as Landsat, SPOT or
IRS. The German Space Agency (DLR) developed their concept for Mars exploration and
made it available for ADS40 aerial camera by Leica Geosystems (formerly LH Systems).
The idea is to use a three line scanner (TLS) principle which allows stereo coverage to
be collected along track in a single flight. The linear arrays are positioned so that the
three lines give forward-, nadir- and backward-pointing image stripes (see figure 3.2 (a)).
Previously, this principle was commonly used by space imagers such as MOMS-02, because
atmospheric turbulence caused serious gaps or double imaging when pushbroom scanners
were used on airplanes. Modern fast acting gyro controlled mounts limit these effects.

Another approach are panoramic cameras which employ a rotating lens swinging
across a cylindrical imaging surface that senses the ground in cross-track direction (see
figure 3.2 (b)). The modern equivalent of the so-called Hyac (High Acuity) film-based
panoramic camera uses linear CCD arrays and is implemented within the digital
camera system by ImageAmerica named Direct Digital Panoramic system (DDP-2)
[ImageAmerica, 2002].

This chapter is structured as follows: Section 3.2 describes the concepts for large format
image acquisition using digital cameras. Then, in section 3.3 the important properties
of aerial cameras are listed. Next, section 3.4 describes a representative set of available
digital airborne cameras, explains their concepts and shows their specifications. All images
of the digital cameras are taken from the manufacturers homepages listed in table 3.2. A
discussion about the different cameras and concepts is given in section 3.5 and a conclusion
is made in section 3.6.

Since no digital sensor exists which is able to capture the same amount of pixels as a
scanned large format film image at once, the concepts are to use multiple area sensors or
line sensors. These concepts are discussed in detail in section 3.2.1. As digital cameras
are beginning to enter into photogrammetric practice, a dimension of complexity is being
added to photogrammetry which has been a main concept in remote sensing for a long time.
While color aerial photography simply is being produced by three perfectly co-registered
emulsion layers, digital images can be obtained in a variety of different approaches. Highresolution
panchromatic imagery may be combined with lower resolution color images.
Color may also get created by sequentially producing its components, thereby introducing
a need to eliminate geometric differences. Several methods for color acquisition are given
in section 3.2.2.

As mentioned in the introduction, digital airborne cameras at a large format can be
divided into two classes: area and pushbroom-based. Area-based cameras are divided
into two sub categories: multi and single head sensors. The main difference between
these approaches is the geometry of the resulting images.
Images from area-based sensors are the digital analog to classical film-based camera
images. The images obey to a central perspective and can be used in exactly the
same photogrammetric workflow as aerial analog film images. The images themselves
contain geometric information, because they are captured by a rigid CCD sensor and
no additionally data, like GPS, is necessary to define the geometry. Classical methods
like the calculation of the epipolar geometry are possible. Figure 3.3 (a) illustrated the
geometry of area-based sensed images.

Pushbroom-based approaches generate images with a central perspective per image
line across flight direction and no perspective along flight direction (see figure 3.3
(b)). This kind of data cannot be used in a standard photogrammetric workflow. For
example the epipolar lines are epipolar curves with difficult geometric aspects (see
[Gupta and Hartley, 1997, Gr¨un and Zhang, 2002, Lee et al., 2003]), so that image
matching using the classical epipolar constraint is not suitable for this new kind of data.
Algorithms for the three line scanner are developed but not always integrated in common
photogrammetric software packages. Unlike to the area-base images the image data
generated by pushbroom scanning have no geometric information and can only be used
by incorporating location and orientation data from extern navigation systems devices
like DGPS and INS.

The digital aerial cameras listed in table 3.1 are described in detail in this section. A
complete list including camera name, manufacturer, web contact and reference is given in
table 3.2. An explanation of each concept is presented and detailed specifications of the
area-based cameras are listed in table 3.3, of pushbroom cameras in table 3.4.

When talking about replacing analog large format film, only three commercially available
digital cameras exist, namely UltraCamD, DMC and ADS40. The UHRFS is a system
from a US-government program and is not commercially available. Other digital aerial
cameras with 16 to 20 megapixels such as IGN, DiMAC or DSS may replace middle
format film, but for sure not the large format. The DiMAC system suggests that multiple
cones can be arranged in a manner analogous to the DMC system. How this results in
photogrammetrically accurate solution is not available from the product literature. The
helicopter mounted pushbroom camera StarImager addresses a niche in the market, like
urban mapping or city modeling, but is not designed for replacing large format aerial
cameras.

Analog film cameras for aerial sensing will be replaced by their digital successors. The
main technical problem is the production of the same amount of pixels, which are collected
by scanning a 23×23 cm2 analog film image. Nowadays, there are no digital image
sensors with the capability to gather this high number of pixels. That is the reason why
two concepts become accepted in the production of a digital aerial camera. The usage of
multiple area sensors or the three line scanner principle using CCD lines are state of the
art. Three commercial products, namely the UltraCamD, DMC and ADS40, implement
these concepts and are replacing analog airborne cameras.

From the two area-based concepts the UltraCamD should be preferred to the DMC, since
the resulting virtual image has a single perspective center and the maximal repeat rate is
higher. If flying at low altitudes within urban areas with large buildings (e.g. skyscrapers),
the UltraCamD is able to produce correct images. The DMC in contrary, produces
distorted images, due to the tilted cameras, that have different perspective centers. When
using the pushbroom concept of the ADS40 it must be considered, that classical software
tools cannot be used, for the photogrammetric softcopy workflow.

This chapter is structured as follows: First, the principle of camera calibration are
given in section 4.2. To be able to determine the geometrical relations of the subimages,
the calibration solves two major tasks. It gives an initial solution of the position, where
the images are overlapping and due to the rectification a linear geometrical transform can
be used in stitching, instead of a non-linear mapping.

Then, the correspondence problem between the images it solved be means of
image matching. One possibility is to extract points of interest (POI) in one image
and reallocated them in the other image. Consequently, section 4.3 presents POI
extraction and section 4.4 deals with the correspondence problem. This is done by image
matching, first with pixel accuracy (section 4.4.2) and second, with subpixel accuracy
(section 4.4.3). An accuracy analysis of the subpixel algorithms is performed in section
4.4.3, since the stitching must be very precise to be used for aerial cameras of image
plane synthesis. With the matching problem solved, the next task is to determine a
transformation between the images using robust statistics shown in section 4.5. Finally,
in the assembling of the large image two additional problems are solved: A global
radiometric adjustment (section 4.6) to avoid global brightness differences and a blending
in the overlapping areas (section 4.7) to avoid local differences in the overlapping regions.

For high precision large format aerial camera calibration the classical photogrammetric calibration
method is for sure the best approach. However, if calibrating a consumer camera,
normally no calibration target is available. For this purpose a special kind of calibration
is proposed in this section. The approach is published in detail by [Klaus et al., 2004].
Instead of extensive setups, the accurate angular positions of fixed stars are exploited and
used as very accurate test targets. This approach uses a star catalog and requires only
one single image of the night sky to extract the parameters of the camera model.
The algorithm is performed in three steps. First, the pixel positions and magnitudes of
the stars are extracted from the input image, with an accuracy of 0.1 pixel. This is done
by image segmentation and centroid calculation. Then, with the extracted position and
magnitudes, an initial estimation of the essential camera parameters can be achieved by
using a RANSAC-based algorithm [Fischler and Bolles, 1981]. Finally, the found solution
is optimized using the Levenberg-Marquardt method [Mor´e, 1978].

The resulting mean pixel errors for several consumer cameras are in the range of 0.13 to
0.21 pixels and enables remote calibration without the need of a test target.
For digital aerial cameras this calibration method is not appropriate, since these cameras
are designed for only very short exposure. When capturing the night sky long exposure
times are necessary, to be able to extract the stars.

The direct linear subpixel correlation algorithm is very robust. It gives an accuracy
for points of interest of 0.02 pixels with a small standard deviation. There are very
few false matches and nearly no outliers in the tests. In comparison the paraboloid
interpolation shows bad accuracy. Figure 4.13 gives the residual errors as histograms
for the paraboloid and direct linear subpixel correlation algorithm. A huge amount
of outliers is observable in the results of the simple method, which is not the case in
the Lan/Mohr algorithm. Figure 4.14 shows the error distribution using the template
based method, with three and seven templates per dimension. The results are similar.
Finally figure 4.15 shows a comparison of performance and accuracy of the algorithm
with three and seven templates. The comparison shows that only three templates do
already improve the accuracy significantly. By using seven templates the accuracy could
not be improved significantly although the time expense is more than quadrupled. One
problem in the evaluation process is that the shifted images are generated by means of
image interpolation, that introduces a geometrical error. Therefore, the accuracy of the
subpixel refinement algorithms can only be tested, if they are of lower accuracy than
the interpolation itself. This is the reason why the accuracy of the template matching
algorithm is not increasing, if using seven instead of three templates.

Out of the homologous points from image matching ,the parameters of a given transformation
can be estimated. Since the cameras are geometrically calibrated, only linear
transformations are discussed in this section. If a non-linear model has to be used, the
computation of the parameters gets complicated and is done by non-linear regression algorithms
like the well-known Levenberg-Marquardt algorithm [Mor´e, 1978].

There are several groups of robust statistics which can handle a different fraction of
outliers. The fraction of outliers that causes the methods to fail is called the breakdown
point and is given in percentage. A group of methods has been derived by extending
the least-squares algorithm (which has a theoretical breakdown point of 0%, which means
that one outlier could lead to a completely wrong solution), so that it can handle a certain
amount of gross errors. The basic assumption is that outliers can be detected, because their
distance from the model is larger than for inliers. This is only true if the inliers dominate
the initial fit. Data points with large residuals are thus regarded as possible outliers and
their influence is reduced with a so-called influence function  (x). The problem with the
simple least squares method is that extreme outliers with arbitrarily large residuals can
have an infinitely large influence on the resulting estimate. Huber divides the least squares
extensions into three groups in his standard work about robust statistics [Huber, 1981]

Images taken from different CCD sensors can vary in brightness due to varying CCD sensitivity.
Since every digital aerial camera is radiometrically calibrated, the CCD sensitivity
is not an issue. However, if the exposure time for the images is different, these images have
different overall brightness. As the CCD sensor is linear in its sensitivity, which means that
a double exposure time results in double gray values (if the values are not saturated), a
multiplicative model over the mean gray values solves the brightness problem. Depending
on the geometric relations of the CCD sensors, an equation system of the form Ax = b is
set up containing the mean values of the overlapping areas and the geometric relations in
matrix A and in vector b. Vector x contains the multiplicative parameters for each sensor.
This equation system may be over-determined and therefore solved by the least squares
approach, or it is quadratic with full rank and is solved directly via matrix inversion.
This issue is illustrated on an example with four images arranged as follows

Ideally, each pixel which corresponds to the same position in object space would have
the same intensity in every image, but in reality this is not the case. The reason for
this can be changes in aperture and exposure time (e.g. due to varying closing of the
shutters)), vignetting, registration errors due to misalignments in the stitching process
or mis-modeling of the camera calibration. Therefore, a good blending strategy that
minimizes seam artifacts by smoothing the transition between the images is necessary. In
feathering or alpha blending [Uyttendaele et al., 2001] the mosaic image I is a weighted
sum of the overlapping input images I1 and I2.

The accuracy of the registration of the color images to the stitched panchromatic image
is, of course limited, since the color images are smaller by a factor of 3.6. This factor comes
from the different focal lengths of 101.4mm for the panchromatic image and 28mm for the
color channels. The pixel errors are given in the panchromatic coordinate system, so that
e.g. a pixel shift of 1 pixel in the color image is given as 3.6 pixels in the panchromatic
image.

The mean global RMSE for the four color channels is 0.22 pixel with a standard deviation
of 0.08 pixel. In the optimal case the RMSE for the color registration should be 3.6
times larger than the RMSE for the stitching, which is not the case, since the factor
is 6.8. Two aspects are influencing the optimal possible quality. First, the error of the
panchromatic image propagates to the registration result and second, the input color image
is not perfectly registered. The reason for this may come from the scanning process and
is observable on edges, that show color fringes. However the RMSE is not bad, since 0.22
pixels in the panchromatic image represents 0.06 pixel (1/16 pixel) in the color images.
Consequently, a RMSE of about 0.2 pixel should be the lower error bound for the color
registration.

A program was implemented in C++ to analyze the behavior of the new polynomial evaluation
in a real world scenario. All calculations are done on a Pentium III, 1 GHz. Of course
the kernel functions could be precalculated using lookup tables. To keep the accuracy at a
high level 10000 grid points per unit length are chosen according to [Ostuni et al., 1997].
Note that memory access is quite slow, therefore the lookup table approach is slower than
direct calculation for the cubic case. To get an average value the calculation is performed
100 million times, which leads to the following results in 2D interpolation

This chapter presents a novel method for efficient implementation of cubic and higher
order interpolation. The proposed polynomial evaluation needs only about 40% for slope
constraint and 60% for continuity or flatness constraint of computation time for cubic
(speed-up factor of 2.6 and 1.7 respectively) in comparison to the standard method. For
higher order polynomials the speed-up factor is about 1.5. The whole image interpolation
process is speeded-up and takes only about 83% and 87% respectively for cubic convolution.
Therefore the classical implementation should be replaced with this novel method.

For digital aerial cameras this contribution allows a faster post processing of the collected
image data, since the interpolation task needs a large amount of the overall image
processing time. In case of the UltraCamD the interpolation used in the image stitching
process takes one third of the whole computation time. This part can be accelerated by the
usage of the proposed method. However, this approach is not limited for the special case
of digital aerial cameras. In contrary, it should replace the standard interpolation method,
which is still implemented in the same way as in 1973, proposed by [Rifman, 1973].

Image fusion received significant attention from researchers in remote sensing and image
processing with the launch of SPOT 1 (1986). This satellite provides panchromatic images
with 10m GSD and multi-spectral images with 20m GSD. Since then, much research has
been done to develop effective image fusion algorithms.

The condition for the pansharpening process is, that the images are geometrically registered
to one an other. Either the low-resolution color image is sampled up to the size
and geometry of the panchromatic one, or the registration is done with a remaining scale
factor which is known. If the registration is not accurate several artifacts will occur and
an image fusion makes no real sense.

Another concept for a neighborhood-based approach is to segment the panchromatic
image and then fill the segments with the corresponding average multi-spectral information.
[Chang et al., 2001] use unsupervised ISODATA classification to get a segmentation.
Then the multi-spectral data is resampled by using the average color per segmented patch.
After that, a traditional color transformation fusing method is used. Since a segmentation
is computationally expensive, [Steinnocher, 1999] suggests to use local object edges
instead of image segments. Both approaches result in pre-segmented images, where the
variation of gray levels within the objects is very low, while objects are clearly segmented
from each other. The fused images look like oil paintings and are therefore very suitable
for a classification process, but do not give the impression of real multi-spectral images.
They look more like median filtered images with sharp edges. These approaches can support
a classification process, but they are not qualified to generate a realistic mapping of
the world (see figure 7.3).

The standard evaluation concept for fusion methods is to start with a high-resolution
color image Iref . This image is downsampled by a given factor which leads to a small
color image IRGB. Additionally Iref is converted to a gray scale image IPAN. Now
different fusion approaches combine the low-resolution color image IRGB and the highresolution
panchromatic image IPAN to a high-resolution color image Ifused. This fused
image can now be compared with the reference image Iref . This setup is straight forward
and gives the possibility to compare fusion results from different algorithms and several
spatial differences to a ground truth image. Figure 7.11 illustrates the evaluation setup.
The question of how to compare the reference and the fused image quantitatively
remains. The same error metrics like in the comparison of Bayer pattern demosaicked
images in section 6.4 can be used, which are the root mean square error (RMSE) for each
color channel (see equation 6.22), the total RMSE which is the sum of the single RMSE
values, the RMSE in the Lab color space (see equation 6.24) and the RMSE at edges.

[Wald, 2002] proposed two more error metrics which are directly related to the RMSE
and therefore cannot reveal more information than the RMSE itself. They are named
relative average spectral error (RASE) and relative adimensional global error in synthesis
(ERGAS). Wald argues that these two error metrics are easier to interpret. Comparing
several fusion methods on the same data set the RMSE is sufficient, so RASE and ERGAS
can be skipped

In a work that involves image analysis to determine three-dimensional
structure, it is necessary to establish a model that describes
projection to 2D image planes.
There are two considerations to take into account: the projective
geometry that determines where a point of the scene would be
projected onto the image plane and the physics of light that
determines how brightness changes as a function of scene
illumination and surfaces properties.

A camera model for the generation of an image is the pinhole camera.
It is a box that has an infinitesimal small hole through which light
enters and forms an inverted image on the camera back plane. To
simplify things, we usually model a pinhole camera by placing the
image plane between the focal point of the camera and the object, so
that the image is not inverted. This mapping of three dimensions onto
two is called perspective projection. Several alternative projection
models exist, like paraperspective or orthographic projection.
Projective geometry is fundamental to the understanding of image
analysis.

The diameter of the circle is proportional to the aperture diameter. If
the aperture diameter decreases the range of world approximately
focused increases but there is a reduction on light intensity. Longer
exposure times can solve the reduction of light intensity but is
accompanied by a loss of time resolution; the trade off is between loss
of spatial resolution or time resolution.

After crossing the lens the light, in digital cameras, reaches the
Charged-Coupled Device (CCD), and here another problem arise due
to the fact that cameras in most cases uses a three color model
technique to capture the light (RGB). For each pixel there are three
detectors in the sensor. This cause a effect called “lateral
displacement”, i.e. there is a spatial displacement from the video
signal and the point in the world.

The epipole is the point of intersection of the line joining the optical
centers (the baseline) with the image plane. The epipole is the image in
one camera of the optical centre of the other camera. The epipolar
plane is the plane defined by a 3D point and the optical centres or
equivalently, by an image point and the optical centres.
The epipolar line is the straight line of intersection of the epipolar
plane with the image plane. It is the image in one camera of a ray
through the optical centre and image point in the other camera. All
epipolar lines intersect at the epipole.

Stereo vision determines the position in space by using triangulation.
That is, by intersecting the rays defined by the centers of projection
and the image. Triangulation depends crucially on the solution of the
correspondence problem.

The disparity measures the difference in retinal position between
corresponding points in two images. Depth is inversely proportional to
disparity. It is possible to verify by looking at moving objects that
distant objects seem to move more slowly than closer ones.

The sequence of operations of most computer vision system begins by
the detection, location and representation of special parts of the
image, called image features, usually corresponding to interesting
elements of the scene.

In computer vision the term image feature refers to two possible
entities:
A global property of an image or part thereof, for instance the average
grey level the area in pixel (global feature).

A part of the image with some special properties, for instance a circle,
a line, or a textures region in an intensity image, planar surface in a
range image (local feature).

How to detect special parts of intensity and range images like points,
curves, particular structures of gray levels or surfaces patches
represents the second definition and the focused one.

Image features are local, meaningful, detectable parts of the image.
Meaningful means that the features are associated to interesting
scene elements via the image formation process, such as sharp
intensity variations created by the contours of the objects in the
scene, or image regions with uniform gray levels, for instance image
planar surfaces.

Detectable means the location algorithms must exist, otherwise a
particular feature is no use. Different features are, of course,
associated to different detection algorithms, these algorithms output
collections of feature descriptors, which specify the position and other
essential properties found in the image.

Image features can be edges, points, corners, surfaces, lines, curves,
etc.

Edges points or simple edges, are pixels at or which values undergo a
sharp variation. Image edges are commonly presented as
discontinuities in the underlying irradiance function, but it seems
more accurate to speak of sharp image variations than discontinuities,
the reason being that the scene radiance is low pass filtered by optics
and the resulting image brightness cannot have real 0-order
discontinuities.

There are various reasons for the interest in edges. The contours of
potentially interesting scene elements like objects, marks in surfaces
and shadows, all generate intensity edges. Moreover, image lines,
curves, and contours are often basic elements for stereopsis,
calibration, motion analysis and recognition, are detected from chains
of edges points.

The edges detection bottleneck is to locate the edges generated by the
scene elements and not by the noise. The trade off is to suppress the
noise as much as possible, without destroying the true edges.

Corners can be characterized more easily than edges in mathematical
terms, but do not correspond necessarily to any geometry entities of
the observed scene. These features can be interpreted as corners, but
not only in the sense of intersections of the image lines. They capture
corners in patterns of intensities. Such features are stable across
sequences of images, and are therefore interesting to track objects
across sequences.

A corner is identified by two strong edges, feature points include high
contrast image corners and junctions generated by the intersection of
objects contours, but also corners of the local intensity pattern not
corresponding to obvious scene features.

In general terms, at corners points the intensity surface has two wellpronounced
and distinctive directions.

Many objects, especially man-made, can be conveniently described in
terms of shape and position, of the surfaces they are made of. Surface
based descriptions are used for classification, pose estimation and
reverse engineering, and are omnipresent in computer graphics.
The solution for several computer problems involving 3D models is
simpler when using 3D features than 2-D features, as image
formation must be taken into account for the latter.
To solve the problem its needed two tools: a dictionary of shape
classes and a algorithm determining which shape class approximates
best the surface at each pixel.

Lines and curves are important features because they define the
contours of objects in the image.
Lines extraction is difficult because of pixelization and errors
introduced by image acquisition and edge detection, there is no line
going exactly through all the points, it has to be found the best
compromise for line.

In Lines and Curves extraction there are problems to overcame. Which
image points in the image compose each instance of the Line or Curve,
and given a set of image points probably belonging to a single
instance of the target Line or Curve, find the best Line or Curve
interpolating the points.

Another problem that arises is due to accidental positioning two 3D
Lines or Curves far apart from each other project onto close image
Lines or Curves.

In 3D computer vision feature extraction is an intermediate step, not
the goal of the system. We do not extract features, just to obtain
features representations, we extract deatures to navigate robots, to
decide whether an image contain a certain object, to calibrate
cameras, etc.

Correspondence consists in determining which item in the left camera
corresponds to the item in the right camera. A rather subtle difficult is
that some arts of the scene are visible by one camera only. Therefore,
a stereo system must also be able to determine the image parts that
should not be matched.

The correspondence problem involves two decisions: which image
element to match and which similarity measure to adopt.

The correspondence algorithms can be classified in two classes,
correlation-based and feature-based methods. The correlation-based
methods apply to the totally image points, while feature-based
methods attempt to establish a correspondence between sparse sets of
image features.

In correspondence correlation-based methods, the elements to match
are image windows of fixed size, and the similarity criterion is a
measure of the correlation between windows in the to images. The
corresponding element is given by the window that maximizes the
similarity within a search region.

In correspondence feature-based method, there is correspondence
search restricted for a sparse set of features. Instead of windows, they
use numerical and symbolic properties of features available from
feature descriptors. Instead of correlation like measures, they use a
measure of the distance between feature descriptors.
Unfortunately there is no correspondence method giving optimal
results under all possible circumstances. Choosing the method
depends on factors like the application, the available hardware, or
software requirements.

Correlation-based method is easier to implement and provide dense
disparity maps for the purpose of reconstructing surfaces. They need
textured images to work well. However, due to foreshortening effects
and change in illumination direction, they are inadequate for
matching image pairs taken from very different viewpoints. Also, the
interpolation necessary to refine correspondences from pixel to
subpixel precision can make correlation-based matching quite
expensive.

Feature-based methods are suitable when a priori information is
available about the scene, so that optimal feature can be used. A
typical example is the case of indoor scenes, which usually contain
many straight lines but rather untextured surfaces. Feature-based
algorithms can also prove faster than correlation-based ones, but any
comparison of specific algorithms must take into account the cost of
producing the feature descriptors. The sparse disparity maps
generated by these methods may look inferior to the dense maps of
correlation-based matching, but in some applications (e.g., visual
navigation) they may well be all you need in order to perform the
required tasks successfully. Another advantage of feature-based
techniques is that they are relatively insensitive to illumination
changes and highlights.

If the geometry of the stereo system is known, the disparity map can
be converted to a 3D map of the viewed scene that is a 3D
reconstruction. Our 3D perception of the world is due to the
interpretation that the brain gives of the computed difference in the
retinal position, named disparity, between items. The disparities of all
the image points form the disparity map, which can be displayed as
an image.

So given a number of corresponding parts of the left and right image,
and possibly information on the geometry of the stereo system, it is
possible to obtain the 3D location and structure of the observed
objects.

Reconstruction up to a scale factor is when only intrinsic parameters
of both cameras are known, extrinsic parameters as well as the 3D
structure of the scene are derived. Unlike triangulation, in which the
geometry of the stereo system was fully known, the solution cannot
rely on sufficient information to locate 3D points unambiguously.
Since the baseline of the system is unknown it isn’t possible to recover
the true scale of the viewed scene. Consequently, the reconstruction is
unique only up to an unknown scaling factor.This factor can be
determined if we know the distance between to points in the observed
scene.

Reconstruction up to a projective transformation is a 3D
reconstruction even in the absence of any information on the intrinsic
and extrinsic parameters. This reconstruction is unique only up to
unknown projective transformation of the world. It is worth noticing
that, if no estimates of intrinsic and extrinsic parameters are available
and non-linear deformations can be neglected the accuracy of the
reconstruction is only affected by the algorithms computing the
disparities, not by calibration.

A real-time application is one that can respond in a predictable, timely
way to external events. Real-time system requirements are typically
classified as hard or soft real-time. For a hard real-time system,
events must be handled predictably in all cases; a late response can
cause a catastrophic failure. For a soft real-time system, not all
events must be handled predictably; some late responses are
tolerated. For many real-time computer vision applications, a soft
real-time system is sufficient. For example, in a real-time gesture
recognition system, it may be tolerable to occasionally or
systematically drop video frames, as long as the system is designed to
robustly handle frame drops.

For fast tracking systems a drop video frame is not tolerable.
Real time vision is interested the visual information that can be
extracted from spatial and temporal changes occurring in an image
sequence.

The temporal dimension in visual processing is very important, the
apparent motion of objects onto the image is a strong visual cue for
understanding structure and 3D motion.

In order to acquire fast image sequences it is necessary a frame
grabber capable of storing frames at fast rate. If the grabbing rate is
allowed to be chosen, then it should be fast enough to guarantee that
the discrete sequence is a representative sampling of the continuous
image evolving over time. In order to process the images inside the
time rate fast algorithms must be implemented. In stereopsis it should
be taken into account the image grabbing time, to synchronize the
cameras.

The architecture of the robot software is an open architecture.
An open robot architecture allows the system to be connected easily to
devices and programs made by researchers and manufacturers. A
system with a closed architecture, on the other hand, is one whose
design is proprietary, making it difficult to connect the system to other
systems.

Sun workstations are used for software development, control
engineering, as well as for robot operator interaction. The robots IRB-6
and IRB-2000 are controlled from VME-based embedded computers.
Signals from the internal sensors of the robot to the VME system go
via the sensor interface to the DSP board connected to the VME bus.

The robot characteristics of the IRB-2000: it has 6 DOF (Degrees of
freedom) and a precision of 0.1 mm. Joint 1, 4, 6 are cylindrical joints
and the others three joints 2, 3, 5 are revolute joints. This robot can
reach all the points in the possible work area with arbitrary
orientation.

The IRB-6 is a robot with five degrees of freedom and a precision of 0.2
mm. The joints are connected with six links, the joints 1 and 5 are
cylindrical and the others (2,3,4) are revolute joints.
The control system used in RobotLab is composed in three modules:
IgripServer, Trajec and Regul. This control system is for both robots
and only it is different in Cartesian coordinate system because they
have different degree of freedom, Figure 3.1.

Matcomm is software that was developed in Department of Automatic
Control. This software works with protocol TCP/IP to transmit data
between computers in network and with different systems. The reason
to use Matcomm is because it is a easy and fast way to connect with
another machine without necessity of the setting all the parameters.
Matcomm can be use in Unix and Windows environment. Data are
sent using sockets in array format.

In this experiment, we address the problem of visually guiding and
controlling a robot in projective 3D-space using stereo vision. The
experimental setup is composed by one stereo rig mounted on the
end-effector of IRB-6 robot, and two cross drawings. One in the object
and the other in the end-effector of the IRB-2000 robot.
The method is entirely formulated using comparison of the disparity
between the cameras and applying it to the lattice built in the
calibration movement.

The IRB-6 was used to position the cameras in a position where the
two crosses were in the image area covered by both cameras.
The IRB-2000 is used to position the cross centre, mounted in the
end-effector, right above the cross centre located in the object.
This was performed in real time, and the control feedback minimizes
the error computed by the depth, x and y difference between the
crosses centre.

The experiment is done with the object stopped and with the object in
motion.

In this stage the goal is to find the center of the cross. It is important
to exclude feature points that aren’t included in the cross.
The x and y position stored for each feature point is now used to build
a relation between every point and its closest neighbor, this is done by
calculating the euclidean distance to a every feature point with no
closest neighbor already associated.
The feature point with the minimum distance associated is store to an
array of closest neighbors.

The reason why almost all vectors have their direction to the right or
to the bottom is because the feature points were stored from left to the
right, top to the bottom when the LineScan was made.
Now we have all points with a vector associated, but we still don’t
know which vector belongs or not to the cross.

In order to get the cross, several filters were implemented, the first one
was based on the average length of the vectors.

As it is possible to see from the Figure 3.8 the vectors that belong to
the cross have a shorter length than the others do. Since the vectors
belonging to the cross have a bigger rate of occurrences than the
others do, if an average is computed, the value of the average will be
close to the size of vectors belonging to the cross. This is used as a
term of comparison to exclude more points.

It must be taken into account that several aberration vectors will ruin
the average, because they are thousands of times bigger than the
vectors that belong to the cross. In order to avoid the bad effect of the
aberration vectors a previous filter is superimposed, it takes the
previous average and excludes large vectors. This filter introduces a
problem by using old averages, but still it is very useful if the first
measure could be rejected.

In order to exclude more points that could be noise or points that
aren’t noise but don’t belong to the cross, a direction filter is required.
The direction filtering takes into account that all the vectors have an
angle associated, which can vary from 0 to 90 degrees. This is very
important to state, because other ranges must have different
approaches. The angle of a vector is taken by the x and y size of the
vector and then clustered to the first quadrant.

All the points included on the chain, (i.e. all the points associated with
vectors included on the chain) were stored to an array, where they
were clustered to the respective chain.
It is important to notice that all relevant information related to each
chain is saved to a database of connected vectors, such as feature
points included in the chain, the total number of points included in
the chain, the start/end position of the chain.

At this stage the information stored in the database is enough to
estimate the center of the cross, but still before finding the cross
center the connected vectors have to be classified as Horizontal Line
(HL) or Vertical Line (VL). Finding the cross center is then a question
of solving the intersection of two lines.

The group of CV that belong to HL are defined as the ones that have a
longer x than y length. The inclusion on the HL is not straightforward,
there is a filtering process in between. For instance the x versus y
length can satisfy the requirements but there are situations in which
it shouldn’t be included in that set.

The first Connected Vector CV1 has a similar angle to CV2 but is
obvious that it shouldn’t belong to the same set as CV1. The two CV
are parallel and the way to exclude from a set and include in another
set, no longer can be their angle. From CV1 a normal vector is traced
until it intersects the line that includes CV2, the length d1 is the
resulted prolongation of the normal vector.
The length of the normal vector is a relevant variable, due to its
importance the normal vector length is used in grouping the CV in
different sets.

In Figure 3.11 the previous fact can be stated, CV1 and CV3 are
included in the same set, but CV2 is not included, CV4 is obviously
not included in none of the sets for the HL.
The CV4 is a vector in which the x length component is smaller than
the y so this one will be clustered in one of VL sets.
After fitting every CV in the correct group and line (HL or VL), it is time
to assign the correct set as function of the line. This is done in a very
simple way, the set with greater amount of feature points associated
to the CVs belonging to that set; is the chosen one.

To fill the lattice it is necessary to know which feature point in the left
camera corresponds to the feature point in the right camera
A better accuracy of the lattice means as many feature point as
possible in the image area covered by both cameras.
The centre of the cross as already been found and it is possible to use
it as feedback in order to center robot above the centre of the plate,
which means more feature point as desired. It is important to state
that the centre of the cross is also the center of the plate.
Before finding the correspondence of feature point, it is important to
group them for further processing.

The grouping of the feature point was performed by override all feature
point and fit them into a parallel lines to the VL, the HL parallel lines
weren’t suitable to cluster feature point. It was used VL parallel lines
to cluster feature point because the real lines made by the holes in
plate, were still parallel in the projective image plane, but the same
didn’t happen with the horizontal lines.

The horizontal lines when projected to the image plane suffer a tilting
effect, and that makes them harder to use as a method for clustering
feature point.

The image was override by a parallel line to the VL belonging to the
cross and the feature point were inserted into groups, first line groups
and further into left line groups and right line groups, this is VL
parallel (VLP) that were situated to the left or right side of the VL
belonging to the cross (VLC).

The VPL to each side were numbered increasing from their distance to
the VLC, this is first VLP to the left side was the immediate line to the
left side and so on to left border of the image. The same numbering
process was used for the right side VLP.

In order to have a easier correspondence, the feature point were again
clustered into two new set inside the VPL left and right. The feature
point above the HLC were grouped into a new set, HCL up and the
feature point below the HLC were grouped into a new set, HCL down,
and they numbering followed the assumption that more far from the
HL more high would be their index.

The vision algorithms in the investigation are tested with a simulation
of the robot IRB-6. The main feature of this robot is the image shown
from the end-effector, that is a view of the plane perpendicular to the
Z-axis of the gripper. The virtual image is a view of the camera
mounted in the end-effector, representing the visual sensor that
performs the image acquisition to feedback the robot controller.

cube centre. The scan starts on the first row and column and scans
from the upper left corner to the right bottom corner. The image
background is black and the cube colors are sharp. For example to
find a blue color a scan is done in blue matrix searching for a blue
component higher than 240 and in red and green component with
value below 10. These reference values are chosen because it is a 8
bit color range. To find blue, the values should be R=0, G=0 and
B=255.

The control of the IRB-6 and IRB-2000 was done in two main fields,
the kinematics control and the controllers implemented.

The kinematics control was done mainly in inverse kinematics. This
means that is given the end point of the structure, in coordinates, to
calculate the joint angles to achieve that end point.

There were implemented different solutions for the controllers to find a
good relation between fast response, to reach the desired point and a
good stability. The controllers tested were proportional and
proportional with integral part.

The system architecture of the robot control visual feedback can be
seen in Figure 3.33. This system is built in four main modules: robots,
robot system, robot control and vision.

The robot control module and the vision module that we built are
connected with the robots and robot system module, forming the close
loop that we will describe. These units receive, processes and send the
control information that takes some time delays and errors. These are
observed and measure to get the sources error.

The transfer information between the different modules takes some
time. This time is not constant and depends of the network load,
electrical disturbances and data amount.

The information processing takes some time too. In general depends of
the amount of image, the way to process it, such as, if it is recursive
or how many times is process it, the image area, the points extracted,
and the different parameters of the analysis in the same time.
Another delay is added when the image is printed on the screen that
means time spent in on plotting to the desktop

The acquiring camera images delays the system because the cameras
are not synchronised and then the data acquiring can have different
timings.

The aim in this control is to make the movement of the robot fast,
smooth and stable as possible in a way to realize the desired
trajectory.

A flow chart for the virtual robot feedback loop is showed in Figure
3.36. The loop starts in the virtual robot and it sends images all the
time. This image is a view from the end-effector camera.

The image is processed in Visual C++ in the Windows environment,
where it is received and processed. This function is explained in more
detail in the Virtual Java Robot Image Processing section. It returns
the Coordinates X and Y to give the path that the robot should move
for.

The Cartesian-space coordinates have to be converted to joint angles
by application of the IRB-6 inverse kinematics giving the position and
orientation of the end-effector of the robot. After that the joint values
are sent to the robot to close the feedback loop.

The user can interact with the robot by dragging the cube to another
position and which activates the control system and its feedback .
The feedback is provided from the cameras, so the inputs are x, y in
image space coordinates and the outputs are the joint angles.
It is assumed that there is a perfect relation between the image
Coordinates and the robot Coordinates. This means that one pixel
correspond one 1 mm in the robot coordinates

Two platforms are used in the investigation, a Windows NT platform
and a Solaris platform.

The reason for choosing Windows is that the Fire-i API was only
available for this platform. Windows has a large number of hardware
peripherals available (with supported drivers), particularly for frame
grabbers (essential for real-time vision applications).
Windows also provides source software development tools that aren’t
available on Linux.

Some reasons for choosing Windows NT over the more popular
Windows 98 are: Windows NT has a better architecture for real-time
applications than Windows 98; Windows NT is more robust than
Windows 98.

A problem found in this experiment was that of different orientations
between the robot coordinate system and the cameras system. A first
task was to align the system coordinates of systems, robot and stereo
rig x and y coordinates.

This experiment starts to use a proportional controller with different
gains in each coordinate, Figure A.3. The step response of the closed
loop system is shown in Figure 4.2. The figure shows clearly that there
is a steady state error. The error decreases when the controller gain is
increased, but the system then becomes oscillatory.

The positioning movement is made with the two robots, IRB 6 and IRB
2000. The IRB 6 put the stereo rig in a pre-defined position to have a
full vision of the two crosses. The IRB 2000 starts the control in start
position pre-defined also.

This chapter introduces the theoretical background of this thesis and reviews the state-of-
the-art in colour based skin detection and modelling. Since physics-based approaches will
be used in this thesis an introduction to physics-based vision and the image formation
process is given in section 2.1 including the involved components: light source, re°ection,
and sensor. The optical properties of human skin are described and several approaches to
model skin re°ectance are presented in section 2.2. The state-of-the-art in colour based
skin modelling and detection for computer vision is then reviewed in section 2.3 focusing
on work that considers the problem of unconstrained environments with changing illumi-
nation, e.g., for real-time tracking of faces and hands. Optical skin detection methods
that use other than standard RGB are brie°y reviewed in section 2.4. Finally, section 2.5
summarises this chapter and gives some concluding remarks on the current state-of-the-art
in this field.

Computer vision aims at analysing images, detecting and determining the objects in the
images as well as their positions, and doing higher level scene interpretation. E.g., a robot
interacting with humans may have to ¯nd faces and do an interpretation of their facial
expressions. Traditionally the computer vision based image understanding problem was
divided into two phases: low-level segmentation with feature extraction, and a higher-level
reasoning phase relating the image features to object features that are described in object
models of the scene [86; 115]. The low-level segmentation has often been considered to be
a statistical image processing problem determining statistically signi¯cant changes of pixel
values under the presence of noise and under the assumption that such signi¯cant changes
correspond to the boundaries of objects. In other words, the image acquisition and the
structure of the optical energy in the scene were not considered. Since the mid-1980's a
part of the computer vision community began to analyse images considering the laws of
optics including the process of illumination, re°ection, and sensing, and this approach was
called physics-based vision [86]. The goal was to model the behaviour of light, starting
from the light sources, travelling through the scene, interacting with di®erent objects, and
¯nally reaching the camera, as illustrated in ¯gure 2.1. This involves, e.g., the spectral
composition and re°ectance of the light sources and surfaces, respectively, their positions
and orientations, the optical roughness of the surfaces, and camera characteristics.

The image formation process within a colour video camera can be modelled by spectral
integration. Knowing the spectrum of the incoming light, the spectral characteristics of
the camera's sensing elements, and the spectral transmittance of ¯lters in front of the
sensing elements one can model, e.g., the red, green, and blue (RGB) pixel values of the
camera's output. The incoming light is a result of the light source properties and the
characteristics of the re°ecting materials. The following subsections explain and model
the above mentioned process and discuss the illumination of everyday scenarios.

The modelling of light re°ection has been addressed in many areas, among those com-
puter graphics and computer vision. Computer vision methods, e.g., may derive 3D shape
information from a 2D image using a re°ection model and knowing the illumination, or
they may estimate the scene illumination including its colour using a 3D model and a
re°ection model. Computer graphics aims at realistic rendering of scenes using 3D mod-
els and re°ection models. Modelling the re°ection and illumination conditions for scene
interpretation and rendering, respectively, is usually divided into two di®erent processes,
called global and local illumination [179]. Global illumination tries to collect the contri-
butions of all parts of the environment which are illuminating a given point of the scene,
whilst local illumination computes the transformation that occurs at this re°ecting point
between incoming and outcoming light, i.e., the re°ection. The latter will be focused on
in this subsection.

The reflections of dielectric non-homogeneous mate-
rials may be modelled by the Dichromatic Reflection Model [183], which describes the
reflected radiance or light L as an additive mixture of the light LS reflected at the mate-
rial's surface (interface or surface re°ection) and the light LB reflected from the material's
body

The light, which is not re°ected at the surface, penetrates into the material body where it
is scattered and selectively absorbed at wavelengths that are characteristic of the material.
Some fraction of the light arrives again at the surface and exits the material (figure 2.4).
The body reflection provides the characteristic colour of the material.

This section has introduced the basics behind the image formation process modelling light
sources and reflections. Light sources were mainly described by their spectral composition
and direction. When viewed by an RGB colour camera they may be approximated with
Blackbody radiators. Reflections may be modelled as a linear combination of surface and
body reflections. Surface reflections have approximately the same spectral composition
as the light sources whereas body reflections will change the spectral composition, except
for materials with constant spectral reflectance. Materials with different releectance will
result in different colours when viewed by a camera. This is very convenient and used
in human and computer vision to distinguish between objects based on their colour for
example between a green and a red pepper.

The optical analysis of human skin as well as its modelling are important in areas as
diverse as medical applications, cosmetic development, computer graphics, and computer
vision. Skin is, unlike most other organs, directly visible, which allows the dermatolo-
gist often to diagnose and follow its condition simply by visual inspection. Changes in
skin colour are often due to abnormal structures or depositions of pigmented substances
within the skin, thus, understanding of the optical properties of skin helps explaining such
changes and can be useful in diagnosis of skin disease [8]. An increasing number of devices
based on the interaction of visible and non-visible radiation with tissue are employed in
medical diagnostic and therapeutic procedures like laser surgery [180]. A quantitative
understanding of the optics of skin yields knowledge of the optical radiation doses received
This chapter, as well as the next one, covers the classical problem of image segmentation:
given an image, the task is to segment it into regions that “belong” together. Although at
this abstract level the task is easy to understand, it is actually very hard to formalize.
The difficulty here is that it is not at all clear how to formalize the notion of “belonging
together” in mathematical terms. Humans seem to have a clear intuition of this notion, but
it is hard to grasp and at least to some extent it relies on exemplar-based learning. The
integration of such prior knowledge is deferred to the next chapter.

The present chapter deals with the fully automated (or unsupervised) case where apart
from the image no other user input is given. We will start with an overview of the two
major approaches: the region-based and the edge-based approach. For the latter, the notion
of curvature will be of importance as it allows to deal with missing edges. This notion and
its appearance in computer vision is then reviewed separately before we turn to the major
contribution of this chapter: a contour-based approach which combines edge consistency and
curvature regularity into a ratio functional.

Above we have met two region-based data terms, where the first assumed that the intensities
in a segment vary gradually and the second that they are all in the vicinity of some given
mean value. This latter term can be interpreted as the negative logarithm of a Gaussian
probability density with variance one.
It turns out that any other kind of probability density can be used [215]. Examples include
a Gaussian density where the variance is a free parameter, a mixture of Gaussians or a nonparametric
Parzen density [167]. These densities can also be formulated in higher dimensions,
which allows to include color images and texture information.
The specific forms (or parameters) of these densities are estimated in alternation with the
segmentation.

The above given edge-based criteria have traditionally been minimized locally using gradient
descent. Both explicit [123] and implicit [36] curve evolution methods have been used.
In this field also global minimization methods are known and in fact they appeared quite
early: in 1990 Amini et al. [5] showed how to optimize a modified snakes functional via
distance calculations in graphs. Yet, their method has very high run-time demands and was
therefore only applied to rather small search spaces. The method proposed in this chapter
uses an algorithm proposed by Lawler [141] that allows large search areas in practice and was
introduced to computer vision by Jermyn and Ishikawa [117]. The latter work is discussed in
the section on hybrid methods below. Details on the optimization algorithm are presented in
conjunction with the novelty later on in this chapter.

Many more edge-based methods have been proposed. One line of work is based on force fields,
where an early work is given by the generalized gradient vector flow of Xu and Prince [211].
Jalba et al. [115] mimic the notion of electrostatic forces, Xie and Mirmehdi [210] instead
use magnetostatic forces. The arising functionals are minimized via curve evolution and the
results are often comparable to those produced by region-based methods.
Another line of work addresses ratio functionals. Many of these works can also integrate
region terms and are discussed in the subsequent section. A purely edge-based approach is
given by the normalized cuts of Shi and Malik [191], which extends to an arbitrary number
of segments. The underlying optimization problem is NP-hard. Using relaxation-techniques
a solution is obtained that does not depend on initialization.

This optimization task therefore favors curves that coincide both with the location and the
orientation of image edges. Since the absolute is outside the integral it also requires that the
enclosed foreground area is consistently lighter or consistently darker than the background.
That is why the left image in Figure 2.4 is segmented correctly. In contrast, the right image is
not treated as one would desire: here the foreground is partially lighter and partially darker
than the background. The numerator terms will then cancel out and the result is near zero.

The contribution of this chapter is the extension of the class (2.7) of globally optimizable
ratio functionals to include curvature regularity. The proposed method is very general as it
allows almost arbitrary functional dependences of the integrands on location, tangent angle
and curvature.
This is achieved by combining the graph-based ratio minimization algorithm used in [117]
with the product graph used for curvature. Ways to efficiently handle the rather large graphs
are presented.
It will be shown that the proposed method gives rise to substantially larger regions than
the average outward flux (2.9): curvature is an important prior to deal with gap closure. The
method also compares favorably to region-based methods.
Lastly, the proposed method can be used to find parameter sets where the parameterizationinvariant
re-formulation (2.6) of the snakes functional has a meaningful global optimum. In
fact, such an optimum is output simultaneously.
This is joint work with Simon Masnou and Daniel Cremers. A short version was published
in [182], an extended version has been submitted to a journal.

Hence, by regularly checking the parent graph one is able to detect if negative cycles are
present. It also allows to extract such a cycle, which is necessary to update the ratio.
While the basic algorithm in Figure 2.7 must be carried out sequentially, the negative cycle
detection in Figure 2.8 allows a lot of freedom for the implementation. We now discuss how
to efficiently implement negative cycle detection, both in a sequential and in a parallel way.
The key for efficiency lies in how to implement step 2 in Figure 2.8. Concerning the numerical
implementation we noticed that both double precision and integer optimization lead to the
global optimum. We use integer operations since they guarantee global optimality and double
precision became only recently available on graphic processing units (GPUs), which are used
as parallel platforms.

State-of-the-art graphics hardware allows highly parallel implementations of a certain class
of algorithms. This class does not contain the queue-based implementation just described.
However, in the form given in Figure 2.8, step 2 can be implemented in parallel. To this end
one uses two buffers of distance labels, where the second is updated based on the first one.
The nodes can then be processed in parallel.
Distances and parent pointers are stored in matrices as opposed to list structures. The
cycle check is done on the CPU every 25 iterations, its computational cost (including memory
transfer between GPU and CPU) is negligible in practice.

For the Moore-Bellman-Ford algorithm for distance calculation (Fig. 2.8) a root node must be
fixed. Since the described graph is connected, the choice of this root node does not affect the
optimality property of the ratio optimization process. Yet, it can have significant influence
on the performance.
For the parallel implementation it is useful to add an extra root node and connect it to
every node by an edge weighted with 0. Equivalently, all distance labels can be initialized
with 0. After k iterations the distance label of any node contains the cost of the cheapest
path of length k passing through it. While in theory one can still have |V| iterations until
a negative cycle arises, in practice one can expect a number of iterations in the order of the
length of the most negative cycle in the graph.
This choice of the root node could be used for the sequential implementation as well.
However, this would imply a high memory consumption since initially every node in the
graph is added to the queue. One will also have to visit every node in the graph at least once,
which reduces the efficiency of the queue-based implementation in practice. As a consequence
we pick a node inside the graph as the root node. For the first negative cycle detection we
choose a node in the center of the image. In subsequent calls the root node is selected as one
of the nodes in the previously found cycle.

In Section 2.8 it was shown that Lawler’s algorithm minimizes a sequence of line integrals
where a weighted version of the denominator is subtracted from the numerator. For the final
line integral a global minimizer with energy 0 is found.
This implies that minimizing a ratio allows to find a parameter setting where the associated
line energy has a meaningful global optimum. Precisely this observation was made by Jermyn
and Ishikawa in [117].

Due to computational issues (i.e. to be able to use integer optimization without causing
overflows) we will state results for the snakes ratio with absolute image gradient (p = 1) only.
The results with squared absolutes are unlikely to be better as places with low contrast are
even more strongly disfavored than before.

This chapter has presented an edge-based approach for fully unsupervised image segmentation.
It combines a product graph for minimizing curvature-based functionals with a model that
guarantees meaningful global optima and an efficient algorithm to compute these optima. In
practice it is fast enough to work also on large images.
For moderately complex images the method is able to identify objects and due to its edgebased
nature it excels in cases where region-based methods get confused. In these cases one
even observes a strong robustness to the weight of length regularity in the objective function.
It should be clarified, however, that region-based methods are applicable to a larger class
of problems. In particular, they extend to an arbitrary number of segments, whereas the
presented method can only find one region. Hence, for more complex images we recommend
the use of region-based approaches.
Still, a significant advantage of the presented contour-based approach is that it readily
extends to the integration of shape knowledge while preserving global optimality – and with
it the independence of initialization. Precisely this is the topic of the next chapter.

In the previous chapter we have dealt with the task of fully unsupervised image segmentation.
We have presented a contour-based approach which integrates curvature regularity and hence
establishes gap closure.
In practice fully unsupervised methods are far from satisfactory: so far it has not been
possible to capture the notions of human scene interpretation in a mathematically precise
way. Moreover, humans heavily rely on prior knowledge: they actually interpret images
instead of grouping brightnesses.
Consequently, this chapter deals with the integration of shape knowledge into segmentation
processes. The program will be given the outline (or contour / silhouette) of an object. The
task is then to locate a – possibly deformed – equivalent of this contour in the image. This is
achieved with translation invariance, i.e. the method does not rely on an initialization.
Many methods require a set of training shapes to accurately model the deformation processes.
In this chapter it will be shown that actually a single shape suffices. It presents two
shape measures, the basic one allowing the contour to locally stretch and shrink and also to
rotate globally. The refined model then also incorporates strong deformations by allowing
parts of the shape to rotate locally.
The chapter starts with an overview of related work, then proceeds to the presentation of
the basic method. Subsequently, extensions to the tracking of objects and the handling of
highly deformable shape measures are presented.

State-of-the-art methods in computer vision allow to incorporate various levels of prior knowledge.
High level methods introduce 3D-object models such as stick figures to model humans.
The associated problem is known as pose estimation and is not covered in this work.
In a simplified setting one does not care about the position of parts in 3D-space but only
about their location in the image. These models are known as pictorial structures and date
back to the work of Fischler and Elschlager [86]. They require a decomposition of the shape
into meaningful parts. Recent publications include [84, 174].
This chapter focuses on methods that do not require an object model. The addressed
methods are based on planar shapes which is abbreviated to “shapes” in the following. As
illustrated in Figure 3.1, a (planar) shape is a binary image where the black parts indicate
points that belong to the shape. There can be multiple disconnected regions and regions with
holes.

This field is divided into methods that work with a single shape and those that require a
set of training shapes.
Methods requiring training shapes estimate the deformation characteristics of the shapes
from the training data. This field was started by the seminal work of Grenander et al. [100]
and refined by Cootes and Taylor [46]. Cremers et al. [56] model shapes via explicit contours
and model the deformations of the control points. More recent works are based on the level
set method, including works of Leventon et al. [144], Tsai et al. [198], Rousson and Paragios
[177] and Cremers et al. [52]. Recently Cremers et al. [53] proposed a solution based on
probability functions. Subsequently Lempitsky et al. [142] proposed a related method which
requires substantially more training data.
All of these methods rely on an area-based shape similarity measure which needs to preestimate
the deformation characteristics of the shape from the training shapes.
In contrast, a few methods exist which incorporate sophisticated shape measures based on
point correspondences and work with a single shape template. So far all methods are limited
to simply-connected shapes, i.e. shapes without holes.
Felzenszwalb [83] models the shape via its Delaunay triangulation and allows deformations
of the triangles. This allows to combine region-based deformation terms with edge-based data
terms. The method is globally optimal and hence translation-invariant. In practice it requires
to reduce the search space since otherwise the computational cost would be too high - see the
paragraph on minimizing shape-based energies below.
Coughlan et al. [48] locate an open contour (the outline of a shape) in an image and
establish translation-invariance. This method allows to incorporate a sophisticated elastic
shape similarity measure [148, 153] based on point correspondences and also allows to find
multiple instances of the shape. It differs from all the above mentioned ones in that it does
not perform segmentation – this is not possible with open contours. In [47] Coughlan and
Ferreira give an extension to contours with holes.

The main contribution of this chapter is an efficient procedure to handle closed contours for
the problem of image segmentation with elastic shape priors. This problem is handled in a
globally optimal manner. The memory consumption of the method is linear in the number of
pixels and although the running time is a high order polynomial in the worst case, for practical
problem instances a linear dependence is observed. The combination of all these properties
allows the treatment of the entire image information during the optimization process. That
is, there is no need to reduce the search space before optimization is started.
Compared to the work of Coughlan et al. [48] there are three contributions: firstly, the
proposed method addresses the computationally much harder case of closed contours, which
is solved in a computational time that is effectively linear in the number of pixels and also
linear in the number of template points. Secondly, the presented model is a ratio functional
and therefore exhibits a lower bias towards short curves. Thirdly, the extension to a highly
deformable shape model is presented.
This is joint work with Daniel Cremers. The contributions were published in [181, 183, 185].
An extended version has been submitted to a journal.

Figure 3.2 gives a rough overview of the presented algorithm: based on a single template only,
the method simultaneously locates a deformed version of a closed input contour in the image
and computes a matching between the two contours. It incorporates translation-invariance
and optionally allows rotational invariance.
The method is based on globally minimizing a ratio functional which incorporates a sophisticated
elastic shape similarity measure [148, 153]. To this end, the problem is first mapped
to optimizing over cycles in a product space spanned by the image and the template. By
discretizing this space into a graph, the problem is ultimately mapped to finding the globally
optimal cycle in a product graph. Such a cycle is found via an efficient combinatorial
algorithm which gives an effectively linear dependence of the running times on the number of
input pixels.

In the previous chapter we have presented a real-time approach to motion segmentation. By
combinedly estimating displacements and segments the algorithm was able to identify the
shape of coherently moving objects in the images.
Still, motion segmentation does not account for the fact that the video is generated by a
single, deforming scene: one cannot obtain a reconstruction of the scene from the algorithm.
In addition, it is assumed that all points remain visible in the next frame. In practice this
assumption is usually violated: objects will occlude each other.
In this chapter we address a topic that resolves both these issues: the task of layer decomposition.
Here the video is modeled as a superposition of a set of moving images, called
layers. These layers represent the scene. In addition occlusion can be handled very naturally
as one deals with a generative model of the video: an occlusion order disambiguates cases
where several layers would be visible in the same position. Before we enter the details, a
survey of the two most closely related lines of works is given.

Wang and Adelson [204] set out with the aim to decompose a sequence into a set of images.
Yet, instead of treating a single model (or energy functional), they perform a multi-step
optimization involving the clustering of non-parametric velocity fields.
Subsequent methods were able to treat a single energy functional: Jojic and Frey [119] introduce
a multi-layer decomposition method with an accurate occlusion model. They model
the video as a real-valued superposition of layer images and solve this via generalized expectation
maximization. Despite convincing results, a limitation of this method is that it neither
includes spatial smoothness nor favors hard decisions to determine which video pixel belongs
to which layer - two important issues to get a true decomposition. These limitations also
apply to the subsequent works of Frey et al. [92] and Williams and Titsias [207] which use
robust penalizers.
Kumar et al. [139] propose a seven-step approach to minimize a model including motion
blur and changes in lighting. This involves a combination of graph cuts and belief propagation.
While from a theoretical point of view this method is rather hard to analyze, it leads to good
results for articulated motion.

The objective of the work presented in this chapter is to develop algorithms to automatically
analyse the features of species’ flight behaviour, and quantifying wing beat frequency
using spectral analysis. Physical characteristics, such as body mass and species,
are known to vary with wing beat frequencies (Bullen and McKenzie, 2002; Norberg and
Norberg, 2012), and may potentially be used for automated classification. The work presented
here uses spectral analysis techniques to quantify wing beat frequencies, using a
single imaging device in low-light. Two modified techniques based on bounding box metrics
were proposed, and similarity matrices, for measuring periodic and cyclical motion
as a 1D time domain signal. These are transformed to the frequency domain using Fast
Fourier Transform (FFT) and the techniques evaluated against the baseline algorithm proposed
by Cutler and Davis Cutler and Davis (2000), using expert-annotated ground-truth
data. Bats were initially used for this study partly because the data was readily available
but also because bat motion is of a higher frequency and so is more challenging for
analysis.

The rest of the Chapter is organised into the following. In section 3.1, a description
of the dataset used for the experiments and a discussion of low-level image processing
techniques for dealing with low-light video data of bats are presented, In section 3.2 the
experimental setup is detailed and results presented in section 3.3 and finally section 3.4
summarises key conclusions.

The proposed method were derived from Cutler and Davis (2000), but with a number
of proposed modifications. An investigation shows that a similarity metric based on silhouette
shape is not as effective with a target which can change orientation arbitrarily.
Therefore, a coarser metric derived from the oriented bounding box fitted to bat’s 2D
object silhouette was proposed to better capture the periodicity of motion (as well as being
computationally less expensive). Further investigation shows that the selection of the
dominant frequency proposed by Cutler and Davis is often inconclusive; therefore, two
techniques were proposed to replace it. The first technique is called the diagonal selection
(DS), which is based on the correlation of the signal with individual components,
reconstructed from the peaks of the signals’ frequency spectra and the second is the selfsimilarity
technique.

For each video, the bats’ silhouettes (Figure 3.1) using the background Gaussian
mixture model proposed by Zivkovic and van der Heijden (2006) was extracted . This
method was used as it’s been shown (Zivkovic and van der Heijden, 2006; Bouwmans
et al., 2008) to be more suitable for real-time processing, whilst at the same time improving
classification accuracy. To detect the connected components, contours were obtained
from the binary image using the contour algorithm proposed by Suzuki et al. (1985). An
oriented bounding box was fitted to each silhouette using Algorithm 1 and a selection of
the bounding box metrics (height, width and hypotenuse) were measured.

To solve the problem of broken silhouettes, contours are merged based on the minimum
perpendicular distances from the four corner points of each minimum fitted rectangle
to the boundaries of the other bounding boxes. This was repeated for all fitted rotated
bounding boxes to merge broken silhouettes (3.1). Broken silhouettes are due to noise in
the video, which in some cases is resolved with the Fast Fourier Transforms (FFT).

For comparison, the bounded box metrics are used to form self-similarity matrices
which are used to compute the FFT. The self-similarity matrices, in this case, are computed
using absolute correlation (3.4). To determine the wing beat frequency, each column
of the similarity matrix is linearly de-trended and a Hanning filter applied. The result is
then used to compute the power spectra for all columns of the self-similarity matrices.
For accuracy, the skewness of each of the fix columns of the similarity matrices was obtained
and either their spectra averaged or median estimated depending on the results of
the skewness in Equation 3.5. The highest peak of P( fk) is then selected to represent the
wing beat frequency.

wo sets of experiments were performed: the bounding box with a diagonal selection and
the bounding box with self-similarity techniques. The results from these were then evaluated
against the baseline algorithm proposed by Cutler and Davis (2000) and preliminary
results using bats data presented. For evaluation the methods in Cutler and Davis were
coded in MATLAB and since their dataset was not available the dataset introduced in this
chapter was used and the results were compared. The sample videos were divided into
64 frames each with an overlap of 32 frames. For example, in sample 2, the video was
divided into three 64 time frames (three windows) with 50% overlap (32 frames overlap).
Therefore, window 2 of sample 2, which is made up of 64 frames, had the first 32 frames
in common with window 1. Three bats experts were used to manually count wingbeat cycles
(to the nearest quarter) from the videos. The results of the ground truth can be found
in Section 3.3 of this chapter. To find out if window size can improve the resolution of the
wing beats, the window size was increased to 128 frames with an overlap of 64 frames
and again, the three bat experts made to manually count the wingbeat cycles (to the nearest
quarter). The results of this have again been reported in Section 3.3. For all these
videos, the median wingbeat cycles were estimated and used to calculate the frequencies
which represent ground truth data. The frequencies produced by the various algorithms
that were closest to this value were then counted and the results reported

Preliminary investigation shows that both improved techniques achieved better results
than the baseline line algorithm by Cutler and Davis (2000) in the estimation of wing
beat frequencies when the bat’s orientation changes. The self-similarity matrices with the
bounding box technique used were also as good as the baseline algorithm when the bat’s
orientation does not change much.

When only one peak is above the threshold value on the frequency plot, all methods
are able to find the correct wingbeat frequency and this situation is considered to be ideal
by the baseline algorithm Cutler and Davis (2000). When there are more than one peak
values above the threshold, the proposed methods are able to find the wingbeats that
closely match the ground truth compared with the baseline algorithm. Finally, when all
peaks are below the threshold, the proposed methods are able to estimate more wingbeats
frequencies which are closer to the ground truth than the baseline algorithm.
Increasing the window size can help improve the frequency resolution, thus improving
the accuracy of the wingbeat frequencies. Two ways of improving the frequency resolution
are increasing the time domain sample and or using spectral interpolation Lyons
(2010). The fast Fourier transform (FFT) was used in Chapter 5 to extract wingbeat frequencies
features for the bird species classification algorithms.

The following chapter will explore the classification of bird species using appearancebased
features and will propose an extended set of appearance features which will be
evaluated with the feature set proposed by Marini et al. (2013).
To extract the number of visitors (V ) from images, a wide range of computer vision
techniques may be considered. In this study, three different concepts, arranged in order
of complexity were examined. First, a change based algorithm utilizing the temporal
variability (section 5.1). As CD returned the number of changed pixels, subsequently
the results needed to be calibrated to reveal V . Then, an object detection pipeline
using shape based features (HOG-Descriptor) was consulted in section 5.2. Its results
claim to be the real number of pedestrians, which consequently had to be assessed
using established accuracy measures. As Cessford et al. (2002) identified a need for
counters delivering specialized management information, last but not least an advanced
object recognition system using artificial intelligence (CNN) was tested, returning a
list of detected objects per image (section 5.3). Also its results were assessed using the
accuracy measures introduced before. For a final comparison, the latter methods were
aggregated according to the change detection. Figure 11 illustrates the research design.
For both test sites, the exact same methods were conducted. Therefore in this chapter,
the methods will only be presented once, while in the results chapter each site is handled
individually.

All the methods used in this study were conducted using the open-source software
R, as it is flexible and platform independent. Where additional software was used,
i.e. OpenCV for HOG and YOLO as a CNN, it was wrapped into R functions to
ensure a consistent interface. To port the tools to other systems, they subsequently
were condensed into a new package, available on GitHub. As it was developed at
the University Würzburg and in alignment with another package for webcam imagery
(phenopix), it will be called wuepix. The packages vignette and documentation can be
found on the appended documentations.

This first method, change detection, analyses the image archive by comparing the
two latest pictures against each other. Therefore, as a first step, the image archive
was ordered chronologically, so that processing could be batched. Within a loop, per
iteration two images got selected. The latest image will further be called It, the other
respectively It−1 served as background. t denotes the timestamps at which the image
was captured.
Methodically the technique was split into three steps. First the images were set off
against each other (differencing / rationing). Then the results got post-processed
(aggregation). Thirdly, the aggregated values were calibrated (regression), whereby the
models accuracy (R²) becomes apparent.

Differencing the images is a classic, low-level image operation, which partly has already
been introduced in the conceptual framework. To preserve the RGB information, it has
deliberately refrained from gray-scaling the images. To apply a multi-spectral change
detection, the spectral layers k 2 1, , 3 were added to the equation (Radke et al., 2005).

As this algorithm rolled through the image archive, Idt turned into Idt−1 in the subsequent
iteration. Like this unfortunately every person got counted twice. Therefore the results
will need to be aggregated and divided by two. On the other hand however, this
double-count also had some advantages. For example, if hypothetically two individuals
with same colored clothes stood at the exact same spot, this would neutralize their
presence in the central iteration. With one iteration before and one after however,
both get recognized at least once. Two aggregation operators were compared. An
initial assumption, that medians robustness to outliers would comply with illumination
changes, had to be revised. Especially for longer integration scales, during calm periods,
it significantly underestimated the visitor numbers. Simply integrating Pt proved to
work best. As aggregation scale intervals of two, six, twenty and sixty minutes were
tested. Their performance will be indicated in the results section.

The second method uses Histogramms of Oriented Gradients to detect pedestrians
(Dalal & Triggs, 2005). The HOG-Descriptor, focuses on a feature class of the
same name. Figure 13 illustrates, how these histograms are computed. The image is
separated into blocks and is further subdivided into individual cells containing some
pixels (e.g. 8x8). A sliding window computes the gradients (direction and amplitude of
increasing color intensity). The directional gradient is unsigned and ranges from 0° to
180° (ibid.). These gradients then are accumulating into local 1-D histograms per cell,
weighted by their amplitude.

Next, a linear SVM classifies these block-wise histograms (Dalal & Triggs, 2005).
This machine learning method seperates the input data into two classes (pedestrian /
no-pedestrian), by arranging the feature space in such a way, that the margins to the
hyperplane are maximized. Hereby the SVM autonomously selects its features (called
support vectors here) (Kotsiantis et al., 2007).

In reality however, it likely returns multiple, overlapping bounding boxes per object.
Therefore the Non-Maximum Suppression (NMS) is a key post-processing step in
computer vision. This technique selects a bounding box with the maximum detection
score and suppresses its neighboring boxes (Bodla et al., 2017; Dollár et al., 2012;
Rothe et al., 2014).

When Dalal & Triggs (2005) trained their method, they started with a wellestablished
pedestrian database of city scenes (MIT) and archived “essentially perfect
results” (Dalal & Triggs, 2005, p. 888). Therefore they initialized a new, more
challenging, dataset INRIA (ibid.), which became an important benchmarking set
(e.g. used by Benenson et al., 2014). It contains 1805 images of humans cropped
from personal photos, 64x128 pixels each (Dalal & Triggs, 2005). As many were
bystanders taken form the photos background, they appear in any orientation and
against a variety of backgrounds (ibid.). In average, the pedestrians height in this
dataset is 96-100 pixels, with an additional, small padding (Dollár et al., 2012).

In this chapter, we present a unified three-layer hierarchical approach for solving tracking
problems in multiple non-overlapping cameras. Given a video and a set of detections
(obtained by any person detector), we first solve within-camera tracking employing
the first two layers of our framework and, then, in the third layer, we solve acrosscamera
tracking by merging tracks of the same person in all cameras in a simultaneous
fashion. To best serve our purpose, a constrained dominant sets clustering (CDSC)
technique, a parametrized version of standard quadratic optimization, is employed to
solve both tracking tasks. The tracking problem is caste as finding constrained dominant
sets from a graph. That is, given a constraint set and a graph, CDSC generates
cluster (or clique), which forms a compact and coherent set that contains a subset of
the constraint set. In addition to having a unified framework that simultaneously solves
within- and across-camera tracking, the third layer helps link broken tracks of the same
person occurring during within-camera tracking. We have tested this approach on a very
large and challenging dataset (namely, MOTchallenge DukeMTMC [128, 129, 145])
and show that the proposed framework outperforms the current state of the art. Even
though the main focus of this work is on multi-target tracking in non-overlapping cameras,
proposed approach can also be applied to solve re-identification problem. Towards
that end, we also have performed experiments on MARS [189], one of the largest and
challenging video-based person re-identification dataset, and have obtained excellent
results. These experiments demonstrate the general applicability of the proposed framework
for non-overlapping across-camera tracking and person re-identification tasks

Object tracking is a challenging computer vision problem and has been one of the
most active research areas for many years. In general, it can be divided in two broad
categories: tracking in single and multiple cameras. Single camera object tracking
associates object detections across frames in a video sequence, so as to generate the
object motion trajectory over time. Multi-camera tracking aims to solve handover
problem from one camera view to another and hence establishes target correspondences
among different cameras, so as to achieve consistent object labelling across
all the camera views. Early multi-camera target tracking research works fall in different
categories as follows. Target tracking with partially overlapping camera views
has been researched extensively during the last decade [4, 20, 44, 81, 103, 159]. Multi
target tracking across multiple cameras with disjoint views has also been researched
in [18, 31, 34, 73, 75, 80, 115, 162]. Approaches for overlapping field of views compute
spatial proximity of tracks in the overlapping area, while approaches for tracking targets
across cameras with disjoint fields of view, leverage appearance cues together with
spatio-temporal information.
Almost all early multi-camera research works try to address only across-camera
tracking problems, assuming that within-camera tracking results for all cameras are
given. Given tracks from each camera, similarity among tracks is computed and target
correspondence across cameras is solved, using the assumption that a track of a target in
one camera view can match with at most one target track in another camera view. Hungarian
algorithm [85] and bipartite graph matching [73] formulations are usually used
to solve this problem. Very recently, however, researchers have argued that assumptions
of cameras having overlapping fields of view and the availability of intra-camera
tracks are unrealistic [162]. Therefore, the work proposed in this chapter addresses the
more realistic problem by solving both within- and across-camera tracking in one joint
framework.
In the rest of this section, we first review the most recent works for single camera
tracking, and then describe the previous related works on multi-camera multi-view
tracking.
Single camera target tracking associates target detections across frames in a video
sequence in order to generate the target motion trajectory over time. Zamir et al. [176]
formulate tracking problem as generalized maximum clique problem (GMCP), where
the relationships between all detections in a temporal window are considered. In [176],
a cost to each clique is assigned and the selected clique maximizes a score function.
Nonetheless, the approach is prone to local optima as it uses greedy local neighbourhood
search. Deghan et al. [43] cast tracking as a generalized maximum multi clique
problem (GMMCP) and follow a joint optimization for all the tracks simultaneously. To
handle outliers and weak-detections associations they introduce dummy nodes. However,
this solution is computationally expensive. In addition, the hard constraint in their
optimization makes the approach impractical for large graphs. Tesfaye et al. [151] consider
all the pairwise relationships between detection responses in a temporal sliding
window, which is used as an input to their optimization based on fully-connected edgeweighted
graph. They formulate tracking as finding dominant set clusters. They follow
a pill-off strategy to enumerate all possible clusters, that is, at each iteration they 
remove the cluster from the graph which results in a change in scale (number of nodes
in a graph) of the original problem. In this chapter, we propose a multiple target tracking
approach, which in contrast to previous works, does not need additional nodes to
handle occlusion nor encounters change in the scale of the problem.

In our formulation, in the first layer, each node in our graph represents a short-tracklet
along a temporal window (typically 15 frames). Applying constrained dominant set
clustering here aim at determining cliques in this graph, which correspond to tracklets.
Likewise, each node in a graph in the second layer represents a tracklet, obtained
from the first layer, and CDSC is applied here to determine cliques, which correspond
to tracks. Finally, in the third layer, nodes in a graph correspond to tracks from different
non-overlapping cameras, obtained from the second layer, and CDSC is applied
to determine cliques, which relate tracks of the same person across non-overlapping
cameras.

Input data are image sequences (or corresponding frames therein) acquired by the top-level block. 
The background subtraction Mixture of Gaussians method (MoG) separates the foreground object, from 
a predicted background such that a person, animal or other labelled object (commonly referred to in 
latter sections as a “class”) eventually presenting a silhouette, after the corresponding morphological 
operations.

A silhouette enters a pre-processing manipulation stage in order to increase robustness of the inferred 
rule-based system as described in Section 3.5.. The said silhouettes generate a dataset that is labelled 
by means of manually setting respective classes. This forms our database compiled of multiple labelled 
silhouettes and for each class. The dataset order is randomly rearranged in order to avoid bias and overfitting 
in the training and testing stages respectively.

The dataset is split in two subsets according to a percentage (holdout); a training set (sample of 60% cases) 
and a testing set (sample of 40% cases). The training set is used to ‘train’ the classification trees, from which 
subsequently a set of rules will be extracted. The actual rule-based system specificity and accuracy measures are 
provided in application of said rules to the test set (unseen data). In Figure 3.1, each of the blocks of the 
process are depicted in more detail

There are many revised and improved [33] MoG methods, statistical methods which introduce updating their 
parameters through an Expectation Maximisation algorithm. The algorithm infers continuous time and three 
colour spaces (RGB), so after the necessary customisation of the code, time has been replaced by samples 
and it is assumed that the source is grayscale.

The images are now separated into a black background and the foreground object of interest in grayscale. The 
image is converted from grayscale to binary image using Otsu’s thresholding method[20]. The foreground object 
is often sub-optimal in terms of consistency as a silhouette, with open boundaries and artefacts. Also, noise 
or inconsistencies in the images before the background subtraction or even after the MoG, could result in false 
foreground objects.

The colouring technique is apparent in Figure 3.4. If the objects are too many, the same colours will reappear 
when far enough from each other. To extract the biggest blobs of the connected components image, the method 
described in Algorithm 3.3 is used.

In order to increase the quality but also the repeatability of the Supervised Training, and subsequently 
the results of the whole algorithm, it is of paramount importance to include a method which manipulates 
the extracted silhouettes in a that the size of the frames, the size of the silhouettes, their orientation 
would not make our decision trees make too many rules and thus result in a case of overfitting.

In order to streamline this procedure and automate it, Theta is calculated automatically. The angle is 
calculated by simulating a right triangle, formed by the coordinates of the uppermost pixel of non-zero value, 
the hypothetical origin (mid of height, mid of width of image) and the y'y axis. Then, the angle formed by the 
imaginary line connecting the uppermost pixel of non-zero value and the origin of the axis is the angle which is 
needed. Assuming the highest point of a human silhouette is the top ot the head, and the lowest is the bottom of 
the feet,if a silhouette does not have its highest non-zero pixel in the hypothetical y’y axis, it needs to be 
rotated by that angle. This is a general rule to ignore posture recognition.

After the silhouettes finished processing, they get categorised in classes manually. Since the video was short 
enough, it was possible for each frame to be compared directly to the original video and hand pick the silhouettes. 
The initial trial had two classes; human and car. The classification method uses the standard C4.5 algorithm to 
induct the trees. The algorithm is presented in Algorithm 3.5, in pseudo-code.

In order to defend the system’s results in Section 3, a number of further trials were deemed essential. Instead 
of only training the Classification Trees with silhouettes extracted from one video, it underwent training using 
silhouettes of multiple videos with different camera settings and backgrounds.

The goal was to introduce the classifier to two additional tests; one by training the model with a larger training 
dataset and applying the new rules to the old testing dataset, and the other changing the test dataset and applying 
the rules inducted by the initial training dataset.

As a first step, the whole methodology discussed in Section 3 was executed again on different videos in order 
to add more samples to the classes Person, Dog, Car. Using different videos but the exact same methodology, the 
classifier got trained with the newly compiled training dataset, the new rules were formed and then tested upon 
the new test dataset. The results were as expected by the performance of the sample video, quite high in terms of 
accuracy, at 95% and a negligible misclassification rate of human class ‘0’. The classification tree is apparently 
more complex and with larger distance due to more classes being trained as in Figure 4.1, due to the assumption of 
binary criteria for best splitting.

Another paper utilising the MuHAVi database for their test, is [1], where a form of voting scheme of different 
camera angles focusing on the same object of interest is proposed for Rank Minimisation, and a Matrix Completion 
for Classification method. Their method is not designed to classify whether the object is human or not, but the 
pose of the humans it detects. Their results are more than promising but fall behind Wu and Jia in [40] on the 
MuHAVi dataset. The usage of HoG, HoF histogram vectors, eliminate the need for any silhouette extraction along 
with the multi-camera setting and their consensus system for rank minimisation (least absolute error for n 
cameras), provide an object-centred classification model which yields better results than this dissertation’s 
method. This is partly to better model construction as well as the computational error minimisation error.

In comparison, multi-view camera settings perform better due to their ability to provide their respective human 
recognition methods and classifiers with much more data than the single-view counterparts, but they require 
extensive set-up and are highly expensive. In Table 3, a comparison of the above methods with the proposed work 
is presented.The Horn and Schunck method uses the OFC equation (2.2) and a regularization term
that solves the aperture problem. Equation (2.3) shows the Horn and Schunck functional.
This energy model uses quadratic functionals in both terms. This assumes that the image
noise and the flow derivatives are expected to follow a Gaussian distribution. A direct
consequence of this kind of functionals is that the method is very sensitive to the presence
of noise and the computed flow fields are very smooth.

The energy model proposed by Nagel-Enkelmann was a great contribution in the field
of optical flow. Their model is similar to Horn-Schunck but it includes an anisotropic
smoothing term. This term varies the direction of diffusion according to the gradient of
the image. In regions where the gradient is small it acts isotropically and in those where
the gradient is high the diffusion is anisotropic along the edges.

Robust functions provide methods that reduce the influence of failures in the input data.
In optical flow, many strategies use robust functions to remove outliers. In 1996, Black and
Anandan [Black96] proposed a method that robustified the OFC and the regularization.
Their idea was to reduce the problems due to violations in the brightness and spatial
smoothness assumptions caused by multiple motions.

The seminal work of Horn and Schunck [Horn81] proposed a basic idea about temporal
coherence. They proposed to compute the flow field between the first two images and
then use this estimate as initial guess for the following frame. This is a simple predicting
technique, that introduces one of the desirable ideas for temporal coherence methods: the
temporal continuity [Black94] of the calculations that should result in less computations.
This idea is exploited later in some other works, where the flow is predicted and updated
according to the observed data.

This is the strategy followed with Kalman filtering approaches like in [Singh91]. In
this work the filter was used to incrementally compute the optical flow. The process is
divided in prediction and update phases: during the prediction phase, the previous flow is
extrapolated to an intermediate state vector and, in the update phase, the measurements
are mixed with the predicted state to create a final state. The method is based on a
correlation approach, so the solutions are expected to be sparse. The Kalman filter allows
to reduce the uncertainty and improve the flow field in subsequent predictions.

The main handicap of using the temporal derivative is the implicit assumption that
the motion field must be continuous. This poses a sort of incongruence with respect to
the nonlinear attachment of the data, which is specially designed for large displacements.
The use of robust functions may mitigate the effect of temporal smoothing, since it avoids
taking into account the outliers from the continuous model. When the temporal derivative
is coupled with the spatial derivatives, it may also mitigate the effect of the spatial
regularization. Isolating the temporal term helps solving this problem, although it is only
effective in the presence of small motions.

Another method to propose a time formulation were Black and Anandan [Black91].
They proposed a temporal coherence constraint based on the assumption of constant
accelerations. The flow field was predicted from the averaged acceleration and the previous
estimation of the optical flow. In the energy model they introduced a new temporal term
based on the attachment of the flow to the estimated flow field that incorporates the
information of acceleration. Nagel [Nagel90b] also proposed a temporal method based on
an extension of his spatial diffusion operator as a 3D diffusion matrix.

More recently, some authors have generalized the use of regularization schemes based
on the flow temporal derivative. This was analyzed in [Weickert01] and later used in
other related approaches, e.g. [Brox04], [Bruhn05b] or [Papenberg06]. In these cases,
the temporal information is coupled with the spatial gradient in the form of a nonquadratic
3D smoothing operator. Empirical results have demonstrated that the temporal
information should have a lower penalty weight than the spatial counterpart. This
is probably due, in part, to the fact that, while the spatial information is continuous
and the gradient can be numerically approximated with enough precision, the temporal
information is not in general continuous, specially in the presence of large displacements.
The use of temporal information has demonstrated to provide important improvements
with respect to the spatial formulation. As stated in [Weickert01] it is also more
robust against noise and produces smoother flow fields. However, it treats the temporal
information in the same way as the spatial one.

One of the firsts to propose a nonlinear temporal coherent model were Salgado and
S´anchez [Salgado06]. In that case, they proposed to separate the temporal constraint
from the spatial regularization and not to use the temporal derivative. They showed that
it has a stabilizing effect on the estimated errors through the sequence. They devised
the need to introduce the backward flow to improve the result in the whole sequence.
Here we show that it is not necessary to explicitly introduce the backward flow in the
energy model since it naturally appears during the minimization process. Additionally,
we propose a nonlinear temporal regularization scheme.

During the years, various optical flow works have proposed the use of color
information in their models. In this sense, Ohta [Ohta89] presented a method that
calculates its solutions directly from an image pixel without imposing any other
assumption [Fennema79, Horn81]. Therefore, the model derived multiple conditions
from the multi-channel information of a single image point. Later, Markandey and
Flinchbaugh [Markandey90] proposed a numerical scheme that uses the visible and the
infra-red spectrum from a multi-spectral image.

Seven years later, Golland and Bruckstein [Golland97] presented two methods that
increase the robustness of their energy models based on the color information. The
first one assumes brightness conservation under motion considering a multi-channel
image as a set of three different grayscale images. The second introduced the idea
of color conservation under the premise that its components (not only the luminance)
are conserved. Their experiments confirmed that, in regions with strong gradient, good
solutions can be obtained, whereas in regions of uniform chrominance these methods failed.
This work used RGB, normalized RGB and HSV images to evaluate their proposals

The seminal work of Horn and Schunck [Horn81] proposed a very basic idea on temporal
coherence. They compute the flow field between the first two images and use this
estimation as initial guess for the following frame. This is a simple predicting technique
that introduces the idea of temporal continuity [Black94] of the calculations that should
result in less computations. This idea is exploited later in several works [Weickert01,
Brox04, Bruhn05b, Papenberg06] where the flow is predicted in the following time instant
and updated with the observed data.

The most widely accepted case is a very simple formulation based on the regularization
of the temporal derivative. This has shown to be effective when the motion field is
continuous, as has been originally demonstrated in [Weickert01]. Although this is a simple
idea, it is not realistic for many optical flow applications.
The main handicap of using the temporal derivative is the implicit assumption that
the motion field must be continuous. This poses a sort of incongruency with respect to
the nonlinear attachment of the data, which is specially designed for large displacements.
When the temporal derivative is coupled with the spatial derivatives, it may also mitigate
the effect of the spatial regularization.

The former contribution is motivated by the results presented in [Salgado06], where
a similar method was proposed based on the Nagel-Enkelmann diffusion operator. The
numerical results showed that the use of a nonlinear temporal formulation of the flow field
provided very good results. That was the first time that such a nonlinear flow assumption
was introduced.

The second contribution introduces a non-continuous flow regularization scheme. In
this case, we propose a new scheme, at the PDE level, which is similar in spirit to the
temporal laplacian. In these two new assumptions, we devise the need to use the backward
flow in order to find appropriate correspondences in previous frames. This is key to
tracking the right motion of the objects in the sequence.

We estimate the backward correspondences once we have previously calculated the
optical flow. In appendix I, we present four algorithms that use directly the forward
optical flow and the image intensities for computing the backward flow. In our temporal
method, we use the second algorithm due to its good results.

In order to recover large displacements, we implement a coarse-to-fine strategy. The
optical flow is initialized at the coarsest scale and incrementally refined in the following
scales. This is a common approach to deal with large displacements in variational optical
flow methods. Another approach, which is very similar in spirit to this technique, is the
use of a linear scale-space focusing strategy, as presented in [Alvarez00].
In the experiments, we observe that the temporal dimension allow finding continuous
flow fields in time and the noise is considerably reduced with respect to the solution given
by the spatial method.

In this chapter, we focus on improving the management of the temporal information
in optical flow methods. The presence of large motions in natural scenes is very common,
so it is necessary to investigate on more realistic temporal models. We propose a new
way to handle the temporal information. In our opinion, this is more congruent than
previous approaches. We claim that the separation between the spatial and temporal
regularizations is necessary. The temporal formulations must also consider the non
continuous nature of optical flow.

Next, we examine the behavior of the temporal models explained in this chapter. In
the first row of figures 3.1 and 3.4, we show the third frames of the small square and
Urban2 sequences, respectively; their true flows, and the best spatial solutions found. In
the second row, we show three temporal solutions: the first for the nonlinear temporal
attachment defined in (FCA, eq. (3.4)); the second, for the nonlinear temporal smoothness
approach defined in (FRS, eq. (3.15)); and, finally, using both temporal terms. In these
experiments, we refer to this combination as Temporal Continuity Optical Flow (TCOF).
The small square is a simple sequence where the object is moved over a textured
background. The square is moving 15 pixels per frame, while the background moves 3
pixels in the same direction. The most important features are its translational motion
and a large displacement. On the other hand, we use the Urban2 sequence from the
Middlebury database due to its greater displacements (of about 20 pixels in some areas)
respect to the other sequences of the dataset.

We show the AAE and EPE errors for every frames of the small squares in graphics of
figures 3.3 and 3.2, respectively. In figure 3.5, we use Urban2 to extend the comparison
by showing motion details for the corresponding spatial and TCOF solutions regarding
to its true flow. Then, we show the solutions for the spatial and TCOF approaches using
the test and evaluation sequences from Middlebury in figures 3.6 and 3.7, respectively.
Finally, we use the sequence of Karl-Wilhelm strasse and a scene extracted from the
movie ’Godfather: Part II’ in order to analyze the behavior of the proposed method in
real world sequences. In figure 3.8, we depict the flow fields obtained with the spatial and
spatio-temporal methods for several frames of the original sequences.
We represent the motion of these two last experiments with the IPOL color scheme
while, the previous ones, are represented with the scheme of Middlebury.

According to the results, the improvement of the temporal methods with respect to
the spatial solution is important. As expected, the spatial method produces higher errors
at the motion discontinuities and, more significantly, at the occlusions. Table 3.1 shows
the average End-point (EPE) and Angular (AAE) errors for the geometric sequence. The
first temporal result, corresponding to the first image in the second row of figure 3.1,
provides an important improvement on the EPE and, more noticeable, on the AAE.
The improvement in accuracy is still more important if we use the nonlinear temporal
smoothing scheme (3.15) or a combination of both.

We observe that the nonlinear temporal smoothing scheme (3.15) behaves better than
the temporal attachment, even at the motion boundaries. The graphics in figure 3.2 show
the EPE for every frame on the square sequence. Frame by frame, the optical flows are
more accurate in the temporal methods. We also observe that the results are very stable,
specially in the middle of the FRS line. Reasonably, the frames at the beginning and at
the end of the sequence present higher errors, due to the Neumann boundary conditions.
The AAE graphic in table 3.3 shows a similar behavior.

Next, we show the results for the test sequences of the Middlebury benchmark
database [Baker07b], except Dimetrodon and Venus. This is because there are not enough
images to use TCOF properly. The numerical results depicted in table 3.2 reflect that the
temporal approach always improves the AAE respect to its spatial counterpart, specially
in Urban2 and Urban3. However, its EPE is worst in Grove2, Urban2 and RubberWhale
but not significantly. On the other hand, figure 3.6 depicts the original image, the true flow
and the flow fields for the two approaches. Here, we observe accuracy in both solutions
without strong differences. The TCOF method was sent to the Middlebury web page for
its evaluation. Figure 3.7 contains the original image, the true flow and our solution. In
general, we observe good results in the motion fields.

The optical flows obtained for the synthetic sequences of Middlebury do not show many
differences between the spatial and temporal solution. This situation is the opposite in the
real sequences depicted in figures 3.8 and 3.9. We observe that the temporal dimension
allows finding continuous flow fields in time and the noise is reduced with respect to the
solution given by the spatial method, specially in the Karl-Wilhelm strasse sequence. We
also observe an improvement in the Godfather scene when the camera begin to move to
the left. For instance, the details of the trees are better preserved in the temporal solution
showed in the third row of figure 3.9. On the other hand, the first car that appears in
the sequence leaves dust in the air when it gets out of the scene. When this happens, the
temporal solution is more consistent than the spatial one. Both figures demonstrate that
TCOF performance is better dealing with the complexity of real sequences.

This type of methods produce piecewise-smooth flow fields. In particular, Brox et al.
proposed a technique that is more robust to outliers due to a continuous L1 function that
creates a TV regularization. It also deals with constant brightness variations by including
a gradient constancy term in its energy functional in conjunction with the brightness
constancy assumption [Horn81].

In our experiments, we first conduct a thorough analysis of the method with grayscale
images from the Middlebury benchmark database. We observe that this kind of methods
creates rounding shapes at the borders of the moving objects, which is a typical
consequence of TV-L1 strategies.

In relation to this, the color information could prevent this problem by associating the
pixel information with various brightness intensities. Furthermore, this type of images
may avoid the use of additional constraints in the flow calculation [Golland97, Ohta89]
and contains more photometric information that can be useful against shadows, shading
and image specularities [Barron02, vdW04, Mileva07]. On the other hand, we observe a
better accuracy of the Spatial approach for the Middlebury sequences in our experiments.
Thus, we introduce the color information in the Spatial version and compare with the
basic approach.

The previous experiments were made with a grayscale optical flow model. However,
a multi-channel approach should provide better results, so we extended extended our
original method into a multichannel scheme. Now, we evaluate the possible benefits of
color against grayscale information in a variational optical flow method. The experiments
are made with some synthetic sequences using RGB and grayscale images from the
Middlebury benchmark database and the MPI-Sintel dataset.
Our interest is to compare motion details between the best flows achieved for the same
sequence. We have made our evaluation using the best configuration of α and γ for every
sequence. The remaining parameters are set as in the previous experiments.
The best flows (and its motion details) for the Middlebury sequences of Grove3,
RubberWhale and Urban2 are shown in figures 4.14, 4.15 and 4.16, respectively. On
the other hand, the results for Bandage 1, Bandage 2 and Ambush 5 from MPI-Sintel
dataset are depicted in the figures 4.17, 4.18 and 4.19.
Once again, we represent the motion with the IPOL color scheme, that appears in
the first column of these figures. In the second column, we depict the true flow and the
motion fields for grayscale and RGB, respectively. The remaining columns present motion
details for the corresponding solutions.
The results are conclusive and confirm that the color information benefits the optical
flow estimation. In general, the motion contours are better preserved and the error
decreases perceptibly, especially in figures 4.14 and 4.19.
Figure 4.14 is a good example of this improvement, where the solution achieved by
the color image is closer to the true flow than its grayscale counterpart. If we observe the
motion details, we can see a more realistic solution that achieves a better preservation of
the leafs. The reasons behind this enhancement is that the difference between the leafs
and the background is much pronounced in the color image. In this sense, the grayscale
picture makes it hard for the method to distinguish the moving objects with respect to
the background.
For the same reasons, the definition of the contour is more accurate in figures 4.15 and
4.16. For instance, we observe an improvement in the details exposed in the red box of the
color solution of RubberWhale. The multi-channel information helps in the the shading
region. Nevertheless, the benefits are not so clear in both figures as the previous one.
The flows of figure 4.17 are more precise in its third row. Here, the diffusion in the
dragon tail (green box) is ameliorated due to the color information. On the other hand,
we can also see a significant decrease of the error in the head of the dragon at figure 4.18.
Finally, although neither of the motion fields of figure 4.19 offer good results, we
observe fewer outliers in the color solution.

In this section, we compare the numerical results (AAE and EPE) provided by the best
configurations of α and γ using color or grayscale in these two datasets. Furthermore, the
computational cost is very important if we need to choose the best scheme; therefore, we
take into account the running times required by the methods to justify if the computational
cost its reasonable. The values for the remaining parameters have been fixed as in the
previous section.
Tables 4.5 and 4.6 show the best AAE and the EPE results for both color spaces
while, its fourth and seventh columns, depict the percentage of error variation between
them. Once again, it is clear that the RGB information improves the numerical errors,
especially in the Middlebury sequences where most of the amelioration in the EPE exceeds
in 5.50% and the AAE in 4%.
Moreover, the enhancement is not so evident in the MPI Sintel sequences. It can
be appreciated that, in a few occasions, the color worsens the EPE. Nevertheless, this
deterioration is not excessive and the color information provides a reasonable improvement
in a high percentage of cases, especially in Ambush 5 and Sleeping 1.
The error amelioration is not enough to determine if a multi-channel scheme is justified.
Thus, Figures 4.7 and 4.8 show the execution times needed for obtaining all the previous
results. The size of the sequences from MPI Sintel is 1024 × 436 while, in the case of
Middlebury, the size of the images is variable: Grove2, Grove3, Urban2 and Urban3 are
640 × 480, while Hydrangea, RubberWhale and Dimetrodon are 584 × 388.
Tables 4.7 and 4.8 show the running times of each sequence and the percentage of
improvement provided by the grayscale with respect to the RGB images. As expected,
less information means less execution time. However, the difference is not excessively
pronounced and, even, in the case of Ambush 2 sequence, less time is required if we
use color information. This means that, using more information from the images, the
algorithm usually converges in less iterations. Moreover, we must consider that the
increment in the calculations has only affected the data term, so that more image channels
does not mean that the operations grow up proportionally. From these experiments,
we conclude that if our main goal resides in the precision of the results, the RGB
information is quite interesting but, if the performance is required, a grayscale method
is an alternative to consider. Besides, we can parallelize the multi-channel framework or
use other numerical schemes such as multigrid [Bruhn06].
The experiments were realized in a computer with the following features: Intel(R)
Core(TM) i7 CPU 860 @2.80GHz. We have used a single core in our tests.

The preservation of motion discontinuities is still one of the main challenges in optical
flow methods. The variational solutions are obtained as the minimization of a continuous
functional where a smoothness term ensures that our optical flow is piece-wise continuous.
In the other hand, it is critical for the correct preservation of motion boundaries. This
is an important problem due to the fact that a poor preservation of flow edges makes
difficult to separate different moving regions.

These boundaries are normally associated with the contours of the objects.
Nevertheless, an object border does not necessarily imply a flow boundary. For example, it
is not strange that a scene presents adjacent objects that are moving in the same direction.
In this case, these ones probably belong to the same optical flow region without motion
discontinuities between them. Besides, it is challenging to differentiate between object
contours and textures.

Many works have proposed alternatives in order to cope with motion discontinuities.
Several optical flow methods [Proesmans94, Black93, Cohen93, Brox04, Zach07] use
anisotropic diffusion to reduce the effect of outliers and produce sharp boundaries. In
other works [Nagel86, Alvarez00], the regularization process is steered by a diffusion tensor
that depends on the image structures.

This idea has been used in more recent methods like [Sun08, Werlberger09]. The
difference is that they include robust function in order to avoid oversegmenting the flow
field. In [Zimmer09, Zimmer11], the authors include also a motion tensor in the data term.
Other smoothing strategies include bilateral filtering [Xiao06] or non-local regularizations,
like in [Werlberger10].

Our first approach ensures a minimum isotropic regularization that prevents the
cancellation of the smoothness term by using a small constant. A similar idea was
introduced in [Ayvaci12] but, in that case, the regularization was carried out for each
component of the optical flow independently. This strategy allows a better performance at
flow discontinuities and it is more insensitive to the discontinuity parameter. Nevertheless,
user intervention is still necessary for choosing it.

In this sense, our second proposal estimates its value from the gradient of the image
and the regularization parameter. This means that the method is automatically adapted
to the image range at the same time that it provides good motion contours. In the
experiments, we observe that it usually obtains accurate flow fields which are close to the
best solutions, without any user intervention.

We also analyze a scheme similar to the robust diffusion tensors proposed
in [Werlberger09] or [Zimmer11]. The diffusion across image contours is controlled with
a robust decreasing function and maintained along the isocontours of the objects. In
this case, the method is very stable with respect to the anisotropic parameter. In the
experiments, we show the capabilities of these strategies and the benefits and drawbacks
with respect to a standard TV-L1 method.

The chapter is organized as follows: in Section. 5.1, we introduce a general framework
for the regularization strategies and we explain each proposal in detail. In Section 5.2, we
show our experimental results. First, we analyze each strategy and their behavior with
respect to the instabilities problem; then, we compare all the strategies together. Finally,
a summary of the main ideas and conclusion are given in Section 5.5.

This diffusion tensor resembles a robust variant of the Nagel-Enkelmann operator,
with decreasing functions to mitigate the diffusion across the boundaries of the objects.
The diffusion is performed unconditionally along the isocontours while in the gradient
direction it is modulated by a robust function.
The behavior of the diffusion tensors is depicted in Fig. 5.3. We see how robustification
techniques reduce the influence of noise and allow to obtain piecewise-smooth flow fields.
The solution given by the Nagel-Enkelmann method improves the Horn-Schunck solution
at the edges, as can be seen in the head of the girl.

Both, Brox and Nagel-Enkelmann methods, obtain piecewise-smooth motion fields.
However, the Nagel-Enkelmann method cannot preserve discontinuities when the gradient
of the image is small, as we see in the beard and the eyebrow. The Brox method detects
both of them, although it does not distinguish the hair tufts. The Total Variation tends
to smooth the flow destroying details. It also creates rounded shapes in high curvature
boundaries.

The flow fields obtained by the Alvarez and DF methods are very similar. The most
significant difference between them is the magnitude of the flow, especially in the eyebrow
and beard. Furthermore, the robustification has allowed to detect discontinuities with
small image gradients. In the DF approach, we see sharp discontinuities but small artifacts
close to the old man ear.

Note that, in these flows, we have intentionally taken a value of λ which is slightly
bigger than its optimal value for comparing the stability of DF, DF-β and RADT. These
flows are represented in the graphics by the vertical lines and the X marks. Note that
these points do not always appear for the DF graphic because the error is normally too
big.
In general, the flow discontinuities obtained with the Brox method are not aligned
with the object contours. For instance, the method cannot distinguish between the two
rectangles of figure 5.15 and the leafs in Grove2 and the hair tufts of the woman in
Alley 1 are heavily smoothed. The motion contours are better preserved using the other
strategies, principally dealing with the geometric figures.
On the other hand, these four figures are a good example of the high parametric
dependence of the DF method, where numerous instabilities appear while the others
schemes offer a reasonable quality. In fact, the solution of our first proposal is quite
similar to the DF method, but with an important reduction of blobs.
The RADT strategy provides good solutions, like for instance, in figure. 5.18 where
the method correctly detects the woman’s arm and the apple in her hand. Nevertheless,
the diffusion is not completely stopped at the object contours, especially at the skyline
in the Yosemite sequence. Note that this behavior is similar in the Shaman 2 results
showed in figure 5.3, where the small gradients do not allow to steer the diffusion process
conveniently.
Finally, we observe an interesting issue in the graphic of figure 5.17 that we must
consider. According to the error evolution, the differences between the Brox results and
the other approaches are not remarkable. However, the flow fields demonstrate better
boundaries definition but, with some small instabilities that worsen the average error of
the whole flow. In this sense, it is possible for an oversmoothed solution to have the same
error as a flow which is overall more accurate but with some outliers due to instabilities.
In summary, we observe the same behavior as in the previous section. From the
Rectangles sequence we can appreciate the limitations of the Brox method and the benefits
of using the other strategies. The motion contours are much better preserved even though
the EPE errors are not so different with respect to Brox. The graphics show the instability
problems of the DF method and we can clearly see the blobs in the flow images. This
means that there is a correlation between the choice of λ, the appearance of instabilities
and the increase of EPE errors. The RADT method preserves good motion discontinuities
but the flow field is smoother than DF-β and DF-Auto. However, the latter still introduce
some instabilities around the contours, probably at occluded regions, which are not present
at RADT.

OpenMP is an Application Program Interface (API) that supports multi-platform shared memory 
multi-processing programming in C, C++ and Fortran. In this project OpenMP is chosen as the 
standard for multi-core CPU programming. For the motivation of this choice, see section 4.2. 
An in-depth treatment of the OpenMP API is outside the scope of this project. Only the topics 
necessary to understand the main line in this work are discussed here. All details of OpenMP 
API can be found in the definition of the standard (OpenMP Architecture Review Board, 2011). A 
good introduction to OpenMP can be found in Chapman, Jost and van de Pas (2008). A tutorial can 
be found in Barney (2011b).

Static scheduling not specifying the chunk size will divide the work in N approximately equally 
sized chunks over the N available cores. This will give the least scheduling overhead. However 
if some iterations take more time to calculate than other iterations it is quite possible that 
one of the threads will need significantly more time to finish its work than the other threads. 
The other threads which have finished their work must wait for this last thread before the threads 
can be joined and the parallel construct finishes. This means that the work load is not evenly balanced 
over the threads. This indicates that static scheduling will be the favourable strategy if the time needed 
for calculating each chunk is fairly constant. However, this is only true if the computer running the OpenMP 
application is dedicated to the application. Let’s assume that N chunks are divided over N threads on a machine 
with N cores and one of the cores is interrupted by a higher priority background task. In this case the thread 
on the interrupted core will take more wall clock time to finish and will delay the total parallel construct.

Guided and dynamic scheduling will give a much better load balancing if the time needed for calculations of an 
iteration is expected to fluctuate or if it is likely that one or more cores will be interrupted by higher priority 
background tasks. The difference between guided and dynamic scheduling is that in dynamic scheduling the size of 
the chunk is fixed and in guided scheduling the size of the chunk will gradually decrease. Guided scheduling will 
give a more fine-tuned load balancing than dynamic scheduling at the cost of more scheduling overhead.

The preliminary research and experiments described in section 2.2 indicated that on large images 
parallelization can give a significant performance benefit. Due to the overhead involved in parallelization, 
the use of parallelization on small images can lead to a performance loss compared to running sequentially

As stated in the previous section, calibration is not an easy challenge and it is expected that it will take 
a long time if all parallelized operators must be calibrated for all image types. A perfect calibration will 
not be possible for all operators because the calibration can be dependent on the content of the image

Based on experiences with the Computer Vision projects mentioned in section 1.2 it is quite clear that the 
image type that is most often used for greyscale operators is the Int16Image and for colour operators the 
HSV888Image. Furthermore it is expected that the calibration result for a specific operator will be similar 
for all images types. This assumption must be validated by benchmarking. If this assumption is invalid, a 
calibration for all image types will be necessary.

The full calibration procedure must determine the break-even point in number of pixels for each parallelized 
operator where the parallel execution of the operator is gain-factor times faster than the sequential version. 
For one basic operator the result of the calibration is stored as the number of pixels for the break-even point. 
All other operators will store their results relative to the result of the basic operator. The quick calibration 
procedure only benchmarks the basic operator and uses default values for relative factors for the other operators.

The idea behind the simple procedure is that this procedure will give a quick and fairly good idea of the general 
performance of the system based on the most dominant factors like clock speed and number of cores available. The 
idea behind the complex procedure is that this will give a more fine-tuned and machine-specific calibration. The 
preliminary research and experiments described in section 2.2 indicated that different CPU architectures will give 
variations in the relative factors for the operators. It is plausible that these differences can be explained by 
factors like differences in efficiency of the executing of the instruction set and differences in memory latency 
and bandwidth.

In the author’s experience, parallel programming is more vulnerable for making errors than sequential programming. 
During the calibration process both the parallel result and the sequential result of the operator are calculated. 
The calibration procedure must have a built-in regression test, which always compares the sequential result with 
the parallel result. The overhead involved for this test is negligible.
After calibration the results can be saved to disk. The next time at start-up the calibration file is read from 
disk, so that the Automatic Operator Parallelization mechanism can be used without executing the calibration 
procedure.

When the kernel is submitted to the compute device for computation an index space is defined. An instance of the 
kernel called the work-item is created for each index. Work-items can be identified by this index, which provides 
a global ID for a work-item. Work-items are organized into work-groups. The work-groups provide a more coarse-grained
 decomposition of the index space. Work-groups are assigned a unique work-group. Work-items are assigned a unique 
local ID within a work-group so that a single work-item can be uniquely identified by its global ID or by a 
combination of its local ID and work-group ID. The work-items in a given work-group execute concurrently on the 
processing elements of a single compute unit.

The indexing space used to partition work-items in OpenCL is called an N-Dimensional Range (NDRange). The NDRange, 
as the name suggests, supports multidimensional indexing. OpenCL supports up to and including three-dimensional 
indexing.

The application running on the host uses the OpenCL API to create memory objects in global or constant memory 
and to enqueue memory commands that operate on these memory objects. To copy data explicitly, the host enqueues 
commands to transfer data between the memory objects and host memory.

OpenCL uses a relaxed consistency memory model; i.e. the state of memory visible to a work-item is not guaranteed 
to be consistent across the collection of work-items at all times. Within a work-item memory has load/store 
consistency. Local memory is consistent across work-items in a single work-group at a work-group barrier. Global 
memory is consistent across work-items in a single work-group at a work-group barrier, but there are no guarantees 
of memory consistency between different work-groups executing a kernel.

Memory objects are categorized into two types: buffer objects and image objects. A buffer object stores a 
one-dimensional collection of elements whereas an image object is used to store a two- or three-dimensional 
texture, frame-buffer or image. Elements of a buffer object can be a scalar data type (such as an int, float), 
vector data type, or a user-defined structure. Elements in a buffer are stored in sequential fashion and can be 
accessed using a pointer. Elements of an image are stored in a format that is opaque to the user and cannot be 
directly accessed using a pointer. Built-in functions are provided by the OpenCL kernel language to allow a kernel 
to read from or write to images.

Synchronization between work-items in a single work-group is done using a work-group barrier. All the work-items 
of a work-group must execute the barrier before any are allowed to continue execution beyond the barrier. Note that 
the work-group barrier must be encountered by all work-items of a work-group executing the kernel or by none at 
all. There is no mechanism in the Compute Device for synchronization between work-groups.

This work is about how to parallelize a large generic Computer Vision library in an efficient and effective way. 
In section 3.6, a classification of basic low level image operators is described. This chapter describes the 
implementations of one representative for each class. These implementations can be used as a skeleton to implement 
the other instances for each class. Many of the high level operators are built using the basic low level operators. 
These operators will directly benefit of the parallelization of the basic low level operators.

In order to gain experience with OpenMP and OpenCL two simple vision operators, Threshold and Histogram, were 
selected for the first experiments. The results of these experiments were used to limit the scope of experiments 
with the more complex operators.

Several studies found in the literature review concluded that loop unrolling is beneficial. However, in all 
the researches fixed masks are used. Because both the size of the mask and the position of the origin of the 
mask are known at compilation time, both for loops can be completely unrolled. This will eliminate the complete 
overhead of both for loops. This approach is not possible in this work because the sizes of the mask and/or the 
origin of the mask are not known at compilation time.

The last For loop in the “UnrollV” (Antao and Sousa, 2010) approach can be eliminated if the width of the 
mask is enlarged to the next multiple of 4, 8 or 16 and the origin of the mask remains in the same position. 
The result of the convolution will remain unchanged if the “extra” mask values are set to zero. Typically the 
image will be much larger than the mask, so if the origin of the mask is not at the bottom row, it will be 
impossible to use pixels outside the image in the calculation. This approach was implemented for short4, short8 
and short16 vectors and is referenced in this work as the “UnrollV2” optimization. This proposed approach appears 
to be novel. The literature search has not found any previous use of this approach.

The UnrollV2 optimization is not using the vector capabilities in an optimal way, because the extra zeros in 
the last vector for each row calculation are dummies. Antao and Sousa (2010) and Antao, Sousa and Chaves (2011) 
introduce approaches that do not have this disadvantage. According to their tests these approaches are beneficial 
on CPUs. These approaches require to use a auxiliary image, in which the order of the pixels is extensively 
rearranged. Future work will have to investigate whether these approaches are beneficial on GPUs.

As mentioned in section 2.1, VisionLab supports a wide variety of image types. The Histogram operator must work 
with all greyscale image types. The generic implementation of the Histogram operator in VisionLab will work without 
a predefined range for the pixels and also supports negative pixel values; before calculating the histogram, the 
minimum and maximum pixel value in the image will be calculated. In many cases this is an overkill because the 
minimum pixel value is zero and the maximum pixel value is known. For these cases VisionLab has the faster 
Histogram0 operator. Implementation of the Histogram0 operator is straightforward. First a straightforward OpenCL 
implementation is described and then an optimized implementation. In the next sections the kernels are described 
and discussed. The idea of the implementation is derived from Gaster et al. (2012, Chapter 9). Their implementation 
only works for fixed size histograms. The implementation presented here will work with variable size histograms, 
so it is compatible with VisionLab’s Histogram0 operator. The idea of Gaster et al. is to parallelize the histogram 
calculation over a number of work-groups. In a work-group all work-items summarize their sub-images into a 
sub-histogram in local memory using an atomic increment operation. Subsequently each work-group copies its 
sub-histogram to local memory. Thereafter a reduction kernel performs a global reduction operator to produce 
the final histogram. The idea behind the CPU implementation is that each work-group has only one work-item and 
the number of work-groups is equal to the number of cores. This implicates that there are no race conditions 
for the local histogram and there is no need for expensive atomic increment operations. The implementation is 
quite similar to the OpenMP implementation described in section 6.7.2.3. The sequential implementation is based 
on a Two passes approach as described in section 3.6.6.2. In many applications the LabelBlob operator is followed 
by the BlobAnalyse operator or a derivative of this operator. BlobAnalyse performs all kinds of measurements for 
each blob. The result of the BlobAnalyse is a table, with for each blob a record containing all measurements for 
that blob. The index in the table is the label number. In order to keep the memory usage of the table as small as possible it is imperative that the LabelBlobs operator label the blobs with successive label numbers. This means that after resolving equivalence provisional labels, the table with the resolved provisional labels must be renumbered with successive label numbers starting at label number 1. After this renumbering the second pass can assign the successive label numbers to the pixels of the blobs. All parallel implementations found in the literature review (section 3.6.6.3) are multiple iteration approaches. Kalentev, Rai, Kemnitz, and Schneider (2011), abbreviated to Kalentev et al., report that their test set their algorithm needs on average 5 iterations. Each iteration of their algorithm consists of 2 passes; a Link and LabelEqualize pass, see section 6.8.2.4. Including the initial and final pass their algorithm needs on average 12 passes. Kalentev et al. claim that, because of the reduction algorithm they use, their algorithm is efficient in terms of the number of iterations needed.

Measurements with the sequential implementation were performed in order to get an impression of the order of 
magnitudes for the execution times of the three parts of the sequential algorithm: Pass1, Resolving equivalences 
and Pass2. The processing time of the LabelBlob operator will depend on the contents of the image; the number of 
object pixels and the morphology of the blobs. In section 7.9.2.1 the Int16Image cells.jl (see Appendix B) is 
considered to be a ‘typical’ image. The sequential LabelBlobs operator with eight-connectivity was executed on 
image cells.jl after Thresholding with different sizes of the image. These tests were performed on an Intel 
Core i7-2640M processor at 2.8 GHz. The median of the execution time in micro seconds over 30 measurements for 
each part was:On average Kalentev et al. approach needs 5 iterations. In total: one simple initial pass, 10 
neighbourhood search passes and one simple final pass. It is expected that the initial and final pass will have 
similar complexity as Pass2. As will be explained in section 6.8.2.4 Kalentev et al. approach needs a post 
processing step with two passes, which are expected to have a similar complexity as Pass2. The execution time 
will be dominated by the 10 neighbourhood search passes.

If the sequential version takes 1 unit of execution time for an image, it is estimated that, Kalentev et al. 
will take more than 7.9 units of (sequential) execution time. In order to get a speedup bigger than 1, it is 
to be expected that more than 8 cores will be required.

According to the author the proposed parallel algorithms in literature only work for “many-core” systems and 
not for “few-core” systems like contemporary CPUs who have typical 2 to 8 cores. The substantial speedup claimed 
by Niknam, Thulasiraman, Camorlinga (2010) with their OpenMP implementation was obtained by comparing a sequential 
multi-pass algorithm with a parallel multi-pass algorithm. In this work the much faster 2 pass sequential algorithm 
is compared with parallel multi-pass algorithms.

So, another approach is necessary for “few-core” systems. The proposed approach is to split the image into 
sub-images. This approach is inspired by the work of Park, Looney and Chen (2000) to limit the amount of memory 
used for the equivalences table in sequential implementations. For each sub-image a label equivalences table is 
calculated in the same manner as in Pass1 of the sequential algorithm. All used label equivalence numbers must be 
unique for the whole image. Thereafter the label equivalences tables are repaired for the blobs that cross the 
boundaries of the sub-images. Note that one blob can be in more than two sub-images. Next, the label equivalences 
tables are merged into one label equivalences table and renumbered with successive label numbers. Finally, a last 
pass is required to assign the successive label numbers to the pixels of the blobs. This proposed approach appears 
to be novel. The literature search has not found any previous use of this approach.

The computational expensive part is the calculation of the label equivalences tables, this involves a neighbourhood 
search of the sub-images. Because there are no data dependencies this part is embarrassingly easy to parallelize. 

Each sub-image can be processed by a separate core. The repairing and merging of the label equivalences tables 
followed by the renumbering is not computationally expensive because only a small amount of data is to be processed.
In the current implementation this is done sequentially, but an iterative parallel approach is possible too. The 
last pass, similar to Pass2 of the sequential algorithm, for assigning the label numbers to the pixels is 
implemented with a shared read-only lookup table and is embarrassingly easy to parallelize. OpenCL uses wavefronts 
in order to hide memory latency (section 3.5.2.3.4). The number of the wavefronts is determined by the work-group 
size (section 5.3.2.3). The work-group size for a kernel is an important parameter in achieving a good performance. 

For each kernel described in the next sections, the work-group size that resulted in the highest speedup was 
experimentally found. The speedup graphs show the speedup for the optimal work-group size for each kernel. Tables 
with optimal work-group size for each benchmark are available in electronic form.

Many of the OpenCL kernels used will only work with images with restrictions on the size of those images, like 
height and/or width, which must be a multiple of 4, 8 or 16. It is possible to avoid this restriction at the 
expense of performance.
In many cases the scalar and vector variations of a OpenCL kernel were benchmarked. In the scalar variation the 
processing is performed pixel by pixel. In the vector variation N pixels are grouped to a vector and processed 
as one vector. In the case of an Int16Pixel the scalar variation is denominated ‘Short’ and the vector variations 
‘Short4’, ‘Short8’ and ‘Short16’. The digit in ‘ShortX’ specifies the size of the vector. When appropriate for a 
benchmark these names are used to distinguish between the scalar and vector variations of a kernel. All benchmarks 
were performed using the ‘standard’ OpenCL copy transfer method. Because the wrapper around the host API interface 
does not yet support ‘zero copy transfer’, no benchmarks could be performed using this option. All benchmarks were 
performed using OpenCL buffers. The results in section 7.6 suggest that, in general, it is not benificial for 
Computer Vision operators to use OpenCL images instead of OpenCL buffers. It is future work to benchmark data 
transfer with ‘zero copy transfer’ and OpenCL images.

Probabilistic methods have been extensively applied in robotics and computer vision
to handle noisy perception of the environment and the inherent uncertainty
in the estimation. There is a variety of solutions to the estimation problems in today’s
literature. Filtering and Maximum Likelihood Estimation (MLE) are among
the most used in robotics. Since filtering easily becomes inconsistent when applied
to nonlinear processes [160], MLE gained a prime role among the estimation solutions.
In Simultaneous Localization and Mapping (SLAM) [45, 95, 106, 98] or other
mathematically equivalent problems such as Bundle Adjustment (BA) [4, 105] or
Structure from Motion (SfM) [14], the estimation problem is solved by finding the
MLE of a set of variables (e.g. camera/robot poses and 3D points in the environment)
given a set of observations. Assuming Gaussian noises and processes, the
MLE has an elegant Nonlinear Least Squares (NLS) solution.

In practice, the initial problem is nonlinear and it is usually addressed by repeatedly
solving a sequence of linear systems. The linear system can be solved either
by matrix factorization or gradient methods. The latter are more efficient from the
storage point of view, since they only require access to the gradient, but they can
suffer from poor convergence, slowing down the execution. Matrix factorization,
on the other hand, produces more accurate solutions and avoids convergence difficulties
but typically requires a lot of storage.

SLAM is a central problem in robotics and relates to navigation of a robot which
at the same time builds the map that it uses to determine its location and to plan
further movement [144], both the location and the map being unknown initially.
There are many formulations of SLAM, to some degree dependent on the sensor
used, the representation of the map and the underlying method for dealing with
sensor noise. The most common sensor types include range finders (e.g. LIDAR,
RGBD or time of flight cameras or sonar) and monocular, stereo or spherical cameras,
GPS, IMU, as well as various combinations of those.

Kalman Filtering (KF) is an efficient method for dealing with noisy measurements
of a linear variable. However, problems in robotics and computer vision
are highly nonlinear due to projections and rotations and rather than baseline KF,
its extensions are commonly used. Extended Kalman Filter (EKF) is a nonlinear
version of KF which uses a linear approximation around the current linearization
point, and has been popular in SLAM literature [35, 54, 43, 109].
One disadvantage of EKF is lower precision or even divergence, if the underlying
model is highly nonlinear. For that, Unscented Kalman Filter (UKF) uses a sampling
approach [92] in order to calculate the mean and the distribution of the estimate
more accurately. Several SLAM approaches were formulated using UKF [119, 32, 84,
86], yielding a better run time and consistency than that of EKF-based approaches.
Information Filter (IF) is another variation where information matrix, the inverse
of the state covariance matrix, is being propagated. The advantage is in simple
integration of new observations as the information is additive, leading to more
accurate estimates and higher stability. One disadvantage of IFs is the need to invert
the information matrix often but despite that, IFs are relevant in SLAM [165, 166, 57].
Particle filtering is a popular method based on Monte Carlo sampling. It is very
simple to implement and can inherently handle multiple hypotheses. The position
of the robot (the estimated variable) is represented by a set of particles which
are uniformly distributed initially, as the robot position is unknown. At each step,
robot control commands are applied to all the particles which are then re-sampled
using the posterior distribution of particle positions conditioned by map observations.
The particles typically quickly converge to one or more clouds (hypotheses)
where the robot could be located. FastSLAM and its variants [124, 145, 10, 100] are
archetypal representatives of particle filter implementations.

Both BA and SfM deal with noise much like SLAM, to which these problems are
mathematically equivalent. Although the matters are perhaps more complicated,
we refer as BA to problems dealing with unstructured databases of images – often
from multiple different cameras with potentially unknown parameters and as SfM
to problems of reconstruction from an ordered sequence of images from a single
moving camera – possibly a video-sequence of smooth motion. This makes the
two different from the image track processing point of view but very alike from
the optimization point of view.

The distinguishing trait that sets BA apart from SLAM is the space where the error
is minimized: in SLAM, the space in which the measurements (and thus the error)
is defined is the same as that of the poses. On the other hand, in BA, the error of
the reprojection in 2D image space is minimized while the poses and landmarks
exist in 3D space.

Photo tourism [155] is an application of BA to building sparse reconstructions
from unstructured collections of photographs for the purpose of interactive navigation
in such collection. It makes use of the EXIF image tags to get the intrinsic
camera parameters rather than solving an uncalibrated problem. The camera
poses are primarily used for photo placement in the user interface while the sparse
structure is rendered as textured points (rather than performing triangulation), optionally
in non-photo-realistic mode. Two techniques for view interpolation for
animated transitions are suggested

Parameterizations taking advantage of the incremental solving were proposed as
well. In [153], relative camera and pose formulation is employed, rather than using
a single global Euclidean coordinate frame. After adding a new camera pose and
the associated observations, it is possible to find the variables where this addition
induced a significant change and only a reduced system consisting of those variables
and their neighbors is solved. The size of the system that needs to be solved
is only a fraction of the full system, making the optimization faster. A standard
Schur complement solver is employed.

Another approach is acceleration via graph sparsification. In [91], rather than
optimizing the entire problem only the camera poses are optimized, with the
observations taking form of three-view constraints related to the tri-focal tensor.
A similar generalized approach is proposed in [27] where the structure variables
would be represented implicitly by the corresponding triangulation functions and
therefore only the camera poses and optionally also their calibrations would be
optimized. In both cases, the structure points can be triangulated after-the-fact in
the least squares fashion from all the cameras that observe each given point. Since
these methods effectively solve a pose graph, it is possible to use the appropriate
incremental algorithms [95, 98], [PIŠ+13b] as well.

This chapter describes existing implementations of general Nonlinear Least
Squares (NLS) solvers which are used in robotics, with the focus on the algorithms
and data structures. The list is definitely not an exhaustive one, but an attempt was
made to have the most significant implementations present. There is a considerable
overlap with other scientific communities that solve computer vision, surveying,
photogrammetry or other similar problems, often using their own methods and
software tools.

Despite using elementwise matrices, iSAM makes a limited use of the block
structure by calculating the fill-reducing ordering on variables (which corresponds
to block columns) and then expanding this ordering to the individual elements
(columns). It uses either CSparse [41] or Cholmod [42] internally for batch matrix
factorizations – the disadvantage is that it needs to convert from its own sparse
format to standard CSC before such libraries can be used, and then to convert the
result back.

Another important feature is hierarchical graph optimization. A graph is represented
at the highest level as well as on several lower levels of detail. The first level
of detail is obtained by dividing vertices of the original graph into subgraphs and
treating each subgraph as a vertex. For that, a representative vertex is selected from
each subgraph. There is an edge between representative vertices of two graphs in
case there were edges between any two vertices between those graphs. The measurement
and its associated covariance are calculated by solving for a problem
consisting of the union of the two graphs, with the representative vertex from the
first graph being in the origin and the representative vertex from the second graph
providing the mean as well as covariance. The hierarchical optimization starts at
the highest level (the smallest graph) and the significant changes are propagated
downwards via rigid body transformations. When needed, a lower level subgraph
can be refined using another optimization round, with the additional constraint on
the representative vertices. Covariances can be calculated for the data association.
Written in C++, HOG-Man uses the graph as its primary representation rather
than a matrix. It stores an associative array of vertices and an ordered set of
pointers to the edges. Such structure allows for simple graph modifications but
requires multiple indirections to access any element of the graph. It makes use of
CSparse [41] for Cholesky decomposition and solving, while the graph or its subgraphs
are converted into sparse matrix form on the fly, rather than maintaining a
matrix and updating it incrementally. No advantage is taken of the block structure
of the matrices

General Framework for Graph Optimization (g2o) [106] is the culmination of
the research done on sSBA and SPA and has quickly become a popular framework
for nonlinear optimization in robotics. It contains several optimizers, based
on Gauss-Newton, Levenberg-Marquardt or Dogleg methods. While designed to
be easily extensible, g2o can sole BA and SLAM problems out-of-the-box. It uses
Lie algebra [159] group SE(3) to correctly calculate derivatives involving spatial
rotations and optimizes such variables in the tangent space vectorial form se(3).
In addition, it contains numerical differentiation functions to calculate derivatives
automatically if needed. It can also recover covariances of the estimate, using the
same recursive formula implementation as described in [93]. The support for robust
solving is also implemented

Compared to sSBA, the sparse block matrix scheme is changed, the diagonal
element storage was removed and now all the matrices are represented as a vector
of block columns where each block column is row-indexed associative array of matrix
blocks. This is similar to sSBA, with the exception that in g2o the blocks can
take any size, including matrices with blocks of mixed size. The (incomplete, to
save space) C++ prototype of the matrix storage can be seen in Listing 4.1. Several
operations are implemented on this matrix format, including addition, matrix and
vector products, transpose and scalar multiplication. Diagonal matrix view is implemented
for faster access to the block diagonal (but it is not useful for representing
block diagonal matrices by itself). Linear solving is accomplished using one of
CSparse [41], Cholmod [42], Eigen [73] or CG. Conversion to elementwise sparse
matrix is again required (except for CG) every time linear solving takes place.

The solving is thresholded by a small constant on backsubstitution (which can
skip cliques of the Bayes Tree where the change in the solution is to be low). Thanks
to that, the backsubstitution usually runs in better than linear time. Another several
orders of magnitude larger, threshold is on the increment of the variables to
be relinearized. The relinearization is only performed every ten steps of the algorithm
by default. Constrained column Approximate Minimum Degree (AMD)
(CCOLAMD) is employed for variable ordering, with the most recent variables ordered
last in order to reduce the size of the incremental updates (since the new
observations are most likely to reference those variables). Unlike iSAM, more precise
Lie-algebraic derivatives calculated using the exponential map paradigm are
employed throughout.

Chapter 2 contains a brief introduction into nonlinear least squares methods and
their extensions, the problems those methods are applicable or have been traditionally
applied to and some of the state of the art solvers. A strong focus is on
data structures: the problems discussed here are all sparse, with non-trivial sparsity
patterns and the choice of the data representation affects the algorithms and
ultimately the efficiency of the solution. Chapter 3 discusses the standard formats
for storing and manipulating sparse matrices.

Sparse matrices are often used in the implementations described in this chapter,
as representing the problems by dense matrices would bring significant computational
overhead and would quickly become impractical. Standard libraries for
elementwise sparse matrices are popular, with Tim Davis’ SuiteSparse being used
notably often (10 out of 24 projects in the OpenSLAM2 repository use it, Google’s
Ceres solver does as well).

Another important feature is hierarchical graph optimization. A graph is represented
at the highest level as well as on several lower levels of detail. The first level
of detail is obtained by dividing vertices of the original graph into subgraphs and
treating each subgraph as a vertex. For that, a representative vertex is selected from
each subgraph. There is an edge between representative vertices of two graphs in
case there were edges between any two vertices between those graphs. The measurement
and its associated covariance are calculated by solving for a problem
consisting of the union of the two graphs, with the representative vertex from the
first graph being in the origin and the representative vertex from the second graph
providing the mean as well as covariance. The hierarchical optimization starts at
the highest level (the smallest graph) and the significant changes are propagated
downwards via rigid body transformations. When needed, a lower level subgraph
can be refined using another optimization round, with the additional constraint on
the representative vertices. Covariances can be calculated for the data association.
Written in C++, HOG-Man uses the graph as its primary representation rather
than a matrix. It stores an associative array of vertices and an ordered set of
pointers to the edges. Such structure allows for simple graph modifications but
requires multiple indirections to access any element of the graph. It makes use of
CSparse [41] for Cholesky decomposition and solving, while the graph or its subgraphs
are converted into sparse matrix form on the fly, rather than maintaining a
matrix and updating it incrementally. No advantage is taken of the block structure
of the matrices.

Many applications ranging from physics, computer graphics, computer vision to
robotics rely on efficiently solving large nonlinear systems of equations, as illustrated
in the previous chapter. In the case of using a Gauss-Newton-like algorithm,
the solution can be approximated by iteratively solving a series of linearized problems.
In some applications, the size of the system can be considerably large. The
most computationally demanding part is to assemble and solve the linearized system
at each iteration. This chapter shows solutions that exploit both, the block
structure and the sparsity of the corresponding matrices and offers very efficient
methods to manipulate, assemble and perform arithmetic operations on them.
A block matrix is a matrix which is interpreted as partitioned into sections called
blocks that can be manipulated at once. A matrix is called sparse if many of its entries
are zero. Considering both, the block structure and the sparsity of the matrices
can bring important advantages in terms of storage and operations.

An overlapping block matrix may be obtained e.g. by a procedure for finding
block structure in general sparse matrices which aims at covering all matrix
nonzeros by the minimum number of blocks possible, see e.g. Figure 1.1c or Figure
5.1a. Unaligned block matrices (Figure 1.1b or Figure 5.1c) arise naturally e.g.
in LOTs [30, 31] in image processing, where each two adjacent blocks overlap in
order to avoid discontinuities in the processed image.

Assuming aligned, non-overlapping matrices has its benefits. Each block of the
matrix can be treated as a (scalar) variable in an ordinary (elementwise) sparse
matrix and formulas applicable to the elements can be automatically extended
to blocks (see Figure 5.2), with the difference that scalar operations become operations
on matrices: addition becomes elementwise addition of the blocks, multiplication
becomes matrix multiplication, division becomes linear solving or backsubstitution
in case the blocks are triangular, square root becomes Cholesky factorization.
The only issue is that the blocks interacting in an arithmetic operation must have
compatible dimensions. Fortunately, for most of the matrix algorithms, only the
blocks in the same block row or block column are interacting and the dimensions
are therefore guaranteed to match.

Similarly, operations taking multiple matrices as input (e.g. matrix addition or
multiplication) can rely on the blocks of the two matrices to be aligned with each
other. This makes the implementation of the arithmetic operations simpler and
faster as only entire blocks interact (rather than the overlapping parts of the blocks
interacting in case the matrices weren’t aligned). In our implementation, it is required
for the matrices to be aligned with each other, or in different words, to have
a compatible block layout.

The advantage of elementwise sparse matrix schemes is that the arithmetic operations
can be performed efficiently. Compressed sparse column (CSC) format [146]
used in CSparse is an efficient way to store the sparse data in memory. The disadvantage
of this format is its inability or impracticality to change a matrix structurally
or numerically once it has been compressed. The block-wise schemes are
complementary, their advantages include both easy numeric and sometimes also
structural matrix modification, at the cost of slight memory overhead and reduced
arithmetic efficiency, speed-wise.

Matrix assembly is a notable bottleneck in many situations: the time needed for
putting the matrix together is comparable to the numerical operations which follow.
The elementwise CSC representation [146] can be as efficient as any block matrix
structure, in case of assembling a set of structurally-different matrices. The NLS
solvers, however, involve operating iteratively on matrices where large portions
of the matrix structure do not change between the iterations. In such case, block
matrix schemes can be very proficient, as they allow for modifying parts of the
block structure as well as efficiently modifying the numeric content.
In this chapter, a fast and cache efficient data structure for sparse block matrix
representation is proposed, which combines the advantages of elementwise
and block-wise schemes. It enables simple matrix modification, be it structural or
numerical, while also maintaining, and often even exceeding the speed of elementwise
operations schemes. Another important advantage of the proposed scheme is
the overall robustness of the structure, allowing for validation and error-checking

An indispensable tool for solving linear systems, most of the matrix factorizations
borrow from, is Gaussian elimination. Gaussian elimination modifies a matrix into
its upper-triangular form by performing linear combinations of rows and at the
same time modifies the right-hand side. The solution of a triangular system is
easily found by backsubstitution: the last variable does not depend on any other
and the solution is a simple ratio. The second last variable depends only on the
last but now that it is known, it can be substituted to get a simple linear equation.
The rest of the variables are solved for in similar manner, proceeding backwards,
from the right to the left – hence the name back-substitution.

The long tradition of abusing the graphics pipeline for purposes other than
graphics was finally ended by the introduction of Application Programming Interfaces
(APIs) for general purpose computations. Compute Device Unified Architecture
(CUDA) was introduced by NVIDIA in 2008 and was intended as an extension
of the C++ language for NVIDIA GPUs. In 2009, it was followed by a more
general Open Compute Library (OpenCL) which targets many different kinds of
parallel platforms, including GPUs. Both CUDA and OpenCL expose functionality
hidden from the graphics APIs, such as random memory access (scatter in addition
to gather), inter-thread communication using shared memory, atomics or doubleprecision
instructions. This compelled most of the authors to abandon writing new
implementations in shaders. Note that some of those features were later introduced
to the graphics APIs in form of the compute shaders.

Efficient parallel sorting is an important building stone of many algorithms. Although
parallel sorting algorithms have been researched extensively in the past,
implementing the same algorithms on GPU presents a significant challenge, due to
the necessary amount of communication and synchronization, not to mention high
irregularity of memory accesses. In this chapter, a highly efficient implementation
of radix sort is discussed. The ultimate goal is to support sparse linear algebra
calculations, where sorting is often employed as a preprocessing step of matrix
compression [38] in order to improve load balancing and to increase utilization of
parallel processors [16] or e.g. in sparse matrix transpose. In Figure 11.1, there is a
breakdown of the execution time of the current sparse matrix multiplication algorithms,
running on GPU. In there, it is clearly visible that sorting takes a substantial
portion of the time.

Sparse matrix multiplication is characteristic by scattering the elementwise products
in not easily predicted pattern. In order to be efficient, it must calculate products
in the order in which the matrices are stored (such as compressed sparse
column [41]). When implemented in parallel, this scattering would cause a lot of
conflicts where different threads would require access to the same element of the
output matrix. To resolve this, the current implementations calculate the product
as a set of destination coordinates and associated values, which are then sorted
and compacted.

This puts the problem in a different perspective: the data to be sorted is produced
by the GPU (e.g. by a matrix multiplication routine), and the sorted results
are consumed by the GPU. Therefore, we are not burdened by having to transfer
the data between CPU and GPU, much to the contrary: the conventional approach
would be to only use GPU for large enough problems and to process small problems
on the CPU. In our perspective, such processing would involve the prohibitive
cost of data transfers and CPU-GPU synchronization. On the other hand, there is
some prior knowledge about the range and distribution of the sorted data. The
radix sort algorithm is able to use such knowledge to significantly accelerate the
sorting, but still remains general.

Some of the first attempts on efficient sorting on GPU [143, 102, 101] were implemented
using the programmable shading pipeline, and were based on sorting
network [13] approach. Govindaraju et al. [70] extended the idea to fully utilize the
vector pipeline of the shading units and implemented a large-scale out-of-core sorting.
The obvious disadvantage is a considerable overhead of using a graphics API,
but general-purpose computing APIs did not exist yet. Sorting networks furthermore
require relatively large number of passes, which grows with the size of the
sorted sequence. These passes required communication through global (texture)
memory, and the upper bound of performance was therefore relatively low.

Sintorn and Assarsson [154] were able to develop a method based on a combination
of merge sort and bucket sort. The bucket sort is used to improve parallelism
at the later stages of sorting, where the number of lists to be merged becomes lower
than the number of parallel processors. Their implementation, although based on
comparison sorting algorithms, outperformed the work of Satish et al. [149] for
arrays of 8 M elements, or more. One disadvantage of this method is the use of
atomic counters to perform the bucket sort, and as such it depended on the distribution
of the sorted data, as atomic operations on the same counter are subject to
serialization in many parallel architecture, including GPUs.

The efficiency of radix sorting was improved by Ha et al. [76] by focusing on the
arithmetic intensity of the sorting. To reduce the number of arithmetic operations
in sorting, several optimizations such as the accumulation of three 10-bit histogram
bins in a single 32-bit integer or the use of mixed-data structure are applied. It is
based on the observation that bigger value types suffer less from irregular memory
access patterns at the scatter phase. Therefore, array of key-value structures is
preferred for this step, rather than the commonly used structure of arrays. As a
result, about 30% greater sorting rate is achieved, compared to the Satish et al.
[149] implementation.

Currently the fastest state of the art implementation is that of Merrill and
Grimshaw [122] and [121], which was greatly influential also to the proposed
method. They build on the work of Satish et al. [149] and also use the idea of
accumulating four 8-bit histogram bins in a single 32-bit integer. Several novel
ideas are introduced in these works, one of the most important ones being the reduction
of the number of steps per radix to three, as in Algorithm 11.1 where lines
3, 4 and 5 – 7 can run each as one step that only requires global communication at
the beginning or at the end. This reduction in global memory traffic increases the
upper bound on sorting throughput. It is achieved by performing local sorting at
the end of the scattering step, where it can be done in work-efficient manner.

Radix sort [103] is a stable sorting algorithm, suitable for sorting keys that map to
integral values, such as integers or to certain extent the floating-point values. Note
that this is converse to the widely used sorting paradigm that uses a comparison
predicate and is implemented in e.g. C++ Standard Template Library. It works
by grouping the given integer keys by their corresponding digits. This is done in
successive fashion, starting with the least significant digits. Once grouped, the keys
are then read out, starting with the group corresponding to the lowest value and
maintaining the relative order of the keys in the same group. After going through
all of the digits, the sequence is sorted. The parallel version of this algorithm, called
split radix sort [19], relies on parallel prefix sum primitive extensively, to facilitate
grouping of the sorted elements. Parallel prefix sum, or scan, can be implemented
efficiently on GPU [150]. In order to extend radix sort algorithm to run efficiently
on multi-processor machines such as GPUs, a notion of segments [19] is introduced.
The sort can be broken down to local operations on the individual segments of the
input sequence, which can be performed with reduced amount of communication
between processors, working on different segments. The final sorting algorithm is
described in Algorithm 11.1. A similar algorithm was used in [121].

for space, having each thread accumulate in its private histogram, and have the
threads reduce the histograms at the end. Segmented histogram is highly advantageous
for GPU implementation, as there is no communication between the segments,
and the reduction can take place entirely in the fast shared memory. The
size of the segments is of a great importance, as it directly affects the performance.
If the segments are too small, the costs of each thread initializing its private histogram
with zeros and of the final reduction will easily outweigh the time, spent
in the actual accumulation of the values, rendering the calculation inefficient. If,
on the other hand, the segments are too large, there may not be enough segments
to occupy all the streaming multiprocessors of the GPU. Many of the previous
implementations restrict the size of the segment to a constant, for example implementation
of Satish et al. [149] uses tiles of 1024 items. Instead, the proposed
implementation, similarly to [121], uses variable length segments. The number of
segments is chosen as a minimum that can keep the GPU fully utilized.

Instead, the proposed histogram algorithm accumulates the histogram in registers.
Due to the nature of the GPU execution model, to use branching to decide
which histogram bin should be incremented would result in thread divergence,
serialization and again dependence of execution speed on the data. On GPU, it is
better to compare the data at the input to all histogram bins, and use the results
of the comparison to increment all the histogram bins, for every item of the data.
This approach, however, yields high arithmetic intensity and is only efficient if
there are enough threads running to cover up the latency. Instead, bit masking operations
are employed to calculate the comparison. That enables accumulation of
several different values at once by simply or-ing their masks together. Special care
needs to be taken for accumulating duplicate values. The final accumulation part
is summarized in Algorithm 11.3.

After accumulating the segment histograms, their prefix sum is calculated much
like in [149], which will be used as a global destination offset for the sorted elements.
Since the number of segments required to occupy the GPU is small, this step
is not large enough to be efficiently issued as a separate kernel, and is fused with
the last scattering step. Note that although this saves kernel execution, it does not
save significant amount of communication and Table 11.1 still applies.
In order to perform scattering of the sorted sequence, global indices need to be
calculated for each of the elements. Segmented prefix sum of histogram bin affiliation
flags yields local ranks of the sorted elements. By adding value of histogram
prefix sum for the corresponding segment and histogram bin, global position in
the output sequence is obtained, as illustrated on Figure 11.2. This requires us
to calculate 2b prefix sums, each of the size of the segment, or alternately more
shorter prefix sums with carry.

One of the disadvantages of register histogram accumulation described in Section
11.2.2 is the number of registers it uses (34 in our case). That directly affects
possible number of workgroups, running on a single multiprocessor, and affects
the capability to hide computational latency. In order to reduce the number of
registers, a simple novel technique called volatile stripping is proposed. It is based
on an observation that the OpenCL compiler allocates registers in a manner that
will yield high processing speed, while the programmer has very little control over
it. Declaring variables as register has no effect, and the compiler (NVIDIA 331.82)
seems to ignore the ’-cl-nv-maxrregcount’ option

In this section we compare the timing results of radix sorting performed using the
proposed implementation with similar state of the art implementations such as
CUDPP 2.1, Thrust 1.6.0, CLOGS 1.2.0, CLpp v1 beta 3 and libCL 1.2.1. All of those
libraries use the radix sort algorithm. Some of them also implement predicatebased
sorting, but it is slower than radix sorting, and therefore of no interest in
our application. The evaluation was performed by sorting vectors of random numbers
of varying lengths (the same sequences were used for all the implementations).
We also performed evaluation on sequences, produced by multiplying sparse matrices
from The University of Florida Sparse Matrix Collection [39]. This collection
was chosen because it contains sparse matrices corresponding to a diverse set of
problems, and as such it is suitable for testing of general purpose linear algebra
implementations.

Summative results can be found in Table 11.2. These were measured on random
unsigned 32-bit numbers (care was taken so that the random numbers are not
banded, but indeed span the whole 32 bits) and optionally 32-bit values. More
detailed benchmarks are seen at Figures 11.3 (keys only) and 11.4 (key-value pairs).
Since different implementations might react differently on the distribution of
the sorted numbers, we also performed benchmarks by sorting element indices,
obtained by performing sparse matrix multiplication, and recording destination
row and column indices of results of every scalar product (see [38] for more details).
Row and column indices are combined to a single key by multiplying column
index by the number of rows and adding row index. Average runtime results on
data generated by multiplying 160 of randomly chosen matrices from University
of Florida Sparse Matrix Collection with their respective transposes are plotted in
Figure 11.5. Note that the proposed implementation consistently gains the fastest
saturated sorting rates, only outperformed by CUDPP for very short sequences.
Also note that the authors of Thrust and CUDPP report greater sorting rates
than measured, comparable with the proposed implementation. This is most likely
due to the behavior on the particular GPU models, where our implementation is
better optimized.

This chapter presents a novel and highly efficient parallel algorithm for sparse matrix
multiplication. Sparse matrix-matrix multiplication is an important algorithm,
useful in a wide variety of scientific tasks, including among others computational
chemistry and physics, graph contraction, breadth-first search from multiple vertices,
algebraic multigrid methods, finite element methods or solving (non)linear
systems using Schur complement [178].
The sparse matrix algorithms are usually tightly coupled to the sparse matrix
storage formats they use. Two of the popular formats are compressed sparse column
(CSC) [41] and compressed sparse row (CSR). Those are closely related; matrices
stored in one are transposes of the matrices stored in the other. CSC stores
matrices as a vector of prefix sums of numbers of nonzero elements in each column
and two vectors storing element values and their respective rows. It is common for
the elements in each column to be ordered by their row number. The use of the
CSC format is assumed in the rest of this chapter, unless specified otherwise.
Let us recall that in matrix multiplication C = A B, each element of the product
Ci,j is a sum of products of the corresponding elements in the ith row of A and
the jth column of B. The number of columns of A must match the number of rows
of B. In CSC, it is straightforward to look up elements by column (O(1)) but not to
look up elements by row (O(n) in the number of nonzero elements), which would
be needed to calculate the elements of C in ordered fashion (gather).
The original algorithm for sequential sparse matrix multiplication [75] is implemented
e.g. in the popular CSparse package [41] (used by Google’s Ceres solver
and Street View), and is work-efficient in terms of its complexity being proportional
to the number of Floating Point Operations (FLOPs). It is worth mentioning
that this level of efficiency is only reached for the price of calculating a partially
unordered representation of the product, which is still useful in practice, but it is
not the canonical form.
The algorithm [75] is efficient by traversing the elements of B column by column
(assuming the CSC storage is used; for CSR all the terms are transposed), where
each element Bi,j multiplies all the elements of A in the ith column (the one corresponding
to the row of the particular element Bi,j). Many of the other sparse
matrix multiplication algorithms use this strategy. It produces partially ordered
partial products (scatter), which need to be summed up. Gustavson [75] came up
with an elegant way of quickly merging these partially ordered sequences

Parallel sparse matrix multiplication algorithms (PSpGEMM in BLAS terminology),
however, generally decompose the matrices to band or block submatrices and distribute
the computation of the partial products to different processors. Similarly
like in the previous case, the results need to be merged to form the final product,
using sparse matrix addition in this case. This approach is further referred as a
coarse-grain work subdivision, since the submatrices are typically relatively large.
Packages [11, 59] use this approach.

The radix sort is efficient on GPU if the sorted elements are only reordered by
small distances, as such reordering can be performed in the local memory. This
is achieved by sorting it in segments corresponding to the individual columns of
the product (Figure 12.2b top), instead of sorting the whole expansion at once. The
individual segments can be sorted in parallel. Now the elements are only reordered
by relatively short distances, leading to better write coalescing and leaving ample
opportunity to do the sorting in the local memory. However, load balancing issues
arise, as the lengths of expansions of the individual columns can vary wildly [38].
In the context of GPU computing, some operations have segmented variants, e.g.
a segmented scan. Its input is a vector of values to calculate the scan of, and a vector
of head flags, a binary vector with ones at the positions of segment starts. Note that
segmented operation is performed on the bulk of data rather than on the individual
segments, and thus requires no explicit load balancing. Unfortunately, radix
sort is not a good candidate for segmented implementation, as it would lead to
both runtime and space tolls: the key bit histograms would need to be evaluated
per each segment and the reordering would also need to take place per segment,
leading to more load balancing issues. Fortunately, for merge sort, segmented variants
exist2, and the performance toll, compared to the non-segmented variant, is
negligible. By using segmented sort, the time of the sorting stage was significantly
reduced; one can compare Figure 12.1 where sorting takes only 34%, with Figure 4
in [38] where it is closer to 63% of the total runtime.

Objects are the basic run-time entities in an object-oriented system. They may
represent a person, i. e. a bank account, a table of data or any item that the program must
handle. Program objects should be chosen such that they match closely with the real-world
objects. A class provides the blueprints for objects, so basically an object is created from a
class [4]. We declare objects of a class with exactly the same sort of declaration that we
declare variables of basic types

The wrapping up of data and functions into a single unit (called class)
is known as encapsulation. Data encapsulation is the most striking feature of a class.
The data is not accessible to the outside world and only those functions which are
wrapped in the class can access it. These functions provide the interface between the
object’s data and the program. This insulation of the data from direct access by the
program is called ‘data hiding`.
Abstraction refers to the act of representing essential features without including the
background details or explanations. Classes use the concept of abstraction and are
defined as a list of abstract attributes such as size, weight and cost, and functions to
operate on these attributes. They encapsulate al1 the essential properties of the
objects that are to be created. Since the classes use the concept of data abstraction,
they are known as Abstract Data Types (ADT).

Polymorphism is another important concept in OOP. Polymorphism
means having several different forms and is mainly related to object methods. For
example, an operation may exhibit different behavior in different instances but the
behavior depends upon the types of data, used in the operation. For example,
consider the operation of addition. For two numbers, the operation will generate a
sum. If the operands are strings, then the operation would produce a third string by
concatenation.

Coming to the introduction of open source libraries let's get to know
about the Opencv library which is used in our thesis. Opencv is nothing but open
source computer vision library including hundreds of computer vision algorithms and
was introduced originally by the Intel [10].The library has more than 2500 optimized
algorithms and it is extensively downloaded with 2.5M downloads worldwide. This
library is written in object oriented languages like C and C++ and can be deployed on
Windows, Linux and Mac Osx platforms.
Opencv library mainly focuses on efficiency and optimization and the
content of this library is interesting for students and researchers in image processing
and computer vision area. The opencv library is divided into various modules and
each module will deal with different functions.
Opencv has the machine learning library which is very important for
the opencv library and now coming to the core part of the Opencv library it mainly
contains MLL [11]. Most programmers and students come to know about the role
computer vision plays. Now this computer vision is widely being used for processing
images and videos on the web. First it was very difficult to have an organized and
portable library for various applications and later this was developed by a large
number of programmers. Now with the increase of multicore processors and
development of computer vision applications there is a large demand for Opencv in
several institutions and many are trying to use this library for filtering the image,
camera calibration, Motion analysis and Object Tracking.
Now opencv is being downloaded by more than a million users
according to statistics. It is even increasing day by day with an average of 30k
downloads per month .Generally opencv is written in either C and C++ code in an
optimized way and is independent of the Intel performance primitive (IPP), but it
will automatically load dynamic link libraries if they are present.

Pcl_features library mainly contains the mechanism that is used to
estimate the 3D features from the point cloud data. 3D features are nothing but the
representation at certain 3D point in space, which are nothing but the geometrical
pattern in space This library mainly helps us to find the surface normal's and
curvatures in space and to find this, we have a set of points in space and we need to
find the closest point in space and depending on the application we opt to use the
closest point in the vicinity of p or all the points. The easiest method that we follow
to determine the surface normal changes is that Eigen decomposition method.
The pcl_registration library is mainly deals with the process of
registration .Here a group of datasets are combined and they are modeled using a
process called registration. The main thing here is to find points in these datasets and
try to minimize the error between these datasets and if the error falls below some
threshold value then the process of registration is said to be complete
The surface library is another one which mainly deals with the surface
reconstruction from the 3D scans .The surface reconstruction techniques are
generally convex hull, mesh representation or a smoothed resample surface with
normal's. The smoothing and resampling method is used when there is a lot of noise
in the cloud .Meshing is used when we want to create a surface out of the points and
the other method that is convex hull which is used when only the boundaries are need
to be extracted
The Io library is similar to that of the library present in Opencv and this
library mainly consists of classes and functions that are used for reading and writing
data files and also capturing the point clouds from the devices
The common library contains the common data structures and methods
used by the most of PCL libraries. The core data structures include the Point Cloud
class and a multitude of point types that are used to represent points, surface normals,
RGB color values, feature descriptors, etc.

A Collaborative Virtual Environment (CVE) is used for collaboration and interaction of
participants that may be spread over large distances. The applications are usually based on
networked virtual environments where the users interact through avatars. An avatar is a
representation of a person in the virtual environment. Avatars can communicate with each
other as well as doing activities that people do in their daily life. Nowadays, virtual
environments like Second Life (SL) attract many people to experience 3D virtual life.
In Second Life, residents can explore, meet other residents, socialize, participate in individual
and group activities, and create and trade virtual property and services with one another, or
travel throughout the world. Virtual environments can be also used to increase interaction and
immersion in e-learning systems (Li, et al., 2009) or teleconferencing (Figure 1-1). People can
attend virtual meeting rooms (teleconferencing) or virtual classrooms (e-learning). They can
discuss with other people (avatars) and interact through virtual resources (e.g. blackboards,
shared tables, etc.).

Motion capture technology has a wide variety of highly demanding applications. At present,
the film industry is the application where motion capture is most extensively used. Motion
capture techniques allow large amounts of animation data to be produced; the movements of
human subjects (athletes, dancers, actors) are captured and transferred directly or indirectly to
computer-generated animation models. This gives animator the ability to produce more
realistic movements. Movies use mocap to replace traditional animation by hand and also to
produce computer-generated creatures that interact visually with real actors (e.g. the Gollum
character from the Lord of the Rings film). Recently, several films and television series have
been produced almost entirely using motion capture animation (e.g. The Polar Express,
Avatar, etc.). Similar motion capture techniques are applied in videogame industry to allow
more convincing and realistic character movements and expressions.

Recently, some entertainment applications have started using motion capture in order to
provide more intuitive forms of interaction in human-machine or human-software interfaces.
In the videogames industry, the next step is to allow players to interact with games using only
body motion or gestures (hands-free gaming) without holding any physical device (e.g. Kinect
from Microsoft, EyeToy from Sony). Human-robot interaction is another example of a motion
capture application. A major challenge in robotics is providing humanoid robots with
embedded intelligence so they can autonomously interact with people by using natural nonverbal
communication (human gestures). Embedding motion capture in a robotic system
would provide two advantages: 1) understanding meaningful human gestures and movements
in order to serve humans in their daily life (Kanda, et al., 2003), 2) communicating effectively
with humans by imitating as closely as possible their natural motions (Kim, et al., 2009),
(Nakaoka, et al., 2003). This would allow robots to participate in many human society
activities.

Automated video surveillance is another demanding application for human motion capture.
Video surveillance can be used in geriatric-care, alarm security systems, home-nursing, etc.
The goal of automated surveillance is to reduce the large number of human operators required
to monitor many real time video feeds simultaneously by using software that can analyze
video content automatically. Currently, automated video surveillance is an active research
area in computer vision with many remaining technical challenges (Dick, et al., 2003). In
order to develop automated surveillance system, motion capture could be used to detect, track,
identify people, and generally, to analyze human behavior in real-time without the need to
attach any joint markers. These systems must be able to operate in public spaces
(unconstrained environments) where lighting variations, occlusions and changes reflectance
represent a significant challenge for the computer vision analysis.

The use of motion capture technology is relatively new; it began in the late 1970‟s for military
purposes (tracking movements of pilots) (Maureen, 2000). Later, in 1980, Vicon Motion
Systems created the first system for gait analysis, which the captured motion of children in
order to detect disabilities (Sutherland, et al., 1988). Since the 1990‟s, advances in computer
processing power and research in algorithms have made possible a wide variety of new
applications (see section 1.2) including real-time motion capture computation (Molet, et al.,
1996).

Nowadays, motion capture is rapidly becoming cheaper and many more systems have
emerged in the market. However, it still faces challenges, such as lack of precision of the
motion data and complicated calibration procedures (e.g. special environments,
uncomfortable user equipment). In the following sections, current technologies for motion
capture are described (section 2.2). Section 2.3 summarizes existing techniques for motion
capture by computer vision without markers. Finally, section 2.4 briefly describes our baseline
approach for 3D motion capture by monocular vision.

Mocap technologies generally include synchronized cameras and special suits with markers or
sensors worn by the performers. Markers are set at body parts (or joints) and are tracked in
order to identify the motion by their positions or angles. Only the motion of the actors, not
their visual appearance, is recorded and this motion is the mapped to a 3D model by
computer.

Motion capture techniques can be classified by their input methods, namely optical,
mechanical, magnetic, acoustic and inertial. Each of these inputs (or a combination of them) is
tracked, ideally at least twice the frequency of the desired motion

Passive markers: Passive optical system use markers coated with a retro-reflective material
to reflect back light that is generated near the cameras lens. The operating principle is similar
to radar: the cameras emit radiation (usually infrared), which is reflected by the markers and
returned to the same camera. The cameras are sensitive to a narrow band of wavelengths and
perceive the markers as bright spots. Such a system typically consists of 6 to 24 cameras.
Passive systems do not require the user to wear wires or electronic equipment. The markers
are usually attached directly to the skin, or to a full body lycra suit designed for motion
capture (Qualisys, 2010).

Active markers: Rather than reflecting back externally generated light, the markers
themselves are powered to emit their own infrared light. Active optical systems triangulate
positions by illuminating one LED at a time very quickly or multiple LEDs with software to
identify them by their relative positions. Some systems can modulate the amplitude or pulse
width in order to provide marker ID. This unique marker ID provides much cleaner data than
passive marker systems (PhaseSpace, 2010).

Magnetic systems use sensors placed on the body to measure a low-frequency magnetic field
generated by a transmitter source (Figure 2-3). The outputs of these sensors are 3D positions
and rotational information. The number of sensors used is usually from 6 to 11. The sensors
and source are attached to an electronic control unit that correlates their reported locations
within the magnetic field (AMM, 2010). Inverse kinematics (IK) is then used to recover the
angles for the different body joints. A big advantage of these systems is that useful real-time
results can be obtained from only 6 sensors. However, the sensors are susceptible to magnetic
and electrical interference from metal objects in the environment. In addition, the wiring from
the sensors tends to limit the actor‟s movements. Recently, the development of new wireless
magnetic systems has been reported (Kanetaka, et al., 2010).

2.2.6 Computer vision based systems
Using computer vision techniques to acquires the human body motion is one of the most
attractive and practical solutions as it does not require any expensive or invasive hardware or
markers (only cameras are required) and it can work outdoors (in streets, offices, parks).
Algorithms have been proposed that capture human motion at near real-time frame rates;
however, they mostly rely on multi camera systems under controlled conditions, which limit
their applicability. Some monocular vision approaches (Agarwal, et al., 2006), (Urtasun, et
al., 2006) aim at capturing specific motions (walking, golf swinging, jumping, etc.) using
some learning model or tracking motion for certain parts of the body. Some other systems can
track unconstrained motion, but do not run in real-time (Sminchisescu, et al., 2003).
Recently, 3D image sensors have been used to capture the 3D body shape and disambiguate
poses using depth measurements. Time-of-flight sensors (Ganapathi, et al., 2010), (Kolb, et
al., 2010) and active triangulation system (e.g. Microsoft Kinect (Kinect, 2010)) are such
dedicated sensors.

Real-time markerless tracking of human pose by monocular computer vision remains a hard
yet relevant problem.

In order to estimate the 3D pose, image features are extracted from input images. A feature or
image descriptor can be defined as a piece of low-level visual information extracted from an
image to solve a specified task. Image descriptors are generally extracted using probability
distributions (e.g. color histograms), neighborhood operations (e.g. edges, optical flow) or
thresholding the pixel values (e.g. foreground or color classification). The choice of features
depends greatly on the problem to be solved. In human motion capture, the image descriptors
are generally used as cues to find the position of each body part and thus to estimate the full
3D human pose. Image descriptors commonly used in the literature include color, silhouettes,
edges and motion (Poppe, 2007), (Moeslund, et al.).

Skin color is a common cue for head and hand detection (Vezhnevets, et al., 2003).
Broekhuijsen et al. (2006) extract skin regions by converting images to the HSV color space,
thresholding each channel empirically, and finally selecting the largest connected
components, which are deemed to belong to the head and the hands. Bernier et al. (2009) use
an initial face detection algorithm to obtain a normalized skin color histogram in the UV
chrominance space (from YUV color space). A skin color probability is estimated from the
histogram and then used to detect the head and hands.
Some human motion capture works combine different color cues into a single probability;
Fontmarty et al. (2007) define a color-based likelihood that combines clothing color and skin
color. For clothing probability, a learned reference histogram of the clothing color is
compared using a Bhattacharyya distance with a targeted ROI in the image. In addition, they
calculate a homogenous distance by measuring the standard deviation of the color distribution
in RGB space from uniformly sampled points inside the color region.
Unfortunately, color may lack robustness in cases where significant local changes in
illumination can occur. In addition, cluttered scenes that contain colors similar to regions of
interest (skin or clothing) can generate noise in color segmentation.

In computer vision, an edge is a significant variation or discontinuity in the gray level of a
digital image. Edges detector algorithms provide the set of connected curves that indicate the
boundaries of objects of interest (Ziou, et al., 1998). In human motion capture image edges
are a useful cue for the boundaries of body limbs (e.g. hand, arm, head, forearm, etc.). Some
works (Fontmarty, et al., 2007), (Chen, et al., 2005) compute a distance transform to measure
the distance between edge points in the image and their nearest pixel on a candidate silhouette
boundary. Other works (Ramanan, et al., 2003) use edgeS as a cue to detect or track body
parts (e.g. arms, forearms, hands).

Broekhuijsen et al. (2006) estimate the hand position by computing the number of edge pixels
along a skin region of the full arm. They also train edge response histograms of limb-like
edges and background-edges. They compute the probability that an edge belongs to a body
part according to the edge orientation. Noriega et al. (2007) use the orientation of the edges to
estimate the position and orientation of a limb.

Edges have the advantage that they are very robust to changes in lighting conditions and can
be extracted at a very low computational cost. However, undesirable edges resulting from
background clutter, clothing textures edges or noise may be a problem.

Some works integrate different cues to achieve more robustness in the 3D pose estimation.
Several methods have been used to combine the information from different descriptors. Chen
et al. (Chen, et al., 2005) combine the silhouette information with edges extracted from the
same silhouette. They extract edges from the foreground silhouette and compute a distance
transform to the silhouette boundaries. Sminchisescu et al. (2001) combine edge and motion
information in a negative log-likelihood function. Edges are weighted according to their
importance qualified by a motion boundary map extracted from an optical flow method.
Fontmarty et al. (2007) combine edge distance, color, and 3D blob distance (acquired from
two cameras) into a single observation function. They assume that all observations are
mutually independent probabilities.

Combining image descriptors proves to be more robust as the advantages of each descriptor
can be used in one or more likelihood functions. However, care must be taken because each
descriptor can give incompatible likelihoods.

Pose estimation refers to the process of searching for the set of model parameters that
minimizes the similarity between features extracted from the model and the input image using
local optimization. The model parameters can be the joint angles or the global positions of
each body part. Due to the high-dimensionality of the model parameters (20 or more joint
angles), the search method used must be very efficient. In the state of the art, many works
take several minutes per frame and only few can achieve quasi-real-time results (Hua, et al.,
2007), (Fontmarty, et al., 2008), (Bernier, et al., 2009) In general, there are two techniques for
generative pose estimates: top-down estimation and bottom-up estimation. Related work
using these approaches will be described below.

PCA-based motion models have proven to be effective in reducing the dimensionality of the
pose space, however PCA is not optimal as human motions are generally multimodal and
have nonlinear correlations. Therefore non-linear mappings between pose space and latent
space are predefined. Urtasun et al. (2005) used a Scaled Gaussian Process Latent Variable
Model (SGPLVM) to learn prior non-linear models for 3D person tracking from monocular
video sequences. SGPLVMs have the advantage that they can be learned from much smaller
amounts of training data than other techniques. In SGPLVM, likelihoods of the training data
points are modeled as Gaussian processes for which the corresponding latent positions are
initially unknown. As a consequence, one must now both the unknown latent positions and
the mapping from the latent space to the original pose space. A kernel function is introduced
to allow for nonlinear mappings. Scaling of individual data dimensions is used to account for
the different variances of the different dimensions of the data. 3D tracking is accomplished
with simple MAP estimators with SGPLVM used to encourage poses to be close to the
training data.
Later, Urtasun et al. (2006) proposed a more sophisticated latent variable model named the
Gaussian Process Dynamical Model (GPDM). Specifically, a GPDM is a latent variable
dynamical model, comprising a low-dimensional latent space, a probabilistic mapping from
the latent space to the pose space, and a dynamical model in the latent space (Figure 2-12). It
provides continuous density functions over poses and motions that are generally non-
Gaussian and multimodal. Given training sequences, one simultaneously learns the latent
embedding, the latent dynamics, and the pose reconstruction mapping. GPDM has the
advantage over GPLVM that it usually produces much smoother latent trajectories. GPDMs
were shown to be effective for tracking a range of human walking styles, despite weak and
noisy image measurements and partial occlusions. Raskin et al. (2007) combined GPDMs
with the Annealed Particle Filter body tracker proposed by Deutscher et al. (2000). They used
GPDM to reduce the effective dimensionality of the pose vector (joint angles). The particles
were drawn in the latent space and then a mapping to the pose space for evaluation. This
reduction improved the performance of the particle filter, increasing its stability and its ability

Recently, several nonlinear probabilistic motion models have been proposed to better encode
the sophisticated dynamics and spatial information of human poses. Pang et al. (2007)
introduced the Gaussian Process Spatio-Temporal Variable Model (GPSTVM); this model
comprises a low dimensional latent space with associated spatio-temporal process. They argue
that GPSTVM provides a more genuine embedding of the human poses from both spatial and
temporal perspectives than GPDM. They showed that GPSTVM produces smoother
configuration of latent positions. They track the 3D configuration from monocular video
sequences by particle filter propagation over time in the latent space, avoiding the highdimensionality
of the pose space. Lu et al. (2008) proposed the Laplacian Eigenmap Latent
Variable Model (LELVM) to build priors for motion tracking. LELVM combines the
advantages of latent variable models (multimodal probability density, nonlinear mappings for
reconstruction and dimensionality reduction) with those of spectral manifold learning
methods (no local optima, unfolding highly nonlinear manifolds and scaling to latent spaces
of high dimension). LELVM uses a different type of dimensionality reduction; it defines just
a correspondence between points in latent space and pose space and not a nonlinear mapping
as GPDM or SGPLVM. Lu et al. (2008) compared the performance of LELVM with PCA
and GPLVM in a Particle Filter framework, showing that LELVM is superior in terms of
accuracy, robustness and computation time.
Low-dimensional motion models are becoming more and more accurate and robust in
representing complex human motion; however they are currently restricted to specific
activities like walking, running, dancing, etc. Further research is needed to deal with
transitions between different specific motions and extending motion models to broader classes
of human movements and general unconstrained motions.

Our registration process iteratively optimizes the match between the primitives from the
model and those from image with respect to the model parameters (Figure 2-13). In our
region-based registration, the primitives extracted are the color regions from the image and
the 3D model. Three classes of regions are considered: skin, head and clothes. The human
model in a candidate 3D pose is projected onto the segmented image and a cost function is
used to measure the match between corresponding regions. The matching cost function is
minimized using an optimization algorithm that requires only function evaluations (not
gradients). In the following subsections, we describe in detail the method implemented in our
region-based registration process (Marques Soares, et al., 2004).

Color samples are extracted from the first image captured. Basically, samples are extracted
automatically, in contrast with the approach previously proposed in (Marques Soares, et al.,
2004), where samples had to be extracted manually. In the current system, a skin color sample
is taken in the face region found with Adaboost face detector (Viola, et al., 2001) and a
clothes sample is taken in rectangle estimated under the face.

In the next chapters, we present the methods proposed to attack the limitations and
disadvantages found in our baseline approach. Basically, a first disadvantage in the prototype
system previously developed (Marques Soares, et al., 2004) is the absence of a background
subtraction method that allows working only with the region of interest (actor) in the image.
A second disadvantage is the fact that the 3D model cannot adapt automatically to the
morphology of the actor. These disadvantages limit the applicability of the system for several
users and different environments. In a following step, we will propose new methods to
achieve a more accurate and robust tracking under limited real-time computation. In this case,
exhaustive experiments must be done on several video sequences in order to validate the
accuracy and robustness achieved by our proposed algorithms.

In this chapter, we develop new algorithms to enhance the tracking performance of our baseline
approach (section 2.4). First, we implement new modules to extract the silhouette of the
user and automatically calibrate our 3D model. In order to improve the accuracy of the pose
estimation, a new registration step that works by matching color then edges is proposed.
Combining these features allow us to achieve robustness and accuracy in 3D motion capture
limited by real-time computation. Combining color and edges may be difficult as each
descriptor provides different image information; moreover, edges are much more localized
image features, giving improved accuracy but potentially causing mismatches and errors in
the pose estimation process. The proposed algorithms typically require less computation than
the existing approaches, making them suitable for real-time use in consumer computers. An
careful experimental analysis validated the proposed approach with respect to these
challenges.

First, we briefly describe the modules implemented for our system. Then, a robust
background subtraction algorithm for the extraction of the human silhouette is proposed.
After this, we introduce our 2 step approach based on matching color regions and edges. The
experimental performance of each step is studied and we discuss how a balance between the
two steps that makes the best use of the limited computation resources

Extracting the human silhouette allows to be processed the motion capture only those regions
of the image that contain the subject of interest or actor. This helps to ignore static objects in
the background that are not of interest (walls, tables, windows, etc.).

Silhouettes can be extracted using a background subtraction algorithm (Herrero, et al., 2009)
provided that the appearance of the environment or background is different from that of the
object of interest or foreground. The background usually consists of still objects while, when
considering human pose estimation, the foreground is the person in the image. Background
subtraction models the appearance of the empty scene (background) using pixel-wise image
features and compares this background model with the features observed at the same pixel of
the input image where the object of interest (e.g. human) may appear (Li, et al., 2004), (Guha,
et al., 2006). Features that appear to have changed significantly are thresholded, with classical
post-processing to output the human silhouette region.

The naive approach to background subtraction assumes the temporal constancy of each
background pixel intensity (or RGB color), with very little variation caused by image noise.
However, in most practical situations, temporal changes do occur due to variations in
illumination (shadows, changing sunlight, etc.). Several works have experimented with
different color spaces to handle illumination changes. Apart from using RGB intensity values
(Stauffer, et al., 1999), researchers have experimented with normalized RGB (Paragios, et al.,
2001), HSV (McKenna, et al., 1999), YCrCb (El Baf, et al., 2008), etc. Some works obtain
performance improvements using image gradients (Javed, et al., 2002) or optical flow (Mittal,
et al., 2004) features. Some contributions also implement complex statistical modeling of
feature distributions (mixture of Gaussians (Stauffer, et al., 1999), PCA (Rymel, et al., 2004),
Hidden Markov Models (Stenger, et al., 2001)). To date, no algorithm or feature has proven to
be robust to all changing environment conditions or complex backgrounds. Moreover, the
computational cost of some background subtraction algorithms is quite high, making them
inappropriate for real-time work.

In this section, we propose a relatively simple real-time algorithm for background subtraction
in order to extract human silhouettes under common lighting variations. Two robust features
are combined: color chrominance and gradient. Both features provide some robustness to
variations in lighting conditions. We use the chrominance components of the YCrCb color
space. The Y (luminance) component is ignored for better robustness to lighting variations.
However chrominance remains somewhat sensitive to illumination changes (Liévin, et al.,
2004). In order to gain robustness, we also include gradient-based features, these features
exploit differential relationship within the neighborhood of each pixel and therefore they are
less sensitive to lighting changes (Bernier, 2006). Both features are modeled by Gaussian
densities, which describe the background scene statistically taking into account the variations
of the image features due to illumination changes. The final foreground region is extracted by
applying probability thresholds and using morphological operators

Because the edge-based distance has multiple local minima, we need carefully to constraint
the search space to match only the body and limbs. Therefore, the initial simplex for the edgebased
registration is usually the small simplex at final iteration of the region-based
registration, so that the edge-based registration starts searching in a reduced space around the
3D pose estimated by region-based matching. However, if the region-based registration
reaches convergence, the size of this simplex will tend to zero. In this case we use a small
simplex size experimentally optimized for edge-based registration. Figure 3-12 shows the
results. By examining the last column of this figure, we can see how the edge-based
registration achieves more accurate 3D pose estimation by matching the correct edges of the
body limbs.

The next figure (4-33) shows the gain of robustness of our particle filter method with respect
to the standard CONDENSATION algorithm. We see that our algorithm reduces significantly
the number of mistrackings for all the video sequences, including those where the actor is
moving in the scene (figures 4-33e and 4-33f) and turning around himself (figure 4-33c).
We note that CONDENSATION algorithm presented the highest number of failures in the
video sequences where the actor is not facing to the camera (figures 4-33d and 4-33f). The
reason is that depth ambiguities remain for almost all images of these video sequences even if
the actor is not executing any motion. In these cases, the proposed algorithm achieved the
highest gain of robustness (figures 4-33d and 4-33f).
We see also that our algorithm achieved the lowest gain of robustness for the video sequence
in which the actor is turning around himself (figure 4-33c). This is because full body rotation
motions are the most difficult motion to track since the turning direction can hardly be
detected with the image feature we used (color region and edges). Some other features,
possibly optical flow, would help here.

